# Hypothesen testen 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```


## Wahrscheinlichkeitstheorie


## Rechenregeln zum Erwartungswert und der Varianz

### Erwartungwert

Für eine diskrete Zufallsvariable $X$ auf einer endlichen Menge $\{x_i, i = 1, \ldots, n\}$ mit $n$ Elementen ist der Erwartungswert definiert mit:

$$
E[X] = \sum_{i=1}^n x_i P(x_i)
$$

D.h. jedes mögliche Ereignis wird mit seiner Wahrscheinlichkeit multipliziert und die Summe über alle diese Möglichkeiten wird gebildet. Da der eine zentrale Rolle in der Wahrscheinlichkeitstheorie und der Statistik spielt, hat er ein eigenes Symbol bekommen $\mu$. Daher wird uns immer wieder die Schreibweise:

$$
E[X] = \mu_X
$$

Oder wenn der Zusammenhang klar ist und nur von einer bestimmten Zufallsvariablen gesprochen wird, dann auch nur $\mu$. Es hat sich eingebürgert, die Größe $\mu$ als den Mittelwert der Population zu bezeichnen auch wenn es sich dabei nicht unbedingt um den Mittelwert handelt wie er üblicherweise verstanden wird und z.B. bei der Stichprobe berechnet wird ($\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$). Bei dem Erwartungswert handelt es sich um den gewichteten Mittelwert und wird daher manchnal die Unterscheidung vorgenommen wenn von dem Mittelwert der Population $\mu$ und dem Mittelwert der Stichprobe $\bar{x}$ gesprochen wird. 

Im folgenden werden verschiedene Rechenregeln mit dem Erwartungswert aufgelistet. Diesen Regeln werden wir immer wieder begegnen wenn wir später Erwarungswerte für Statistiken berechnen. Die erste Regel bezieht sich darauf, wenn eine Zufallsvariable mit einer Konstanten $a$ multipliziert wird. Konstant heißt, bei $a$ handelt es sich nicht um eine Zufallsvariable und $a$ hat immer den gleichen Wert. Der Erwartungswert berechnet sich dann mittels:

$$
E[aX] = \sum_{i=1}^n a x_i P(x_i) = a \sum_{i=1}^n x_i P(x_i) = a E[X]
$$

In den meisten Fällen sind wir nicht an einer einzelnen Zufallsvariablen interessiert, sondern, beispielsweise wenn wir eine Stichprobe untersuchen, es liegen mehrere Zufallsvariablen vor. Im einfachsten Fall starten wir mit zwei unabhängigen Zufallsvariablen $X$ und $Y$. Die beiden Variablen können auf der gleichen Ereignismenge definiert sein, können aber auch auf unterschiedlichen Ereignismengen, z.B. $\{x_i, i = 1, \ldots, n\}$ und $\{y_j, j = 1, \ldots, m\}$ definiert sein. Wollen wir den Mittelwert von $X$ und $Y$ berechnen und davon den Erwartungswert berechnen, müssen wir verstehen wie sich die Addition unabhängiger Zufallsvariblen auf den Erwartungswert auswirkt. Tatsächlich ist diese Operation relativ einfach zu verstehen, der Erwartungswert von $E[X + Y]$ berechnet sich mittels:

$$
E[X + Y] = \sum_{i=1}^n x_i P(x_i) + \sum_{j=1}^m y_j P(x_j) = E[X] + E[Y]
$$

Diese Formel generalisiert für unabhängige $X_i, i = 1, \ldots, n$ zu:

$$
E[X_1 + X_2 + \ldots + X_n] = E[X_1] + E[X_2] + \ldots + E[X_n]
$$

In Kombination mit der Regel für konstante Terme mit den Konstanten $a_1, a_2, \ldots, a_n$ folgt:

$$
E[a_1 X_1 + a_2 X_2 + \ldots + a_n X_n] = a_1 E[X_1] + a_2 E[X_2] + \ldots + a_n E[X_n]
$$

#### Beispiele {-}

Nehmen wir zur Veranschaulichung ein einfaches Beispiel mit einer Zufallsvariable $X$ welche die folgende Verteilung hat (siehe @tbl-stats-hypo-px):

| x | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(x)$ | $\frac{1}{8}$ |$\frac{5}{8}$ |$\frac{1}{8}$ |$\frac{1}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-px}

Dann berechnet sich der Erwartungswert $E[X]$ mittels:

$$
E[X] = \sum_{i=1}^4 x_i P(x_i) = \frac{1}{8} \cdot 0 + \frac{1}{8}\cdot 1 + \frac{5}{8}\cdot2 + \frac{1}{8}\cdot3 = 1.25
$$

Hier kann auch eine interessante Eigenschaft des Erwartungswerts beobachtet werden, nämlich das der berechnete Wert gar nicht in der Menge der möglichen Werte der Zufallsvariablen vorkommen muss. In der Ereignismenge von $X$ sind nur ganzzahlige Werte.

Haben wir eine zweite Zufallsvariable $Y$ mit der Verteilung (siehe @tbl-stats-hypo-py)


| y | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(y)$ | $\frac{2}{8}$ |$\frac{2}{8}$ |$\frac{1}{8}$ |$\frac{3}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-py}

Mit $E[Y]$:

$$
E[Y] = \sum_{i=1}^4 y_i P(y_i) = \frac{2}{8} \cdot 0 + \frac{2}{8}\cdot 1 + \frac{1}{8}\cdot2 + \frac{3}{8}\cdot3 = 1.625 
$$

Dann folgt für den Erwarungswert von $E[X + Y]$:

$$
E[X + Y] = E[X] + E[Y] = 1.25 + 1.625 = 2.875 
$$

Definieren wir eine neue Zufallsvariable $Z$ mit $Z := a \cdot X$ mit der Konstanten $a := 2$. Dann folgt für den Erwartungswert von $E[Z]$:

$$
E[Z] = E[aX] = aE[X] = 2 \cdot 1.25 = 2.5
$$

Ein ganz anderes Beispiel, welches noch mal den Begriff Erwartungswert veranschaulicht, bezieht sich auf ein Glückspiel mit dem Namen Chuck-a-Lcuk. Das Beispiel ist @gross2019 entnommen. Das Spiel wird mit einem 1 € Einsatz gespielt. Es werden drei Würfel geworfen und die folgende Regeln bestimmen den Gewinn (siehe @tbl-chuck-a-luck). 

| Ausgang | Gewinn |
| --- | --- |
| keine 6 | 0 EU |
| min. eine 6 | 2 EU | 
| 3 x 6 |  27 EU |

: Gewinnauschüttung bei Chuck-a-Luck {#tbl-chuck-a-luck}

Die Frage die sich nun stellt, ist ob dieses Spiel fair ist bzw. lohnt es sich einen 1 € Einsatz zu setzen? Diese Frage kann mit dem Erwartungswert beantwortet werden. Um den Erwartungswert zu berechnen benötigen wir allerdings zunächst die Wahrscheinlichkeiten für die verschiedenen Ausgänge.

Die Wahrscheinlichkeit keine $6$ zu werfen ist für jeden Würfel einzeln $\frac{5}{6}$, dementsprechend, da die Würfel unabhängig voneinander sind, kann diese Wahrscheinlichkeit dreimal miteinander multipliziert werden.

$$
P(0 \times 6) = \left(\frac{5}{6}\right)^3 = \frac{125}{216} \approx 0.579
$$

D.h. in knapp 60% der Fälle wird beim dem Spiel kein Gewinn ausgeschüttet. Berechnen wir zunächst den Fall, dass drei Sechsen geworfen werden. Der ist Parallel zu keiner Sechs, nur das jetzt für einzelnen Würfel die Wahrscheinlichkeit $\frac{1}{6}$ ist. Es folgt. 

$$
P(3 \times 6) = \left(\frac{1}{6}\right)^3 = \frac{1}{216} \approx 0.005
$$

D.h. die Wahrscheinlichkeit für $3 \times 6$ ist gerade einmal ein halbes Prozent. D.h. in 500 Spielen, würde wir dieses Ereignis nur ein einziges Mal erwarten.

Letzlich bleibt noch das Ereignis mindestens eine $6$. Hier nehmen wir das Komplementärereignis zu mindestens eine Sechs heißt, nämlich keine Sechs und ziehen dessen Wahrscheinlichkeit von 1, dem sicheren Ereignis, ab. Da diese Menge auch die drei Sechsen beinhaltet, müssen wir dessen Wahrscheinlichkeit auch noch abziehen.

$$
P(\text{min. eine } 6) = 1 - P(0 \times 6) - P(3 \times 6) = \frac{216}{216} - \frac{125}{216} - \frac{1}{216} = \frac{90}{216} = 0.41\bar{6}
$$

Die Wahrscheinlichkeit für mindestens eine Sechs ist dementsprechend etwas über 40%. Jetzt wenden wir wieder die Formel für den Erwartungswert an um die zu erwartende Gewinnsumme zu bestimmen. Die Gewinnsumme nimmt jetzt den Wert der Zufallsvariablen ein.

$$
E[X] = \frac{125}{216}\times 0 + \frac{90}{216}\times 2 + \frac{1}{216}\times27 = \frac{207}{216} \approx 0.958
$$

Im Mittel erwarten wir bei dem Spiel einen Gewinn von $0.958$€ bei einem Einsatz von $1$ €. Daher wird im Mittel ein Verlust bei dem Spiel gemacht. 

Als letztes Beispiel schauen wir uns den Erwartungswert des Mittelwerts $\bar{x}$ an.

$$
E[\bar{x}] = E\left[\frac{1}{n}\sum_{i=1}^n x_i\right] = \frac{1}{n}\sum_{i=1}^n E[x_i] = \frac{1}{n}\sum_{i=1}^n \mu = \frac{1}{n}n \mu = \mu
$$

### Varianz

## Schätzer

Erwartungstreue



## Hypothesentestung

### Der t-Test

Das Verhältnis einer standardnormalverteilten Variable z und eine $\chi^2$-verteilten Variable s folgt einer $t$-Verteilung.

$$
T = \frac{\hat{\Delta}}{\hat{s}_e(\hat{\delta})} \sim t\text{-Verteilung}
$$

### $\chi^2$-Test der Varianz 

Sei $\hat{\sigma}^2$ ein Schätzer für eine Varianz und $H_0: \sigma^2 = \sigma_0^2$ die Nullhypothese, dann lässt sich eine Teststatistik über die folgende Formel konstruieren:

$$
T = d \frac{\hat{\sigma}^2}{\sigma_0^2} \sim \chi^2(d\text{ Freiheitsgrade})
$$


### F-Test von Varianzverhältnissen

Seien zwei normalverteilte Stichproben gegeben und deren Varianzen über $\hat{\sigma}_A^2$ und $\hat{\sigma}_B^2$ abgeschätzt werden dann kann eine Teststatisk über die Gleichheit der beiden Varianzen $\sigma_A^2 = \sigma_B^2$ über die folgende Formel konstruiert werden. 

$$
T = \frac{\hat{\sigma}^2_A}{\hat{\sigma}^2_B} \sim F(df_A, df_B)
$$

Die beiden Varianzen folgen dabei jeweils einer $\chi^2$ Verteilung mit Freiheistgraden $df_A$ und $df_B$, so dass die Statistik $T$ einer $F$-Verteilung mit $(df_A, df_B)$ Freiheitsgeraden folgt und die $H_0$ lautet $H_0: \frac{\sigma_A^2}{\sigma_B^2} = 1$

