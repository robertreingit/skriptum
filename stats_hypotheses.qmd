# Hypothesen testen 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```


## Der Erwartungwert $\mu$ einer Zufallsvariable

Für eine diskrete Zufallsvariable $X$ auf einer endlichen Menge $\{x_i, i = 1, \ldots, n\}$ mit $n$ Elementen ist der Erwartungswert definiert mit:

\begin{equation}
E[X] = \sum_{i=1}^n x_i P(x_i)
\label{eq-stats-hypo-expected-01}
\end{equation}

D.h. jedes mögliche Ereignis wird mit seiner Wahrscheinlichkeit multipliziert und die Summe über alle diese Möglichkeiten wird gebildet. Da der Erwartungswert eine zentrale Rolle in der Wahrscheinlichkeitstheorie und der Statistik spielt, hat er ein eigenes Symbol spendiert bekommen $\mu$. Daher wird uns immer wieder die folgende Schreibweise begegnen:

$$
E[X] = \mu_X
$$
Der Erwarungswert der Zufallsvariable $X$ wird mit $\mu_X$ bezeichnet. Wenn der Zusammenhang klar ist und nur von einer bestimmten Zufallsvariablen gesprochen wird, dann auch nur $\mu$.

Es hat sich eingebürgert, die Größe $\mu$ als den Mittelwert der Population zu bezeichnen auch wenn es sich dabei nicht unbedingt um den Mittelwert handelt wie er üblicherweise verstanden wird und z.B. bei der Stichprobe berechnet wird ($\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$). Bei dem Erwartungswert handelt es sich um den gewichteten Mittelwert. Es wird unterschieden zwischen dem Mittelwert in der Population $\mu$, der theoretisch ist, dem Mittelwert der Stichprobe $\bar{X}$ als Zufallswert, ebenfalls theoretisch, und dem tatsächlich beobachtet Mittelwert $\bar{x}$ in der Stichprobe. 

Im folgenden werden verschiedene Rechenregeln mit dem Erwartungswert aufgelistet. Diesen Regeln werden wir immer wieder begegnen wenn wir später Erwarungswerte für Statistiken berechnen. Die erste Regel bezieht sich darauf, wenn eine Zufallsvariable mit einer Konstanten $a$ multipliziert wird. Konstant heißt, bei $a$ handelt es sich nicht um eine Zufallsvariable und $a$ hat immer den gleichen Wert. Der Erwartungswert berechnet sich dann mittels:

\begin{equation}
E[aX] = \sum_{i=1}^n a x_i P(x_i) = a \sum_{i=1}^n x_i P(x_i) = a E[X]
\label{eq-stats-hypo-expected-mult}
\end{equation}

In den meisten Fällen sind wir nicht an einer einzelnen Zufallsvariablen interessiert, sondern, beispielsweise wenn wir eine Stichprobe untersuchen, es liegen mehrere Zufallsvariablen vor. Im einfachsten Fall starten wir mit zwei unabhängigen Zufallsvariablen $X$ und $Y$. Die beiden Variablen können auf der gleichen Ereignismenge definiert sein, können aber auch auf unterschiedlichen Ereignismengen, z.B. $\{x_i, i = 1, \ldots, n\}$ und $\{y_j, j = 1, \ldots, m\}$ definiert sein. Wollen wir den Mittelwert von $X$ und $Y$ berechnen und davon den Erwartungswert berechnen, müssen wir verstehen wie sich die Addition unabhängiger Zufallsvariblen auf den Erwartungswert auswirkt. Tatsächlich ist diese Operation relativ einfach zu verstehen, der Erwartungswert von $E[X + Y]$ berechnet sich mittels:

\begin{equation*}
E[X + Y] = \sum_{i=1}^n x_i P(x_i) + \sum_{j=1}^m y_j P(x_j) = E[X] + E[Y]
\end{equation*}

Diese Formel generalisiert für unabhängige $X_i, i = 1, \ldots, n$ zu:

\begin{equation}
E[X_1 + X_2 + \ldots + X_n] = E[X_1] + E[X_2] + \ldots + E[X_n]
\label{eq-stats-hypo-expected-add}
\end{equation}

In Kombination mit der Regel für konstante Terme mit den Konstanten $a_1, a_2, \ldots, a_n$ folgt:

$$
E[a_1 X_1 + a_2 X_2 + \ldots + a_n X_n] = a_1 E[X_1] + a_2 E[X_2] + \ldots + a_n E[X_n]
$$

Der Erwartungswert ist daher ein linearer Operator.

Nur der Vollständigkeit halber auch noch kurz die Definition des Erwartungswertes für reelle Verteilungen, bei der das Summenzeichen mit einem Integral ausgetauscht ist.

\begin{equation*}
E[X] = \int_{x=-\infty}^\infty x f(x) dx
\end{equation*}

Das Integral geht über den gesamten Definitionsbereich von $X$ und die Wahrscheinlichkeitsfunktion wird durch die Dichtefunktion $f(x)$ ausgetauscht. Die Linearität bleibt für alle uns interessierenden Fälle erhalten (keine Panik, es werden im Weiteren keine Integrale berechnet).

### Beispiele {-}

Nehmen wir zur Veranschaulichung ein einfaches Beispiel mit einer Zufallsvariable $X$ die nur vier verschiedene Werte annehmen kann, die die folgenden Wahrscheinlichkeiten haben (siehe @tbl-stats-hypo-px):

| x | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(x)$ | $\frac{1}{8}$ |$\frac{5}{8}$ |$\frac{1}{8}$ |$\frac{1}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-px}

Anwendung von Formel \eqref{eq-stats-hypo-expected-01} führt zu der Berechnung des Erwartungswerts $E[X]$ mittels:

$$
E[X] = \sum_{i=1}^4 x_i P(x_i) = 0 \cdot \frac{1}{8}  + 1 \cdot \frac{1}{8} + 2\cdot \frac{5}{8} + 3\cdot \frac{1}{8} = 1.25
$$

Hier kann auch eine interessante Eigenschaft des Erwartungswerts beobachtet werden, nämlich das der berechnete Wert gar nicht in der Menge der möglichen Werte der Zufallsvariablen vorkommen muss. In der Ereignismenge von $X$ sind nur ganzzahlige Werte.

Haben wir eine zweite Zufallsvariable $Y$ mit der Verteilung (siehe @tbl-stats-hypo-py)


| y | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(y)$ | $\frac{2}{8}$ |$\frac{2}{8}$ |$\frac{1}{8}$ |$\frac{3}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-py}

Mit Formel \eqref{eq-stats-hypo-expected-01} folgt wiederum:

$$
E[Y] = \sum_{i=1}^4 y_i P(y_i) = 0 \cdot \frac{2}{8} + 1 \cdot \frac{2}{8} + 2 \cdot \frac{1}{8} + 3 \cdot \frac{3}{8} = 1.625 
$$

Wenn wir eine neue Zufallsvariable $Z$ definieren mit $Z = X + Y$, dann folgt für den Erwarungswert von $E[Z] = E[X + Y]$ mittels \eqref{eq-stats-hypo-expected-add}:

$$
E[Z] = E[X + Y] = E[X] + E[Y] = 1.25 + 1.625 = 2.875 
$$

Definieren wir dagegen $Z$ mit $Z := a \cdot X$ mit der Konstanten $a := 2$. Dann folgt für den Erwartungswert von $E[Z]$ mit Formel \eqref{eq-stats-hypo-expected-mult}:

$$
E[Z] = E[aX] = aE[X] = 2 \cdot 1.25 = 2.5
$$

Ein ganz anderes Beispiel, welches noch mal den Begriff Erwartungswert veranschaulicht, bezieht sich auf ein Glückspiel mit dem Namen Chuck-a-Luck. Das Beispiel ist @gross2019 entnommen. Chucia-Luck wird mit einem Einsatz von 1 € gespielt. Es werden drei Würfel geworfen und die folgende Regeln bestimmen den Gewinn (siehe @tbl-chuck-a-luck). 

| Ausgang | Gewinn |
| --- | --- |
| keine 6 | 0 EU |
| min. eine 6 | 2 EU | 
| 3 x 6 |  27 EU |

: Gewinnauschüttung bei Chuck-a-Luck {#tbl-chuck-a-luck}

Die Frage die sich nun stellt, ist ob dieses Spiel fair ist bzw. lohnt es sich einen 1 € Einsatz zu setzen? Diese Frage kann mit dem Erwartungswert beantwortet werden. Um den Erwartungswert zu berechnen benötigen wir allerdings zunächst die Wahrscheinlichkeiten für die verschiedenen Ausgänge.

Die Wahrscheinlichkeit keine $6$ zu werfen ist für jeden Würfel einzeln $\frac{5}{6}$, dementsprechend, da die Würfel unabhängig voneinander sind, kann diese Wahrscheinlichkeit dreimal miteinander multipliziert werden.

$$
P(0 \times 6) = \left(\frac{5}{6}\right)^3 = \frac{125}{216} \approx 0.579
$$

D.h. in knapp 60% der Fälle wird beim dem Spiel kein Gewinn ausgeschüttet. Berechnen wir zunächst den Fall, dass drei Sechsen geworfen werden. Dieser Fall kann parallel zudemjenigen für keine Sechs gelöst werden. Einziger Unterschied, ist die Wahrscheinlichkeit des Ereignisses. Die Wahrscheinlichkeit für einen Würfel eine Sechs zu würfeln ist $\frac{1}{6}$. Es folgt daher analog:

$$
P(3 \times 6) = \left(\frac{1}{6}\right)^3 = \frac{1}{216} \approx 0.005
$$

D.h. die Wahrscheinlichkeit für $3 \times 6$ ist gerade einmal ein halbes Prozent. D.h. in 200 Spielen, erwawrten wir dieses Ereignis nur ein einziges Mal.

Letzlich bleibt noch das Ereignis mindestens eine $6$. Hier nehmen wir das Komplementärereignis zu mindestens eine Sechs. Das Komplementärereignis ist nämlich keine Sechs zu würfeln. Diese Wahrscheinlichkeit ziehen wird dann von 1, dem sicheren Ereignis, ab. Da diese Menge auch die drei Sechsen beinhaltet, für das eine andere Gewinnberechnung gilt, müssen wir dessen Wahrscheinlichkeit noch subtrahieren.

$$
P(\text{min. eine } 6) = 1 - P(0 \times 6) - P(3 \times 6) = \frac{216}{216} - \frac{125}{216} - \frac{1}{216} = \frac{90}{216} = 0.41\bar{6}
$$

Die Wahrscheinlichkeit für mindestens eine Sechs ist dementsprechend etwas über 40%. Jetzt wenden wir wieder die Formel für den Erwartungswert an um die zu erwartende Gewinnsumme zu bestimmen. Die Gewinnsumme nimmt jetzt den Wert der Zufallsvariablen ein.

$$
E[X] = \frac{125}{216}\times 0 + \frac{90}{216}\times 2 + \frac{1}{216}\times27 = \frac{207}{216} \approx 0.958
$$

Im Mittel erwarten wir bei dem Spiel einen Gewinn von $0.958$€ bei einem Einsatz von $1$ €. Daher wird im Mittel ein Verlust bei dem Spiel gemacht. 

Als letztes Beispiel betrachten wir den Erwartungswert des Mittelwerts $\bar{x}$ und wenden dabei die obigen Rechenregeln an.

$$
E[\bar{x}] = E\left[\frac{1}{n}\sum_{i=1}^n x_i\right] = \frac{1}{n}\sum_{i=1}^n E[x_i] = \frac{1}{n}\sum_{i=1}^n \mu = \frac{1}{n}n \mu = \mu
$$

## Die Varianz $\sigma^2$ einer Zufallsvariablen

Die Varianz $\sigma^2$ beschreibt die Streuung der Zufallsvariablen $Y$. Formal definiert ist die Varianz über: 

\begin{equation}
Var(Y) = \sigma^2 = E[(Y-E[Y])^2]
\label{eq-def-variance}
\end{equation}

Wenn der quadratische Term ausmultipliziert, die Rechenregeln für den Erwartungswert und berücksichtigen, das $E[Y]$ eine konstante ist, dann erhalten wir eine oft praktische, alternative Formulierung für die Varianz.

\begin{align*}
\sigma^2 &= E[(Y-E[Y])^2] \\
&= E[Y^2 + E[Y]^2 - 2YE[Y]] \\
&= E[Y^2] - E[E[Y]^2] - E[2E[Y]Y] \\
&= E[Y^2] - E[Y]^2 - 2E[Y]E[Y] \\
&= E[Y^2] - E[Y]^2 - 2E[Y]^2 \\
&= E[Y^2] - E[Y]^2
\end{align*}


Wenn wir die Terminologie für den Erwartungswert $E[X] = \mu$ anwenden, erhalten wir dementsprechend.

\begin{equation*}
Var(Y) = \sigma^2 = E[Y^2] - \mu^2
\end{equation*}

Für die Varianz gibt es auch paar einfache Rechenregeln ähnlich wie für den Erwartungswert.

Für eine Konstante $a$ gilt

\begin{align*}
Var(a) &= 0 \\
Var(a + Y) &= Var(Y) \\
Var(a \cdot Y) &= a^2 \cdot Var(Y)
\end{align*}

#### Beispiele {-}

Für zwei unabhängige Zufallsvariablen $X$ und $Y$ gilt:

\begin{equation*}
Var(X + Y) = Var(X) + Var(Y)
\end{equation*}

Eine überraschende Eigenschaft aus den Rechenregeln für die Varianz ist, dass die Varianz der Differenz zweier unabhängigen Variablen gleich der Varianz ist, wenn die beiden Variablen addiert werden.

\begin{align*}
Var(X - Y) &= Var( 1\cdot X + (-1)\cdot Y) \\
&= Var(X) + Var((-1)\cdot Y) \\
&= Var(X) + (-1)^2 \cdot Var(Y) \\
&= Var(X) + Var(Y)
\end{align*}

Mit diesen Regeln können wir nun auch die Varianz des Mittelwerts $\bar{Y}$ bestimmen. Die Variablen $Y_i, i = 1,\ldots,n$ seien wieder unabhängig voneinander.

\begin{align*}
Var(\bar{X}) &= Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) \\
&= \left(\frac{1}{n}\right)^2 Var\left(\sum_{i=1}^n X_i\right) \\
&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i) \\
&= \frac{1}{n^2}\sum_{i=1}^n \sigma^2 \\
&= \frac{1}{n^2}n \sigma^2 = \frac{\sigma^2}{n}
\end{align*}

Wie beim Erwartungswert auch noch mal kurz die kontinuierliche Version der Definition der Varianz.

\begin{equation}
Var(X) = \int_{-\infty}^{\infty}(X-\mu)^2 f(x) dx
\end{equation}

## Schätzer

Schätzer sind uns bereits in verschiedenen Varianten begegnet. Als ein Schätzer wird eine Funktion bezeichnet mit deren Hilfe anhand einer Stichprobe ein Wert, ein Schätzwert, berechnet wird, der ermöglicht Rückschlüsse über unbekannte Parameter $\phi$ in der Population zu erhalten. D.h wir haben zum Beispiel eine Population mit den Parametern $\mu$ und $\sigma^2$ und wollen anhand der beobachteten Werte der Stichprobe aussagen über diese beiden Parameter machen. Dazu benutzen wir Schätzer.

Beispielsweise ist der Mittelwert einer Stichprobe $\bar{x}$ ein Schätzer für den Mittelwert $\mu$ in der Population. Die Standardabweichung $s$ der Stichprobe ist ein Schätzer für die Standardabweichung $\sigma$ in der Population. Es gibt nun immer eine Reihe von möglichen Schätzern auf einer gegebenen Stichprobe. Zum Beispiel, könnte ich anstatt den Mittelwert über alle Stichprobenwerte zu rechnen, könnte ich nur die drei kleinsten Werte nehmen. Oder jeden zweiten Wert usw.. D.h. nicht alle Schätzer sind gleich gut. Manche sind besser als andere. Das Konzept des Schätzer erlaubt Unterschiede zwischen verschiedenen Schätzer zu formalisieren.

Eine wichtige Eigenschaft von Schätzern ist die zum Beispiel die sogenannte Erwartungstreue. Die Erwartungstreue ist eng mit dem Erwartungswerts einer Statistik verknüpft. Wir hatten oben gesehen, dass für den Erwartungwert des Mittelwerts $E[\bar{x}] = \mu$ gilt. Die zeigt, dass der Mittelwert der Stichprobe eine erwartungstreuer Schätzer für den Populationsparameter $\mu$ ist.

Mit $\sigma^2 = E[X^2] - E[X]^2 = E[X^2] - \mu^2 \Rightarrow E[X^2] = \sigma^2 + \mu^2$ und den Rechenregeln zum Erwartungswert gilt:

\begin{align*}
E\left[\sum_{i=1}^{n} (x_i-\bar x)^2\right] &= E[\sum_{i=1}^{n} x_i^2-2\bar x\sum_{i=1}^{n} x_i+n\bar{x}^2]\\
&=E[\sum_{i=1}^{n} x_i^2-2\bar{x}n\bar{x}+n\bar{x}^2]\\ 
&= E[\sum_{i=1}^{n} x_i^2-n\bar{x}^2] \\
&= n E[x_i^2]- n E[\bar{x}^2]\\
&= n (\mu^2 + \sigma^2) - n(\mu^2+\sigma^2/n)\\
&= (n-1) \sigma^2
\end{align*}

Daraus folgt:
\begin{equation*}
E[s^2] = E\left[\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}\right] = \sigma^2
\end{equation*}

Was dann auch erklärt warum die Summe der quadrierten Abweichung durch $n-1$ und nicht durch $n$ geteilt wird.

Würden nämlich die quadrierten Abweichungen durch $n$ geteilt werden, dann hätten wir keinen erwartungstreuen Schätzer mehr.

\begin{equation*}
E\left[\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n}\right] = \frac{1}{n}E\left[\sum_{i=1}^n (x_i - \bar{x})^2\right] = \frac{n-1}{n}\sigma^2
\end{equation*}

Würden wir durch $n$ teilen, dann würden wir die Varianz unterschätzen, da $\frac{n-1}{n}<1$ gilt. 

Wir benötigen erwartungstreue Schätzer um die Verknüpfung zwischen unserer Stichprobe und den Parametern der theoretischen Stichproben durchführen zu können.

## Hypothesentestungszoo

Es folgt noch eine kurze Übersicht über verschiedenen Teststatistiken die uns immer wieder begegnen wird.

### Der t-Test

Das Verhältnis einer standardnormalverteilten Variable z und eine $\chi^2$-verteilten Variable s folgt einer $t$-Verteilung.

$$
T = \frac{\hat{\Delta}}{\hat{s}_e(\hat{\delta})} \sim t\text{-Verteilung}
$$

In den meisten Fällen hat der t-Test die Form, dass ein Unterschied durch eine Streuung geteilt wird. Zum Beispiel bei dem t-Test der uns in unserer ersten Statistikvorlesung begegnet ist, wurde der Unterschied zwischen zwei Gruppenmittelwerten durch die gewichtete Standardabweichung geteilt. 

### $\chi^2$-Test der Varianz 

Sei $\hat{\sigma}^2$ ein Schätzer für eine Varianz und $H_0: \sigma^2 = \sigma_0^2$ die Nullhypothese, dann lässt sich eine Teststatistik über die folgende Formel konstruieren:

$$
T = d \frac{\hat{\sigma}^2}{\sigma_0^2} \sim \chi^2(d\text{ Freiheitsgrade})
$$


### F-Test von Varianzverhältnissen

Seien zwei normalverteilte Stichproben gegeben und deren Varianzen über $\hat{\sigma}_A^2$ und $\hat{\sigma}_B^2$ abgeschätzt werden dann kann eine Teststatisk über die Gleichheit der beiden Varianzen $\sigma_A^2 = \sigma_B^2$ über die folgende Formel konstruiert werden. 

$$
T = \frac{\hat{\sigma}^2_A}{\hat{\sigma}^2_B} \sim F(df_A, df_B)
$$

Die beiden Varianzen folgen dabei jeweils einer $\chi^2$ Verteilung mit Freiheistgraden $df_A$ und $df_B$, so dass die Statistik $T$ einer $F$-Verteilung mit $(df_A, df_B)$ Freiheitsgeraden folgt und die $H_0$ lautet $H_0: \frac{\sigma_A^2}{\sigma_B^2} = 1$

