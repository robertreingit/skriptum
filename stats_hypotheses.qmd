# Hypothesen testen 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```


## Wahrscheinlichkeitstheorie


## Rechenregeln zum Erwartungswert und der Varianz

### Erwartungwert

Für eine diskrete Zufallsvariable $X$ auf einer endlichen Menge $\{x_i, i = 1, \ldots, n\}$ mit $n$ Elementen ist der Erwartungswert definiert mit:

$$
E[X] = \sum_{i=1}^n x_i P(x_i)
$$

D.h. jedes mögliche Ereignis wird mit seiner Wahrscheinlichkeit multipliziert und die Summe über alle diese Möglichkeiten wird gebildet. 

Wird eine Zufallsvariable mit einer Konstanten $a$ multipliziert, dann ist der Erwartungswert:

$$
E[aX] = \sum_{i=1}^n a x_i P(x_i) = a \sum_{i=1}^n x_i P(x_i) = a E[X]
$$

Werden zwei unabhängige Zufallsvariablen $X$ und $Y$ auf den Ereignismengen $\{x_i, i = 1, \ldots, n\}$ und $\{y_j, j = 1, \ldots, m\}$ addiert, dann berechnet sich der Erwartungswert von $E[X + Y]$ mittels:

$$
E[X + Y] = \sum_{i=1}^n x_i P(x_i) + \sum_{j=1}^m y_j P(x_j) = E[X] + E[Y]
$$

Diese Formel generalisiert für unabhängige $X_i, i = 1, \ldots, n$ zu:

$$
E[X_1 + X_2 + \ldots + X_n] = E[X_1] + E[X_2] + \ldots + E[X_n]
$$

In Kombination mit der Regel für konstante Terme mit den Konstanten $a_1, a_2, \ldots, a_n$ folgt:

$$
E[a_1 X_1 + a_2 X_2 + \ldots + a_n X_n] = a_1 E[X_1] + a_2 E[X_2] + \ldots + a_n E[X_n]
$$

#### Beispiele {-}

Nehmen wir zur Veranschaulichung ein einfaches Beispiel mit einer Zufallsvariable $X$ welche die folgende Verteilung hat (siehe @tbl-stats-hypo-px):

| x | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(x)$ | $\frac{1}{8}$ |$\frac{5}{8}$ |$\frac{1}{8}$ |$\frac{1}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-px}

Dann berechnet sich der Erwartungswert $E[X]$ mittels:

$$
E[X] = \sum_{i=1}^4 x_i P(x_i) = \frac{1}{8} \cdot 0 + \frac{1}{8}\cdot 1 + \frac{5}{8}\cdot2 + \frac{1}{8}\cdot3 = 1.25
$$

Hier kann auch eine interessante Eigenschaft des Erwartungswerts beobachtet werden, nämlich das der berechnete Wert gar nicht in der Menge der möglichen Werte der Zufallsvariablen vorkommen muss. In der Ereignismenge von $X$ sind nur ganzzahlige Werte.

Haben wir eine zweite Zufallsvariable $Y$ mit der Verteilung (siehe @tbl-stats-hypo-py)


| y | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(y)$ | $\frac{2}{8}$ |$\frac{2}{8}$ |$\frac{1}{8}$ |$\frac{3}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-py}

Mit $E[Y]$:

$$
E[Y] = \sum_{i=1}^4 y_i P(y_i) = \frac{2}{8} \cdot 0 + \frac{2}{8}\cdot 1 + \frac{1}{8}\cdot2 + \frac{3}{8}\cdot3 = 1.625 
$$

Dann folgt für den Erwarungswert von $E[X + Y]$:

$$
E[X + Y] = E[X] + E[Y] = 1.25 + 1.625 = 2.875 
$$

Definieren wir eine neue Zufallsvariable $Z$ mit $Z := a \cdot X$ mit der Konstanten $a := 2$. Dann folgt für den Erwartungswert von $E[Z]$:

$$
E[Z] = E[aX] = aE[X] = 2 \cdot 1.25 = 2.5
$$

### Varianz

## Schätzer

Erwartungstreue

## Hypothesentestung

### Der t-Test

Das Verhältnis einer standardnormalverteilten Variable z und eine $\chi^2$-verteilten Variable s folgt einer $t$-Verteilung.

$$
T = \frac{\hat{\Delta}}{\hat{s}_e(\hat{\delta})} \sim t\text{-Verteilung}
$$

### $\chi^2$-Test der Varianz 

Sei $\hat{\sigma}^2$ ein Schätzer für eine Varianz und $H_0: \sigma^2 = \sigma_0^2$ die Nullhypothese, dann lässt sich eine Teststatistik über die folgende Formel konstruieren:

$$
T = d \frac{\hat{\sigma}^2}{\sigma_0^2} \sim \chi^2(d\text{ Freiheitsgrade})
$$


### F-Test von Varianzverhältnissen

Seien zwei normalverteilte Stichproben gegeben und deren Varianzen über $\hat{\sigma}_A^2$ und $\hat{\sigma}_B^2$ abgeschätzt werden dann kann eine Teststatisk über die Gleichheit der beiden Varianzen $\sigma_A^2 = \sigma_B^2$ über die folgende Formel konstruiert werden. 

$$
T = \frac{\hat{\sigma}^2_A}{\hat{\sigma}^2_B} \sim F(df_A, df_B)
$$

Die beiden Varianzen folgen dabei jeweils einer $\chi^2$ Verteilung mit Freiheistgraden $df_A$ und $df_B$, so dass die Statistik $T$ einer $F$-Verteilung mit $(df_A, df_B)$ Freiheitsgeraden folgt und die $H_0$ lautet $H_0: \frac{\sigma_A^2}{\sigma_B^2} = 1$

