# Hypothesen testen 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

Im diesem Kapitel werden ein paar übergeordnete Konzepte der Statistik auf die im späteren Verlauf immer wieder zurück gegriffen wird noch einmal etwas eingehender eingegangen. Dies ist vor allem der Begriff des Erwartungswerts und auch noch mal die Varianz. Dazu wird kurz der Begriff eines Schätzers formalisiert. Am des Kapitels werden in einer Art Liste verschiedene Varianten von Hypothesentests kurz vorgestellt.

## Der Erwartungswert $\mu$ einer Zufallsvariable

Bisher wurde oft der Begriff **Mittelwert** verwendet und zum Beispiel vom Mittelwert $\mu$ einer Normalverteilung gesprochen. Eigentlich ist der $\mu$ nicht *nur* der Mittelwert sondern wird als Erwartungswert bezeichnet. Bei einer diskrete Zufallsvariable $X$, d.h. einer Zufallsvariable die auf einer endlichen Menge von Ereignissen $\{x_i, i = 1, \ldots, n\}$ mit $n$ Elementen definiert ist, ist der Erwartungswert wie folgt definiert:

::: {.def-erwartungswert}
### Erwarungswert $\mu$

\begin{equation}
E[X] = \sum_{i=1}^n x_i P(x_i)
\label{eq-stats-hypo-expected-01}
\end{equation}

Der Erwartungswert\index{Erwartungswert} wird üblicherweise mit dem Symbol $\mu$ bezeichnet. 
:::

D.h. jedes mögliche Ereignis wird mit seiner Wahrscheinlichkeit multipliziert und die Summe über alle diese Möglichkeiten wird gebildet um den Erwartungswert $\mu$ zu bestimmen.

::: {#exm-stats-hypo-cube}
## Erwartungswert eines Würfels

Sei ein normaler Würfel mit sechs Seiten geben und die Zufallsvariable $X$ ist die jeweilige Augenanzahl der Würfelseiten. Da bei einem fairen Würfel alle Seiten gleichwahrscheinlich sind, hat jedes Ereignis $x_i \in \{1,2,3,4,5,6\}$ eine Wahrscheinlichkeit vom $P(X = x) = \frac{1}{6}$ somit errechnet sich der Erwartungswert.

\begin{equation*}
E[X] = \sum_{i=1}^6 x_i P(x_i) = 1\cdot\frac{1}{6} + 2\cdot\frac{1}{6} +3\cdot\frac{1}{6} +4\cdot\frac{1}{6} +5\cdot\frac{1}{6} +6\cdot\frac{1}{6} = 3.5
\end{equation*}

:::

::: {.callout-tip}
Wenn der Fall auftritt, dass mehrere Zufallsvariablen verwendet werden, z.B. $X$ und $Y$, dann wird die folgende Schreibweise verwendet um die Erwartungswerte voneinander abzugrenzen. Zum Beispiel der Erwarungswert der Zufallsvariable $X$:

$$
E[X] = \mu_X
$$
Der Erwartungswert der Zufallsvariable $X$ wird mit $\mu_X$ bezeichnet. Wenn der Zusammenhang klar ist und nur von einer bestimmten Zufallsvariablen gesprochen wird, dann auch nur $\mu$.
:::

In der statistischen Literatur ist es üblich, die Größe $\mu$ als den Mittelwert der Population zu bezeichnen auch wenn es sich dabei nicht unbedingt um den Mittelwert handelt wie er üblicherweise verstanden wird und z.B. bei der Stichprobe berechnet wird ($\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$). Bei dem Erwartungswert $\mu$ handelt es sich um den gewichteten Mittelwert. Es wird unterschieden zwischen dem Mittelwert in der Population $\mu$, der theoretisch ist, und dem Mittelwert der Stichprobe $\bar{X}$ der eine Zufallsvariable darstellt und ebenfalls von theoretischer Natur ist, und dem tatsächlich **beobachteten Mittelwert** $\bar{x}$ in der Stichprobe. In den meisten Fällen im alltäglichen Umgang ist diese Unterscheidung nicht von Bedeutung sondern wird oft erst bei der Herleitung von theoretischen Zusammenhängen wichtig.

Für den späteren Umgang mit dem Erwartungswert ist die Kenntnis von ein paar Rechenregeln notwendig. Diese werden im Folgenden aufgelistet. 

Die erste Regel behandelt denn Fall, wenn eine Zufallsvariable $X$ mit einer Konstanten $a$ multipliziert wird. Konstant bedeutet, dass es sich bei $a$ nicht um eine Zufallsvariable handelt und $a$ immer den gleichen Wert hat. D.h. es soll der Erwartungswert von $aX$ berechnet werden:

\begin{equation}
E[aX] = \sum_{i=1}^n a x_i P(x_i) = a \sum_{i=1}^n x_i P(x_i) = a E[X]
\label{eq-stats-hypo-expected-mult}
\end{equation}


::: {#exm-stats-hypo-mult}
### Erwartungswert der Summe der Augenzahl mal $100$

Es soll nicht die Augenzahl des Würfels, sondern die Augenzahl mal $100$ berechnet werden, und davon der Erwartungswert. Somit gilt $a = 100$ und mit Formel \eqref{eq-stats-hypo-expected-mult}:

\begin{equation*}
E[aX] = a E[X] = 100 \cdot 3.5 = 35
\end{equation*}
:::

In vielen Fällen ist nicht nur eine einzelne Zufallsvariablen beteiligt, sondern, beispielsweise wenn eine Stichprobe untersuchen wird, liegen mehrere Zufallsvariablen vor. Der einfachsten Fall ist dabei natürlich wenn nur zwei unabhängigen Zufallsvariablen $X$ und $Y$ benötigt werden. Der allgemeine Fall, lässt sich dann aus diesem Spezialfall herleiten. Die beiden Variablen können auf der gleichen Ereignismenge definiert sein, können aber auch auf unterschiedlichen Ereignismengen, z.B. $\{x_i, i = 1, \ldots, n\}$ und $\{y_j, j = 1, \ldots, m\}$ definiert sein. Soll der Erwartungswert des Mittelwerts der beiden Zufallsvariablen $X$ und $Y$ berechnet, kann eine Regel angewendet werden die die Addition unabhängiger Zufallsvariablen regelt. Tatsächlich ist diese Operation relativ einfach nachzuvollziehen, der Erwartungswert von $E[X + Y]$ berechnet sich mittels:

\begin{equation}
E[X + Y] = \sum_{i=1}^n x_i P(x_i) + \sum_{j=1}^m y_j P(x_j) = E[X] + E[Y]
\label{eqn-stat-hypo-expected-add2}
\end{equation}

::: {#exm-stats-hypo-addmu}
## Erwartungswert der Summe der Augenzahl zweier Würfel

Sei $X$ die Augenzahl des einen Würfels (z.B. ein blauer Würfel) und $Y$ die Augenzahl eines weiteren Würfels (z.B. ein roter Würfel). Dann gilt nach Formel \eqref{eq-stats-hypo-expected-add2}:

\begin{equation*}
E[X + Y] = E[X] + E[Y] = 3.5 + 3.5 = 7
\end{equation*}
:::

Die Formel \eqref{eq-stats-hypo-expected-add2} als Spezialfall mit nur zwei Zufallsvariablen lässt sich relativ direkt für den Fall von $n$ unabhängigen Zufallsvariablen $X_i, i = 1, \ldots, n$ generalisieren. Seien zum Beispiel drei Zufallsvariablen $X, Y$ und $Z$ gegeben und der Erwartungswert der Summe $M = X + Y + Z$ soll berechnet werden, dann lässt sich eine Zwischenvariable, z.B. $A = Y + Z$ definieren und es gilt $M = X + Y$ ist wieder der bereits bekannte Fall. Wiederholte Anwendung führt dann zur folgenden allgemeinen Regel:

\begin{equation}
E[X_1 + X_2 + \ldots + X_n] = E[X_1] + E[X_2] + \ldots + E[X_n]
\label{eq-stats-hypo-expected-add}
\end{equation}

In Kombination mit der Regel für konstante Terme (Formel \eqref{eq-stats-hypo-expected-mult}) mit den Konstanten $a_1, a_2, \ldots, a_n$ folgt dann eine weitere Verallgemeinerung:

\begin{equation}
E[a_1 X_1 + a_2 X_2 + \ldots + a_n X_n] = a_1 E[X_1] + a_2 E[X_2] + \ldots + a_n E[X_n]
\label{eq-stats-hypo-expected-lo}
\end{equation}

Diese Eigenschaft, dass der Erwartungswert einer gewichteten Summe von Zufallsvariablen (eine lineare Operation) gleich der gewichteten Summe der einzelnen Erwartungswerte ist, macht den Erwartungswert zu einem **linearen Operator**.

Nur der Vollständigkeit halber sei auch noch kurz die Definition des Erwartungswertes für reelle Verteilungen erwähnt. Hier ist die Zufallsvariable nicht mehr auf einer endlichen Menge sondern auf einem Intervall definiert. Auf die Herleitung wird nicht eingegangen, sondern er reicht das Summenzeichen durch ein Integral zu ersetzen.

\begin{equation*}
E[X] = \int_{x=-\infty}^\infty x f(x) dx
\end{equation*}

Das Integral geht über den gesamten Definitionsbereich von $X$ und die Wahrscheinlichkeitsfunktion wird durch die Dichtefunktion $f(x)$ ersetzt. Die Linearität der Operation bleibt für alle im weiteren interessierenden Fälle erhalten (keine Panik, es werden im Weiteren keine Integrale berechnet).

::: {#exm-hypo-stat-simple}
Sein eine Zufallsvariable $X$ geben, die nur vier verschiedene Werte annehmen kann $x \in \{0,1,2,3\}$. Diese Ereignisse haben die folgenden Wahrscheinlichkeiten (siehe @tbl-stats-hypo-px):

| x | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(x)$ | $\frac{1}{8}$ |$\frac{5}{8}$ |$\frac{1}{8}$ |$\frac{1}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-px}

Anwendung von Formel \eqref{eq-stats-hypo-expected-01} führt zu der Berechnung des Erwartungswerts $E[X]$ mittels:

$$
E[X] = \sum_{i=1}^4 x_i P(x_i) = 0 \cdot \frac{1}{8}  + 1 \cdot \frac{5}{8} + 2\cdot \frac{1}{8} + 3\cdot \frac{1}{8} = 1.75
$$
:::

::: {#exm-hypo-stats-simple-02}
Haben wir eine zweite Zufallsvariable $Y$ mit der Verteilung (siehe @tbl-stats-hypo-py)


| y | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(y)$ | $\frac{2}{8}$ |$\frac{2}{8}$ |$\frac{1}{8}$ |$\frac{3}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-py}

Mit Formel \eqref{eq-stats-hypo-expected-01} folgt wiederum:

$$
E[Y] = \sum_{i=1}^4 y_i P(y_i) = 0 \cdot \frac{2}{8} + 1 \cdot \frac{2}{8} + 2 \cdot \frac{1}{8} + 3 \cdot \frac{3}{8} = 1.625 
$$

Wenn wir eine neue Zufallsvariable $Z$ definieren mit $Z = X + Y$, dann folgt für den Erwarungswert von $E[Z] = E[X + Y]$ mittels \eqref{eq-stats-hypo-expected-add}:

$$
E[Z] = E[X + Y] = E[X] + E[Y] = 1.25 + 1.625 = 2.875 
$$

Definieren wir dagegen $Z$ mit $Z := a \cdot X$ mit der Konstanten $a := 2$. Dann folgt für den Erwartungswert von $E[Z]$ mit Formel \eqref{eq-stats-hypo-expected-mult}:

$$
E[Z] = E[aX] = aE[X] = 2 \cdot 1.25 = 2.5
$$
:::

::: {#exm-hypo-stats-chuckaluck}
### Chuck-a-Luck
Ein ganz anderes Beispiel, welches noch mal den Begriff Erwartungswert veranschaulicht, bezieht sich auf ein Glückspiel mit dem Namen Chuck-a-Luck. Das Beispiel ist @gross2019 entnommen. Chucia-Luck wird mit einem Einsatz von 1 € gespielt. Es werden drei Würfel geworfen und die folgende Regeln bestimmen den Gewinn (siehe @tbl-chuck-a-luck). 

| Ausgang | Gewinn |
| --- | --- |
| keine 6 | 0 EU |
| min. eine 6 | 2 EU | 
| 3 x 6 |  27 EU |

: Gewinnauschüttung bei Chuck-a-Luck {#tbl-chuck-a-luck}

Die Frage die sich nun stellt, ist ob dieses Spiel fair ist bzw. lohnt es sich einen 1 € Einsatz zu setzen? Diese Frage kann mit dem Erwartungswert beantwortet werden. Um den Erwartungswert zu berechnen benötigen wir allerdings zunächst die Wahrscheinlichkeiten für die verschiedenen Ausgänge.

Die Wahrscheinlichkeit keine $6$ zu werfen ist für jeden Würfel einzeln $\frac{5}{6}$, dementsprechend, da die Würfel unabhängig voneinander sind, kann diese Wahrscheinlichkeit dreimal miteinander multipliziert werden.

$$
P(0 \times 6) = \left(\frac{5}{6}\right)^3 = \frac{125}{216} \approx 0.579
$$

D.h. in knapp 60% der Fälle wird beim dem Spiel kein Gewinn ausgeschüttet. Berechnen wir zunächst den Fall, dass drei Sechsen geworfen werden. Dieser Fall kann parallel zudemjenigen für keine Sechs gelöst werden. Einziger Unterschied, ist die Wahrscheinlichkeit des Ereignisses. Die Wahrscheinlichkeit für einen Würfel eine Sechs zu würfeln ist $\frac{1}{6}$. Es folgt daher analog:

$$
P(3 \times 6) = \left(\frac{1}{6}\right)^3 = \frac{1}{216} \approx 0.005
$$

D.h. die Wahrscheinlichkeit für $3 \times 6$ ist gerade einmal ein halbes Prozent. D.h. in 200 Spielen, erwawrten wir dieses Ereignis nur ein einziges Mal.

Letzlich bleibt noch das Ereignis mindestens eine $6$. Hier nehmen wir das Komplementärereignis zu mindestens eine Sechs. Das Komplementärereignis ist nämlich keine Sechs zu würfeln. Diese Wahrscheinlichkeit ziehen wird dann von 1, dem sicheren Ereignis, ab. Da diese Menge auch die drei Sechsen beinhaltet, für das eine andere Gewinnberechnung gilt, müssen wir dessen Wahrscheinlichkeit noch subtrahieren.

$$
P(\text{min. eine } 6) = 1 - P(0 \times 6) - P(3 \times 6) = \frac{216}{216} - \frac{125}{216} - \frac{1}{216} = \frac{90}{216} = 0.41\bar{6}
$$

Die Wahrscheinlichkeit für mindestens eine Sechs ist dementsprechend etwas über 40%. Jetzt wenden wir wieder die Formel für den Erwartungswert an um die zu erwartende Gewinnsumme zu bestimmen. Die Gewinnsumme nimmt jetzt den Wert der Zufallsvariablen ein.

$$
E[X] = \frac{125}{216}\times 0 + \frac{90}{216}\times 2 + \frac{1}{216}\times27 = \frac{207}{216} \approx 0.958
$$

Im Mittel erwarten wir bei dem Spiel einen Gewinn von $0.958$€ bei einem Einsatz von $1$ €. Daher wird im Mittel ein Verlust bei dem Spiel gemacht. 
:::

::: {#exm-hypo-stats-expected-simple-03}
Als letztes Beispiel betrachten wir den Erwartungswert des Mittelwerts $\bar{X}$ von $n$ identisch verteilten Zufallsvariablen $X_i$. Dadurch das für alle $X_i, E[X_i] = \mu$ gilt ist dies letztendlich nur eine direkte Anwendung der obigen Rechenregeln.

\begin{equation}
E[\bar{X}] = E\left[\frac{1}{n}\sum_{i=1}^n X_i\right] = \frac{1}{n}E\left[\sum_{i=1}^n X_i\right] = \frac{1}{n}\sum_{i=1}^n E[X_i] = \frac{1}{n}\sum_{i=1}^n \mu = \frac{1}{n}n \mu = \mu
\label{eq-stats-hyp-exbar}
\end{equation}

D.h. der Erwartungswert des Mittelwerts ist tatsächlich auch der Erwartungswert $\mu$ der Verteilung.
:::

## Die Varianz $\sigma^2$ einer Zufallsvariablen

Die Varianz $\sigma^2$ beschreibt die Streuung der Zufallsvariablen $X$. 

::: {.def-varianz}
### Varianz $\sigma^2$

\begin{equation}
Var(X) = \sigma^2 = E[(X-E[X])^2] = E[(X - \mu)^2]
\label{eq-def-variance}
\end{equation}

D.h. die Varianz\index{Varianz} ist die quadrierte Abweichung von X vom Erwartungwert $\mu$.
:::

Wie wir schon behandelt haben, wird die Varianz als Skalenparameter bezeichnet. Wobei wir in den meisten Fällen mit der Standardabweichung $\sigma$ arbeiten, da diese die gleichen Einheiten wie die Variable $X$ hat.

Wenn wir den quadratischen Term ausmultiplizieren und dabei die Rechenregeln für den Erwartungswert berücksichtigen ($E[X] = \mu$ wird als eine Konstante betrachten), dann erhalten wir eine praktische, alternative Formulierung für die Varianz $\sigma^2$.

\begin{align*}
\sigma^2 &= E[(X-E[X])^2] \\
&= E[X^2 + E[X]^2 - 2XE[X]] \\
&= E[X^2] - E[E[X]^2] - E[2E[X]X] \\
&= E[X^2] - E[X]^2 - 2E[X]E[X] \\
&= E[X^2] - E[X]^2 - 2E[X]^2 \\
&= E[X^2] - E[X]^2
\end{align*}

Bzw.

\begin{equation}
Var(X) = \sigma^2 = E[X^2] - \mu^2
\label{eq-stats-hypo-var2}
\end{equation}

Die Gleichung \eqref{eq-stats-hypo-var2} wird später immer wieder praktisch sein, wenn wir die Eigenschaften von bestimmten Statistiken formal bestimmen wollen.

Für die Varianz gibt es auch paar einfache Rechenregeln ähnlich wie für den Erwartungswert. Für eine Konstante $a$ gilt

\begin{align*}
Var(a) &= 0 \\
Var(a + X) &= Var(X) \\
Var(a \cdot X) &= a^2 \cdot Var(X)
\end{align*}

Insgesamt gilt für die Linearkombination von $n$ unabhängigen Zufallsvariablen $X_i$:

\begin{equation}
Var\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 Var(X_i)
\label{eq-stats-hypo-sum-var}
\end{equation}

::: {#exm-ed-hypo-var-sum}
Für zwei unabhängige Zufallsvariablen $X$ und $Y$ gilt:

\begin{equation*}
Var(X + Y) = Var(X) + Var(Y)
\end{equation*}
:::

::: {#exm-ed-hypo-var-diff}
Eine überraschende Eigenschaft aus den Rechenregeln für die Varianz ist, dass die Varianz der Differenz zweier unabhängigen Variablen gleich der Varianz ist, wenn die beiden Variablen addiert werden.

\begin{equation}
\begin{split}
Var(X - Y) &= Var( 1\cdot X + (-1)\cdot Y) \\
&= Var(X) + Var((-1)\cdot Y) \\
&= Var(X) + (-1)^2 \cdot Var(Y) \\
&= Var(X) + Var(Y)
\end{split}
\label{eq-stats-hypo-vardiff}
\end{equation}

Das Ergebnis in Formel \eqref{eq-stats-hypo-vardiff} wird uns später wieder begegnen, wenn wir uns mit experimentellen Designs beschäftigen und in diesem Zusammenhang den Unterschied zwischen Gruppenmittelwerten berechnen wollen.
:::

::: {#exm-ed-hypo-mean-se}
Mit den obigen Regeln können wir auch die Varianz des Mittelwerts $\bar{X}$ bestimmen. Die Variablen $X_i, i = 1,\ldots,n$ seien wieder unabhängig voneinander und stammen aus der gleichen Verteilung mit $Var(X_i) = \sigma^2$.

\begin{equation}
\begin{split}
Var(\bar{X}) &= Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) \\
&= \left(\frac{1}{n}\right)^2 Var\left(\sum_{i=1}^n X_i\right) \\
&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i) \\
&= \frac{1}{n^2}\sum_{i=1}^n \sigma^2 \\
&= \frac{1}{n^2}n \sigma^2 = \frac{\sigma^2}{n}
\end{split}
\label{eq-stats-hypo-barxse}
\end{equation}

Die Gleichung \eqref{eq-stats-hypo-barxse} sollte uns bekannt vorkommen, ist diese doch der Standardfehler des Mittelwerts bzw. die Varianz dessen. Hier sehen wir den Wurzelzusammenhang den wir bereits beobachtet haben. Formal haben wir daher hergeleitet, dass die Streuung des Mittelwerts einer Stichprobe mit der Wurzel der Stichprobengröße $n$ abnimmt.
:::

Wie beim Erwartungswert auch noch mal kurz die kontinuierliche Version der Definition der Varianz.

\begin{equation}
Var(X) = \int_{-\infty}^{\infty}(X-\mu)^2 f(x) dx
\end{equation}

## Schätzer

Schätzer sind uns bereits in verschiedenen Varianten begegnet. Als ein Schätzer wird eine Funktion bezeichnet mit deren Hilfe anhand einer Stichprobe ein Wert, ein Schätzwert, berechnet wird, der ermöglicht Rückschlüsse über unbekannte Parameter $\phi$ in der Population zu erhalten. D.h wir haben zum Beispiel eine Population mit den Parametern $\mu$ und $\sigma^2$ und wollen anhand der beobachteten Werte der Stichprobe aussagen über diese beiden Parameter machen. Dazu benutzen wir Schätzer.

::: {.def-schaetzer}
### Schätzer

Ein Schätzer\index{Schäzter} oder eine Schätzfunktion ist eine Statistik auf einer Stichprobe die dazu verwendet wird Information über unbekannte Parameter einer Population zu erhalten. Ein Schätzer $t$ für den Parameter $\theta$ (belieber Parameter) aus der Population wird oft mit einem Hut gekennzeichnet $t = \hat{\theta}$. Da $t$ mittels der Stichprobe $X_i, i = 1, \ldots, n$ ermittelt wird, kann manchmal auch die Schreibweise $t = h(X_1, \ldots, X_n)$ verwendet werden. $h()$ ist dabei eine Funktion (z.B. Mittelwert) auf der Stichprobe.
:::

Beispielsweise ist der Mittelwert einer Stichprobe $\bar{x}$ ein Schätzer für den Mittelwert $\mu$ in der Population. Die Standardabweichung $s$ der Stichprobe ist ein Schätzer für die Standardabweichung $\sigma$ in der Population $s = \hat{\sigma}$.

Es gibt nun immer eine Reihe von möglichen Schätzern auf einer gegebenen Stichprobe. Zum Beispiel, könnte ich anstatt den Mittelwert über alle Stichprobenwerte zu rechnen, könnte ich nur die drei kleinsten Werte nehmen. Oder jeden zweiten Wert usw.. D.h. nicht alle Schätzer sind gleich gut. Manche sind besser als andere. Das Konzept des Schätzer erlaubt Unterschiede zwischen verschiedenen Schätzer zu formalisieren.

Eine wichtige Eigenschaft von Schätzern ist die sogenannte Erwartungstreue. Die Erwartungstreue ist eng mit dem Erwartungswerts einer Statistik verknüpft. Wir hatten oben gesehen, dass für den Erwartungwert des Mittelwerts $E[\bar{x}] = \mu$ gilt. Dies zeigt, dass der Mittelwert der Stichprobe ein erwartungstreuer Schätzer für den Populationsparameter $\mu$ ist.

Mit $\sigma^2 = E[X^2] - E[X]^2 = E[X^2] - \mu^2 \Rightarrow E[X^2] = \sigma^2 + \mu^2$ und den Rechenregeln für den Erwartungswert können wir zeigen warum die Standardabweichung $s$ bzw. die Varianz $s^2$ einer Stichprobe sich als Schätzer für $\sigma$ bzw. $\sigma^2$ eignen, also $s = \hat{\sigma}$ bzw. $s^2 = \hat{\sigma}^2$.

\begin{equation}
s^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}
\end{equation}

Es folgt nämlich

\begin{align*}
E\left[\sum_{i=1}^{n} (x_i-\bar x)^2\right] &= E[\sum_{i=1}^{n} x_i^2-2\bar x\sum_{i=1}^{n} x_i+n\bar{x}^2]\\
&=E[\sum_{i=1}^{n} x_i^2-2\bar{x}n\bar{x}+n\bar{x}^2]\\ 
&= E[\sum_{i=1}^{n} x_i^2-n\bar{x}^2] \\
&= n E[x_i^2]- n E[\bar{x}^2]\\
&= n (\mu^2 + \sigma^2) - n(\mu^2+\sigma^2/n)\\
&= (n-1) \sigma^2
\end{align*}

Daraus folgt:
\begin{equation*}
E[s^2] = E\left[\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}\right] = \sigma^2
\end{equation*}

Was dann auch erklärt warum die Summe der quadrierten Abweichung durch $n-1$ und nicht durch $n$ geteilt wird.

Würden nämlich die quadrierten Abweichungen durch $n$ geteilt werden, dann hätten wir keinen erwartungstreuen Schätzer mehr.

\begin{equation*}
E\left[\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n}\right] = \frac{1}{n}E\left[\sum_{i=1}^n (x_i - \bar{x})^2\right] = \frac{n-1}{n}\sigma^2
\end{equation*}

Würden wir durch $n$ teilen, dann würden wir die Varianz unterschätzen, da $\frac{n-1}{n}<1$ gilt. Der Schätzer weist in diesem Fall einen bias (systematischen Fehler) auf.

::: {.def-bias}
### Bias

Der Bias\index{Bias} ist definiert als die Abweichung zwischen dem Erwartungswert $\hat{\theta}_{t}$ des Schätzers $t$ vom wahren Wert $\theta$ in der Population.

\begin{equation}
\text{bias}(\hat{\theta}_t) = E[\hat{\theta}_t - \theta]
\end{equation}

:::

Zusammenfassend, benötigen wir erwartungstreue Schätzer um die Verknüpfung zwischen unserer Stichprobe und den Parametern einer theoretischen Stichprobenverteilung zu bilden.

## Hypothesentestungszoo

Es folgt noch eine kurze Übersicht über verschiedenen Teststatistiken die uns immer wieder begegnen wird.

### Der t-Test

Wir hatten bereits gesehen, das das Verhältnis einer standardnormalverteilten Variable zu einer $\chi^2$-verteilten Variable folgt einer $t$-Verteilung. In vielen Fällen wird diese Eigenschaft verwendet, wenn ein Unterschied $\hat{\Delta}$ beispielsweise zwischen zwei Mittelwerten $\bar{x} - \bar{y}$ durch eine Streuung $\hat{s}_e(\hat{\delta})$ geteilt wird. 

\begin{equation*}
T = \frac{\hat{\Delta}}{\hat{s}_e(\hat{\delta})} \sim t\text{-Verteilung}
\end{equation*}

Zum Beispiel bei dem t-Test der uns in unserer ersten Statistikvorlesung begegnet ist, wurde der Unterschied zwischen zwei Gruppenmittelwerten durch den Standardfehler der Differenz geteilt. 

### $\chi^2$-Test der Varianz 

Die $\chi^2$-Verteilung taucht öfters im Zusammenhang mit Schätzern von Varianzen auf. Beispielsweise lässt sich zeigen, das bei $n$ unabhängige, standardnormalverteilte Variabnel $X_i, i = 1, \ldots, n$ die Summer der quadrierten Abweichungen vom Mittelwert, also $\sum_{i=1}^n(Y_i - \bar{Y})^2$ einer $\chi^2$-Verteilung mit $n-1$ Freiheitsgraden folgt.

Sei $\hat{\sigma}^2$ ein Schätzer für die Varianz aus einer Stichprobe und es soll überprüft werden, ob diese Varianz einer vorgegebenen Varianz $\sigma_0$ folgt, also $H_0: \sigma^2 = \sigma_0^2$ die Nullhypothese ist. Dann lässt sich eine Teststatistik $T$ über die folgende Formel konstruieren:

\begin{equation*}
T = d \frac{\hat{\sigma}^2}{\sigma_0^2} \sim \chi^2 \quad d = \text{ Freiheitsgrade}
\end{equation*}

$H_0$ wird dann abgelehnt, wenn $T > q_{\chi^2,\text{df}=d,1-\alpha}$ (einseitig) gilt.

### F-Test von Varianzverhältnissen

Sei eine Stichproben aus normalverteilten, unabhängigen Variablen gegeben und die Varianz $\sigma_A^2$ und $\sigma_B^2$ mittels zweier Schätzer $\hat{\sigma}_A^2$ und $\hat{\sigma}_B^2$ bestimmt worden. Dann kann eine Teststatisk $T$ konstruiert werden um die die Gleichheit der beiden Varianzen zu überprüfen. Formal $H_0: \sigma_A^2 = \sigma_B^2 \Leftrightarrow \frac{\sigma_A}{\sigma_B} = 1$.

$$
T = \frac{\hat{\sigma}^2_A}{\hat{\sigma}^2_B} \sim F(df_A, df_B)
$$

Die beiden Varianzen folgen dabei jeweils einer $\chi^2$ Verteilung mit Freiheistgraden $df_A$ und $df_B$. Daher folgt  die Statistik $T$ unter der $H_0$ einer $F$-Verteilung mit $(df_A, df_B)$ Freiheitsgeraden. Die $H_0$ wird daher wiederum abgelehnt wenn $T > q_{F,\text{df}_A,\text{df}_B, 1-\alpha}$ (einseitig) gilt.

Zusammenfassend ist wie schon bei der Auflistung der theoretischen Verteilung ist es zunächst einmal das Wichtigste zur Kenntnis zu nehmen, dass es diese unterschiedlichen Tests gibt. Deren Herleitung ist natürlich mit den gegebenen Informationen nicht nachvollziehbar und maximal undurchsichtig. Die Liste ist daher mehr im Sinne einer Mustererkennungsaufgabe für später zu interpretieren, dass wenn die Tests im weitern Verlauf in konkreten Zusammenhängen auftauchen ein Wiedererkennungswert ersteht. Letztendlich ist das anzuwendende Prinzip immer das Gleiche, Teststatistik $\rightarrow$ theoretische Verteilung $\rightarrow$ kritischer Wert unter der $H_0$ über die Quantile $\alpha$ $\rightarrow$ Testentscheidung und Unsicherheit bestimmen. Was sich ändert sind die Tests und deren Parameter.
