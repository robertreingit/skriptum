# Weitere Grundlagen der Statistik 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

Im diesem Kapitel werden ein paar übergeordnete Konzepte der Statistik auf die im späteren Verlauf immer wieder zurück gegriffen wird noch einmal etwas eingehender eingegangen. Dies ist vor allem der Begriff des Erwartungswerts und auch noch mal die Varianz. Dazu wird kurz der Begriff eines Schätzers formalisiert. Am des Kapitels werden in einer Art Liste verschiedene Varianten von Hypothesentests kurz vorgestellt.

## Der Erwartungswert $\mu$ einer Zufallsvariable

Bisher wurde oft der Begriff **Mittelwert** verwendet und zum Beispiel vom Mittelwert $\mu$ einer Normalverteilung gesprochen. Eigentlich ist der $\mu$ nicht *nur* der Mittelwert sondern wird als Erwartungswert bezeichnet. Bei einer diskrete Zufallsvariable $X$, d.h. einer Zufallsvariable die auf einer endlichen Menge von Ereignissen $\{x_i, i = 1, \ldots, n\}$ mit $n$ Elementen definiert ist, ist der Erwartungswert wie folgt definiert:

::: {.def-erwartungswert}
### Erwarungswert $\mu$

\begin{equation}
E[X] = \sum_{i=1}^n x_i P(x_i)
\label{eq-stats-hypo-expected-01}
\end{equation}

Der Erwartungswert\index{Erwartungswert} wird üblicherweise mit dem Symbol $\mu$ bezeichnet. 
:::

D.h. jedes mögliche Ereignis wird mit seiner Wahrscheinlichkeit multipliziert und die Summe über alle diese Möglichkeiten wird gebildet um den Erwartungswert $\mu$ zu bestimmen.

::: {#exm-stats-hypo-cube}
## Erwartungswert eines Würfels

Sei ein normaler Würfel mit sechs Seiten geben und die Zufallsvariable $X$ ist die jeweilige Augenanzahl der Würfelseiten. Da bei einem fairen Würfel alle Seiten gleichwahrscheinlich sind, hat jedes Ereignis $x_i \in \{1,2,3,4,5,6\}$ eine Wahrscheinlichkeit vom $P(X = x) = \frac{1}{6}$ somit errechnet sich der Erwartungswert.

\begin{equation*}
E[X] = \sum_{i=1}^6 x_i P(x_i) = 1\cdot\frac{1}{6} + 2\cdot\frac{1}{6} +3\cdot\frac{1}{6} +4\cdot\frac{1}{6} +5\cdot\frac{1}{6} +6\cdot\frac{1}{6} = 3.5
\end{equation*}

:::

::: {.callout-tip}
Wenn der Fall auftritt, dass mehrere Zufallsvariablen verwendet werden, z.B. $X$ und $Y$, dann wird die folgende Schreibweise verwendet um die Erwartungswerte voneinander abzugrenzen. Zum Beispiel der Erwarungswert der Zufallsvariable $X$:

$$
E[X] = \mu_X
$$
Der Erwartungswert der Zufallsvariable $X$ wird mit $\mu_X$ bezeichnet. Wenn der Zusammenhang klar ist und nur von einer bestimmten Zufallsvariablen gesprochen wird, dann auch nur $\mu$.
:::

In der statistischen Literatur ist es üblich, die Größe $\mu$ als den Mittelwert der Population zu bezeichnen auch wenn es sich dabei nicht unbedingt um den Mittelwert handelt wie er üblicherweise verstanden wird und z.B. bei der Stichprobe berechnet wird ($\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$). Bei dem Erwartungswert $\mu$ handelt es sich um den gewichteten Mittelwert. Es wird unterschieden zwischen dem Mittelwert in der Population $\mu$, der theoretisch ist, und dem Mittelwert der Stichprobe $\bar{X}$ der eine Zufallsvariable darstellt und ebenfalls von theoretischer Natur ist, und dem tatsächlich **beobachteten Mittelwert** $\bar{x}$ in der Stichprobe. In den meisten Fällen im alltäglichen Umgang ist diese Unterscheidung nicht von Bedeutung sondern wird oft erst bei der Herleitung von theoretischen Zusammenhängen wichtig.

Für den späteren Umgang mit dem Erwartungswert ist die Kenntnis von ein paar Rechenregeln notwendig. Diese werden im Folgenden aufgelistet. 

Die erste Regel behandelt denn Fall, wenn eine Zufallsvariable $X$ mit einer Konstanten $a$ multipliziert wird. Konstant bedeutet, dass es sich bei $a$ nicht um eine Zufallsvariable handelt und $a$ immer den gleichen Wert hat. D.h. es soll der Erwartungswert von $aX$ berechnet werden:

\begin{equation}
E[aX] = \sum_{i=1}^n a x_i P(x_i) = a \sum_{i=1}^n x_i P(x_i) = a E[X]
\label{eq-stats-hypo-expected-mult}
\end{equation}


::: {#exm-stats-hypo-mult}
### Erwartungswert der Summe der Augenzahl mal $100$

Es soll nicht die Augenzahl des Würfels, sondern die Augenzahl mal $100$ berechnet werden, und davon der Erwartungswert. Somit gilt $a = 100$ und mit Formel \eqref{eq-stats-hypo-expected-mult}:

\begin{equation*}
E[aX] = a E[X] = 100 \cdot 3.5 = 35
\end{equation*}
:::

In vielen Fällen ist nicht nur eine einzelne Zufallsvariablen beteiligt, sondern, beispielsweise wenn eine Stichprobe untersuchen wird, liegen mehrere Zufallsvariablen vor. Der einfachsten Fall ist dabei natürlich wenn nur zwei unabhängigen Zufallsvariablen $X$ und $Y$ benötigt werden. Der allgemeine Fall, lässt sich dann aus diesem Spezialfall herleiten. Die beiden Variablen können auf der gleichen Ereignismenge definiert sein, können aber auch auf unterschiedlichen Ereignismengen, z.B. $\{x_i, i = 1, \ldots, n\}$ und $\{y_j, j = 1, \ldots, m\}$ definiert sein. Soll der Erwartungswert des Mittelwerts der beiden Zufallsvariablen $X$ und $Y$ berechnet, kann eine Regel angewendet werden die die Addition unabhängiger Zufallsvariablen regelt. Tatsächlich ist diese Operation relativ einfach nachzuvollziehen, der Erwartungswert von $E[X + Y]$ berechnet sich mittels:

\begin{equation}
E[X + Y] = \sum_{i=1}^n x_i P(x_i) + \sum_{j=1}^m y_j P(x_j) = E[X] + E[Y]
\label{eq-stats-hypo-expected-add2}
\end{equation}

::: {#exm-stats-hypo-addmu}
## Erwartungswert der Summe der Augenzahl zweier Würfel

Sei $X$ die Augenzahl des einen Würfels (z.B. ein blauer Würfel) und $Y$ die Augenzahl eines weiteren Würfels (z.B. ein roter Würfel). Dann gilt nach Formel \eqref{eq-stats-hypo-expected-add2}:

\begin{equation*}
E[X + Y] = E[X] + E[Y] = 3.5 + 3.5 = 7
\end{equation*}
:::

Die Formel \eqref{eq-stats-hypo-expected-add2} als Spezialfall mit nur zwei Zufallsvariablen lässt sich relativ direkt für den Fall von $n$ unabhängigen Zufallsvariablen $X_i, i = 1, \ldots, n$ generalisieren. Seien zum Beispiel drei Zufallsvariablen $X, Y$ und $Z$ gegeben und der Erwartungswert der Summe $M = X + Y + Z$ soll berechnet werden, dann lässt sich eine Zwischenvariable, z.B. $A = Y + Z$ definieren und es gilt $M = X + Y$ ist wieder der bereits bekannte Fall. Wiederholte Anwendung führt dann zur folgenden allgemeinen Regel:

\begin{equation}
E[X_1 + X_2 + \ldots + X_n] = E[X_1] + E[X_2] + \ldots + E[X_n]
\label{eq-stats-hypo-expected-add}
\end{equation}

In Kombination mit der Regel für konstante Terme (Formel \eqref{eq-stats-hypo-expected-mult}) mit den Konstanten $a_1, a_2, \ldots, a_n$ folgt dann eine weitere Verallgemeinerung:

\begin{equation}
E[a_1 X_1 + a_2 X_2 + \ldots + a_n X_n] = a_1 E[X_1] + a_2 E[X_2] + \ldots + a_n E[X_n]
\label{eq-stats-hypo-expected-lo}
\end{equation}

Diese Eigenschaft, dass der Erwartungswert einer gewichteten Summe von Zufallsvariablen (eine lineare Operation) gleich der gewichteten Summe der einzelnen Erwartungswerte ist, macht den Erwartungswert zu einem **linearen Operator**.

Nur der Vollständigkeit halber sei auch noch kurz die Definition des Erwartungswertes für reelle Verteilungen erwähnt. Hier ist die Zufallsvariable nicht mehr auf einer endlichen Menge sondern auf einem Intervall definiert. Auf die Herleitung wird nicht eingegangen, sondern er reicht das Summenzeichen durch ein Integral zu ersetzen.

\begin{equation*}
E[X] = \int_{x=-\infty}^\infty x f(x) dx
\end{equation*}

Das Integral geht über den gesamten Definitionsbereich von $X$ und die Wahrscheinlichkeitsfunktion wird durch die Dichtefunktion $f(x)$ ersetzt. Die Linearität der Operation bleibt für alle im weiteren interessierenden Fälle erhalten (keine Panik, es werden im Weiteren keine Integrale berechnet).

::: {#exm-hypo-stat-simple}
Sein eine Zufallsvariable $X$ geben, die nur vier verschiedene Werte annehmen kann $x \in \{0,1,2,3\}$. Diese Ereignisse haben die folgenden Wahrscheinlichkeiten (siehe @tbl-stats-hypo-px):

| x | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(x)$ | $\frac{1}{8}$ |$\frac{5}{8}$ |$\frac{1}{8}$ |$\frac{1}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-px}

Anwendung von Formel \eqref{eq-stats-hypo-expected-01} führt zu der Berechnung des Erwartungswerts $E[X]$ mittels:

$$
E[X] = \sum_{i=1}^4 x_i P(x_i) = 0 \cdot \frac{1}{8}  + 1 \cdot \frac{5}{8} + 2\cdot \frac{1}{8} + 3\cdot \frac{1}{8} = 1.75
$$
:::

::: {#exm-hypo-stats-simple-02}
Haben wir eine zweite Zufallsvariable $Y$ mit der Verteilung (siehe @tbl-stats-hypo-py)


| y | 0 | 1 | 2 | 3 |
| --- | --- | --- | --- | --- |
| $P(y)$ | $\frac{2}{8}$ |$\frac{2}{8}$ |$\frac{1}{8}$ |$\frac{3}{8}$ |

: Verteilung der Zufallsvariablen $X$ {#tbl-stats-hypo-py}

Mit Formel \eqref{eq-stats-hypo-expected-01} folgt wiederum:

$$
E[Y] = \sum_{i=1}^4 y_i P(y_i) = 0 \cdot \frac{2}{8} + 1 \cdot \frac{2}{8} + 2 \cdot \frac{1}{8} + 3 \cdot \frac{3}{8} = 1.625 
$$

Wenn wir eine neue Zufallsvariable $Z$ definieren mit $Z = X + Y$, dann folgt für den Erwarungswert von $E[Z] = E[X + Y]$ mittels \eqref{eq-stats-hypo-expected-add}:

$$
E[Z] = E[X + Y] = E[X] + E[Y] = 1.25 + 1.625 = 2.875 
$$

Definieren wir dagegen $Z$ mit $Z := a \cdot X$ mit der Konstanten $a := 2$. Dann folgt für den Erwartungswert von $E[Z]$ mit Formel \eqref{eq-stats-hypo-expected-mult}:

$$
E[Z] = E[aX] = aE[X] = 2 \cdot 1.25 = 2.5
$$
:::

::: {#exm-hypo-stats-chuckaluck}
### Chuck-a-Luck
Ein ganz anderes Beispiel, welches noch mal den Begriff Erwartungswert veranschaulicht, bezieht sich auf ein Glückspiel mit dem Namen Chuck-a-Luck. Das Beispiel ist @gross2019 entnommen. Chucia-Luck wird mit einem Einsatz von 1 € gespielt. Es werden drei Würfel geworfen und die folgende Regeln bestimmen den Gewinn (siehe @tbl-chuck-a-luck). 

| Ausgang | Gewinn |
| --- | --- |
| keine 6 | 0 EU |
| min. eine 6 | 2 EU | 
| 3 x 6 |  27 EU |

: Gewinnauschüttung bei Chuck-a-Luck {#tbl-chuck-a-luck}

Die Frage die sich nun stellt, ist ob dieses Spiel fair ist bzw. lohnt es sich einen 1 € Einsatz zu setzen? Diese Frage kann mit dem Erwartungswert beantwortet werden. Um den Erwartungswert zu berechnen benötigen wir allerdings zunächst die Wahrscheinlichkeiten für die verschiedenen Ausgänge.

Die Wahrscheinlichkeit keine $6$ zu werfen ist für jeden Würfel einzeln $\frac{5}{6}$, dementsprechend, da die Würfel unabhängig voneinander sind, kann diese Wahrscheinlichkeit dreimal miteinander multipliziert werden.

$$
P(0 \times 6) = \left(\frac{5}{6}\right)^3 = \frac{125}{216} \approx 0.579
$$

D.h. in knapp 60% der Fälle wird beim dem Spiel kein Gewinn ausgeschüttet. Berechnen wir zunächst den Fall, dass drei Sechsen geworfen werden. Dieser Fall kann parallel zudemjenigen für keine Sechs gelöst werden. Einziger Unterschied, ist die Wahrscheinlichkeit des Ereignisses. Die Wahrscheinlichkeit für einen Würfel eine Sechs zu würfeln ist $\frac{1}{6}$. Es folgt daher analog:

$$
P(3 \times 6) = \left(\frac{1}{6}\right)^3 = \frac{1}{216} \approx 0.005
$$

D.h. die Wahrscheinlichkeit für $3 \times 6$ ist gerade einmal ein halbes Prozent. D.h. in 200 Spielen, erwawrten wir dieses Ereignis nur ein einziges Mal.

Letzlich bleibt noch das Ereignis mindestens eine $6$. Hier nehmen wir das Komplementärereignis zu mindestens eine Sechs. Das Komplementärereignis ist nämlich keine Sechs zu würfeln. Diese Wahrscheinlichkeit ziehen wird dann von 1, dem sicheren Ereignis, ab. Da diese Menge auch die drei Sechsen beinhaltet, für das eine andere Gewinnberechnung gilt, müssen wir dessen Wahrscheinlichkeit noch subtrahieren.

$$
P(\text{min. eine } 6) = 1 - P(0 \times 6) - P(3 \times 6) = \frac{216}{216} - \frac{125}{216} - \frac{1}{216} = \frac{90}{216} = 0.41\bar{6}
$$

Die Wahrscheinlichkeit für mindestens eine Sechs ist dementsprechend etwas über 40%. Jetzt wenden wir wieder die Formel für den Erwartungswert an um die zu erwartende Gewinnsumme zu bestimmen. Die Gewinnsumme nimmt jetzt den Wert der Zufallsvariablen ein.

$$
E[X] = \frac{125}{216}\times 0 + \frac{90}{216}\times 2 + \frac{1}{216}\times27 = \frac{207}{216} \approx 0.958
$$

Im Mittel erwarten wir bei dem Spiel einen Gewinn von $0.958$€ bei einem Einsatz von $1$ €. Daher wird im Mittel ein Verlust bei dem Spiel gemacht. 
:::

::: {#exm-hypo-stats-expected-simple-03}
Als letztes Beispiel betrachten wir den Erwartungswert des Mittelwerts $\bar{X}$ von $n$ identisch verteilten Zufallsvariablen $X_i$. Dadurch das für alle $X_i, E[X_i] = \mu$ gilt ist dies letztendlich nur eine direkte Anwendung der obigen Rechenregeln.

\begin{equation}
E[\bar{X}] = E\left[\frac{1}{n}\sum_{i=1}^n X_i\right] = \frac{1}{n}E\left[\sum_{i=1}^n X_i\right] = \frac{1}{n}\sum_{i=1}^n E[X_i] = \frac{1}{n}\sum_{i=1}^n \mu = \frac{1}{n}n \mu = \mu
\label{eq-stats-hyp-exbar}
\end{equation}

D.h. der Erwartungswert des Mittelwerts ist tatsächlich auch der Erwartungswert $\mu$ der Verteilung.
:::

## Die Varianz $\sigma^2$ einer Zufallsvariablen

Die Varianz $\sigma^2$ beschreibt die Streuung der Zufallsvariablen $X$. 

::: {.def-varianz}
### Varianz $\sigma^2$

\begin{equation}
Var(X) = \sigma^2 = E[(X-E[X])^2] = E[(X - \mu)^2]
\label{eq-def-variance}
\end{equation}

D.h. die Varianz\index{Varianz} ist die quadrierte Abweichung von X vom Erwartungwert $\mu$.
:::

Wie wir schon behandelt haben, wird die Varianz als Skalenparameter bezeichnet. Wobei wir in den meisten Fällen mit der Standardabweichung $\sigma$ arbeiten, da diese die gleichen Einheiten wie die Variable $X$ hat.

Wenn wir den quadratischen Term ausmultiplizieren und dabei die Rechenregeln für den Erwartungswert berücksichtigen ($E[X] = \mu$ wird als eine Konstante betrachten), dann erhalten wir eine praktische, alternative Formulierung für die Varianz $\sigma^2$.

\begin{align*}
\sigma^2 &= E[(X-E[X])^2] \\
&= E[X^2 + E[X]^2 - 2XE[X]] \\
&= E[X^2] - E[E[X]^2] - E[2E[X]X] \\
&= E[X^2] - E[X]^2 - 2E[X]E[X] \\
&= E[X^2] - E[X]^2 - 2E[X]^2 \\
&= E[X^2] - E[X]^2
\end{align*}

Bzw.

\begin{equation}
Var(X) = \sigma^2 = E[X^2] - \mu^2
\label{eq-stats-hypo-var2}
\end{equation}

Die Gleichung \eqref{eq-stats-hypo-var2} wird später immer wieder praktisch sein, wenn wir die Eigenschaften von bestimmten Statistiken formal bestimmen wollen.

Für die Varianz gibt es auch paar einfache Rechenregeln ähnlich wie für den Erwartungswert. Für eine Konstante $a$ gilt

\begin{align*}
Var(a) &= 0 \\
Var(a + X) &= Var(X) \\
Var(a \cdot X) &= a^2 \cdot Var(X)
\end{align*}

Insgesamt gilt für die Linearkombination von $n$ unabhängigen Zufallsvariablen $X_i$:

\begin{equation}
Var\left(\sum_{i=1}^n a_i X_i\right) = \sum_{i=1}^n a_i^2 Var(X_i)
\label{eq-stats-hypo-sum-var}
\end{equation}

::: {#exm-ed-hypo-var-sum}
Für zwei **unabhängige** Zufallsvariablen $X$ und $Y$ gilt:

\begin{equation*}
Var(X + Y) = Var(X) + Var(Y)
\end{equation*}
:::
In der Rechenregel wurde der Begriff unabhängig verwendet der bisher noch nicht definiert wurde. Da die Unabhängigkeit ein zentrales Konzept in der Statistik ist, wird dies etwas weiter unten noch einmal spezifischer aufgegriffen. Zunächst reicht jedoch eine intuitives Verständnis von Unabhängigkeit, im Sinne von wenn Variablen keinen Einfluss aufeinander haben.

::: {#exm-ed-hypo-var-diff}
Eine überraschende Eigenschaft aus den Rechenregeln für die Varianz ist, dass die Varianz der Differenz zweier unabhängigen Variablen gleich der Varianz ist, wie wenn die beiden Variablen miteinander addiert werden.

\begin{equation}
\begin{split}
Var(X - Y) &= Var( 1\cdot X + (-1)\cdot Y) \\
&= Var(X) + Var((-1)\cdot Y) \\
&= Var(X) + (-1)^2 \cdot Var(Y) \\
&= Var(X) + Var(Y)
\end{split}
\label{eq-stats-hypo-vardiff}
\end{equation}

Das Ergebnis in Formel \eqref{eq-stats-hypo-vardiff} wird im späteren Verlauf immer wieder auftauchen, beispielsweise im Rahmen von experimentellen Untersuchungsdesigns.
:::

::: {#exm-ed-hypo-mean-se}
Mit den obigen Regeln können wir auch die Varianz des Mittelwerts $\bar{X}$ bestimmen. Die Variablen $X_i, i = 1,\ldots,n$ seien wieder unabhängig voneinander und stammen aus der gleichen Verteilung mit $Var(X_i) = \sigma^2$.

\begin{equation}
\begin{split}
Var(\bar{X}) &= Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) \\
&= \left(\frac{1}{n}\right)^2 Var\left(\sum_{i=1}^n X_i\right) \\
&= \frac{1}{n^2}\sum_{i=1}^n Var(X_i) \\
&= \frac{1}{n^2}\sum_{i=1}^n \sigma^2 \\
&= \frac{1}{n^2}n \sigma^2 = \frac{\sigma^2}{n}
\end{split}
\label{eq-stats-hypo-barxse}
\end{equation}

Die Gleichung \eqref{eq-stats-hypo-barxse} sollte uns bekannt vorkommen, ist diese doch der Standardfehler des Mittelwerts bzw. die Varianz dessen. Hier sehen wir den Wurzelzusammenhang den wir bereits beobachtet haben. Formal haben wir daher hergeleitet, dass die Streuung des Mittelwerts einer Stichprobe mit der Wurzel der Stichprobengröße $n$ abnimmt.
:::

Wie beim Erwartungswert auch noch mal kurz die kontinuierliche Version der Definition der Varianz.

\begin{equation}
Var(X) = \int_{-\infty}^{\infty}(X-\mu)^2 f(x) dx
\end{equation}

## Kovarianz $Cov(X,Y)$

Die Kovarianz ist ein weiteres zentrales Konzept in der Statistik. Wie die sprachliche Nähe zur Varianz ausdrückt, sind die beiden Konzepte miteinander verwandt. 

::: {#def-covarianz}

### Kovarianz 
Die Kovarianz $Cov(X,Y)$ ist statistisches Maß, das die lineare Beziehung zwischen zwei Zufallsvariablen beschreibt $X$ und $Y$. Die Kovarianz gibt an, wie stark und in welche Richtung sich die beiden Zufallsvariablen gemeinsam verändern.

In der Population wird für die Kovarianz das Symbol $\sigma_{xy}$ verwendet.
:::

Ist die Kovarianz positiv, bedeutet dies, dass hohe Werte der einen Variablen tendenziell mit hohen Werten der anderen Variablen einhergehen. Eine negative Kovarianz hingegen weist darauf hin, dass hohe Werte einer Variablen mit niedrigen Werten der anderen Variable zusammenhängen. Liegt die Kovarianz bei null, gibt es keine lineare Abhängigkeit zwischen den Variablen, was jedoch nicht ausschließt, dass andere (z. B. nicht-lineare) Beziehungen bestehen können.

Mathematisch wird die Kovarianz durch den Erwartungswert des Produkts der Abweichungen der Variablen von ihren jeweiligen Erwartungswerten berechnet. Seien zwei Zufallsvariablen $X$ und $Y$ gegeben, dann berechnet sich die Kovarianz $Cov(X,Y)$ nach: 

\begin{equation}
\text{Cov}(X, Y) = \sigma_{xy} = E[(X - \mu_X)(Y - \mu_Y)],
\label{eq-stats-hypo-cov-def}
\end{equation}

Ausmultiplizieren der Terme und Anwendung der Rechenregeln zum Erwartungswert führt zu folgender alternativen Darstellung mit der oft einfacher zu arbeiten ist.

\begin{equation*}
\sigma_{xy} = E[XY] - E[X]E[Y]
\end{equation*}

Die Kovarianz hat die Einheit des Produkts der Variablen, was sie oft schwer interpretierbar macht, wenn die Variablen unterschiedliche Skalen haben. In der allgemeinen Anwendung spielt die Kovarianz ähnlich wie die Varianz eher eine untergeordnete Rolle ist aber in der Herleitung von Gesetzmäßigkeiten zentral.

Die Kovarianz einer Variable $X$ mit sich selbst ist die Varianz.

\begin{equation*}
\text{Cov(X,X)} = E[(X-\mu_x)(X-\mu_x)] = E[(X-\mu_x)^2] = \text{Var}(X)
\end{equation*}

Die Kovarianz zwischen Variablen wird oft in Form einer Kovarianzmatrix dargestellt. Seien wieder die beiden Zufallsvariablen $X$ und $Y$ gegeben dann kann die folgende Kovarianzmatrix erstellt werden.

\begin{equation*}
\text{Cov(X,Y)} = \begin{pmatrix} \sigma_x^2 & \sigma_{xy} \\ \sigma_{xy} & \sigma_y^2 \end{pmatrix}
\end{equation*}

D.h. auf der Hauptdiagonalen sind die Varianz der beiden Variablen eingetragen während die anderen beiden Elemente jeweils die Kovarianz angeben. Da $Cov(X,Y) = Cov(X,Y)$ gilt, sind die beiden Elemente gleich und es handelt sich um eine symmetrische Matrize. Kovarianzmatrizen werden oft mit dem Symbol $\Sigma$ bezeichnet.

Wenn mehr als zwei Zufallsvariablen beteiligt sind, dann erweitert sich die Kovarianzmatrix entsprechend. Zum Beispiel bei drei Zufallsvariablen $X,Y$ und $Z$.

\begin{equation*}
\Sigma = \begin{pmatrix}
\sigma_x^2 & \sigma_{xy} & \sigma_{xz} \\
\sigma_{xy} & \sigma_{y}^2 & \sigma_{yz} \\
\sigma_{xz} & \sigma_{yz} & \sigma_{z}^2 
\end{pmatrix}
\end{equation*}

### Kovarianz in `R`

In `R` kann die Kovarianz von Variablen mittels der Funktion `cov()` berechnet werden.

```{r}
#| echo: true

x <- rnorm(10) 
y <- rnorm(10) 
cov(x,y)
```

Um eine Kovarianzmatrix zu erhalten müssen die Daten in Form einer Matrix an die Funktion übergeben werden.

```{r}
#| echo: true

cov(cbind(x,y))
```

Sollen dagegen multivariate Zufallsvariablen mit einer gegebene Kovarianzstruktur erzeugt werden, dann kann für normalverteilte Variablen die Funktion `mvrnorm()` aus dem package `MASS` verwendet werden.

```{r}
#| echo: true

mu_s <- 1:2
sigma <- matrix(c(0.4,0.2,0.2,0.5), nr=2)
MASS::mvrnorm(n = 3, mu = mu_s, Sigma = sigma)
```

Hier wurden drei Zufallsstichproben mit jeweils zwei Elementen erzeugt, basierend auf einer multivariaten Normalverteilung mit $\mu = [1,2]$ und der Kovarianzmatrix.

\begin{equation*}
\Sigma = \begin{pmatrix}
`r sigma[1,1]` & `r sigma[1,2]` \\
`r sigma[2,1]` & `r sigma[2,2]` \\
\end{pmatrix}
\end{equation*}

## Statistische Unabhängigkeit 

Unabhängigkeit\index{Unabhängigkeit} ist ein fundamentales Konzept in der Statistik und beschreibt eine vollständige Abwesenheit eines Zusammenhangs zwischen zwei Zufallsvariablen. Zwei Variablen $X$ und $Y$ sind unabhängig, wenn das Eintreten eines bestimmten Wertes oder Ereignisses der einen Variablen keinen Einfluss auf die Wahrscheinlichkeitsverteilung der anderen Variablen hat.

::: {#def-stats-hypo-indenpdence}

### Statistische Unabhängigkeit

Statistische Unabhängigkeit beschreibt eine Beziehung zwischen zwei Ereignissen oder Zufallsvariablen. Zwei Ereignisse bzw. Zufallsvariablen sind unabhängig voneinander wenn das Eintreten des einen Ereignisses oder der Wert der einen Variablen keinen Einfluss auf das Eintreten oder den Wert des anderen Ereignisses bzw. der anderen Zufallsvariable hat.
:::

Formal ausgedrückt bedeutet dies, dass ihre gemeinsame Verteilung durch das Produkt ihrer einzelnen (marginalen) Verteilungen dargestellt werden kann:

\begin{equation}
P(X  \cap Y ) = P(X) \cdot P(Y)
\label{eq-stats-hypo-independence}
\end{equation}

::: {#exm-stats-hypo-ind-01}
Wenn ein Experiment mit zwei Würfel durchgeführt wird, dann hat der Ausgang des einen Würfelwurfs keinen Einfluss auf das Ergebnis des anderen Würfelwurfs. Seien die Würfel farblich zu unterscheiden, z.B. blau ($X$) und grün ($Y$), dann ist die Wahrscheinlichkeit mit dem blauen Würfel eine Sechs zu würfeln, vollkommen unbeeinflusst vom Ausgang des anderen Würfels. Anders herum die Wahrscheinlichkeit mit dem grünen Würfel eine eins zu Würfel ist unbeeinflusst vom Ausgang des anderen Würfels. Formal:

\begin{equation*}
P(X = 6 \cap Y = 1) = P(X=6) \cdot P(Y=1) = \frac{1}{6} \cdot \frac{1}{6} = \frac{1}{36}
\end{equation*}
:::

Unabhängigkeit ist eine stärkere Bedingung als die Abwesenheit linearer Beziehungen, wie sie durch eine Kovarianz von null ausgedrückt wird. Tatsächlich impliziert Unabhängigkeit, dass die Kovarianz zwischen $X$ und $Y$ gleich null ist. Das Umgekehrte gilt jedoch nicht: Eine Kovarianz von null bedeutet lediglich, dass keine lineare Beziehung besteht; es könnten dennoch komplexere, nicht-lineare Abhängigkeiten vorliegen.

In der Praxis spielt Unabhängigkeit eine zentrale Rolle, da viele statistische Methoden und Modelle auf der Annahme beruhen, dass Zufallsvariablen unabhängig voneinander sind.

Die Unabhängigkeit zwischen Variablen wirkt sich auf die Form der Kovarianzmatrix $\Sigma$ aus. Seien drei Zufallsvariablen $X,Y$ und $Z$ gegeben, die voneinander unabhängig sind. Die Kovarianzmatrix $\Sigma$ nimmt dann die folgende, besondere Form an.

\begin{equation*}
\Sigma = \begin{pmatrix}
\sigma_x^2 & 0 & 0 \\
0 & \sigma_{y}^2 & 0  \\
0 & 0  & \sigma_{z}^2 
\end{pmatrix}
\end{equation*}

Alle Elemente abseits der Hauptdiagonalen, also alle Kovarianzen sind null.

::: {#exm-hypo-stats-id-02}
Sei eine normalverteilte Stichprobe von Körpergrößen mit $N = 3$ gegeben. Die Stichprobe ist aus einer Verteilung mit $\mu = 178$ und $\sigma = 6$ gezogen worden (Populationswerte). Dann wir die Stichprobe $X$ folgendermaßen Dargestellt.

\begin{equation*}
X = \begin{pmatrix} X_1 \\ X_2 \\ X_3 \end{pmatrix} \sim \mathcal{N}(\mu = 178, \Sigma), \quad \Sigma =  \begin{pmatrix} \sigma^2 & 0 & 0 \\ 0 & \sigma^2 & 0 \\ 0 & 0 & \sigma^2 \end{pmatrix}
\end{equation*}

Die Variablen $X_i, i=1,2,3$ sind somit unabhängig und gleichverteilt (im Englischen idependent identical distributed (IID)).
:::

Mittels der Kovarianz können nun auch Varianzen von Variablen berechnet werden wenn, die beteiligten Variablen nicht mehr unabhängig voneinander sind. Es gilt für die Zufallsvariablen $X$ und $Y$:

\begin{align*}
\text{Var}(aX + bY) = a^2 \text{Var}(X) + b^2 \text{Var}(Y) + 2ab \text{Cov}(X,Y)
\end{align*}

Im Fall der Unabhängigkeit von $X$ und $Y$ vereinfacht sich die Rechenregel auf den Fall von oben.

Die Unabhängigkeit ist im weiteren Verlauf zentral, da sie eine der Voraussetzungen für die Inferenz im Rahmen der einfachen und multiplen Regressionsanalyse ist. Im späteren Verlauf wird diese Annahme dann jedoch wieder gelockert. 

## Schätzer

Schätzer sind bereits in verschiedenen Varianten verwendet werden. Als ein Schätzer wird eine Funktion bezeichnet mit deren Hilfe anhand einer Stichprobe ein Wert, ein **Schätzwert**, berechnet wird. Der Schätzwert ermöglicht Rückschlüsse über unbekannte Parameter $\phi$ in der Population zu erhalten. D.h es sei zum Beispiel eine Population mit den Parametern $\mu$ und $\sigma^2$ gegeben und es soll anhand der beobachteten Werte der Stichprobe eine Aussage über die Werte der beiden Parameter gemacht werden. Dazu kann Schätzer verwendet werden.

::: {.def-schaetzer}
### Schätzer

Ein Schätzer\index{Schäzter} oder eine Schätzfunktion ist eine Statistik auf einer Stichprobe die dazu verwendet wird Information über unbekannte Parameter einer Population zu erhalten. Ein Schätzer $t$ für den Parameter $\theta$ (belieber Parameter) aus der Population wird oft mit einem **Hut** gekennzeichnet $t = \hat{\theta}$. Da $t$ mittels der Stichprobe $X_i, i = 1, \ldots, n$ ermittelt wird, kann manchmal auch die Schreibweise $t = h(X_1, \ldots, X_n)$ verwendet werden. $h()$ ist dabei eine Funktion (z.B. Mittelwert) auf der Stichprobe.
:::

Beispielsweise ist der Mittelwert einer Stichprobe $\bar{x}$ ein Schätzer für den Mittelwert $\mu$ in der Population. Die Standardabweichung $s$ der Stichprobe ist ein Schätzer für die Standardabweichung $\sigma$ in der Population $s = \hat{\sigma}$.

Es gibt nun immer eine Reihe von möglichen Schätzern auf einer gegebenen Stichprobe. Zum Beispiel, soll eine Aussage über einen Populationsmittelwert $\mu$ getroffen werden. So könnte nun der Mittelwert der drei kleinsten Werte $\bar{x}_{123}$ anstatt des Mittelwerts über alle Stichprobenwerte verwendet werden $\bar{x}$. Oder es könnte der Mittelwert über jeden zweiten Wert $\bar{x}_{1,3,...n}$ berechnet werden. D.h. in diesem Fall wäre drei verschiedene Schätzer für den Populationsmittelwert $\mu$ bestimmt worden.

\begin{align*}
\bar{x}_{123} &\rightarrow \hat{\mu}? \\
\bar{x} &\rightarrow \hat{\mu}? \\
\bar{x}_{1,3,...n} &\rightarrow \hat{\mu}?
\end{align*}

Allerdings tritt nun der Fall ein, dass nicht alle Schätzer sind gleich gut. Das Konzept des Schätzers erlaubt nun Unterschiede zwischen verschiedenen Schätzer zu formalisieren. Eine wichtige Eigenschaft von Schätzern ist die sogenannte **Erwartungstreue** \index{Erwartungstreue}. Die Erwartungstreue ist eng mit dem Erwartungswerts einer Statistik verknüpft. Wir hatten oben gesehen, dass für den Erwartungwert des Mittelwerts $E[\bar{x}] = \mu$ gilt. Dies zeigt, dass der Mittelwert der Stichprobe ein erwartungstreuer Schätzer für den Populationsparameter $\mu$ ist. Im Englischen wird die Erwartungstreue als *unbiased* bezeichnet. Damit folgt formal.

::: {.def-bias}
### Bias

Der Bias\index{Bias} ist definiert als die Abweichung zwischen dem Erwartungswert $\hat{\theta}_{t}$ eines Schätzers $t$ vom wahren Wert $\theta$ in der Population.

\begin{equation}
\text{bias}(\hat{\theta}_t) = E[\hat{\theta}_t - \theta]
\label{eq-stats-hypo-bias}
\end{equation}

Der Bias wird auch als systematischer Fehler bezeichnet.
:::

Der Einfluss der Wahl eines Schätzers kann anhand des Beispiels mit den drei Schätzern mittels einer Simulation in `R` verdeutlicht werden. Sei die Population eine Standardnormalverteilung aus der Stichproben der Größe $N = 10$ gezogen werden. Der *wahre* Populationsparameter ist somit $\mu = 0$.

```{r}
#| echo: true
#| layout-nrow: 3
#| fig-cap: "Erwartungstreu der drei Schätzer"
#| fig-subcap:
#|   - "Schätzer $\\bar{x}_{123}$"
#|   - "Schätzer $\\bar{x}$"
#|   - "Schätzer $\\bar{x}_{1,3,...n}$"
#| label: fig-stats-hypo-expec-3

n_sim <- 1000
n <- 10
mu <- 0
schaetzer <- function() {
  stich <- rnorm(n, mu)
  x_123 <- mean(sort(stich)[1:3])
  x_bar <- mean(stich)
  x_13n <- mean(stich[seq(1,10,2)])
  c(x_123, x_bar, x_13n)
}
x_bar_s <- replicate(n_sim, schaetzer())

hist(x_bar_s[1,], xlim=c(-2, 2), breaks=13)
hist(x_bar_s[2,], xlim=c(-2, 2), breaks=13)
hist(x_bar_s[3,], xlim=c(-2, 2), breaks=13)
```

In @fig-stats-hypo-expec-3 ist zu erkennen, dass der Schätzer $\bar{x}_{123}$ im Gegensatz zu den anderen beidern Schätzern **nicht** erwartungstreu ist. D.h. $\bar{x}_{123}$ ist ein biased Schätzer. Der Mittelwert der Werte ist nicht auf den tatsächlichen Populationsmittelwert $\mu=0$ zentriert. Die beiden anderen Schätzer $\bar{x}$ und $\bar{x}_{1,3,...n}$  sind dagegen auf null zentriert. Daher sind diese beiden Schätzer erwartungstreu. Anwendung von Formel \eqref{eq-stats-hypo-bias} führt zu:

```{r}
#| echo: true

apply(x_bar_s, 1, mean) - mu
```

Wird die Streuung der Histogramme betrachtet, dann ist weiterhin zu erkennen, dass der Schätzer $\bar{x}$ eine geringere Streuung als $\bar{x}_{1,3,...n}$ aufweist. Die Standardabweichung von $\bar{x}$ und somit der Standardfehler ist geringer als diejenige von $\bar{x}_{1,3,...n}$. Generell versucht man Schätzer zu finden die möglichst wenig streuen bzw. optimal im Sinne ihrer Varianz sind, d.h. Schätzer deren Standardfehler möglichst klein sind damit der Schätzer möglichst wenig um den gesuchten tatsächlichen Populationswert streut.

Zum gleichen Ergebnis kommt eine formale Herleitung der Varianzen der beiden Schätzer $\bar{X}$ und $\bar{X}_{1,3,...n}$. Es wurde vorher bereits gezeigt, dass der Standardfehler des Mittelwerts den bekannten Wert $\frac{\sigma}{\sqrt{n}}$ hat. Da der Schätzer $\bar{X}_{1,3,...n}$ nur die halbe Stichprobe verwendet resultiert daraus anhand der gleichen Herleitung der Standardfehler $\frac{\sigma}{\sqrt{n/2}} = \frac{\sigma\sqrt{2}}{\sqrt{n}}$. D.h. der Standardfehler des Schätzers $\bar{x}_{1,3,...n}$ ist um den Faktor $\sqrt{2}$ größer als derjenige von $\bar{x}$. 

```{r}
#| echo: true

s_es <- apply(x_bar_s, 1, sd)
s_es
s_es[2]*sqrt(2)
```

Wieder ist zu erkennen, dass im Rahmen der Stichprobenvariabilität der korrekte Wert erhalten wird.

::: {#exm-hypo-stats-expec-sigma}
### Erwartungstreue von $s$

Eine wiederkehrende Frage in der Statistikvorlesung ist die Frage nach dem Dividenten $n-1$ bei der Berechnung der Standardabweichung in der Stichprobe. Mit $\sigma^2 = E[X^2] - E[X]^2 = E[X^2] - \mu^2 \Rightarrow E[X^2] = \sigma^2 + \mu^2$ und den Rechenregeln für den Erwartungswert können wir zeigen warum die Standardabweichung $s$ bzw. die Varianz $s^2$ einer Stichprobe sich als Schätzer für $\sigma$ bzw. $\sigma^2$ eignen, also $s = \hat{\sigma}$ bzw. $s^2 = \hat{\sigma}^2$.

\begin{equation}
s^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}
\end{equation}

Es folgt nämlich

\begin{align*}
E\left[\sum_{i=1}^{n} (x_i-\bar x)^2\right] &= E[\sum_{i=1}^{n} x_i^2-2\bar x\sum_{i=1}^{n} x_i+n\bar{x}^2]\\
&=E[\sum_{i=1}^{n} x_i^2-2\bar{x}n\bar{x}+n\bar{x}^2]\\ 
&= E[\sum_{i=1}^{n} x_i^2-n\bar{x}^2] \\
&= n E[x_i^2]- n E[\bar{x}^2]\\
&= n (\mu^2 + \sigma^2) - n(\mu^2+\sigma^2/n)\\
&= (n-1) \sigma^2
\end{align*}

Daraus folgt:
\begin{equation*}
E[s^2] = E\left[\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}\right] = \sigma^2
\end{equation*}

Was dann auch erklärt warum die Summe der quadrierten Abweichung durch $n-1$ und nicht durch $n$ geteilt wird.

Würden nämlich die quadrierten Abweichungen durch $n$ geteilt werden, dann hätten wir keinen erwartungstreuen Schätzer mehr.

\begin{equation*}
E\left[\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n}\right] = \frac{1}{n}E\left[\sum_{i=1}^n (x_i - \bar{x})^2\right] = \frac{n-1}{n}\sigma^2
\end{equation*}

Würden wir durch $n$ teilen, dann würden wir die Varianz unterschätzen, da $\frac{n-1}{n}<1$ gilt. Der Schätzer weist in diesem Fall einen bias (systematischen Fehler) auf.
:::

Zusammenfassend, werden möglichst erwartungstreue Schätzer benötigt um die Verknüpfung zwischen einer Stichprobe und den Parametern einer theoretischen Stichprobenverteilung zu bilden.

## Hypothesentestungszoo

Es folgt noch eine kurze Übersicht über verschiedenen Teststatistiken die im weiteren Verlauf des Skripts immer wieder auftauchen werden.

### Der t-Test

Wir hatten bereits gesehen, das das Verhältnis einer standardnormalverteilten Variable zu einer $\chi^2$-verteilten Variable folgt einer $t$-Verteilung. In vielen Fällen wird diese Eigenschaft verwendet, wenn ein Unterschied $\hat{\Delta}$ beispielsweise zwischen zwei Mittelwerten $\bar{x} - \bar{y}$ durch eine Streuung $\hat{s}_e(\hat{\delta})$ geteilt wird. 

\begin{equation*}
T = \frac{\hat{\Delta}}{\hat{s}_e(\hat{\delta})} \sim t\text{-Verteilung}
\end{equation*}

Zum Beispiel bei dem t-Test der uns in unserer ersten Statistikvorlesung begegnet ist, wurde der Unterschied zwischen zwei Gruppenmittelwerten durch den Standardfehler der Differenz geteilt. 

### $\chi^2$-Test der Varianz 

Die $\chi^2$-Verteilung taucht öfters im Zusammenhang mit Schätzern von Varianzen auf. Beispielsweise lässt sich zeigen, das bei $n$ unabhängige, standardnormalverteilte Variabnel $X_i, i = 1, \ldots, n$ die Summer der quadrierten Abweichungen vom Mittelwert, also $\sum_{i=1}^n(Y_i - \bar{Y})^2$ einer $\chi^2$-Verteilung mit $n-1$ Freiheitsgraden folgt.

Sei $\hat{\sigma}^2$ ein Schätzer für die Varianz aus einer Stichprobe und es soll überprüft werden, ob diese Varianz einer vorgegebenen Varianz $\sigma_0$ folgt, also $H_0: \sigma^2 = \sigma_0^2$ die Nullhypothese ist. Dann lässt sich eine Teststatistik $T$ über die folgende Formel konstruieren:

\begin{equation*}
T = d \frac{\hat{\sigma}^2}{\sigma_0^2} \sim \chi^2 \quad d = \text{ Freiheitsgrade}
\end{equation*}

$H_0$ wird dann abgelehnt, wenn $T > q_{\chi^2,\text{df}=d,1-\alpha}$ (einseitig) gilt.

### F-Test von Varianzverhältnissen

Sei eine Stichproben aus normalverteilten, unabhängigen Variablen gegeben und die Varianz $\sigma_A^2$ und $\sigma_B^2$ mittels zweier Schätzer $\hat{\sigma}_A^2$ und $\hat{\sigma}_B^2$ bestimmt worden. Dann kann eine Teststatisk $T$ konstruiert werden um die die Gleichheit der beiden Varianzen zu überprüfen. Formal $H_0: \sigma_A^2 = \sigma_B^2 \Leftrightarrow \frac{\sigma_A}{\sigma_B} = 1$.

$$
T = \frac{\hat{\sigma}^2_A}{\hat{\sigma}^2_B} \sim F(df_A, df_B)
$$

Die beiden Varianzen folgen dabei jeweils einer $\chi^2$ Verteilung mit Freiheistgraden $df_A$ und $df_B$. Daher folgt  die Statistik $T$ unter der $H_0$ einer $F$-Verteilung mit $(df_A, df_B)$ Freiheitsgeraden. Die $H_0$ wird daher wiederum abgelehnt wenn $T > q_{F,\text{df}_A,\text{df}_B, 1-\alpha}$ (einseitig) gilt.

Zusammenfassend, wie schon bei der Auflistung der theoretischen Verteilung, ist es zunächst einmal wichtig zur Kenntnis zu nehmen, dass es diese unterschiedlichen Tests gibt. Deren Herleitung ist natürlich mit den gegebenen Informationen nicht nachvollziehbar und maximal undurchsichtig. Die Liste ist daher mehr im Sinne einer Mustererkennungsaufgabe für später zu interpretieren. D.h. wenn die Tests im weitern Verlauf des Skripts in konkreten Zusammenhängen auftauchen sollte ein Wiedererkennungswert ersteht. Letztendlich ist das anzuwendende Prinzip immer das Gleiche, Teststatistik $\rightarrow$ theoretische Verteilung $\rightarrow$ kritischer Wert unter der $H_0$ über die Quantile $\alpha$ $\rightarrow$ Testentscheidung und Unsicherheit bestimmen. Was sich ändert sind die Tests und deren Parameter.
