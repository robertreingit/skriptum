# Statistische Signifikanz 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

```{r stats_significance_defs}
n <- 20
set.seed(123)
world <- tibble(ID = paste0('P',stringr::str_pad(1:n, width=2, pad="0")),
                Kraft = sample(2000:2500, n))
world$Kraft[13] <- 1800
world$Kraft[17] <- 3200
d_gr <- 100
d_x <- 2 
mu_lummer <- 0
sd_lummer <- 230
```

Im vorherigen Kapitel wurde gezeigt, wie Unsicherheit ein zentrales Problem bei der Interpretation von experimentellen Ergebnissen oder Daten im Allgemeinen ist. Im folgenden Abschnitt wird nun ein Prozess hergeleitet, der es ermöglicht, vor dem Hintergrund dieser Unsicherheit rationale Entscheidung zu treffen.

## Wie treffe ich eine Entscheidung 

In dem kleinen Weltbeispiel bestand die komfortable Position, dass bekannt war , was passiert bzw. welcher Prozess die beobachteten Datenpunkt erzeugt hat. Das heißt, der **datengenerierenden Prozess** war bekannt.

::: {#def-dgp}
## Datengenerierender Prozess (DGP)

Der Prozess in der realen Welt, der die beobachteten Daten und damit die daraus folgende Statistik erzeugt, wird als datengenerierender Prozess\index{Datengenerierender Prozess} bezeichnet.
:::

Letztendlich zielt jedwede Untersuchung darauf ab, Informationen über den DGP zu erhalten. Diese Information erlaubt es, Aussagen über die reale Welt zu treffen und eben Prozesse zu verstehen bzw. Voraussagen zu treffen. Dabei muss allerdings beachtet werden, dass dieser Prozess in den allermeisten Fällen eine starke Vereinfachung des tatsächlichen Prozesses in der Realität darstellt. Meistens sind die Abläufe in der Realität zu komplex, um sie in ihrer Gänze abzubilden. Somit wird fast immer nur ein vereinfachtes Modell verwendet.

Wenn nun ein Experiment durchgeführt wird, wird normalerweise nur eine einzige Statistik beobachtet. Im bisherigen Beispiel war dies der berechnete Unterschied $D$ in der Kraftfähigkeit nach der Intervention zwischen der Kontroll- und der Interventionsgruppe.

```{r}
#| fig-cap: "Beobachteter Unterschied nach der Durchführung unseres Experiments"
#| label: fig-sts-sig-result-1 
#| fig-height: 1.5

D_obs <- tibble(Kraft = 50)
ggplot(D_obs,
       aes(Kraft, 1)) +
  geom_point(size=3, color = 'red') +
  scale_x_continuous('D[N]', limits = c(-500, 700), breaks = seq(-500, 700, 200)) +
  scale_y_continuous('', breaks = NULL)
```

In @fig-sts-sig-result-1 ist der beobachtete Wert $d = `r D_obs$Kraft[1]`$ abgetragen. Es ist von vorneherein bekannt, dass dieser Wert beeinflusst ist durch die zufällige Wahl der Stichprobe und die daran geknüpfte Streuung der Werte in der Population. Wie kann nun überhaupt eine Aussage darüber getroffen werden, ob das Krafttraining effektiv ist? Ob es nur einen sehr kleinen Effekt zeigt? Oder ob es möglicherweise sogar schädlich ist und zu einer Abnahme der Kraft führt?

Um sich der Lösung zu nähern, ist zunächst eine Überlegung hilfreich, welche Prozesse den beobachteten Wert generiert haben könnten. Im vorherigen Kapitel wurden bereits zwei Prozesse vorgestellt. Einmal der Prozess mit $\Delta = 100$ sowie den Prozess mit $\Delta = 0$.

```{r}
#| fig-cap: "Mögliche datengenerierende Prozesse für den beobachteten Unterschied $D$ (rot)"
#| label: fig-sts-sig-dgp-1
#| fig-height: 2

x <- seq(-1000, 1000, length.out = 100)
tibble(x = x,
       d_0 = dnorm(x, 0, sd_lummer),
       d_100 = dnorm(x, 100, sd_lummer)) |> 
  pivot_longer(-x, names_to = 'Verteilung', values_to = 'd') |> 
  ggplot(aes(x, d, ymin=0, ymax=d, group=Verteilung)) +
  geom_ribbon(aes(fill = Verteilung), alpha=.5) +
  geom_line() +
  annotate('point', x = D_obs$Kraft, y = 0, color = 'red', size = 3) +
  labs(x = 'D[N]', y = 'Dichte')

```

In @fig-sts-sig-dgp-1 ist wieder der beobachtete Wert $d = `r D_obs[["Kraft"]][1]`$ und die beiden Verteilungen abgetragen. Leider kann nicht eindeutig gesagt werden, welche der beiden Verteilungen bzw. deren zugrundeliegende Prozesse den beobachteten Wert erzeugt haben könnte. Der beobachteter Wert $d$ liegt genau zwischen den beiden Maxima der Verteilungen. Etwas motiviertes Starren auf die Abbildung könnten einen allerdings auf die Idee bringen, dass der beobachtete Wert nicht nur von diesen beiden Verteilungen erzeugt worden sein muss, sondern durchaus noch mehr Verteilungen in Frage kommen können.

```{r}
#| fig-cap: "Beispiele für weitere mögliche Verteilungen als DGP."
#| label: fig-sts-sig-dgp-2
#| fig-height: 2

p_which_dist <- tibble(x = x,
       d_0 = dnorm(x, 0, sd_lummer),
       d_100 = dnorm(x, 100, sd_lummer),
       d_350 = dnorm(x, 350, sd_lummer),
       d_n250 = dnorm(x, -250, sd_lummer)) |> 
  pivot_longer(-x, names_to = 'Verteilung', values_to = 'd') |> 
  ggplot(aes(x, d, ymin=0, ymax=d, group=Verteilung)) +
  geom_ribbon(aes(fill = Verteilung), alpha=.5) +
  geom_line() +
  annotate('point', x = D_obs$Kraft, y = 0, color = 'red', size = 3) +
  labs(x = 'D[N]', y = 'Dichte')
p_which_dist
```

@fig-sts-sig-dgp-2 zeigt, dass selbst die Verteilungen mit $\Delta = -250N$ und $\Delta = 350N$ nicht unplausibel sind, den beobachteten Wert erzeugt zu haben. Warum aber bei diesen fünf Verteilungen aufhören? Warum sollte $\Delta$ nicht $-50$ oder $127$ sein? Und überhaupt, niemand kann behaupten, die Natur kenne nur ganzzahlige Werte (siehe $\pi$). Warum sollte $D$ also nicht auch $123.4567N$ sein?

Wenn diese Überlegung weitergeführt wird, dann wird schnell klar, dass letztendlich eine unendliche Anzahl von Verteilungen in der Lage ist, unseren beobachteten Wert plausibel zu generieren. Das bedeutet, wir haben ein Experiment durchgeführt, viel Aufwand betrieben und wochenlang mit unseren ProbandInnen Krafttraining durchgeführt, und sind hinterher eigentlich keinen Schritt weiter, da wir immer noch nicht wissen, was der datengenerierende Prozess ist. Also können wir selbst nach dem Experiment nicht sagen, ob unser Krafttraining tatsächlich wirksam ist.

Zum Glück werden wir später sehen, dass unser Unterfangen nicht ganz so aussichtslos ist. Schauen wir uns zum Beispiel die Verteilung für $\Delta = -350N$ an (@fig-sts-sig-dgp-3).

```{r}
#| fig.cap: "Verteilung für $\\Delta = -350N$ und der beobachtete Wert $D$"
#| label: fig-sts-sig-dgp-3
#| fig.height: 2

tibble(x = x,
       d = dnorm(x, -350, sd_lummer)) |>  
  ggplot(aes(x, d, ymin=0, ymax=d)) +
  geom_ribbon(alpha=.5) +
  geom_line() +
  annotate('point', x = D_obs$Kraft, y = 0, color = 'red', size = 3) +
  labs(x = 'D[N]', y = 'Dichte')
```

Unser beobachteter Wert unter der Annahme, dass $\Delta = -350N$ ist, ist nicht vollkommen unmöglich, aber so richtig *wahrscheinlich* erscheint er auch nicht. Der Wert liegt relativ weit am Rand der Verteilung. Die Kurve ist dort schon ziemlich nahe bei Null. Das bedeutet, der beobachtete Wert ist zwar durchaus möglich, aber es wäre schon überraschend, wenn wir bei einer Durchführung des Experiments ausgerechnet so einen Wert beobachten würden, wenn unser angenommenes $\Delta$ korrekt ist.

Wenn wir jetzt dagegen von der Annahme ausgehen, dass dem DGP der Wert $\Delta = 50N$ zugrunde liegen würde, hätten wir die Verteilung in @fig-sts-sig-dgp-4. Zunächst ist dieser Wert unter der Annahme möglich. Zusätzlich liegt der beobachtete Wert mitten in dem Teil der Verteilung, der auch zu erwarten wäre. Das bedeutet, der beobachtete Wert ist durchaus plausibel unter der Annahme, und bei der einmaligen Durchführung des Experiments würde uns der beobachtete Wert nicht unbedingt überraschen.

```{r}
#| fig.cap: "Verteilung für $\\Delta = 50N$ und der beobachtete Wert $D$"
#| label: fig-sts-sig-dgp-4
#| fig.height: 2

tibble(x = x,
       d = dnorm(x, 50, sd_lummer)) |>  
  ggplot(aes(x, d, ymin=0, ymax=d)) +
  geom_ribbon(alpha=.5) +
  geom_line() +
  annotate('point', x = D_obs$Kraft, y = 0, color = 'red', size = 3) +
  labs(x = 'D[N]', y = 'Dichte')
```

Diesen Ansatz können wir verwenden, um mithilfe unseres Experiments doch etwas über den DGP auszusagen. Allerdings müssen wir uns noch einmal eingehender mit Verteilungen auseinandersetzen, um z. B. genauer zu bestimmen, welche Ergebnisse uns überraschen würden. Das bedeutet, wir müssen uns erst einmal ein paar neue Konzepte erarbeiten.

## Lage- und Skalenparameter

In @fig-sts-sig-dgp-2 hatten wir mehrere Verteilungen abgebildet. Die Verteilungen haben die gleiche Form, sind aber gegeneinander verschoben. Das bedeutet, sie unterscheiden sich bezüglich ihrer Position bzw. Lage. Der Parameter, der bei einer Verteilung die Lage steuert, ist der sogenannte Erwartungswert $\mu$, der auch als Mittelwert bezeichnet wird. Dieser Mittelwert $\mu$ unterscheidet sich allerdings von dem uns bereits bekannten Mittelwert $\bar{x}$ in der Stichprobe. In einem späteren Abschnitt werden wir uns genauer anschauen, wie der Mittelwert $\mu$ berechnet wird.

### Mittelwert $\mu$ der Population

Da der Mittelwert $\mu$ die Position der Verteilung bestimmt, ist $\mu$ ein Parameter der Verteilung. Die Beschreibung als Parameter der Verteilung bedeutet somit, dass die Verteilung von $\mu$ abhängt, oder formaler, dass die Verteilung eine Funktion von $\mu$ ist. Wenn wir uns an Funktionen aus der Schule erinnern, wo wir Funktionen $f$ von $x$ kennengelernt haben und als $f(x)$ dargestellt haben, könnte dies auf die Verteilung übertragen mittels $f(\mu)$ dargestellt werden.

```{r}
d_x <- 0.025
x <- seq(-3, 3, d_x)
dat_0 <- tibble(
  x = x,
  v = dnorm(x, 0, 1) 
)
low <- tibble(
  x = seq(-3,-2,d_x),
  v = dnorm(x, 0, 1)
)
up <- tibble(
  x = seq(2, 3, d_x),
  v = dnorm(x, 0, 1)
)
```

Betrachten wir zwei Verteilungen, die sich bezüglich ihrer Mittelwerte $\mu$ unterscheiden. Zum Beispiel sei $\mu_1 = 0$ und $\mu_2 = 3$. Wie in @fig-sts-sig-dist-mu zu sehen ist, führt dies dazu, dass die beiden Verteilungen gegeneinander verschoben sind.

```{r}
#| fig.cap: "Verteilungen mit zwei unterschiedlichen Mittelwerten"
#| label: fig-sts-sig-dist-mu

xx <- seq(-3,6,d_x)
n_pts <- length(xx)
dat_1 <- tibble(
  x = rep(xx,2),
  v = c(dnorm(xx, 0, 1),
        dnorm(xx, 3, 1)),
  pos = rep(c('x0','x3'), c(n_pts,n_pts))
)
ggplot(dat_1, aes(x,v,ymin=0, ymax=v, fill=pos)) +
  geom_ribbon(alpha=.5) +
  geom_line() +
  labs(x = 'Werte', y = 'Dichte') +
  scale_fill_discrete("Lageparameter",
                      labels = c(expression(mu == 0), expression(mu == 3))) 
```

Wie bereits erwähnt, wird der Mittelwert $\mu$ der Verteilung auch als Erwartungswert bezeichnet. Dies kann dahingehend interpretiert werden, dass, wenn Stichproben aus dieser Verteilung gezogen werden, im Mittel der Wert $\mu$ erwartet werden kann. Soweit ist dies eigentlich noch nichts wirklich Neues, sondern wir hatten dies schon vorher gesehen, als wir alle möglichen Unterschiede zwischen der Kontrollgruppe und der Interventionsgruppe ermittelt haben. Hier war der Mittelwert der Verteilung genau derjenige Wert von $\Delta$.

An dieser Stelle noch einmal der Unterschied zwischen $\mu$ und $\bar{x}$. Der Mittelwert $\mu$ ist eine Eigenschaft der Population, also letztendlich ein Wert, den wir niemals kennen werden, ohne die gesamte Population zu untersuchen. Der Mittelwert $\bar{x}$ ist eine Eigenschaft der Stichprobe aus der Population, also der konkrete Wert, den wir anhand der Stichprobe berechnen. In vielen Fällen versuchen wir, über $\bar{x}$ einen Rückschluss auf $\mu$ zu ziehen.

### Standardabweichung $\sigma$ der Population

Als zweite Eigenschaft von Verteilungen schauen wir uns jetzt die Streuung in der Population an. Die Streuung in der Population wird als Varianz bezeichnet und mit dem Symbol $\sigma^2$ dargestellt. Schauen wir uns zunächst an, welchen Einfluss $\sigma^2$ auf die Form der Verteilung hat. In @fig-sts-sig-dist-sigma sind wieder zwei Verteilungen abgetragen. Dieses Mal ist $\mu$ in beiden Fällen gleich, aber die Varianzen $\sigma^2$ sind mit $\sigma_1^2 = 2$ und $\sigma_2^2 = 1$ unterschiedlich.

```{r}
#| fig.cap: "Verteilungen mit unterschiedlichen Varianzen"
#| label: fig-sts-sig-dist-sigma

xx <- seq(-5,5,d_x)
n_pts <- length(xx)
dat_2 <- tibble(
  x = rep(xx,2),
  v = c(dnorm(xx, 0, 1),
        dnorm(xx, 0, sqrt(2))),
  type = rep(c('s1','s0.5'), c(n_pts,n_pts))
)
ggplot(dat_2, aes(x,v,ymin=0, ymax=v,fill=type)) +
  geom_ribbon(alpha=.5) +
  geom_line() +
  labs(x = 'Werte', y = 'Dichte') +
  scale_fill_discrete("Varianz",labels = c(
    expression(sigma^2 == 2),
    expression(sigma^2 == 1)
  )) 
```

In @fig-sts-sig-dist-sigma ist zu sehen, dass beide Verteilungen ihren Mittelpunkt an der gleichen Stelle haben, aber die rote Verteilung mit $\sigma_1^2=2$ breiter ist als die andere Verteilung. Dies bedeutet, dass die Werte in der Verteilung stärker um den Mittelwert streuen. Wenn wir Werte aus der türkisen Verteilung ziehen, dann sollten diese näher um den Mittelwert $\mu = 0$ liegen als bei der roten Verteilung.

Die Varianz $\sigma^2$ ist ebenfalls wie der Mittelwert ein Parameter der Verteilung. Sie bestimmt die Form der Verteilung. Das heißt, wenn wir wieder unsere Schreibweise von eben verwenden und die Funktion $f$, die die Verteilung beschreibt, betrachten, dann gilt $f(\sigma^2)$ oder eben zusammen mit dem Mittelwert $\mu$: $f(\mu, \sigma^2)$.

Wenn aus der Varianz $\sigma^2$ die Wurzel gezogen wird, dann wird der resultierende Wert $\sigma$ als Standardabweichung bezeichnet. Da die Varianz $\sigma^2$ nur positive Werte annehmen kann, ist die Wurzelfunktion bzw. deren Umkehrung, die Quadrierung, eineindeutig. Wenn wir die Standardabweichung kennen, kennen wir auch die Varianz und umgekehrt.

In der Stichprobe wird die Standardabweichung meistens mit dem Zeichen $s$ bezeichnet und mittels der folgenden Formel berechnet:

\begin{equation}
s = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}}
\label{eq-std}
\end{equation}
			  

Das bedeutet, die Standardabweichung ist die mittlere quadrierte Abweichung vom Mittelwert (siehe Formel \eqref{eq-std}). Die Standardabweichung wird verwendet, um die Streuung der Daten zu beschreiben. Der Vorteil der Standardabweichung besteht darin, dass sie dieselbe Einheit wie der Mittelwert hat. Da die Abweichungen quadriert werden und somit quadrierte Einheiten besitzen, hat die Standardabweichung $s$ dieselbe Einheit wie der Mittelwert $\bar{x}$. Da die Varianz die quadrierte Standardabweichung ist, hat die Varianz der Stichprobe $s^2$ daher quadrierte Einheiten.

```{r}
id <- c(3, 8, 9)
x_bar <- round(mean(world$Kraft[id]))
sd_str <- paste("(",world$Kraft[id],"-",x_bar,")^2",sep='',collapse='+')
x_sd <- round(sd(world$Kraft[id]))
```

Erinnern wir uns an unser erstes Beispiel aus der kleinen Welt, in dem wir in der Kontrollgruppe die Personen $i = \{`r paste(id, collapse=',')`\}$ gezogen hatten. Für diese Stichprobe berechnen wir die Standardabweichung und erhalten bei einem Mittelwert von $\bar{x} = `r x_bar`$:

\begin{equation*}
s = \sqrt{\frac{`r sd_str`}{2}} = `r x_sd`
\end{equation*}

Wir erhalten einen Wert von $s = `r x_sd`N. Wenn dieser Wert größer wird, dann streuen die Werte entsprechend weiter um den Mittelwert, und die Streuung verringert sich, wenn die Standardabweichung $s$ abnimmt.

### Mittelwert und Standardabweichung in `R`

Um den Mittelwert und die Standardabweichung bzw. die Varianz zu berechnen, gibt es in `R` entsprechende Funktionen mit den Namen `mean()`, `sd()` und `var()`.

```{r}
#| echo: true
x <- c(1,2,3,4,5)
mean(x)
sd(x)
var(x)
```


## Entscheidungen und $\mu$ und $\sigma$

Zeichnen wir in eine Verteilung die Standardabweichung ein, ergibt sich folgendes Bild (siehe @fig-stats-sig-norm-sigmas).

```{r}
#| fig.cap: "Verteilung mit verschiedenen Vielfachen der Standardabweichung $\\sigma$"
#| label: fig-stats-sig-norm-sigmas

p <- dnorm_plot()
p + 
  geom_vline(xintercept = (-3:3)[-4], linetype='dashed') +
  scale_x_continuous("Werte", breaks=-3:3,
                     labels = c(
                       expression(-3~sigma),expression(-2~sigma),expression(-1~sigma),
                       expression(mu),
                       expression(1~sigma),expression(2~sigma),expression(3~sigma)))
                     
```

Ein Großteil der Werte liegt im Bereich $\mu \pm 1\times\sigma$. Der Bereich $\mu \pm 2\times\sigma$ umfasst fast alle Werte, während der Bereich $\mu \pm 3\times\sigma$ nahezu alle Werte beinhaltet. Wenn wir die Verteilung noch weiter nach links und rechts abtragen würden, könnten wir sehen, dass auch Werte jenseits von $\mu \pm 3\times\sigma$ liegen, jedoch nur sehr wenige. Diese Erkenntnis können wir verwenden, um umgekehrt zu denken: Wenn wir annehmen, dass unsere Statistik dieser Verteilung folgt, welche Werte würden uns überraschen? Welche Werte würden wir als Evidenz sehen, um zu folgern: *Ich glaube nicht, dass die beobachtete Statistik aus der angenommenen Verteilung stammt*?

Ein Beispiel: Wenn der Wert mehr als $3\times\sigma$ vom Mittelwert $\mu$ entfernt ist, wäre es zwar nicht unmöglich, aber ziemlich unwahrscheinlich, einen solchen Wert zu beobachten. Ein weniger strenger Kompromiss wäre, einen Wert, der mehr als $2\times\sigma$ von $\mu$ entfernt liegt, bereits als überraschend zu betrachten. Tatsächlich liegt die Wahrscheinlichkeit, einen Wert jenseits von $2\times\sigma$ zu beobachten, bei etwa 5 %. Das heißt, wir könnten einen Entscheidungsprozess aufstellen, bei dem wir sagen: Wenn wir für unsere Statistik eine bestimmte Verteilung annehmen und bei unserer Stichprobe einen Wert beobachten, der weiter als $2\times\sigma$ von $\mu$ entfernt ist, dann sind wir überrascht und sehen dies als Evidenz gegen die Verteilungsannahme.

Zusammengefasst:

1) Setze eine Verteilung der Statistik mit definierten $\mu$ und $\sigma$ als Annahme an.
2) Ziehe eine Zufallsstichprobe.
3) Berechne die Statistik auf der Stichprobe.
4) Überprüfe, wie viele Standardabweichungen $\sigma$ die Statistik von $\mu$ entfernt liegt.


\begin{table}[]
    \caption{Parameter einer Verteilung und deren Sch\"atzer}
    \centering
    \begin{tabular}{llr}
     \toprule
     Population & Stichprobe & \\
     \midrule
     Mittelwert $\mu$ & $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$  \\
     Varianz $\sigma^2$ & $s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2$ \\
     Standardabweichung $\sigma$ & $s = \sqrt{s^2}$ \\
     \bottomrule
    \end{tabular}
\end{table}

### Detour - Schätzer

Schauen wir uns noch einmal den Mittelwert $\mu$ der Population und den Mittelwert $\bar{x}$ der Stichprobe und deren Zusammenhang an. Der Mittelwert $\bar{x}$ der Stichprobe wird als sogenannter Schätzer verwendet. Diesen Begriff werden wir später noch genauer untersuchen. Im Moment reicht es, sich zu merken, dass ein Schätzer eine Statistik ist, mit der wir einen Parameter der Population, z. B. $\mu$, abschätzen wollen. Wie schon mehrmals erwähnt, werden wir den wahren Wert $\mu$ aus der Population mittels unserer Stichprobe niemals zu 100% korrekt bestimmen können. Wir können aber mittels geschickt gewählter Statistiken Schätzer konstruieren, die bestimmte Eigenschaften haben.

Nehmen wir zum Beispiel den Mittelwert $\bar{x}$. In unserer kleinen Welt kennen wir den Mittelwert $\mu$ unserer Population. Der Wert beträgt $\mu = `r mean(world[["Kraft"]])`$. Schauen wir uns einmal an, was passiert, wenn wir alle möglichen Stichproben der Größe $N = 10$ unserer kleinen Welt bestimmen und die Verteilung der Mittelwerte abtragen (siehe @fig-stats-sig-mean-n3).

```{r}
#| fig.cap: "Verteilung der Mittelwerte von Stichproben der Größe $n=10$, Kleine Welt Population $\\mu$ (rot)"
#| label: fig-stats-sig-mean-n3

bar <- function(world, k=3) {
  n <- dim(world)[1]
  x <- 1:n
  N <- choose(n,k)
  x_bars <- numeric(N)
  # get all permutations
  id_1 <- t(combn(x,k))
  for (i in 1:nrow(id_1)) {
    x_bars[i] <- mean(world$Kraft[id_1[i,]])
  }
  x_bars
}
x_bars <- bar(world, 10)
mu <- mean(world$Kraft)
ggplot(tibble(x=x_bars),aes(x)) +
  geom_histogram(aes(y = after_stat(density)), bins = 30) +
  geom_vline(xintercept = mu,
             color = 'red', linetype = 'dashed') +
  labs(x = 'Mittelwerte[N]', y = 'Häufigkeit') 
```

In @fig-stats-sig-mean-n3 sehen wir, dass im Mittel der Stichprobenmittelwert $\bar{x}$ tatsächlich um den wahren Populationsmittelwert $\mu$ herum zentriert ist. Einzelne Ausgänge des *Experiments* können zwar daneben liegen, der Großteil der Experimente gruppiert sich jedoch um $\mu$ herum. Der Stichprobenmittelwert $\bar{x}$ ist daher eine *gute* Statistik, um den tatsächlichen Populationsmittelwert $\mu$ abzuschätzen.

## Welche Verteilung setzen wir an?

Kommen wir aber wieder zurück zu unserem Ausgangsproblem, dass wir anhand unserer beobachteten Stichprobe etwas über die Effektivität der Kraftintervention aussagen wollen. Wie hilft uns jetzt die Kenntnis von Mittelwert $\mu$ oder $\bar{x}$ und der Standardabweichung $\sigma$ bzw. $s$ weiter? Wenn die Verteilung unserer Statistik der Form folgt, wie wir sie bisher mehrmals beobachtet haben, dann können wir davon ausgehen, dass wir Werte eher in der Nähe des Mittelpunkts erwarten würden. Wir werden selten genau den Mittelpunkt beobachten, aber wir würden schon sehr überrascht sein, wenn wir Werte weit entfernt vom Mittelwert beobachten würden. Ab welcher Entfernung diese Werte als überraschend eingestuft werden, hängt dabei von der Streuung der Verteilung ab. Wenn $\sigma$ groß ist, überraschen uns weit entfernte Werte weniger als wenn $\sigma$ klein ist.

```{r}
#| fig.cap: "Welche Verteilung nehmen wir?"
#| label: fig-stats-sig-which-dist

p_which_dist
```

Spielen wir verschiedene Möglichkeiten einmal durch. Wir vernachlässigen zunächst einmal $\sigma$ und konzentrieren uns auf $\mu$. Wir benötigen eine einzelne Referenzverteilung, um unseren beobachteten Wert $\Delta$, den Unterschied zwischen den beiden Gruppen, mit der Verteilung in Beziehung zu setzen. Wir könnten zum Beispiel sagen, dass wir davon ausgehen, dass der Unterschied zwischen den beiden Gruppen $\Delta_{\text{wahr}} = 75N$ ist. Das heißt, dies wäre der *wahre* Unterschied zwischen den beiden Gruppen. Wir treffen ihn nicht genau, da wir eine Zufallsstichprobe gezogen haben und die Stichprobenvariabilität dazu führt, dass wir nicht genau den Unterschied treffen. Allerdings wird etwas Nachdenken über den Wert $75N$ zu der Einsicht führen, dass $75$ vollkommen willkürlich ist. Warum nicht $85N$ oder $25N$, oder warum überhaupt ganzzahlig? Schließlich ist $\pi$ auch keine ganze Zahl, also könnten wir genauso gut $74.1234N$ nehmen. Schnell wird daher klar, dass keine Zahl so richtig gut begründet werden kann. Wir brauchen aber eine Zahl, um unseren Apparat mit Verteilungen ansetzen zu können. Tatsächlich gibt es eine Zahl, die zwar auch willkürlich ist, aber doch etwas besser begründet werden kann, nämlich die Zahl $\Delta_{\text{wahr}} = 0$. Warum ist der Wert $0$ in diesem Fall speziell? Nun, er bedeutet, dass wir davon ausgehen, dass zwischen den beiden Gruppen kein Unterschied besteht, also die Intervention überhaupt nichts gebracht hat. Dies ist zwar keine wirklich interessante Annahme, aber sie hat trotz ihrer Willkürlichkeit doch etwas mehr Gewicht als eine beliebige andere Zahl. Wir bezeichnen diese Annahme jetzt auch als die $H_0$-Hypothese. Die $0$ bei $H$ bedeutet dabei nicht unbedingt, dass die $H_0$ davon ausgeht, dass nichts passiert, sondern nur, dass dies unsere Ausgangsannahme ist. In vielen Fällen hat die $H_0$ tatsächlich auch die Annahme, dass nichts passiert, dies muss aber nicht immer der Fall sein. Daher ist unsere Referenzverteilung für die Stichproben in unserem Fall die Hypothese (siehe Formel \eqref{eq-stats-sig-H0}):

\begin{equation}
H_0: \Delta = 0
\label{eq-stats-sig-H0}
\end{equation}

oder graphisch (siehe @fig-stats-sig-H0-delta0)

```{r}
#| fig.cap: "Verteilung wenn nichts passiert mit den beiden Bereichen jenseits von zwei Standardfehlern ausgezeichnet."
#| label: fig-stats-sig-H0-delta0

d_x <- 0.025
x <- seq(-3, 3, d_x)
dat_0 <- tibble(
  x = x,
  v = dnorm(x, 0, 1) 
)
low <- tibble(x = seq(-3, -2, length.out=50), v = dnorm(x))
up <- tibble(x = seq(2, 3, length.out=50), v = dnorm(x))
ggplot(dat_0, aes(x,v,ymin=0, ymax=v)) +
  geom_area(data = low, fill='red', alpha=.5) +
  geom_area(data = up, fill='red', alpha=.5) +
  geom_line() +
  scale_x_continuous('Wert', breaks = c(-2,0,2),
                     labels = c(expression(-2~sigma),
                                expression(mu),
                                expression(+2~sigma))) +
  scale_y_continuous('Dichte')
```

Diese Referenzverteilung können wir nun verwenden, um eine Entscheidung bezüglich unseres beobachteten Werts zu treffen. Die Streuung in der Referenz- bzw. Stichprobenverteilung wird als Standardfehler bezeichnet, im Gegensatz zur Streuung in der Population $\sigma$ und in der Stichprobe $s$. Letztendlich ist der Standardfehler $s_e$ nichts anderes als die Standardabweichung der Statistik.

::: {#def-standarderror}
## Standardfehler \index{Standardfehler}

Die theoretische Streuung einer berechneten Statistik, also deren Standardabweichung, wird als Standardfehler bezeichnet und mit dem Symbol $\sigma_e$ gekennzeichnet. Wird dieser Wert anhand der Stichprobe abgeschätzt, dann hat der Standardfehler das Symbol $s_e$.
:::

## Statistisch *signifikanter* Wert

Kommen wir nun zu dem wichtigen Konzept des statistisch signifikanten Werts. Im vorhergehenden Abschnitt haben wir eine Stichprobenverteilung für unsere Statistik, den Unterschied zwischen den Mittelwerten der beiden Gruppen, hergeleitet. Wir gehen von der Verteilung aus, bei der es keinen Unterschied $H_0: \Delta = 0$ zwischen den beiden Gruppen gibt. $\Delta=0$ hat somit die Bedeutung, dass das Krafttraining nicht effektiv war. Dazu haben wir als Kriterium hergeleitet, dass wir Werte, die mehr als $2$ Standardabweichungen vom Mittelwert entfernt sind, als unwahrscheinlich ansehen, da diese Werte etwa eine Wahrscheinlichkeit von $5\%$ haben. Präziser: Werte, die mehr als zwei Standardfehler vom Mittelwertsunterschied $\Delta = 0$ entfernt sind. Da der angenommene Mittelwertsunterschied, die gemessene Statistik, mit $\Delta = 0$ zu $\mu = 0$ wird, bedeutet dies, dass wir Werte, die entweder kleiner als $-2\times$ Standardfehler oder größer als $2\times$ Standardfehler sind, als unwahrscheinlich unter der Annahme von $H_0: \mu = 0$ betrachten. Als Entscheidungsregel folgt somit:

$$
|\text{beobachteter Wert }| > 2\times \sigma_e \Rightarrow \text{ Evidenz gegen } H_0
$$

```{mermaid}
%%| fig-cap: "Entscheidungsregel zur $H_0$"
%%| label: fig-stats-sig-decision-graph

flowchart TD
    A[Statistik T] --> B{Entscheidung: T > 2xs_e}
    B --> D(Nein)
    D --> E[H0 beibehalten]
    B --> F(Ja)
    F --> G[H0 ablehnen]
```

In @fig-stats-sig-decision-regions ist die Entscheidungsregel noch einmal graphisch dargestellt. Wir bestimmen eine Stichprobenverteilung unter der $H_0$, beispielsweise $H_0: \mu = \Delta = 0$ und schneiden nun rechts und links jeweils einen Bereich der Verteilung ab, den wir als *unwahrscheinlich* unter dieser speziellen $H_0$ ansehen. Diesen Bereich bezeichnen wir als kritischen Bereich. Wenn unser beobachteter Wert im kritischen Bereich liegt, dann sehen wir diese Beobachtung als Evidenz **gegen die Korrektheit der Annahme**, dass die $H_0$ gilt. Wie lehnen die $H_0$ ab.

```{r}
#| fig.cap: "Die $H_0$ Verteilung wenn nichts passiert unterteilt in Regionen die zur Entscheidung für die $H_0$ (grün) und gegen die $H_0$ (rot, kritische Regionen) führen."
#| label: fig-stats-sig-decision-regions

ggplot(dat_0, aes(x,v,ymin=0, ymax=v)) +
  geom_area(fill='green', alpha=.5) +
  geom_area(data = low, fill='red', alpha=.8) +
  geom_area(data = up, fill = 'red', alpha=.8) +
  geom_vline(xintercept = c(-2,2), color='red', linetype='dashed') +
  geom_label(data = tibble(x = c(-2,2), v = 0.3,
                           labels=c(expression(k[lower]),
                                    expression(k[upper]))),
             aes(x,v,label=labels), parse=T) +
  geom_line() +
  scale_x_continuous('Wert', breaks = c(-2,0,2),
                     labels = c(expression(-2~sigma[e]),
                                expression(H[0]:mu==0),
                                expression(+2~sigma[e]))) +
  scale_y_continuous('Dichte')
```

Wenn der Stichprobenwert der Statistik in der *kritischen* Region auftritt, dann wird von einem **statistisch signifikanten Ergebnis** gesprochen: *Unter der $H_0$ bin ich überrascht, diesen Wert zu sehen!* Allerdings ist dieser Wert **nicht unmöglich**, sondern lediglich unwahrscheinlich, wenn die Annahme $H_0$ korrekt ist. Unwahrscheinlich ist dabei kein absolutes Maß, sondern nur eine **willkürliche** Festsetzung, die wir selbst getroffen haben.

Vorhin wurde bereits erwähnt, dass Werte jenseits von $2\times \sigma_e$ etwa eine Wahrscheinlichkeit von $5\%$ unter der $H_0$ haben. Dies bedeutet, dass die Wahrscheinlichkeit, Werte im kritischen Bereich zu beobachten, bei etwa $5\%$ liegt, wenn die $H_0$ zutrifft. Oder anders: Wenn die $H_0$ in der Realität zutrifft, also den DGP korrekt beschreibt, und das Experiment $100$-mal wiederholt wird, dann würde ich etwa $5$ Experimente erwarten, bei denen der beobachtete Wert im kritischen Bereich liegt. Anhand unserer Entscheidungsregel entscheide ich mich in diesen $5$ Fällen gegen die $H_0$, obwohl diese zutrifft. Das heißt, in diesen $5$ Fällen würde ich mich irren. Eines der grundlegenden Probleme, das oftmals nicht beachtet wird bei der Interpretation von **statistisch** signifikanten Ergebnissen, bezieht sich darauf, dass ich nicht weiß, welches der $100$ Experimente ich gerade durchgeführt habe. Es ist durchaus möglich, dass ich *Pech* gehabt habe und ausgerechnet mein Experiment eines der fünf Experimente ist.

::: {#def-alpha-err}
## Irrtumswahrscheinlichkeit $\alpha$ \index{Irrtumswahrscheinlichkeit}

Die Wahrscheinlichkeit, mit der fälschlicherweise eine korrekte $H_0$-Hypothese abgelehnt wird, wird als Irrtumswahrscheinlichkeit bezeichnet. Die Irrtumswahrscheinlichkeit wird mit dem Symbol $\alpha$ bezeichnet und auch als Fehler I. Art bezeichnet.
:::


Eine weitere Missinterpretation der Irrtumswahrscheinlichkeit ist, dass sie eine Aussage über die Wahrscheinlichkeit des Zutreffens der $H_0$ erlaubt. Die Irrtumswahrscheinlichkeit ermöglicht dies allerdings nicht. Ob die $H_0$ zutrifft, hat die Wahrscheinlichkeit entweder $P(H_0) = 1$ oder $P(H_0) = 0$. Entweder sie trifft zu oder eben nicht. Darüber wird hier keine Aussage gemacht, sondern nur, ob unter der Annahme, dass $H_0$ zutrifft, der beobachtete Wert in einem *wahrscheinlichen* oder einem *unwahrscheinlichen* Bereich liegt. Nochmal, was *wahrscheinlich* ist, wurde durch eine willkürliche Festlegung bestimmt. Die gewählte Grenze ist keine physikalische Realität!

Kommen wir nun zum nächsten oft missverstandenen Term, dem p-Wert.

## Der p-Wert

Fangen wir dieses Mal mit der Definition an. Da wir mittlerweile hoffentlich schon einiges an Intuition aufgebaut haben, sollte die Definition einigermaßen verständlich sein. 

::: {#def-p-value}
## p-Wert \index{p-Wert}

Der p-Wert gibt die Wahrscheinlichkeit für den beobachteten oder einen noch extremeren Wert unter der $H_0$ an.
:::

In @fig-stats-sig-p-value-01 ist eine Verteilung unter der $H_0$ eingezeichnet, zusammen mit den kritischen Bereichen für ein gegebenes $\alpha$ und dem beobachteten Wert.

```{r}
#| fig.cap: "Der beobachtete Wert der Statistik (schwarzer Punkt) zusammen mit der Verteilung unter der $H_0$. Die gelben Flächen zeigen den p-Wert für den Wert der beobachteten Statistik an."
#| label: fig-stats-sig-p-value-01

p_val <- 2.5
p_dat_up <- tibble(
  x = seq(p_val, 3, length.out=40),
  v = dnorm(x)
)
p_dat_low <- tibble(
  x = seq(-3, -p_val, length.out=40),
  v = dnorm(x)
)
ggplot(dat_0, aes(x,v,ymin=0, ymax=v)) +
  geom_area(data = low, fill='red', alpha=.8) +
  geom_area(data = up, fill='red', alpha=.8) +
  geom_area(data = p_dat_low, fill='yellow', alpha=0.9) +
  geom_area(data = p_dat_up, fill='yellow', alpha=0.9) +
  geom_vline(xintercept = c(-2,2), color='red', linetype='dashed') +
  geom_label(data = tibble(x = c(-2,2), v = 0.3,
                           labels=c(expression(k[lower]),
                                    expression(k[upper]))),
             aes(x,v,label=labels), parse=T) +
  geom_line() +
  geom_point(data = tibble(x = p_val, v = 0), size=2) +
  labs(x = 'Werte', y = 'Dichte') 
```

Der p-Wert ist die Wahrscheinlichkeit (gelbe Fläche), unter der $H_0$, für den beobachteten oder einen extremeren Wert. Ein extremerer Wert bedeutet in diesem Fall einen größeren Wert, also alle Werte rechts vom beobachteten Wert. Jetzt irritiert allerdings, dass wir auf der linken Seite ebenfalls eine gelbe Fläche haben. Was hier passiert, ist, dass der beobachtete Wert an $\mu$ in den anderen kritischen Bereich *gespiegelt* (salopp) wurde. Jetzt wird wieder das gleiche Prinzip mit dem extremeren Wert angewendet. Hier bedeutet "extremer" links vom beobachteten Wert. Wir erhalten dann wieder eine Fläche und somit eine Wahrscheinlichkeit. Die beiden gelben Flächen zusammen ergeben dann den p-Wert. Dass die Wahrscheinlichkeit für eine Seite dazugenommen wird, bei der wir gar keinen Wert beobachtet haben, wird später verständlich, wenn wir den Unterschied zwischen gerichteten und ungerichteten Hypothesen uns anschauen. Zur Begrifflichkeit "extrem" können wir aber schon mal zusammenfassen, dass "extrem" immer in Bezug auf das $\mu$ der Stichprobenverteilung zu verstehen ist.

In @fig-stats-sig-p-value-02 sind verschiedene Beispiele für beobachtete Werte und die dazugehörigen p-Werte und deren Flächen abgebildet.

```{r}
#| fig-cap: "Verschiedene P-Werte und die dazugehörenden Flächen."
#| label: fig-stats-sig-p-value-02

foo <- function(p, lo=-3, up=3, d_x=0.025) {
  len <- 40
  x <- c(seq(lo, -abs(p), length.out=len))
  tibble(x, v= dnorm(x), type=p)
}
foo2 <- function(p, lo=-3, up=3, d_x=0.025) {
  len <- 40
  x <- c(seq(abs(p), up, length.out=len))
  tibble(x, v= dnorm(x), type=p)
}
p_s <- c(-2.7, -1.13, 0.7, 2.1) 
p_vals <- round(2*(1 - pnorm(abs(p_s))),2)
dat_3 <- purrr::map_dfr(p_s, foo)
dat_3_up <- purrr::map_dfr(p_s, foo2)
ggplot(dat_3, aes(x,v,ymin=0, ymax=v)) +
  geom_ribbon(alpha=0.5) +
  geom_ribbon(data=dat_3_up, alpha=0.5) +
  geom_line(data=dat_0) +
  geom_point(data = tibble(x=p_s, v=0,type=p_s), color='red') +
  facet_grid(~type, labeller=as_labeller(function(p){
    paste("Statistik = ", p, "\np-Wert = ", p_vals[which(p_s == p)])
  })) +
  labs(x = 'Werte', y = 'Dichte') + 
  scale_x_continuous(breaks=c(-3,0,3)) +
  theme(strip.text = element_text(size=8))
```

Können wir eigentlich den p-Wert und die Irrtumswahrscheinlichkeit in irgendeiner Form zusammenbringen? Ja, wenn wir wissen, dass die beobachtete Statistik einen p-Wert von kleiner $\alpha$ hat, dann haben wir automatisch ein statistisch signifikantes Ergebnis. Wenn das nicht auf Anhieb einleuchtet, dann schaut euch noch mal @fig-stats-sig-p-value-01 an. Welche Wahrscheinlichkeit hat $\alpha$ (Tipp: Welche Flächen sind das?) und welche Wahrscheinlichkeit hat der p-Wert (Tipp: Gelb?).

Da der p-Wert eines der am meisten missverstandenen Konzepte ist, hier noch mal ein paar Statements und Erklärungen rund um den p-Wert von verschiedenen Autoren und Institutionen.

*"[A] p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value."* [@wasserstein2016, p.131]

*"[T]he P value is the probability of seeing data that are as weird or more weird than those that were actually observed."* [@christensen2018, p.38]
											 

### Signifikanter Wert - Das Kleingedruckte

- **Vor** dem Experiment wird für ein $H_0$ ein $\alpha$-Level angesetzt (per Konvention $\alpha=0,05 = 5\%$).
- Anhand des $\alpha$-Levels können **kritische Werte** ($k_{lower}, k_{upper}$) bestimmt werden. Diese bestimmen die Grenzen der **kritischen Regionen**.
- Wenn der gemessene Wert $w$ der Statistik in die kritische Region fällt, also $w \leq k_{lower}$ oder $w \geq k_{upper}$ gilt, dann wird von einem **statistisch** signifikanten Wert gesprochen und die dazugehörige Hypothese wird **abgelehnt**. Äquivalent: Der p-Wert ist kleiner als $\alpha$.
- Da in $\alpha$-Fällen ein Wert in der kritischen Region auftritt, auch wenn die $H_0$ zutrifft, wird in $\alpha$-Fällen ein $\alpha$-Fehler gemacht.
- Wenn der Wert $w$ der Statistik nicht in den kritischen Regionen liegt, oder gleichwertig der p-Wert größer als $\alpha$ ist, wird die $H_0$ **beibehalten**. D.h. nicht, dass **kein Effekt** vorliegt, sondern lediglich, dass anhand der Daten keine Evidenz diesbezüglich gefunden werden konnte!
- Die **statistische** Signifikanz sagt nichts über die Wahrscheinlichkeit der Theorie aus!
- Ein p-Wert von $p = 0.0001$ heißt nicht, dass mit 99,99\% Wahrscheinlichkeit ein Effekt vorliegt!
- *Statistisch* signifikant heißt nicht automatisch *praktisch* relevant!

Und noch ein paar weitere Erklärungen für den p-Wert nach @wasserstein2016:

1. P-values can indicate how incompatible the data are with a specified statistical model.
2. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.
3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.
4. Proper inference requires full reporting and transparency.
5. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.
6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

In @fig-stats-sig-altman ist ein kurzer Abschnitt aus @altman1995 abgebildet, der noch mal auf eine weitere Missinterpretation eines statistisch signifikanten Ergebnisses eingeht, also wenn der p-Wert $<\alpha$ gilt.

```{r}
#| out-width: 80%
#| fig-cap: "Ausschnitt aus Altman et al. (1995)"
#| label: fig-stats-sig-altman

knitr::include_graphics("pics/altman_1995.png")
```

Kurz gesagt, wenn wir kein statistisch signifikantes Ergebnis gefunden haben, bedeutet dies nicht, dass es keinen Unterschied gibt. Tatsächlich wird der beobachtete Wert der Statistik praktisch nie exakt $=0$ sein, und wir werden daher praktisch immer einen Unterschied finden. Allerdings ist der beobachtete Unterschied nicht *überraschend* unter der $H_0$ aufgrund der Stichprobenvariabilität. Dennoch gilt: Die Abwesenheit von Evidenz ist nicht gleichzusetzen mit der Evidenz für Abwesenheit.

Das gibt uns auch einen schönen Aufschlag für die nächste Etappe. Was passiert eigentlich, wenn die *andere* Hypothese, also nicht die $H_0$, zutrifft?

## Die Fisher vs. Neyman-Pearson Kontroverse

Ein Problem bei der didaktischen Aufbereitung der folgenden Inhalte besteht allerdings darin, dass der konzeptionelle Aufbau leider nicht mit der wissenschaftlichen Realität zusammenpasst. In der wissenschaftlichen Praxis werden die Begrifflichkeiten statistische Signifikanz, $\alpha$-Fehler usw. so verwendet, als ob sie alle zu dem gleichen theoretischen Konstrukt gehören. Leider ist dies nicht der Fall und es wird in der Praxis eine Chimäre eingesetzt die konzeptionelle Widersprüchlich ist und auch innerhalb der statistischen Primärliteratur zu keiner befriedigenden Lösung gebracht wurde.

Der Großteil der in der angewandten Statistik verwendet Methoden geht auf die Arbeiten von Ronald A. Fisher (1890-1962) zurück. Unter Fishers statistischem System war die sogenannte Nullhypothese $H_0$ zentral und die Ablehnung derer basierend auf dem dazugehörenden p-Wert. Aus Fishers Ansatz erfolgte auch die Konzepte der $H_0$ Hypothese als diejenige unter der *nichts* passiert. Der p-Wert ist dabei eine Funktion eines *einzelnen* Datensatzes. Unter Fisher waren die Konzepte Typ-I und Typ-II Fehler nicht vorhanden. Dieser Ansatz wurde unter dem Begriff *inductive inference* zusammengefasst (@fisher1935). Zu diesem Zweck wird eine $H_0$-Hypothese aufgestellt und diese wird anhand der Daten mittels des p-Werts beurteilt. Der p-Wert wird unter Fisher als mögliche Evidenz gegen die $H_0$ Hypothese angesehen. D.h. wenn ein sehr *kleiner* p-Wert beobachtet wird, wird dies als Evidenz gegen die Korrektheit der Annahme der $H_0$ betrachtet. Allerdings hat sich Fisher gegen die *Annahme* der $H_0$ ausgesprochen, sondern fehlende Evidenz als nicht Ausreichend um die $H_0$ abzulehnen, also provisorisch beizubehalten [loucca2008, S.20]. Aus Fishers inductive inference kommt auch die Begrifflichkeit der *Signifikanz*. Der Ansatz wurde als Induktiv angesehen, weil von den Daten auf die Allgemeinheit geschlossen wird und dieser Schritt ist immer mit Unsicherheit behaftet.

Dem Gegenüber steht der Ansatz mit einer Nullhypothese $H_0$ und einer Altervativhypothese $H_1$ der den Arbeiten der Statistiker Jerzy Neyman (1894-1981) und Egon S. Pearson (1895-1980) beruht. Unter diesem System ist die Minimierung eines Entscheidungsfehler bei Wiederholung des Experiments die zentrale Kernidee. Wie können Entscheidungen getroffen werden die im Mittel die Anzahl der fehlerhaften Entscheidungen bei wiederholter Durchführung des Experiments minimieren? Aus diesem Ansatz haben Pearson-Neyman die Konzepte Typ-I und Typ-II Fehler, Power und Konfidenzintervalle entwickelt [@neyman1935; @lehmann1993]. Unter diesem System hat der p-Wert keine ausgezeichnete Bedeutung. Zentrall ist die Entscheidung die $H_0$ anzunehmen bzw. die $H_0$ abzulehnen um über **viele Wiederholungen des Experiments** hinweg optimale Entscheidungen zu treffen. Optimale Entscheidungen bedeutet möglichst wenige falsche Entscheidungen zu treffen. Der Hintergrund vor dem Neyman und Pearson die Theorie entwickelt haben, ist stärker durch einen industriellen Kontext geprägt um zum Beispiel bei einem Fließband sicher zu stellen, dass die Qualität der Produkte einem bestimmten Standard entspricht. In diesem Zusammenhang ist die *Wiederholung* von Experimenten, die zufällige Auswahl von Objekten, dementsprechend einfacher nachvollziehbar. Unter dem System von Pearson-Neyman ist die Durchführung eines Experiments dabei ohne die explizite Berechnung der Power und folglich der expliziten Aufstellung einer $H_1$ Hypothese bedeutungslos. Auch erklärt sich hier die Interpretation von Konfidenzintervallen als eine Überdeckungswahrscheinlichkeit bei Wiederholung des Experiments. Pearson-Neyman haben diesen Ansatz unter dem Begriff *inductive behavior* zusammengefasst. Das konkrete Niveau der Signifikanz, der p-Wert, ist weniger von Bedeutung, sondern nur welche Entscheidung getroffen wird und wie die Anzahl der Fehlentscheidungen minimiert werden können.

D.h. unter Fisher ist die Begriffe der Evidenz und der damit verbundene **p-Wert** und der **Signifikanz**test zentral. Dagegen sind unter Neyman-Pearson die Konzepte Fehler, $\alpha$ bzw. $\beta$, und **Hypothesen**test zentral. Damit verbunden ist auch beispielsweise, das das Konzept der Power unter Fischers Signifikanztest keine Rolle spielt, da keine $H_1$ vorhanden ist. Es lässt sich zeigen, dass diese beiden Ansätze auf Basis der gleichen Daten zu unterschiedlichen Aussagen kommen können, d.h. im Widerspruch zueinander stehen (@berger2003, @lehmann1993). Die Unterschiede in der Auffassung wie Daten statistisch bewertet werden sollen, hat zu einer regelrechten Fehde vor allem zwischen Fisher und Pearson geführt (@neyman1956, @loucca2008). Unglücklicherweise sind im Laufe der Jahre die beiden Ansätze bei Anwendern der Statistik immer mehr verschmolzen und haben dazu geführt, dass ein Großteil der wissenschaftlichen Analysen mit einem in sich widersprüchlichen statistischen System durchgeführt wird (@hubbard2003, @hager2013, @gigerenzer2004, @nuzzo2014). Ironischerweise sind beide Seiten sich einig darüber gewesen, dass das Signifikanzlevel bzw. der Fehlerlevel mit $5$% nicht sakrosankt sein ist, sondern entsprechend der Fragestellung angepasst werden sollte (@lehmann1993). Beide Ansätze haben zudem zwei Abstrahierungen in die statistische Analyse die sich nicht mit der Realität überein bringen lassen. Aus Fishers Ansatz ist dies das Konzept einer unendlich großen Population aus der Stichproben gezogen werden, während bei Neyman-Pearson die (unendlich oft) wiederholte Testung benötigt wird.

Die Verschmelzung der beiden Konzepte hat auch dazu geführt, dass der p-Wert und das $\alpha$-Level miteinander vermischt werden. Der p-Wert ist eine Aussage über die Daten unter der Annahme der $H_0$. Das $\alpha$-Level ist dagegen eine Aussage über die Fehlerrate über viele Experimente hinweg. Bei einem einzelnen Experiment wird entweder ein Fehler oder eben nicht gemacht. Ein p-Wert von $p = 0.03$ ist daher keine Aussage über Fehlerrate, sondern eine Aussage darüber mit welcher Wahrscheinlichkeit ein beobachteter oder ein extremerer Wert unter einer angenommenen $H_0$ auftritt. In dieser Aussage taucht das Konzept eines Fehlers nicht auf. Tatsächlich ist die Missinterpretation nicht nur unter AnwenderInnen sondern auch unter StatistikerInnen weit verbreitet (@hubbard2003). Allerdings sollte auch erwähnt werden, dass die Ansicht, dass die beiden Systeme nicht miteinander vereinbar sind nicht von allen geteilt wird [vgl. @mayo2018, S.173ff]


## Things to know

- Datengenerierender Prozess
- Lage- und Skalenparameter
- Mittelwert, Varianz und Standardabweichung
- Statistische Signifikanz
- Schätzer
- p-Wert
- Referenzverteilung
- Standardfehler

## Weitere Literatur

Ein interessanter Artikel, der die Auswirkungen beleuchtet, wenn Studien zu wenig Power haben: @button2013. In @djulbegovic2007 findet sich eine interessante Diskussion darüber, unter welchen Bedingungen statistisch signifikante Ergebnisse als wahr angesehen werden sollten. In @borg2023 ist eine Untersuchung zu der unerwartet hohen Anzahl von statistisch signifikanten Ergebnissen in der sportwissenschaftlichen Literatur dokumentiert (siehe auch @haeffel2022). In @sandercock2024 wird ein Beispiel dokumentiert, das zeigt, wie häufig der Standardfehler mit der Standardabweichung verwechselt wird (siehe auch @kadlec2022).
