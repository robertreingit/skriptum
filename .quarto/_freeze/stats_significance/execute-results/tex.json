{
  "hash": "8b78a682bba0320ec5fa8ca7871fb9db",
  "result": {
    "markdown": "# Statistische Signifikanz, p-Wert und Power \n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nIm vorherigen Kapitel haben wir gesehen, wie Unsicherheit ein zentrales Problem bei der Interpretation von Ergebnissen von Experimenten oder Daten allgemein ist. Im nun folgenden Abschnitt wollen wir eine Prozess aufbauen, der es uns vor dem Hintergrund dieser Unsicherheit eine Entscheidung zu treffen.\n\n## Wie treffe ich eine Entscheidung? \n\nIn unserem kleine Welt Bespiel waren wir in der komfortablen Position, das wir genau wussten was passiert bzw. welcher Prozess unseren beobachteten Datenpunkt erzeugt hat. D.h wir kannten den datengenerieren Prozesses.\n\n::: {#def-dgp}\n## Datengeneriereden Prozess\n\nDer Prozess in der realen Welt der die beobachteten Daten und damit die daraus folgende Statistik erzeugt wird als datengenerierender Prozess (DGP) \\index{Datengenerierender Prozess} bezeichnet.\n:::\n\nLetztendlich zielt unsere Untersuchung, unser Experiment, darauf ab, Informationen über den DGP zu erhalten, weil diese Information uns erlaubt Aussagen über die reale Welt zu treffen. Dabei muss allerdings beachtet werden, dass dieser Prozess in den allermeisten Fällen ein starke Vereinfachung des tatsächlichen Prozesses in der Realität darstellt. Meistens sind die Abläufe in der Realität zu komplex um sie ins Gänze abzubilden. Somit wird fast immer nur ein Modell verwendet. \n\nZurück zu unseren Problem, wenn wir ein Experiment durchführen, dann haben wir normalerweise nur eine einzige beobachtete Statistik. In unseren bisherigen Beispiel also den berechneten Unterschied $D$ in der Kraftfähigkeit nach der Intervention zwischen der Kontroll- und der Interventionsgruppe.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Beobachteter Unterschied nach der Durchführung unseres Experiments](stats_significance_files/figure-pdf/fig-sts-sig-result-1-1.pdf){#fig-sts-sig-result-1}\n:::\n:::\n\n\nIn @fig-sts-sig-result-1 sehen wir unseren beobachteten Wert. Dieser sei $D = 50$. Wir wissen ja aber von vorne herein schon, das dieser Wert beeinflusst ist durch die zufällige Wahl der Stichprobe und die daran geknüpfte Streuung der Werte in der Population. Wie können wir den nun überhaupt eine Aussage treffen darüber, ob das Krafttraining was bringt oder vielleicht nur einen kleinen Effekt zeigt oder möglicherweise sogar schädlich ist?\n\nÜberlegen wir uns zunächst, welche Prozesse unseren beobachteten Wert zustande gebracht haben könnten. Wir haben schon zwei Prozesse kennengelernt, einmal den Prozess mit $\\Delta = 100$ und auch den Prozess mit $\\Delta = 0$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Mögliche datengenerierende Prozesse für den beobachteten Unterschied $D$ (rot)](stats_significance_files/figure-pdf/fig-sts-sig-dgp-1-1.pdf){#fig-sts-sig-dgp-1}\n:::\n:::\n\n\nIn @fig-sts-sig-dgp-1 ist wieder unser beobachteter Wert $D = 50$ und die beiden Verteilungen abgetragen. Leider können wir nicht sagen, welche der beiden Verteilungen, bzw. deren zugrundeliegende Prozesse, unseren beobachteten Wert erzeugt haben. Da unser beobachteter Wert $D$ genau zwischen den beiden Maxima der Verteilungn liegt. Etwas motiviertes Starren auf die Abbildung wird uns allerdings auf die Idee bringen, dass der Wert ja nicht nur von diesen beiden Verteilungen erzeugt worden sein kann sondern durchaus noch mehr Verteilungen dafür in Frage kommen.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Beispiele für weitere mögliche Verteilungen als DGP.](stats_significance_files/figure-pdf/fig-sts-sig-dgp-2-1.pdf){#fig-sts-sig-dgp-2}\n:::\n:::\n\n\n@fig-sts-sig-dgp-2 zeigt, dass selbst die Verteilung mit $\\Delta = -250N$ und $\\Delta = 350N$ nicht unplausibel sind den beobachteten Wert erzeugt zu haben. Warum aber bei diesen fünf Verteilungen aufhören, warum sollte $Delta$ nicht $-50$ oder $127$ sein. Und überhaupt, ich bin mir nicht sicher, das die Natur nur ganzzahlige Wert kennt (siehe $\\pi$). Warum sollte $D$ nicht auch $123.4567N$ sein?\n\nWenn diese Überlegung weitergeführt wird, dann wird schnell klar, dass letztendlich eine unendliche Anzahl von Verteilung in der Lage ist unseren beobachteten Wert plausibel zu generieren. D.h. wir haben ein Experiment durchgeführt und den ganzen Aufwand betrieben und haben wochenlang mit unseren ProbandInnen Krafttraining durchgeführt und sind hinterher eigentlich keinen Schritt weiter da wir immer noch nicht wissen was der datengenerierende Prozess ist. Also können wir selbst nach dem Experiment nicht sagen ob unser Krafttraining tatsächlich nützlich ist.\n\nZum Glück werden wir sehen und unser Unterfangen ist nicht ganz so aussichtslos. Schauen wir uns zum Beispiel die Verteilung für $\\Delta = -350N$ an (@fig-sts-sig-dgp-3).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Verteilung für $\\Delta = -350N$ und der beobachtete Wert $D$](stats_significance_files/figure-pdf/fig-sts-sig-dgp-3-1.pdf){#fig-sts-sig-dgp-3}\n:::\n:::\n\n\nUnser beobachteter Wert ist jetzt nicht vollkommen unmöglich unter der Annahme das $\\Delta = -350N$ ist, aber so richtig *wahrscheinlich* ist er auch nicht. Der Wert liegt relativ weit am Rand der Verteilung. Die Kurve ist dort schon ziemlich nahe bei Null. D.h. der beobachtete Wert ist zwar schon möglich aber es wäre schon überraschend wenn wir bei einer Durchführung des Experiments ausgerechnet so einen Wert beobachten würden.\n\nWenn wir jetzt dagegen von der Annahme ausgehen, dass dem DGP der Wert $\\Delta = 50N$ zugrundeliegen würde, hätten wir die Verteilung in @fig-sts-sig-dgp-4. Hier ist der beobachtete Wert mitten drin in dem Teil der Verteilung der auch zu erwarten würde. D.h. unser beobachteter Wert ist durchaus plausibel unter der Annahme das $\\Delta = 50N$ gilt.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Verteilung für $\\Delta = 50N$ und der beobachtete Wert $D$](stats_significance_files/figure-pdf/fig-sts-sig-dgp-4-1.pdf){#fig-sts-sig-dgp-4}\n:::\n:::\n\n\nDiesen Ansatz können wir verwenden um mit Hilfe unseres Experiments doch etwas über den DGP auszusagen. Allerdings müssen wir uns noch einmal etwas eingehender mit Verteilungen auseinandersetzen. D.h. wir müssen uns erst ein mal ein paar neue Konzepte erarbeiten.\n\n## Verteilungen - 1. deep dive\n\n::: {.cell}\n::: {.cell-output-display}\n![Eine Dichtefunktion](stats_significance_files/figure-pdf/unnamed-chunk-7-1.pdf)\n:::\n:::\n\n\n## Eigenschaften von Verteilungen - Mittelwert $\\mu$\n\n::: {.cell}\n::: {.cell-output-display}\n![Verteilungen mit unterschiedlichen Mittelwerten](stats_significance_files/figure-pdf/unnamed-chunk-8-1.pdf)\n:::\n:::\n\n\n^[auch Lageparameter oder Erwartungswert]\n\n## Eigenschaften von Verteilungen - Varianz $\\sigma^2$\n\n::: {.cell}\n::: {.cell-output-display}\n![Verteilungen mit unterschiedlichen Varianzen](stats_significance_files/figure-pdf/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n\n^[auch Skalenparameter]\n\n## Formeln \n\n\\begin{table}[]\n    \\caption{Parameter einer Verteilung und deren Sch\\\"atzer}\n    \\centering\n    \\begin{tabular}{llr}\n     \\toprule\n     Population & Stichprobe & \\\\\n     \\midrule\n     Mittelwert $\\mu$ & $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$  \\\\\n     Varianz $\\sigma^2$ & $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2$ \\\\\n     Standardabweichung $\\sigma$ & $s = \\sqrt{s^2}$ \\\\\n     \\bottomrule\n    \\end{tabular}\n\\end{table}\n\nn := Anzahl der Stichprobenelemente, $x_i$ := Messwerte\n\n\n## Nebenbei: Warum der Mittelwert Sinn macht\n\n::: {.cell}\n::: {.cell-output-display}\n![Verteilung der Mittelwerte von Stichproben der Größe $n=10$, Kleine Welt Population $\\mu$ (rot)](stats_significance_files/figure-pdf/unnamed-chunk-10-1.pdf)\n:::\n:::\n\n\n## Mit der Verteilung die annimmt das nichts passiert!\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Verteilung wenn nichts passiert.](stats_significance_files/figure-pdf/unnamed-chunk-11-1.pdf)\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Quantilefunktion wenn nichts passiert.](stats_significance_files/figure-pdf/unnamed-chunk-12-1.pdf)\n:::\n:::\n\n\n## *Signifikanter* Wert\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Verteilung wenn nichts passiert und kritische Regionen.](stats_significance_files/figure-pdf/unnamed-chunk-13-1.pdf)\n:::\n:::\n\n\nWenn der Stichprobenwert der Statistik in der *kritischen* Region auftritt, dann wird von einem **statistisch** signifikanten Effekt gesprochen. *Unter der $H_0$ bin ich überrascht diesen Wert zu sehen!*\n\n## Der p-Wert\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Der gelben Flächen zeigen den p-Wert für den Wert der Statistik von d = 2,5 an.](stats_significance_files/figure-pdf/unnamed-chunk-14-1.pdf)\n:::\n:::\n\n\n## p-Werte\n\nDer p-Wert gibt die Wahrscheinlichkeit für den gefundenen oder einen noch extremeren Wert unter der $H_0$ an.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Verschiedene P-Werte](stats_significance_files/figure-pdf/unnamed-chunk-15-1.pdf)\n:::\n:::\n\n\n## p-Werte\n\n*\"[A] p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.\"* [@wasserstein2016, p.131]\n\n*\"[T]he P value is the probability of seeing data that are as weird or more weird than those that were\nactually observed.\"* [@christensen2018, p.38]\n\n## Signifikanter Wert - Das Kleingedruckte\n\n- **Vor** dem Experiment wird für ein $H_0$ ein $\\alpha$-Level angesetzt (per Konvention $\\alpha=0,05 = 5\\%$)\n- Anhand des $\\alpha$-Levels können **kritische Werte** ($k_{lower}, k_{upper}$) bestimmt werden. Diese bestimmen die Grenzen der **kritischen Regionen**.\n- Wenn der gemessene Wert w der Statistik in die kritische Region fällt, also $w \\leq k_{lower}$ oder $w \\geq k_{upper}$ gilt, dann wird von einem **statistisch** signifikanten Wert gesprochen und die dazugehörige Hypothese wird **abgelehnt**. Äquivalent: Der p-Wert ist kleiner als $\\alpha$.\n- Da in $\\alpha$-Fällen ein Wert in der kritischen Region auftritt, auch wenn die $H_0$ zutrifft, wird in $\\alpha$-Fällen ein $\\alpha$-Fehler gemacht.\n\n## Signifikanter Wert - Das Kleingedruckte\n\n- Wenn der Wert w der Statistik nicht in den kritischen Regionen liegt, oder gleichwertig der p-Wert größer als $\\alpha$ ist, wird die $H_0$ **beibehalten**. D.h. nicht, dass **kein Effekt** vorliegt, sondern lediglich, dass anhand der Daten keine Evidenz diesbezüglich gefunden werden konnte!\n- Die **statistische** Signifikanz sagt nichts über die Wahrscheinlichkeit der Theorie aus!\n- Ein p-Wert von $p = 0.0001$ heißt nicht, dass mit 99,99\\% Wahrscheinlichkeit ein Effekt vorliegt!\n- *Statistisch* signifikant heißt nicht automatisch *praktisch* relevant!\n\n## Nochmal, wenn die $H_0$ nicht abgelehnt wird\n\n![Ausschnitt aus @altman1995](pics/altman_1995.png){height=6cm}\n\n## Nochmal p-Wert (@wasserstein2016)\n\n1. P-values can indicate how incompatible the data are with a specified statistical model.\n2. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\n3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\n4. Proper inference requires full reporting and transparency\n5. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\n6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n## Was passiert nun aber wenn die \"andere\" Hypothese zutrifft? \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Differenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von $\\alpha$ wenn $H_0$ zutrifft.](stats_significance_files/figure-pdf/unnamed-chunk-16-1.pdf)\n:::\n:::\n\n\n## Wir machen einen $\\beta$-Fehler! \n\n::: {.cell}\n::: {.cell-output-display}\n![Differenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von $\\alpha$ wenn $H_0$ zutrifft und $\\beta$ (grün) wenn $H_1$ zutrifft.](stats_significance_files/figure-pdf/unnamed-chunk-17-1.pdf)\n:::\n:::\n\n\n## Snap!(1989) - The Power\n\n\n::: {.cell}\n::: {.cell-output-display}\n![$1-\\beta$ = Power des Tests (blaue Fläche).](stats_significance_files/figure-pdf/unnamed-chunk-18-1.pdf)\n:::\n:::\n\n\n## Terminologie noch mal\n\n- $\\alpha$: Die Wahrscheinlichkeit sich gegen die $H_0$ zu entscheiden, wenn die $H_0$ zutrifft. $\\alpha$-Level wird vor dem Experiment festgelegt um zu kontrollieren welche Fehlerrate toleriert wird.\n- $\\beta$: Die Wahrscheinlichkeit sich gegen die $H_1$ zu entscheiden, wenn die $H_1$ zutrifft.\n- Power := $1 - \\beta$: Die Wahrscheinlichkeit sich für die $H_1$ zu entscheiden, wenn die $H_1$ zutrifft. Sollte ebenfalls **vor** dem Experiment festgelegt werden.\n\n\n## Wie können wir die Power erhöhen? \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Verteilungen wenn $\\delta$=500 und $\\delta$=0 in unserem kleine Welt Beispiel mit n = 3.](stats_significance_files/figure-pdf/unnamed-chunk-19-1.pdf)\n:::\n:::\n\n\n\n## Stichprobengröße von n = 3 auf n = 9 erhöhen?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Stichprobenverteilungen der Differenz unter $H_0$ und $H_1:\\delta=500$N bei einer Stichprobengröße von n = 9](stats_significance_files/figure-pdf/unnamed-chunk-20-1.pdf)\n:::\n:::\n\n\n## Standardfehler \n\nDie Standardabweichung der Statistik wird als **Standardfehler** $s_e$ bezeichnet^[Der Standardfehler schätzt die Reliabilität der Statistik ab (@cohen1988)]. Der Standardfehler ist nicht gleich der Standardabweichung in der Population bzw. der Stichprobe. Es gilt für den Mittelwert:\n\n\n\\begin{table}[]\n    \\caption{Standardfehler des Mittelwerts, n = Stichprobengröße}\n    \\centering\n    \\begin{tabular}{ll}\n     \\toprule\n     Population & Stichprobe \\\\\n     \\midrule\n     $\\sigma_{\\bar{X}} =  \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}$ & \n     $ s_e =  \\sqrt{\\frac{s^2}{n}} = \\frac{s}{\\sqrt{n}}$ \\\\ \n     \\bottomrule\n    \\end{tabular}\n    \\label{tab:s_e}\n\\end{table}\n\n\n\n# Parameterschätzung \n\n## Problem bei einer dichotomen Betrachtung der Daten\n\n![Auszug aus @cumming2013 [p.1]](../pics/cumming_luck.png)\n\n\n## Wie groß ist der Effekt? \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Stichprobenverteilungen der Differenz unter $H_0$ und $H_1:\\delta=500$N bei einer Stichprobengröße von n = 9](stats_significance_files/figure-pdf/unnamed-chunk-21-1.pdf)\n:::\n:::\n\n\n\n## Schätzung der Populationsparameter\n\n\n::: {.cell}\n\n:::\n\n\nKleine Welt: Experiment wird einmal mit n = 9 durchgeführt \n\n### Beobachtete Stichprobenkennwerte\n\n\\begin{align*}\nd = \\bar{x}_{treat} - \\bar{x}_{con} &= 350 \\\\\ns &= 132 \\\\\ns_e &= 44\n\\end{align*}\n\nWie präzise ist meine Schätzung und welche anderen Unterschiedswerte sind anhand der beobachteten Daten noch plausibel?\n\n## Welche $\\delta$s sind plausibel für $d = 350$? \n\n\n::: {.cell}\n::: {.cell-output-display}\n![Verschiedene Verteilungen von Gruppendifferenzen, beobachteter Unterschied (rot)](stats_significance_files/figure-pdf/unnamed-chunk-23-1.pdf)\n:::\n:::\n\n\nPlausibel unter einem gegebenem **$\\alpha$-Level**!\n\n\n## Alle möglichen $\\delta$s die plausibel sind\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Konfidenzintervall (grün), Populationsparameter $\\delta$ und $\\alpha$-Level für die beobachtete Differenz (gelb).](stats_significance_files/figure-pdf/unnamed-chunk-24-1.pdf)\n:::\n:::\n\n\n## Was passiert wenn ich das Experiment ganz oft wiederhole?\n\n\n::: {.cell hash='stats_significance_cache/pdf/unnamed-chunk-25_cfdff383fb33d568f808be38b93429b8'}\n::: {.cell-output-display}\n![Simulation von $n = 100$ Konfidenzintervallen.](stats_significance_files/figure-pdf/unnamed-chunk-25-1.pdf)\n:::\n:::\n\n\n## Konfidenzintervall - Das Kleingedruckte\n\n- Das Konfidenzintervall für ein gegebenes $\\alpha$-Niveau gibt nicht die Wahrscheinlichkeit an mit der der *wahre* Parameter in dem Intervall liegt.\n- Das Konfidenzintervall gibt alle mit den Daten kompatiblen Populationsparameter an.\n- Das $\\alpha$-Niveau des Konfidenzintervalls gibt an bei welchem Anteil von Wiederholungen davon auszugehen ist, das das Konfidenzintervall den wahren Populationsparameter enthält. \n\n## Konfidenzintervall herleiten nach @spiegelhalter2019 [p.241]\n\\scriptsize\n1. We use probability theory to tell us, for any particular population\nparameter, an interval in which we expect the observed statistic to lie\nwith 95% probability.\n2. Then we observe a particular statistic.\n3. Finally (and this is the difficult bit) we work out the range of possible\npopulation parameters for which our statistic lies in their 95\\%\nintervals. This we call a \"95\\% confidence interval\".\n4. This resulting confidence interval is given the label \"95\\%\" since, with\nrepeated application, 95% of such intervals should contain the true\nvalue.^[Strictly speaking, a 95\\% confidence interval does ***not*** mean there is a 95\\% probability that this particular interval contains the true value [...]]\n\nAll clear? If it isn’t, then please be reassured that you have joined\ngenerations of baffled students.\n\n\n## Konfidenzintervall berechnen (Vorschau)\n\n$$\n\\textrm{CI}_{1-\\alpha} = \\bar{x} \\pm z_{\\alpha/2} \\times s_e \n$$\n\n\n## Dualität von Signifikanztests und Konfidenzintervall\n\nWenn das Konfidenzintervall mit Niveau $1-\\alpha\\%$ die $H_0$ nicht beinhaltet, dann wird auch bei einem Signifikanztest die $H_0$ bei einer Irrtumswahrscheinlichkeit von $\\alpha$ abgelehnt.\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}