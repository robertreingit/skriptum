{
  "hash": "663e96f802e4c541c554a1df9b83b546",
  "result": {
    "markdown": "# Vorhersage \n\n\n::: {.cell}\n\n:::\n\n\n## Vorhergesagte Werte $\\hat{y}_i$\n\n\n::: {.cell}\n\n:::\n\n\nWenn ein einfaches lineares Modell gefittet wurde ist eine zentrale Frage welche Vorhersagen anhand des Modell getroffen werden können. Die Vorhersagen $\\hat{y}_i$ liegen auf der vorhergesagten Regressionsgerade und berechnen sich nach dem Modell für einen gegeben $x$-Wert. \n\n$$\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_0} x\n$$\n\nWie schon mehrfach besprochen unterliegt die Regressionsgerade inherent der Unsicherheit bezüglich der geschätzen Modellkoeffizienten $\\hat{\\beta}_0$ und $\\hat{\\beta}_1$. Diese Unsicherheit überträgt sich auf die geschätzen Werte  $\\hat{y}_i$ und muss daher bei deren Interpretation berücksichtigt werden.\n\nIn @fig-pred-01 sind die bereits behandelten Sprungdaten gegen die Anlaufgeschwindigkeiten zusammen mit der Regressionsgeraden und vorhergesagten Werten (rot) abgetragen.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Vorhersagewerte $\\hat{y}_i$ (rote Punkte) für die Sprungdaten.](slm_prediction_files/figure-pdf/fig-pred-01-1.pdf){#fig-pred-01}\n:::\n:::\n\n\nIn `R` können die vorhergesagten Werte des mittels `lm()` gefitteten Modells mit der Hilfsfunktion `predict()` bestimmt werden. Wenn der Funktion `predict()` keine weiteren Parameter außer dem `lm`-Objekt übergeben werden, berechnet `predict()` die vorhergesagten Werte $\\hat{y}_i$ für alle die $x$-Werte die auch zum fitten des Modells benutzt wurden. Die Reihenfolge der Werte $\\hat{y}_i$ enspricht dabei den Werten im Original-`data.frame()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(mod)[1:5] \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3        4        5 \n4.523537 4.725140 4.856256 4.761778 5.416207 \n```\n:::\n:::\n\n\nWir haben uns hier nur die ersten fünf Werte ausgeben lassen, da nur demonstriert werden soll wie die `predict()`-Funktion angewendet werden kann. Um eine Anwendung zu geben, so können mittels `predict()` die Residuen auch von Hand ohne die `resid()`-Funktion erhalten werden.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(jump$jump_m - predict(mod))[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          1           2           3           4           5 \n-0.16267721 -0.41248842 -0.29359256 -0.01047071  0.09927500 \n```\n:::\n\n```{.r .cell-code}\nresid(mod)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          1           2           3           4           5 \n-0.16267721 -0.41248842 -0.29359256 -0.01047071  0.09927500 \n```\n:::\n:::\n\n\nWiederum nur zur Demonstration die ersten fünf Wert um die Äquivalenz der beiden Methoden zu demonstrieren.\n\nMeistens liegt das Interesse jedoch weniger auf den vorhergesagten Werten $\\hat{y}_i$ für die gemessenen Werte, sondern es sollen Werte vorhergesagt werden für $x$-Werte die nicht im Datensatz enthalten sind. Operational ändert sich nichts, es wird immer noch das gefittete Modell verwendetet und es müssen lediglich neue $x$-Werte übergeben werden.\n\nIn `R` kann dies mittels des zweite Parameter in `predict()` erreicht werden. Soll zum Beispiel die Sprungweite für eine Anlaufgeschwindigkeit von $v = 11.5[m/s]$ berechnen werden, muss zunächst ein neues `tibble()` erstellt werden, welches den gewünschten $x$-Wert enthält. Dabei muss der Spaltenname in dem neuen `tibble()` demjenigen im Original-`tibble()` entsprechen. Ansonsten funktioniert die Anwendung von `predict()` nicht.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tibble(v_ms = 11.5)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 1\n   v_ms\n  <dbl>\n1  11.5\n```\n:::\n:::\n\n\nDieses `tibble()` kann nun zusammen mit dem `lm()`-Objekt an `predict()` übergeben werden.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(mod, newdata = df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n8.614136 \n```\n:::\n:::\n\n\nD.h., bei einer Anlaufgeschwindigkeit von $v = 11.5[m/s]$ ist anhand des Modells eine Sprungweite von $8.6m$ zu erwarten.\n\n## Unsicherheit in der Vorhersage \n\nWie schon angesprochen ist unser Modell natürlich mit Unsicherheiten behaftet. Diese drücken sich in den Standardfehler für die beiden Koeffizienten $\\hat{\\beta_0}$ und $\\hat{\\beta_1}$ (siehe @tbl-pred-01).\n\n\n::: {#tbl-pred-01 .cell tbl-cap='Modellparameter und Standardfehler'}\n::: {.cell-output-display}\n|            | Schätzer| $s_e$|\n|:-----------|--------:|-----:|\n|(Intercept) |    -0.14|  0.23|\n|v_ms        |     0.76|  0.02|\n:::\n:::\n\n\n\nDer vorhergesagte Wert $\\hat{y}$ ist daher für sich alleine ist noch nicht brauchbar, da auch Informationen über dessen Unsicherheit notwendig sind um die Ergebnisse korrekt zu interpretieren.\n\nEs können zwei unterschiedliche Anwendungsfälle voneinander unterschieden werden. \n\n1. Der mittlere, erwartete Wert $\\hat{\\bar{y}}_{neu}$\n2. Die Vorhersage eines einzelnen Wertes  $\\bar{y}_{neu}$\n\nIm konkreten Fall werden damit zwei unterschiedliche Fragestellungen beantwortet. Im 1. Fall lautet die Frage, ich habe eine Trainingsgruppe und möchte wissen was der mittlere Wert der Gruppe anhand des Modells ist, wenn alle eine bestimmte Anlaufgeschwindigkeit $v_{neu}$ haben. Im 2. Fall lautet die Frage welche Weite eine einzelne Athletin für die Anlaufgeschwindigkeit $v_{neu}$ springen sollte. In beiden Fällen werden keiner genau den Wert des Regressionsmodells treffen, aber im 1. Fall der Gruppe werden sich Streuungen nach oben bzw. nach unten gegenseitig im Schnitt ausbalancieren während im 2. Fall der einzelnen Athletin dies nicht der Fall ist. Daher hat die Vorhersage im 2. Fall eine höhere Unsicherheit. Diese Unterschied sollte sich dementsprechend in den Varianzen der beiden Vorhersagen wiederspiegeln.\n\nWie bereits erwähnt, der vorhergesagte Wert $\\hat{y}_{neu}$ ist in beiden Fällen gleich und entsprecht der oben beschriebenen Methode anhand des Modell $y_{neu} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times x_{\\text{neu}}$. \n\n\nFür den erwarteten Mittelwert errechnet sich die Varianz nach:\n\n\\begin{equation}\nVar(\\hat{\\bar{y}}_{neu}) = \\hat{\\sigma}^2 \\left[\\frac{1}{n} + \\frac{(x_{neu} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\\right] = \\hat{\\sigma}_{\\hat{\\bar{y}}_{neu}}^2\n\\end{equation}\n\nDas dazugehörige Konfidenzintervall errechnet sich danach mittels:\n\n\\begin{equation}\n\\hat{\\bar{y}}_{neu} \\pm q_{t(1-\\alpha/2;n-2)} \\times \\hat{\\sigma}_{\\hat{\\bar{y}}_{neu}}\n\\end{equation}\n\nDie Varianz für die Vorhersage eines einzelnen Wertes errechnet sich:\n\n\\begin{equation}\nVar(\\hat{y}_{neu}) = \\hat{\\sigma}^2 \\left[1 + \\frac{1}{n} + \\frac{(x_{neu} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\\right] = \\hat{\\sigma}^2 + \\hat{\\sigma}_{\\hat{\\bar{y}}_{neu}}^2 = \\hat{\\sigma}_{\\hat{y}_{neu}}^2\n\\end{equation}\n\nWas wiederum zu dem folgenden Konfidenzintervall führt:\n\n\\begin{equation}\n\\hat{y}_{neu} \\pm q_{t(1-\\alpha/2;n-2)} \\times \\hat{\\sigma}_{\\hat{y}_{neu}}\n\\end{equation}\n\nIn beiden Fällen ist der Term\n\n$$\n\\frac{(x_{neu} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\n$$\n\nenthalten. Anhand des Zählers kann abgeleitet werden, dass die Unsicherheit der Vorhersage mit dem Abstand vom Mittelwert der $x$-Werte zunimmt. Rein heuristisch macht dies Sinn, da davon ausgegangen werden kann, dass um den Mittelwert der $x$-Werte auch die meiste Information über $y$ vorhanden ist und dementsprechend umso weiter die Werte sich vom $\\bar{x}$ entfernen die Information abnimmt. Im Nenner ist wiederum wie auch beim Standardfehler $\\sigma_{\\beta_1}$ des Steigungskoeffizienten $\\beta_1$ zu sehen, dass die Varianz abnimmt mit der Streuung der $x$-Werte. Daher, wenn eine Vorhersage in einem bestimmten Bereich von $x$-Werten durchgeführt werden soll, dann sollte darauf geachtet werden möglichst diesen Bereich auch zu samplen um die Unsicherheit so klein wie möglich zu halten.\n\n\n## Vorhersagen in R mit `predict()`\n\n### Erwarteter Mittelwert\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(v_ms = 11.5) # oder tibble(v_ms = 11.5)\npredict(mod, newdata = df, interval = 'confidence')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 8.614136 8.482039 8.746234\n```\n:::\n:::\n\n\n### Individuelle Werte\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(mod, newdata = df, interval = 'prediction')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 8.614136 8.118445 9.109827\n```\n:::\n:::\n\n\n## Konfidenzintervalle graphisch\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](slm_prediction_files/figure-pdf/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n\nWeiterführende Literatur sind @kutner2005\n\n## $R^2$ und Root-mean-square\n\n\n::: {.cell}\n\n:::\n\n\n## Einfaches Modell\n\n\\scriptsize\n\n::: {.cell}\n\n```{.r .cell-code}\nmod0 <- lm(y ~ x, simple)\nsummary(mod0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = simple)\n\nResiduals:\n      1       2       3       4 \n-0.5817  0.9898 -0.2345 -0.1736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   1.8414     0.7008   2.628    0.119\nx             0.4574     0.3746   1.221    0.346\n\nResidual standard error: 0.8376 on 2 degrees of freedom\nMultiple R-squared:  0.4271,\tAdjusted R-squared:  0.1406 \nF-statistic: 1.491 on 1 and 2 DF,  p-value: 0.3465\n```\n:::\n:::\n\n\n\n## Nochmal Abweichungen\n\n1. **Gesamtvarianz**:\n$$\nSSTO := \\sum_{i=1}^N (y_i - \\bar{y})^2\n$$\n2. **Regressionsvarianz**: \n$$\nSSR :=\\sum_{i=1}^N(\\hat{y}_i - \\bar{y})^2\n$$\n\n3. **Residualvarianz**:\n$$\nSSE := \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Minimalmodell der Abweichungen](slm_prediction_files/figure-pdf/unnamed-chunk-12-1.pdf)\n:::\n:::\n\n\n## Verhältnis von $SSR$ zu $SSTO$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Perfekter Zusammenhang](slm_prediction_files/figure-pdf/unnamed-chunk-13-1.pdf)\n:::\n:::\n\n\n$$\n\\frac{SSR}{SSTO} = 1\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Kein Zusammenhang](slm_prediction_files/figure-pdf/unnamed-chunk-14-1.pdf)\n:::\n:::\n\n\n$$\n\\frac{SSR}{SSTO} = 0\n$$\n\n## Determinationskoeffizient $R^2$\n\nEs gilt: $SSTO = SSR + SSE$\n\n$$\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO} \\in [0,1] \n$$\n^[Bei der einfachen Regression gilt: $r_{xy} = \\pm\\sqrt{R^2}$]\n\n### Korrigierter Determinationskoeffizient $R_a^2$\n\n$$\nR_a^2 = 1 - \\frac{\\frac{SSE}{n-p}}{\\frac{SSTO}{n-1}} = 1 - \\frac{n-1}{n-p}\\frac{SSE}{SSTO}\n$$\n\n",
    "supporting": [
      "slm_prediction_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}