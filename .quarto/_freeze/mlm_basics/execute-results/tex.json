{
  "hash": "4d543dc153347d2599b7ce1c497bebdc",
  "result": {
    "markdown": "# Einführung \n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\nIn vielen Fällen in der Praxis liegt selten der einfache Fall vor, dass eine abhängige Variable mitels nur einer einzigen Variable erklärt bzw. vorhergesagt werden soll. Sondern meisten sind mehrere Variablen an dem Prozess der modelliert werden soll beteiligt. Ein einfaches Beispiel aus der Literatur ist der Zusammenhang zwischen der Wurfgeschwindigkeit beim Handball in Abhängigkeit vom Körpergewicht und der Armspannweite. In @tbl-debanne-2011 ist ein Ausschnitt aus einem möglichen Datensatz abgebildet.\n\n\n\n::: {#tbl-debanne-2011 .cell tbl-cap='Datenausschnitt: Wurfgeschwindigkeit, Körpermasse und Armspannweite bei professionellen Handballern (angelehnt an Debanne \\\\& Laffaye, 2011).'}\n::: {.cell-output-display}\n| Velocity[m/s]| body mass[kg]| arm span[cm]|\n|-------------:|-------------:|------------:|\n|          15.8|          70.7|        189.2|\n|          17.2|          63.7|        182.0|\n|          18.3|          76.2|        192.1|\n|          18.4|          64.9|        171.1|\n|          18.4|          63.0|        181.1|\n:::\n:::\n\n\nIm Prinzip könnte der isolierte Einfluss der beiden Prädiktorvariablen Körpermasse und Armspannweite auf die Wurfgeschwindigkeit untersucht werden. Allerdings ist den meisten Fällen von größerem Interesse wie sich die beiden Variablen zusammen verhalten und ob durch die Kombination der beiden Variablen ein besseres Modell der Daten erstellt werden kann.\n\nAus dieser Problemstellung heraus ergibt sich die Notwendigkeit von der einfachen linearen Regression auf eine multiple multiple lineare Regression überzugehen. Formal, geschieht dies einfach dadurch, dass die Formel der einfachen Regression mit dem Prädiktor $x$ um eine zweite Variable erweitert wird.\n\nDementsprechend wird aus:\n\n\\begin{equation}\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\end{equation}\n\ndie Formel für die multiple Regression mit:\n\n\\begin{equation}\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_K x_{Ki} + \\epsilon_i\n\\end{equation}\n\nDa bei der einfachen Regression nur eine einzige $x$-Variable in der Formel vorhanden war, ist kein zusätzlicher Index notwendig gewesen, bei der mutliplen Regression mit mehreren Prädiktorvariablen $x$ wird jeder $x$ Variabler ein zusätzlicher Index $j$ angehängt um die Variablen eindeutig zu identifizieren. Per Konvention, wobei diese leider nicht global eingehalten wird, wird die Anzahl der Prädiktorvaiablen mit $K$ bezeichnet. Der y-Achsenabschnitt erhält den Index $j=0$ und die weiteren Steigungskoeffzienten $\\beta_1$ bis $\\beta_K$ erhalten den Prädiktorvariablen $x_j$ entsprechden Index.\n\nIn welcher Reihenfolge die Prädiktorvariablen mit $j=1, j=2, \\ldots, j=K$ verteilt werden hat zunächst keine Auswirkung auf das Modell und regelt lediglich die Bezeichnung. In unserem konkreten Fall der Handballwurfdaten wäre zum Beispiel eine mögliche Zuordnung, das $x_1$ die Körpermasse und $x_2$ die Armspannweite kodiert.\n\n\n::: {.cell}\n::: {.cell-output-display}\n| $i$| Velocity[m/s]| body mass[kg] $j=1$| arm span[cm] $j=2$|\n|---:|-------------:|-------------------:|------------------:|\n|   1|          15.8|                70.7|              189.2|\n|   2|          17.2|                63.7|              182.0|\n|   3|          18.3|                76.2|              192.1|\n|   4|          18.4|                64.9|              171.1|\n|   5|          18.4|                63.0|              181.1|\n:::\n:::\n\n\nRein formal haben wir jetzt schon den Übergang zur multiple Regression vollzogen. Die Frage die sich natürlich direkt anschließt bezieht sich nun auf die Bedeutung der Koeffizienten $\\beta_1, \\ldots, \\beta_k$. \n\n## Bedeutung der Koeffizienten bei der multiplen Regression \n\nUm die Bedeutung der Regressionskoeffzienten bei der multiple Regression besser zu verstehen ist es von Vorteil sich noch einmal die Bedeutung der Koeffizienten im einfachen Regressionsmodell zu vergegenwärtigen (siehe @fig-mlm-basics-simple).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Beispiel für eine einfache Regression und der resultierenden Regressiongeraden](mlm_basics_files/figure-pdf/fig-mlm-basics-simple-1.pdf){#fig-mlm-basics-simple}\n:::\n:::\n\n\nBei der einfachen Regression haben mittels der Methode der kleinsten Quadrate eine Regressiongerade durch unsere Punktwolke gelegt. Dabei haben wir die Regressionsgerade so gewählt, dass die senkrechten Abstände der beobachteten Punkte von der Regressionsgerade minimiert werden bzw. die Abstände zwischen denen auf der Gerade liegenden, vorhergesagten Werte $\\hat{y}_i$ und den beobachteten Wert $y_i$.\n\nWenn wir nun den Übergang von einer Prädiktorvariablenzum nächstkomplizierteren Fall nehmen mit zwei Prädiktorvariablen $x_1$ und $x_2$, dann wäre eine mögliche Darstellungsform der Daten eine Punktwolke im dreidimensionalen Raum (siehe @fig-mlm-basics-scatter-1). \n\n\n\n::: {#fig-mlm-basics-scatter .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![3D Punktwolke](mlm_basics_files/figure-pdf/fig-mlm-basics-scatter-1.pdf){#fig-mlm-basics-scatter-1}\n:::\n\n::: {.cell-output-display}\n![3D Punktwolke mit gefitteter Ebene](mlm_basics_files/figure-pdf/fig-mlm-basics-scatter-2.pdf){#fig-mlm-basics-scatter-2}\n:::\n\nPunktwolken bei der multiple Regression\n:::\n\n\nDa jetzt eine einzelne Gerade nicht mehr in der Lage ist die Daten zu fitten, ist die nächst Möglichkeit eine Ebene die in die Punktwolke gelegt wird (siehe @fig-mlm-basics-scatter-2). Dies ermöglicht dann genau die gleiche Herangehensweise wie bei der einfachen linearen Regression anzuwenden. Als Zielgröße wird aus den möglichen Ebenen diejenigen gesucht deren vorhergesagten, auf der Ebene liegenden Punkte $\\hat{y}_i$ die geringsten senkrechten Abstand zu den beobachteten Punkten $y_i$ haben. Anders, wir suchen diejenigen Ebene durch die Punktwolke deren Summe der quadrierten Residuen $e_i = y_i - \\hat{y}_i$ minimal ist.\n\nDiese Herangehensweise hat den Vorteil, dass sie zum einem die einfache lineare Regression  als Spezialfall mit $K=1$ beinhaltet und sich beliebig erweitern lässt mit der Einschränkung, dass bei $K>2$ die dreidimenionale Darstellung mittels einer Grafik nicht mehr möglich ist. Das Prinzip der Minimierung der Abweichungen von $\\hat{y}_i$ zu $y$ bleibt aber immer erhalten. Zusammenfassend hat dieser Ansatz somit die folgenden Vorteile:\n\n- Die Berechnungen bleiben alle gleich\n- Abweichungen $\\hat{\\epsilon_i}$ sind jetzt nicht mehr Abweichungen von einer Gerade sondern von einer $K$-dimensionalen Hyperebene. Die Eigenschaften der Residuen bleiben aber alle erhalten. \n- Die Modellannahmen bleiben gleich: Unabhängige $y_i$ und $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ iid \n- Inferenz für die Koeffizienten mittels $t_k = \\frac{\\hat{\\beta}_k}{s_k} \\sim t(N-K-1)$ (Konfidenzintervall dito)\n- Konzepte für die Vorhersage bleiben erhalten\n- Modelldiagnosetools bleiben alle erhalten\n\nAls nächster Schritt versuchen wir nun die Interpretation der Koeffizienten  im multiplen Regressionsmodell besser zu verstehen.\n\n## Einfaches Beispiel \n\n\n\\begin{align*}\ny_i &= \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon_i \\\\\n\\beta_0 &= 1 ,\\beta_1 = 3, \\beta_2 = 0.7 \\\\\n\\epsilon_i &\\sim N(0,\\sigma = 0.5)\n\\end{align*}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- 50 # Anzahl Datenpunkte\nbeta_0 <- 1\nbeta_1 <- 3\nbeta_2 <- 0.7\nsigma <- 0.5\nset.seed(123)\ndf <- tibble(\n  x1 = runif(N, -2, 2),\n  x2 = runif(N, -2, 2),\n  y = beta_0 + beta_1*x1 + beta_2*x2 + \n    rnorm(N, 0, sigma)) \n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Einfacher Zusammenhang y~x1](mlm_basics_files/figure-pdf/unnamed-chunk-7-1.pdf)\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Einfacher Zusammenhang y~x2](mlm_basics_files/figure-pdf/unnamed-chunk-8-1.pdf)\n:::\n:::\n\n\n\n## Wie sieht der Fit aus?\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.20883 -0.26741 -0.00591  0.27315  1.01322 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.07674    0.06552   16.43  < 2e-16 ***\nx1           2.96537    0.05604   52.91  < 2e-16 ***\nx2           0.70815    0.05961   11.88 9.27e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4604 on 47 degrees of freedom\nMultiple R-squared:  0.9849,\tAdjusted R-squared:  0.9842 \nF-statistic:  1529 on 2 and 47 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Was bedeuten die einzelnen Koeffizienten?\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Modellfit\n\n|            | $\\hat{\\beta}$| $s_e$|\n|:-----------|-------------:|-----:|\n|(Intercept) |         1.077| 0.066|\n|x1          |         2.965| 0.056|\n|x2          |         0.708| 0.060|\n:::\n:::\n\n\nDer Unterschied in der abhängigen Variablen, wenn zwei Objekte sich in $x_i$ um eine Einheit unterscheiden und die paarweise gleichen Werte in den verbleibenden $x_j, j \\neq i$ annehmen.\n\n## Was bedeuten die Koeffizienten in Kombination?\n\n### Full model\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Modellfit\n\n|            | $\\hat{\\beta}$| $s_e$|\n|:-----------|-------------:|-----:|\n|(Intercept) |         1.077| 0.066|\n|x1          |         2.965| 0.056|\n|x2          |         0.708| 0.060|\n:::\n:::\n\n\n### um x2 bereinigt\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_x1_x2 <- lm(x1 ~ x2, df)\nres_mod_x1_x2 <- resid(mod_x1_x2)\nmod_x1_res <- lm(y ~ res_mod_x1_x2, df)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error t value\n(Intercept)       1.25       0.16    7.61\nres_mod_x1_x2     2.97       0.14   20.97\n```\n:::\n:::\n\n\n### um x1 bereinigt\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_x2_x1 <- lm(x2 ~ x1, df)\nres_mod_x2_x1 <- resid(mod_x2_x1)\nmod_x2_res <- lm(y ~ res_mod_x2_x1, df)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error t value\n(Intercept)       1.25       0.51    2.44\nres_mod_x2_x1     0.71       0.47    1.51\n```\n:::\n:::\n\n\n## Was bedeuten die Koeffizienten in Kombination?\n\n- $\\hat{\\beta}_1$: Wenn ich $x_2$ weiß, welche zusätzlichen Informationen bekomme ich durch $x_1$ \n- $\\hat{\\beta}_2$: Wenn ich $x_1$ weiß, welche zusätzlichen Informationen bekomme ich durch $x_2$ \n\nIn Beispiel nicht problematisch, weil nach Konstruktion $x_1$ und $x_2$ unabhängig voneinander sind:\n\n::: {.cell}\n\n```{.r .cell-code}\nround(cor(df),3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      x1    x2     y\nx1 1.000 0.078 0.969\nx2 0.078 1.000 0.289\ny  0.969 0.289 1.000\n```\n:::\n:::\n\n\n## Added-variable plots\n\n::: {.cell}\n::: {.cell-output-display}\n![Zusammenhang zwischen y und x2 bereinigt um den Einfluß von x1.](mlm_basics_files/figure-pdf/unnamed-chunk-17-1.pdf)\n:::\n:::\n\n\n\n## Added-variable plots mit `car::avPlots()`\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::avPlots(mod, ~x2)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](mlm_basics_files/figure-pdf/unnamed-chunk-19-1.pdf){fig-align='center'}\n:::\n:::\n\n\n## Was passiert wenn ich einen Prädiktor weg lasse?\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Modellfit\n\n|            | $\\hat{\\beta}$| $s_e$|\n|:-----------|-------------:|-----:|\n|(Intercept) |         1.077| 0.066|\n|x1          |         2.965| 0.056|\n|x2          |         0.708| 0.060|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm(y ~ x1, df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)          x1 \n   1.007466    3.017589 \n```\n:::\n\n```{.r .cell-code}\ncoef(lm(y ~ x2, df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)          x2 \n  1.3377771   0.9555316 \n```\n:::\n:::\n\n\nIn unserem Beispiel wieder nicht viel, da die Variablen unabhängig (orthogonal) voneinander sind.\n\n\n## Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Ausschnitt von Körperfettdaten\n\n| triceps| thigh| midarm| body_fat|\n|-------:|-----:|------:|--------:|\n|    19.5|  43.1|   29.1|     11.9|\n|    24.7|  49.8|   28.2|     22.8|\n|    30.7|  51.9|   37.0|     18.7|\n|    29.8|  54.3|   31.1|     20.1|\n|    19.1|  42.2|   30.9|     12.9|\n|    25.6|  53.9|   23.7|     21.7|\n:::\n:::\n\n\n^[Beispiel nach @kutner2005]\n\n\n## Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n::: {.cell}\n\n```{.r .cell-code}\nGGally::ggpairs(bodyfat) + theme(text = element_text(size = 10))\n```\n\n::: {.cell-output-display}\n![Korrelationsmatrize](mlm_basics_files/figure-pdf/unnamed-chunk-23-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Alle drei Prädiktoren\nmod_full <- lm(body_fat ~ triceps + thigh + midarm, bodyfat)\n# ohne Arm\nmod_wo_midarm <- lm(body_fat ~ triceps + thigh, bodyfat)\n# Ohne Oberschenkel\nmod_wo_thigh <- lm(body_fat ~ triceps + midarm, bodyfat)\n# Ohne Triceps\nmod_wo_triceps <- lm(body_fat ~ thigh + midarm, bodyfat)\n```\n:::\n\n\n## Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: full model\n\n|            | $\\hat{\\beta}$|  $s_e$|\n|:-----------|-------------:|------:|\n|(Intercept) |       117.085| 99.782|\n|triceps     |         4.334|  3.016|\n|thigh       |        -2.857|  2.582|\n|midarm      |        -2.186|  1.595|\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\nTable: w/o midarm\n\n|            | $\\hat{\\beta}$| $s_e$|\n|:-----------|-------------:|-----:|\n|(Intercept) |       -19.174| 8.361|\n|triceps     |         0.222| 0.303|\n|thigh       |         0.659| 0.291|\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\nTable: w/o thigh\n\n|            | $\\hat{\\beta}$| $s_e$|\n|:-----------|-------------:|-----:|\n|(Intercept) |         6.792| 4.488|\n|triceps     |         1.001| 0.128|\n|midarm      |        -0.431| 0.177|\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\nTable: w/o triceps\n\n|            | $\\hat{\\beta}$| $s_e$|\n|:-----------|-------------:|-----:|\n|(Intercept) |       -25.997| 6.997|\n|thigh       |         0.851| 0.112|\n|midarm      |         0.096| 0.161|\n:::\n:::\n\n\n## Multikollinearität^[informell nach @kutner2005 [pp. 407]]\n\n- Große Änderungen in den Koeffizienten wenn Prädiktoren ausgelassen/eingefügt werden\n- Koeffizienten haben eine andere Richtung als erwartet\n- Hohe (einfache) Korrelationen zwischen Prädiktoren\n- Breite Konfidenzintervalle für \"wichtige\" Prädiktoren $b_j$ \n\n$$\n\\widehat{\\text{Var}}(b_j) = \\frac{\\hat{\\sigma}^2}{(n-1)s_j^2}\\frac{1}{1-R_j^2} \n$$\n\n\n$R_j^2$ = Multipler Korrelationskoeffizient der Prädiktoren auf Prädiktorvariable $j$.\n\n## Variance Inflation Factor (VIF)\n\n$$\n\\text{VIF}_j = \\frac{1}{1-R_j^2}\n$$\n\n\\vspace{1cm}\n\n:::{.callout-tip}\nWenn VIF > 10 ist, dann deutet dies auf hohe Multikollinearität hin.\n:::\n\n^[Manchmal wird auch Tolerance = $\\frac{1}{VIF}$ betrachtet.]\n\n\n## Variance Inflation Factor (VIF)\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(mod_full) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n triceps    thigh   midarm \n708.8429 564.3434 104.6060 \n```\n:::\n:::\n\n^[car::vif berechnet generalized variance inflation factor wenn Prädiktoren Faktoren oder Polynome sind [@fox2011.] ]\n\nÜblicherweise wird der größte Wert betrachtet um die Multikollinearität zu bewerten.\n\n\n## Wenn Prädiktoren sich gegenseitig maskieren^[adaptiert nach @mcelreath2016]\n\n\n::: {.cell}\n::: {.cell-output-display}\n![x_pos maskiert den Einfluss von x_neg](mlm_basics_files/figure-pdf/unnamed-chunk-30-1.pdf)\n:::\n:::\n\n\n## Wenn Prädiktoren sich gegenseitig maskieren\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Modellfit\n\n|            | $\\hat{\\beta}$| $s_e$|\n|:-----------|-------------:|-----:|\n|(Intercept) |         0.235| 0.135|\n|x\\_pos      |         0.218| 0.147|\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Modellfit\n\n|            | $\\hat{\\beta}$| $s_e$|\n|:-----------|-------------:|-----:|\n|(Intercept) |         0.228| 0.116|\n|x\\_neg      |        -0.618| 0.103|\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Modellfit\n\n|            | $\\hat{\\beta}$| $s_e$|\n|:-----------|-------------:|-----:|\n|(Intercept) |         0.135| 0.096|\n|x\\_pos      |         0.850| 0.123|\n|x\\_neg      |        -0.976| 0.099|\n:::\n:::\n\n\n## Multiple Regression\n\nAus der einfachen Regression\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n$$\n\nwird\n\n$$\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_K x_{Ki} + \\epsilon_i\n$$\n\nmit K Prädiktorvariablen und Multikollinearität.\n\n\n## Zum Nacharbeiten\n\n@pos_multiple_regression \\newline\n@kutner2005 [p.278-288] \\newline\n@fox2011 [p.325-327]\n\n",
    "supporting": [
      "mlm_basics_files\\figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}