{"title":"Einführung","markdown":{"headingText":"Einführung","containsRefs":false,"markdown":"\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\nsource('_common.R')\n```\n\n## Back to school\n\nWir beginnen mit ein Konzept das wir schon alle kennen. Nämlich der Punkt-Steigungsform aus der Schule (siehe @eq-slm-psform-1).\n\n$$\ny = m x + b\n$${#eq-slm-psform-1}\n\nWir haben eine abhängige Variable $y$ und eine lineare Formel $mx + b$ die den funktionalen Zusammenhang zwischen den Variablen $y$ und $x$ beschreibt. Um das Ganze einmal konkret zu machen setzen wir $m = 2$ und $b = 3$ fest. Die Formel @eq-slm-psform-1 wird dann zu:\n\n```{r}\nm <- 2\nb <- 3\n```\n\n$$\ny = `r m` x + `r b`\n$${#eq-slm-psform-2}\n\nUm ein paar Werte für $y$ zu erhalten setzen wir jetzt verschiedene Wert für $x$ ein indem wir $x$ in Einserschritten zwischen $[0, \\ldots, 5]$ erhöhen. Um die Werte darzustellen verwenden wir zunächst eine Tabelle (vlg. @tbl-slm-psform)\n\n```{r}\n#| tbl-cap: \"Tabelle der Daten\"\n#| label: tbl-slm-psform\n\ntib_psf <- tibble(x = 0:5, y = m * x + b)\nkable(tib_psf,\n      booktabs = T,\n      linesep = '')\n\n```\n\nWenig überraschend nimmt $y$ für den Wert $x = 0$ den Wert $`r b`$ an und z.B. für den Wert $x = 3$ nimmt $y$ den Wert $`r m` \\cdot 3 + `r b` = `r m*3+b`$ an. \n\nEine andere Darstellungsform ist naturlich eine graphische Darstellung in dem wir die Werte von $y$ gegen $x$ auf einem Graphen abtragen (siehe @fig-slm-psform-1).\n\n```{r}\n#| fig.cap: \"Graphische Darstellung der Daten aus @tbl-slm-psform\"\n#| label: fig-slm-psform-1\n\nggplot(tib_psf, aes(x,y)) +\n  geom_line() + \n  geom_point(size=4)\n\n```\n\nWiederum wenig überraschen sehen wir einen linearen Zuwachs der $y$-Wert mit den größerwerdenden $x$-Werte. Da in der Definition der Formel @eq-slm-psform-2 nirgends festgelegt wurde, dass diese nur für ganzzahlige $x$-Werte gilt, haben wir direkt eine Gerade durch die Punkte gelegt. Hier wird auch die Bedeutung von $m$ und $b$ direkt klar. Die Variable $m$ bestimmt die Steigung der Gleichung während $b$ den y-Achsenabschnitt beschreibt. \n\n::: {#def-slm-basics-y-intercept}\n## $y$-Achsenabschnitt\n\nDer y-Achsenabschnitt ist der Wert den $y$ einnimmt wenn $x$ den Wert $0$ annimmt. Sei $y$ durch eine lineare Gleichung $y = mx + b$ definiert, dann wird der y-Achsenabschnitt durch den Wert $b$ bestimmt.\n\n:::\n\nDie Variable $m$ dahingehend bestimmt die Steigung der Gerade. \n\n::: {#def-slm-basics-slope}\n\nWenn $y$ durch eine lineare Gleichung $y = mx + b$ definiert ist, dann bestimmt die Variable $m$ die Steiung der dazugehörenden Gerade. D.h. wenn sich die Variable $x$ um einen Einheit vergrößert (verkleinert) wird der Wert von $y$ um $m$ Einheiten größer (kleiner). Gilt $m < 0$ dann umgekehrt.\n\n:::\n\nDiese beiden trivialen Konzepte mit eigenen Definitionen zu versehen erscheint im ersten Moment vielleicht etwas übertrieben. Wie sich allerdings später zeigen wird, sind diese beiden Einsichten immer wieder zentral wenn es um die Interpretation von linearen statistischen Modellen geht.\n\nSoweit so gut. Führen wir direkt ein paar Symbole ein, die uns später noch behilflich sein werden. Sei jetzt die Menge der $x$-Werte geben $x = [0, 1, 2, 3, 4, 5]$. Strenggenommen handelt es sich wieder um ein Tupel, da wir jetzt die Reihenfolge nicht mehr ändern. Wir führen nun einen Index $i$ ein, um einzelne Werte in dem Tupel über ihre Position zu bestimmen und wir hängen diesen Index $i$ an $x$ an. Dann wird aus $x$, $x_i$.\n\n```{r}\n#| tbl-cap: \"$x$-Werte und ihr Index $i$\"\n#| label: tbl-slm-basic-index\n\nkable(tibble(i = 1:6, x = 0:5),\n      booktabs = T,\n      linesep = '',\n      col.names = c('Index $i$', '$x$-Wert'))\n```\n\nDamit können wir jetzt einen speziellen Wert zum Beispiel den dritten Wert mit $x_3 = 2$ bestimmen. Wenden wir unseren Index auf unsere @eq-slm-psform-1 an, folgt daraus, dass $y$ jetzt auch einen Index $i$ erhält.\n\n$$\ny_i = m x_i + b \\qquad i \\text{ in } [1,2,3,4,5,6]\n$$\n\nWir bezeichnen die beiden Variablen $m$, die Steigung, und $b$, den y-Achsenabschnitt, jetzt auch mit neuen Variablen die auch noch einen Index erhalten. Aus $m$ wird $\\beta_1$ und aus $b$ wird $\\beta_0$. Damit wird der y-Achsenabschnitt mit $\\beta_0$ bezeichnet und die Steigung wird mit $\\beta_1$ bezeichnet. Dann wir aus unserer Gleichung:\n\n$$\ny_i = \\beta_0 + \\beta_1 x_i\n$$ {#eq-slm-psform-beta}\n\nDas ist immer noch unsere einfache Punkt-Steigungsform, wir haben lediglich den Index $i$ eingeführt um unterschiedliche $y-x$-Wertepaare zu bezeichnen und wir haben den $y$-Achsenabschnitt und die Steigung mit neuen Symbolen versehen.\n\nBei dem bisherigen Zusammenhang handelt es sich um einen funktionalen Zusammenhang\\index{Funktionaler Zusammenhang} zwischen den beiden Variablen $x$ und $y$. Funktional deswegen, weil wir eine definiertes mathematisches Modell angeben können, d.h. wir haben eine mathematische Funktion welche die Beziehung zwischen den beiden Variablen beschreibt. Wenn wir den Wert für $x$ kenne, dann können wir den präzisen Wert für $y$ ausreichen, indem wir ihn in @eq-slm-psform-1 einsetzen. Aus der Schule kennen wir auch noch die Darstellung $y = f(x)$. Streng genommen ist diese Darstellung für @eq-slm-psform-1 nicht ausreichend, denn um den Wert für $y$ auszurechnen benötigen wir auch noch Kenntnis über die Werte $m$ und $b$, bzw. in unsere weiteren Darstellung $\\beta_0$ und $\\beta_1$. Daher sollte der Zusammenhang eigentlich mit $y = f(x, \\beta_0, \\beta_1)$ bezeichnet werden. Es gilt aber immernoch, für gegebene $x, \\beta_0$ und $\\beta_1$ ist der Wert für $y$ fest determiniert.\n\nWenn wir mit realen Daten arbeiten, dann funktioniert dieser Ansatz leider nicht ganz. Selbst wenn wir ein Experiment gleich durchführen werden wir immer etwas unterschiedliche Werte im Sinne der Messungenauigkeit messen. Wenn wir biologische Systeme messen, kommt dazu das diese in den seltensten Fällen zeitstabil sind sondern immer bestimmte Veränderungen von einem Zeitpunkt zum nächsten auftauchen. In @fig-slm-basics-jump-1 sind Sprungweiten von mehreren Weitspringerinnen gegen die Anlaufgeschwindigkeit abgetragen. Bei der Betrachtung der Daten erscheint ein linearer Zusammenhang zwischen diesen beiden Variablen durchaus als plausibel. \n\n```{r defs_reg_01}\njump <- readr::read_delim('data/running_jump.csv',\n                          delim=';',\n                          col_types = 'dd')  \njump <- jump |> add_column(i = 1:dim(jump)[1], .before=1)\nmod_jump <- lm(jump_m ~ v_ms, jump)\njump <- jump |> mutate(y_hat = predict(mod_jump))\n```\n\n```{r}\n#| fig.cap: \"Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite beim Weitsprung\"\n#| label: fig-slm-basics-jump-1 \n#| fig.height: 4\n\nid_mark <- which(jump$v_ms > 9 & jump$v_ms < 9.1)[-2]\nggplot(jump, aes(v_ms, jump_m)) +\n  geom_point(data = jump[id_mark,], color = 'red', size = 3) +\n  geom_point(size=2) + \n  labs(x = 'Anlaufgeschwindigkeit[m/s]',\n       y = 'Sprungweite[m]') \n```\n\nIn @fig-slm-basics-jump-1 sind zwei Punkte rot markiert. Die beiden Werte haben praktisch die gleichen $x$-Werte allerdings unterscheiden sich die $y$-Werte deutlich von einander. Und dies sind nicht die einzigen Beispielpaare bei denen die $x$-Werte nahe beiandern liegen, während die $y$-Werte deutlich weiter voneiander entfernt liegen als bei einen funktionalen Zusammenhang nach @eq-slm-psform-1 zu erwarten wäre. Diese Abweichungen kommen durch zufällige Einflussfaktoren wie eben zum Beispiel die Veränderungen angesprochener biologischer Faktoren, Messunsicherheiten, beim Weitsprung draußen sind auch immer externe Einflüsse mögliche, vielleicht wenn es sich um den gleichen Springer handelt, hat er auch beim zweiten Mal keine Lust mehr gehabt. Wenn die Punkte zwei unterschiedliche Springer sind, dann kommt auch dazu, dass zwei Weitspringer bei identischer Anlaufgeschwindigkeit unterschiedliche Sprungfähigkeiten haben oder auch technisch nicht gleich gesprungen sind und so weiter und so fort. Insgesamt führen alle diese Einflüsse dazu, dass wir nicht mehr einen streng funktionalen Zusammenhang zwischen unseren beiden Variablen $x$ der Anlaufgeschwindigkeit und $y$ der Sprungweite vorfinden. Wie wir mit diesen Einflüssen umgehen ist das zentrale Thema des nächsten Abschnitts und markiert auch unseren Eingang zur einfachen linearen Regression.\n\n## Die einfache lineare Regression\n\nBleiben wir bei unserem Beispiel aus @fig-slm-basics-jump-1 und interpretieren das als praktisches Problem. Wir sind eine Weitsprungtrainerin und stehen jetzt vor der Aufgabe in unserem Training etwas zu verändern um die Weitsprungleistung zu verbessern. Wir haben wir haben uns dazu entschlossen am Anlauf etwas zu verbessern wissen jetzt aber nicht ob, das wirklich lohnenswert ist. Von einer befreundeten Trainerin haben wir einen Datensatz bekommen von Anlaufgeschwindigkeiten und den dazugehörigen Sprungweiten. Schauen wir uns zunächst die einmal die Struktur der Daten an.\n\n```{r}\n#| label: tbl-slm-basics-jump-1\n#| tbl-cap: \"Ausschnitt der Sprungdaten\"\n\njump |> select(jump_m, v_ms) |>\n  head(n = 7) |>  \n  kable(booktabs=TRUE,\n               linesep=\"\",\n               digits = 2)\n```\n\nIn @tbl-slm-basics-jump-1 ist ein Ausschnitt Sprungdaten abgebildet. Wir haben eine einfache Struktur der Daten. Wir haben eine Tabelle mit zwei Spalten. `jump_m` bezeichnet die Sprungweiten und `v_ms` die Anlaufgeschwindigkeiten. Damit wir die Datenpaare voneinander unterscheiden bzw. identifzieren können führen wir unseren bereits besprochenen Index $i$ und können so einzelne Paare ansprechen.\n\n```{r}\n#| label: tbl-slm-basics-jump-2\n#| tbl-cap: \"Ausschnitt der Sprungdaten\"\njump |> select(i, jump_m, v_ms) |>\n  head(n = 7) |>  \n  kable(booktabs=TRUE,\n               linesep=\"\",\n               digits = 2)\n```\n\nDas waren bisher aber nur Formalitäten. Wir wollen jetzt denn Zusammenhang zwischen den beiden Variablen modellieren. Wir könnten wahrscheinlich auch einfach Pi-mal-Daumen abschätzen wie groß der Zusammenhang ist. Wenn wir jetzt aber einen unserer Läufer haben, der z.B. etwa $9m/s$ anläuft, welchen Vergleichswerte nehmen wir dann aus @fig-slm-basics-jump-1. Den unteren oder den oberen der beiden roten Werte? Oder vielleicht den Mittelwert? Welchen Wert nehmen wir wenn unserer Athlete $9.7m/s$ anläuft. Da haben wir leider keinen Vergleichswert in unserer Tabelle. Daher wäre es schon ganz praktisch eine Formel nach dem Muster von @eq-slm-psform-beta zu haben. Wie wir allerdings schon festgestellt haben, geht dies nicht so einfach da wir eben das Problem mit den Einflussfaktoren haben, die dazu führen, dass die Werte eben nicht streng auf eine Gerade liegen. Somit liegt die Herausforderung nun eine Gerade zu finden die möglichst *genau* die Daten wiederspiegelt.\n\n\n```{r}\n#| fig.cap: \"Mögliche Geraden um den Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite zu modellieren\"\n#| label: fig-slm-basics-line-1\n\nbeta_0 <- coef(mod_jump)[1]\nbeta_1 <- coef(mod_jump)[2]\nggplot(jump, aes(v_ms, jump_m)) + \n  geom_abline(slope = beta_1, intercept = beta_0, col = 'blue', alpha=.3)  +\n  geom_abline(data = tibble(s = beta_1 + seq(-0.1, 0.1, 0.2), y_in = beta_0),\n              aes(slope = s, intercept = y_in), alpha=.1, col='blue') +\n  geom_abline(intercept = 3, slope = 0.3, col = 'blue', alpha=.3) +\n  geom_abline(intercept = -3, slope = 1.2, col = 'blue', alpha=.3) +\n  #geom_segment(aes(x = v_ms, xend = v_ms, yend = jump_m, y = y_hat),\n  #             col = 'red') +\n  geom_point() +\n  #lims(y = c(-10, 10), x = c(-2, 12)) +\n  labs(x = 'Anlaufgeschwindigkeit[m/s]',\n       y = 'Sprungweite[m]') \n```\n\nIn @fig-slm-basics-line-1 sind die Daten zusammen mit verschiedenen möglichen Geraden abgebildet. Eine kurze Überlegung macht schnell klar, dass es im Prinzip unendlich viele unterschiedliche Geraden gibt die durch die Datenpunkte gelegt werden können. D.h. es gibt unendlich viele Kombinationen von $\\beta_0$ und $\\beta_1$, die die jeweiligen Geraden bezeichnen. Daher muss jetzt eine Kriterium gefunden werden, welches ermöglicht aus diesen unendlich vielen Geraden eine auszuwählen die im Sinne des Kriterium optimal ist.\n\nTatsächlich gibt es dort auch verschiedene Möglichkeiten Kriterien anzuwenden, dasjenige dass jedoch am weitesten verbreitet ist aus verschiedenen Gründen sind die quadratierten Abweichungen von der Gerade. Schauen wir uns die Herleitung dazu schrittweise an. In @fig-slm-basics-line-2 ist zur Übersicht nur ein Ausschnitt der Daten zusammen mit einer möglichen Gerade eingezeichnet. Die senkrechten Abweichungen der Geraden zu den jeweiligen Datenpunkten sind rot eingezeichnet. Es ist ersichtlich, dass für diese Wahl der Geraden es zwei Punkte gibt die tatsächlich auch ziemlich genau auf der Geraden liegen während die anderen Punkte zum Teil oberhalb bzw. unterhalb der Geraden liegen. Das Kriterium wäre jetzt dementsprechen die jenige Geraden aus den unendlich vielen zu finden, bei der diese Abweichung ein Minimum annehmen. \n\n$$\n\\text{min}\\sum_{i=1}^n y_i - (\\beta_0 + \\beta_1 x_i) = \\sum_{i=1}^n y_i - \\beta_0 - \\beta_1 x_i\n$$\n\n```{r}\n#| fig.cap: \"Abweichungen der Gerade von der Datenpunkten für die Daten mit eine Anlaufgeschwindigkeit zwischen $8m/s$ und $10m/s$.\"\n#| label: fig-slm-basics-line-2\n\nbeta_0 <- coef(mod_jump)[1]\nbeta_1 <- coef(mod_jump)[2]\nggplot(jump |> filter(v_ms > 8, v_ms < 10), aes(v_ms, jump_m)) + \n  geom_abline(slope = beta_1, intercept = beta_0, col = 'blue', alpha=.3)  +\n  geom_segment(aes(x = v_ms, xend = v_ms, yend = jump_m, y = y_hat),\n               col = 'red') +\n  geom_point(size=3) +\n  labs(x = 'Anlaufgeschwindigkeit[m/s]',\n       y = 'Sprungweite[m]') \n```\n\nUnglücklicherweise haben die einfachen Abweichungen die unhandliche Eigenschaft, dass dann die *Gerade* $y_i = \\hat{y}$ optimal ist. \n\n$$\n\\sum_{i=1}^n y_i - \\hat{y} = \\sum_i^n y_i - \\sum_{i=1}^n \\hat{y} = \\sum_{i=1}^n y_i - n\\hat{y} = \\sum_{i=1}^n y_i - n\\frac{1}{n}\\sum_{i=1}^n y_i = \\sum_{i=1}^n y_i - \\sum_{i=1}^n y_i= 0\n$$\n\nWir können das Kriterium aber auch noch etwas schärfer machen. Wenn wir sagen, dass wir größere Abweichungen stärker gewichten wollen als kleinere Abweichungen. D.h. große Abweichungen zwischen der Gerade und den Datenpunkten sollten stärker berücksichtigt werden, als kleine Abweichungen. Dies können wir erreichen indem wir die Abweichungen noch zusätzlich quadrieren. Dies hat auch noch den Vorteil noch verschiedene andere mathematische Vorteile, unter anderem führt dies dazu, dass wir eine Gerade erhalten, die auch tatsächlich die Steigung der Punkte berücksichtigt und nicht einfache nur eine horizontale Gerade durch die Punkte zeichnet. Dementsprechend erhalten wir die folgende Funktion, die es zu minimieren gilt:\n\n$$\n\\text{min} \\sum_{i=1}^n(y_i - (\\beta_0 + \\beta_1 x_i))^2\n$$ {#eq-slm-basics-rms-1}\n\nDie Abweichungen zwischen der zu findenden Gerade und den Datenpunkten werden als Residuen $e_i$ bezeichnet. Dementsprechend ist die Minimierungsgleichung auch als:\n\n$$\n\\text{min} \\sum_{i=1}^n e_i^2\n$$\ndarzustellen, mit $e_i := y_i - (\\beta_0 + \\beta_1 x_i)$. Führen wir noch eine weitere Bezeichnung $E$ ein, mit der wir die Minimierungsfunktion bezeichnen ($E$ nach englisch error).\n\n$$\nE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2\n$$\n\nDas Minimum läßt sich finden, indem die partiellen Ableitungen von $E$ nach $\\beta_0$ und $\\beta_1$ berechnet werden und, wie wir es aus der Schule kennen, die Ableitungen gleich Null gesetzt werden.\n\n\\begin{align*}\n\\frac{\\partial E}{\\partial \\beta_0} &= -2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) = 0 \\\\\n\\frac{\\partial E}{\\partial \\beta_1} &= -2 \\sum_{i=1}^n x_i (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\end{align*}\n\nDiese Gleichungen lassen sich umstellen und nach $\\beta_0$ und $\\beta_1$ auflösen:\n$$\n\\begin{align}\n\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1} \\bar{x}\n\\end{align} \n$$ {#eq-slm-basics-norm1}\n\n$\\bar{x}$ und $\\bar{y}$ sind wieder die Mittelwerte von $x_i$ und $y_i$. Diese beiden Gleichungen werden als die Normalengleichungen bezeichnet.\n\nWir führen noch einen weiteren Term ein, den vorhergesagten Wert $\\hat{y}_i$ von $y_i$ anhand der Geradengleichung. Das Hütchen über $y_i$ ist dabei immer das Signal dafür, das es sich um einen abgeschätzten Wert handelt. Wenn wir $\\beta_0$ und $\\beta_1$ anhand der Normalengleichung bestimmen, dann sind das mit großer Wahrscheinlichkeit nicht die *wahren* Werte aus der Population, sondern wir haben sie nur anhand der Daten abgeschätzt. Daher bekommen die berechneten Werte ebenfalls ein Hütchen $\\hat{\\beta}_0$ und $\\hat{\\beta}_1$. Insgesamt nimmt die lineare Geradengleichung dann die folgende Form an:\n\n$$\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i\n$$\n\nGraphisch sind die $\\hat{y}_i$s die Werte auf der Geraden für die gegebenen $x_i$-Werte.\n\n```{r}\n#| fig.cap: \"Die vorhergesaten Werte $\\\\hat{y}_i$ auf der Gerade.\"\n#| label: fig-slm-basics-line-3\n\nbeta_0 <- coef(mod_jump)[1]\nbeta_1 <- coef(mod_jump)[2]\nggplot(jump |> filter(v_ms > 8, v_ms < 10), aes(v_ms, jump_m)) + \n  geom_abline(slope = beta_1, intercept = beta_0, col = 'blue', alpha=.3)  +\n  geom_segment(aes(x = v_ms, xend = v_ms, yend = jump_m, y = y_hat),\n               col = 'red') +\n  geom_point(size=3) +\n  geom_point(aes(y = y_hat), color = 'yellow', size=3) +\n  labs(x = 'Anlaufgeschwindigkeit[m/s]',\n       y = 'Sprungweite[m]') \n```\n\nFür den vorliegenden Fall der Weitsprungdaten erhalten wir die Werte für die Koeffizienten nach Einsetzen der beobachteten Werte in @eq-slm-basics-norm1 mit $\\hat{\\beta}_0 = -0.14$ und $\\hat{\\beta}_1 = 0.76$. Somit folgt für die Geradengleichung:\n\n$$\n\\hat{y}_i = -0.14 + 0.76 \\cdot x_i\n$$\n\nWir erhalten die graphische Darstellung der Geradengleichung indem die $x_i$-Werte eingesetzt werden und eine Gerade durch die Punkte gezogen wird. Oder auch einfacher für den größten und den kleinsten $x_i$-Wert.\n\n```{r}\n#| fig.cap: \"Die Regressionsgerade der Sprungdaten.\"\n#| label: fig-slm-basics-line-4\n\nbeta_0 <- coef(mod_jump)[1]\nbeta_1 <- coef(mod_jump)[2]\nggplot(jump, aes(v_ms, jump_m)) + \n  geom_abline(slope = beta_1, intercept = beta_0, col = 'blue')  +\n  geom_point(size=3) +\n  labs(x = 'Anlaufgeschwindigkeit[m/s]',\n       y = 'Sprungweite[m]') \n```\n\nUm uns auch zu vergewissern, dass unsere Berechnungen korrekt sind, schauen wir uns noch einmal an, wie sich $E$ verhält, wenn wir unterschiedliche Kombinationen von Werten für $\\beta_0$ und $\\beta_1$ in die lineare Gleichung einsetzen.\n\n```{r}\n#| fig.cap: \"Heatmap von $log(E)$ für verschiedene Werte von $\\\\beta_0$ und $\\\\beta_1$\"\n#| fig.height: 4\n#| fig.width: 4\n#| label: fig-slm-basics-gradient\n\ncoefs <- coef(mod_jump)\nfoo <- function(ab) {\n  res <- numeric(nrow(ab))\n  for (i in 1:nrow(ab)) {\n    res[i] <- sum((jump$jump_m - ab$a[i] - ab$b[i] * jump$v_ms)**2)\n  } \n  ab |> mutate(z = res)\n}\nmin_coef <- sum( (jump$jump_m - coefs[1] - coefs[2]*jump$v_ms))\nn <- 41 \na <- seq(coefs[1]-.4, coefs[1]+.4, length.out=n)\nb <- seq(coefs[2]-.1, coefs[2]+.1, length.out=n)\nab <- expand_grid(a = a,\n                  b = b)\nab <- foo(ab)\nii_ <- ceiling(41/2)+ ((1:6)-1)*(n*4)\nii <- c(125, 210, 340, 510, 675, 841) \nab_ii <- ab[ii,]\nggplot(ab, aes(a, b, z = log(z))) +\n  geom_contour_filled(show.legend=F) +\n  geom_contour(color='red', show.legend = F) +\n  geom_hline(yintercept = coefs[2], linetype = 'dashed', color = 'black') +\n  geom_vline(xintercept = coefs[1], linetype = 'dashed', color = 'black') +\n  geom_line(data = ab_ii) +\n  geom_point(data = ab_ii, col='yellow', size=2) +\n  geom_point(data = tibble(a = coefs[1], b = coefs[2], z = min_coef), color='black', size=6) +\n  theme(panel.background = element_rect(fill='white')) +\n  scale_colour_manual(aesthetics = 'fill', drop = FALSE,\n                      values = colorRampPalette(c('white','red'))(10)) +\n  labs(x = expression(hat(beta)[0]), y = expression(hat(beta)[1]))\n\n```\n\nIn @fig-slm-basics-gradient sind verschiedene Werte für $E$ in Form einer heatmap dargestellt. Die Abweichungen wurden $log$-transformiert (d.h. der Logarithmus der $E$-Werte wurde berechnet), da sonst die Unterschiede in der diagnaolen Bildrichtung zu schnell wachsen und die Unterschiede nicht mehr so einfach zu erkennen sind. Werte näher an Weiß bedeuten kleine Werte und Werte näher an Rot bedeuten größere Werte von $E$. Das berechnete Paar für $(\\hat{\\beta}_0, \\hat{\\beta}_1)$ mit $\\hat{\\beta}_0 = -0.14$ und $\\hat{\\beta}_1 = 0.76$ ist schwarz eingezeichnet. Die Abbildung zeigt, dass dieses Wertepaar tatsächlich ein Minimum bezüglich der Funktion $E$ ist, da in alle Richtung weg von dem schwarzen Punkt die Werte für $E$ zunehmen. Da wir nur einen Ausschnitt der möglichen Werte sehen, handelt es sich zunächst um eine lokales Minimum aber es lässt sich zeigen, dass es sich dabei auch um ein globales Minimum handelt. Diese Eigenschaft hängt mit der Form der Funktion $E$ zusammen. In @tbl-slm-basics-gradient sind beispielhaft ein paar Werte für $log(E)$ für Paare von $\\beta_0$ und $\\beta_1$ angezeigt, die in @fig-slm-basics-gradient gelb eingezeichnet sind.\n\n```{r}\n#| label: tbl-slm-basics-gradient\n#| tbl-cap: \"Werte von $log(E)$ für verschiedenen Kombinationen von $\\\\beta_0$ und $\\\\beta_1$.\"\n \nkable(ab_ii,\n      booktabs = T,\n      linesep = '',\n      col.names = c('$\\\\beta_0$', '$\\\\beta_1$', '$log(E)$'),\n      digits = 2)\n```\n\n\n### Schritt-für-Schritt Herleitung der Normalengleichungen\n\nUm die Herleitung der Normalengleichungen Schritt-für-Schritt nachvollziehen zu können benötigen wir zunächst einmal ein paar algebraische Tricks.\n\nFür den Mittelwert gilt:\n$$\n\\bar{x} = \\frac{1}{n}\\sum x_i \\Leftrightarrow \\sum x_i = n \\bar{x}\n$$\n\nBei Summen und konstanten $a$ konstant gilt:\n\\begin{align}\n    \\sum a &= n a \\\\\n    \\sum a x_i &= a \\sum x_i \\\\\n    \\sum (x_i + y_i) &= \\sum x_i + \\sum y_i\n\\end{align}\n\nWenn eine Summe abgeleitet wird, kann in die Ableitung in die Summe reingezogen werden.\n$$\n\\frac{d}{d x}\\sum f(x) = \\sum\\frac{d}{d x} f(x)\n$$\n\nHier ein zwei Umformungen bei Summen und dem Kreuzprodukt bzw. dem Quadrat.\n\\begin{alignat}{2}\n && \\sum(x_i-\\bar{x})(y_i-\\bar{y}) \\\\\n \\Leftrightarrow\\mkern40mu && \\sum (x_iy_i-\\bar{x}y_i-x_i\\bar{y}+\\bar{x}\\bar{y}) \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_i y_i - \\sum\\bar{x}y_i - \\sum x_i \\bar{y} + \\sum \\bar{x} \\bar{y} \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu&&  \\sum x_iy_i - n\\bar{x}\\bar{y}-n\\bar{x}\\bar{y}+n\\bar{x}\\bar{y} \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_iy_i - n\\bar{x}\\bar{y} \\nonumber\n\\end{alignat}\n\\begin{alignat}{2}\n&& \\sum(x_i - \\bar{x})^2 \\\\\n \\Leftrightarrow\\mkern40mu && \\sum(x_i^2 - 2 x_i \\bar{x} + \\bar{x}^2) \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum\\bar{x}^2 \\nonumber\\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_i^2 - 2\\bar{x}n\\bar{x} + n\\bar{x}^2 \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_i^2 - n \\bar{x}^2 \\nonumber\n\\end{alignat}\n\\subsection{Herleitung}\n\nZurück zu unserem Problem. Es gilt $E$ zu minimieren:\n\n\\begin{alignat}{2}\n&& E = \\sum e_i^2 = \\sum (y_i - \\hat{y}_i)^2 \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2 \\nonumber\n\\end{alignat}\n\nDie Gleichung hängt von zwei Variablen $\\beta_0$ und $\\beta_1$. Um das Minimum der Gleichung zu erhalten, verfährt man wie in der Schule, indem man die Ableitung gleich Null setzt. Der vorliegenden Fall ist jedoch etwas komplizierter, da die Gleichung von zwei Variablen abhängt. Daher müssen wir die partiellen Ableitungen $\\frac{\\partial}{\\partial \\beta_0}$ und $\\frac{\\partial}{\\partial \\beta_1}$ verwendet. Wir erhalten dadurch ein Gleichungssystem mit zwei Gleichungen (die jeweiligen Ableitungen) in zwei Unbekannten ($\\beta_0$ und $\\beta_1$). Die Lösung erfolgt, indem zuerst eine Gleichung nach der einen Unbekannten umgestellt wird und das Ergebnis dann in die andere Gleichung eingesetzt wird.\n\nWir beginnen mit der partiellen Ableitung nach $\\beta_0$ für den y-Achsenabschnitt.\n(Zurück an die Schule erinnern: Äußere Ableitung mal innere Ableitung)\n\n\\begin{alignat}{2}\n&& \\frac{\\partial \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2}{\\partial \\beta_0} \\\\\n\\Leftrightarrow\\mkern40mu && \\sum\\frac{\\partial}{\\partial \\beta_0}(y_i - \\beta_0- \\beta_1 x_i)^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum 2(y_i - \\beta_0- \\beta_1 x_i) (-1) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && -2 \\sum (y_i - \\beta_0- \\beta_1 x_i) \\nonumber\n\\end{alignat}\nZum minimieren gleich Null setzen.\n\\begin{alignat}{2}\n&& -2 \\sum (y_i - \\beta_0- \\beta_1 x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0- \\beta_1 x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i - \\sum \\beta_0- \\sum \\beta_1 x_i = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && n \\bar{y} - n \\beta_0- \\beta_1 n \\bar{x} = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\bar{y} - \\beta_0- \\beta_1 \\bar{x} = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\bar{y} - \\beta_1 \\bar{x} = \\beta_0\\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\beta_0= \\bar{y} - \\beta_1 \\bar{x} \n\\end{alignat} \n\nEs folgt nach dem gleichen Prinzip die Herleitung für die Steigung $\\beta_1$ und indem die Lösung für $\\beta_0$ eingesetzt wird.\n\n\\begin{alignat}{2}\n&& \\frac{\\partial \\sum (y_i - \\beta_0 - \\beta_1x_i)^2}{\\partial \\beta_1} \\\\\n\\Leftrightarrow\\mkern40mu && \\sum\\frac{\\partial}{\\partial b}(y_i - \\beta_0 - \\beta_1x_i)^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum2(y_i - \\beta_0 - \\beta_1x_i) -x_i \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && -2 \\sum(y_i - \\beta_0 - \\beta_1x_i)x_i\n\\end{alignat}\nWiederum gleich Null setzen.\n\\begin{alignat}{2}\n&& -2 \\sum(y_i - \\beta_0 - \\beta_1x_i)x_i = 0 \\nonumber\\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0 - \\beta_1x_i)x_i = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i x_i - \\beta_0 x_i - \\beta_1x_i x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - \\beta_0 \\sum x_i - b\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n \\beta_0 \\bar{x} - \\beta_1\\sum x_i^2 = 0 \\nonumber\n\\end{alignat}\nEinsetzen der Lösung für $\\beta_0$ führt zu:\n\\begin{alignat}{2}\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n (\\bar{y} - \\beta_1 \\bar{x}) \\bar{x} - \\beta_1\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n\\bar{y}\\bar{x} + n \\beta_1\\bar{x}^2 - \\beta_1\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n\\bar{y}\\bar{x} = \\beta_1 \\sum x_i^2 - \\beta_1n \\bar{x}^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (x_i-\\bar{x})(y_i-\\bar{y}) = \\beta_1 (\\sum x_i^2 - n\\bar{x}^2) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum x_i^2 - n\\bar{x}^2} = \\beta_1\\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\beta_1= \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2} \\nonumber\n\\end{alignat}\n\nSomit erhält man die beiden Normalengleichungen der Regression.\n\nÜber diese beiden Gleichungen erhalten wir die gewünschten Koeffizienten $\\hat{\\beta_0}$ und $\\hat{\\beta_1}$. Die Methode wird als die als die Methode der kleinsten Quadrate\\index{Methode der kleinsten Quadrate} bezeichnet oder im Englischen Root-Mean-Square (RMS)\\index{RMS}.\n\n\n## Was bedeuten die Koeffizienten?\n\nGehen wir zurück nun zu unseren Ausgangsproblem der Weitspringer, was haben wir jetzt durch die Berechnung der Gerade eigentlich gewonnen? Dazu müssen wir erst einmal verstehen was die beiden Koeffizienten $\\hat{\\beta}_0$ und $\\hat{\\beta}_1$ bedeuten. Wenn wir zurück zu @eq-slm-psform-1 gehen, haben die beiden Koeffzienten den $y$-Achsenabschnitt und die Steigung der Geraden beschrieben. In unserem Beispiel haben wir anhand der Daten einen $y$-Achsenabschnitt $\\hat{\\beta}_0$ von $-0.14$ berechnet. D.h ein Weitspringer der mit einer Anlaufgeschwindigkeit von $x = 0$ anläuft, landet $14$cm hinter der Sprunglinie. Dies macht offensichtlich nicht viel Sinn (warum?). Der Grund warum hier ein offensichtlich unrealistischere Wert berechnet wurde, werden wir später noch genauer betrachten. Wir können trotzdem zwei Eigenschaften von $\\hat{\\beta}_0$ beobachten. 1) der Koeffizient hat eine Einheit, nämlich die gleiche Einheit wie die Variable $y$. 2) Ob der Wert zu interpretieren ist, hängt von der Verteilung der Daten ab. Schauen wir uns nun den Steigungskoeffizienten $\\hat{\\beta}_0$ an. Der Steigungskoeffizient in @eq-slm-psform-1 zeigt an, wie sich der $y$-Wert verändert, wenn sich der $x$-Wert um einen Einheit verändert. In unserem Fall welcher Unterschied zu erwarten ist zwischen zwei Weitspringern die sich in der Anlaufgeschwindigkeit um eine $m/s$ unterscheiden. D.h. der Steigungskoeffizient ist ebenfalls in der Einheit der $y$-Variable zu interpretieren.\n\nUnsere Trainerin kann jetzt die berechnete Gerade dazu nehmen um zu überprüfen ob es sich lohnen würde Trainingszeit in den Anlauf zu stecken und welche Verbesserung dort zu erwarten sind. Allerdings fehlt dazu noch etwas, wir wissen nämlich noch nicht ob die berechnete Gerade auch wirklich die Daten gut wiederspiegelt. Im Beispiel erscheint dies anhand der Grafik als relativ plausibel. Das muss aber nicht immer so sein. Wir können nämlich für alle möglichen Daten eine Gerade berechnen ohne das diese Gerade die Daten wirklich auch nur annährend korrekt wiedergibt. In @eq-slm-basics-norm1 steht nirgends für welche Daten die Berechnung nur erlaubt ist.\n\n```{r}\n#| fig.cap: \"Gerade durch einer Parabel folgenden Kurve\"\n#| label: fig-slm-basics-parabel\n\n```\n\n\nWir nehmen noch eine weitere Eigenschaft der Gerade mit, die zunächst nichts mit der Interpretation der Koeffizienten zu tun hat, aber später noch mal von Interesse sein wird. Die Gerade hat nämlich die Eigenschaft durch den Punkt ($\\bar{x}$, $\\bar{y}$) zu gehen. Dies kann daran gesehen werden wenn in die Gleichung $\\bar{x}$ für $x_i$ eingesetzt wird. Anhand der Normalgleichungen kann die Geradengleichung in der Form.\n\n$$\ny_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i = \\underbrace{\\bar{y} - \\hat{\\beta}_1 \\bar{x}}_{\\text{Def. }\\hat{\\beta}_0} + \\hat{\\beta}_1 \\cdot x_i\n$$\n\nWird jetzt für $x_i$ der Wert $\\bar{x}$ eingesetzt folgt daher.\n\n$$\ny_i = \\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 \\bar{x} = \\bar{y}\n$$\n\nD.h. für den Wert $\\bar{x}$ nimmt die Geradengleichung der Wert $\\bar{y}$ an.\n\n## Regression in `R`\n\n### Model fitten mit `lm()`\n```{r, echo=T}\nmod <- lm(jump_m ~ v_ms, data = jump)\nmod\n```\n\n## Formelsyntax in `lm(y ~ x, data)`\n\n\nTable: Formelsyntaxbeispiele für `lm()`\n\n| Modell | Formel |  Erklärung |\n|---|---|---|\n| $y=\\beta_0$ | `y ~ 1` | y-Ab\n| $y=\\beta_0+\\beta x$ | `y ~ x` | y-Ab und StKoef |\n| $y=\\beta_0+\\beta_1x_1+\\beta_2x_2$ | `y ~ x1 + x2` | y-Ab und 2 StKoe |\n\ny-Ab = y-Achsenabshnitt, StKoef = Steigungskoeffizient\n\n## `lm()`-fit mit `summary()` inspizieren\n\n```{r, echo=T}\nsummary(mod)\n```\n\n## `lm()` und ein paar friends...\n\nKoeffizienten und Standardschätzfehler\n```{r, echo=T}\ncoef(mod)\nsigma(mod)\n```\n\nResiduen\n```{r, echo=T}\n# Nur die ersten beiden\n# Residuen\n# damit der Ausdruck\n# auf das Slide passt.\nresid(mod)[1:2]\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":2,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"slm_basics.html"},"language":{},"metadata":{"lang":"de","fig-responsive":true,"quarto-version":"1.2.258","bibliography":["bibliography.bib"],"theme":"cosmo","callout-icon":true},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":4,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":{"text":"\\usepackage{makeidx,multirow}\n\\makeindex\n"},"include-after-body":{"text":"\\printindex\n"},"output-file":"slm_basics.pdf"},"language":{},"metadata":{"block-headings":true,"bibliography":["bibliography.bib"],"documentclass":"scrreprt"},"extensions":{"book":{}}}}}