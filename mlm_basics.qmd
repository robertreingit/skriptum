# Einführung 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

```{r defs_reg_04}
require(plot3D)
lm_coef_ex <- function(mod, max_co=3) round(summary(mod)$coefficients[,1:max_co],2)
```

In vielen Fällen in der Praxis liegt selten der einfache Fall vor, dass eine abhängige Variable mitels nur einer einzigen Variable erklärt bzw. vorhergesagt werden soll. Sondern meisten sind mehrere Variablen an dem Prozess der modelliert werden soll beteiligt. Ein einfaches Beispiel aus der Literatur ist der Zusammenhang zwischen der Wurfgeschwindigkeit beim Handball in Abhängigkeit vom Körpergewicht und der Armspannweite. In @tbl-debanne-2011 ist ein Ausschnitt aus einem möglichen Datensatz abgebildet.


```{r}
#| label: tbl-debanne-2011
#| tbl-cap: 'Datenausschnitt: Wurfgeschwindigkeit, Körpermasse und Armspannweite bei professionellen Handballern (angelehnt an Debanne \\& Laffaye, 2011).' 

handball <- readr::read_delim(
  "vel;body_mass;arm_span\n15.77;70.66;189.16\n17.22;63.68;182\n18.29;76.23;192.11\n18.43;64.94;171.07\n18.43;63;181.06\n18.55;67.24;168.05\n18.91;73.02;178.1\n19.25;63.23;179.05\n19.42;65.98;175.03\n19.73;68.5;170.06\n19.86;67.47;187.09\n20.15;71.42;171.07\n20.18;80.42;181.49\n20.19;63.86;182.06\n20.39;83.74;185.08\n21.12;74.06;185.08\n21.34;82.31;189.1\n21.44;88.27;190.1\n21.63;81.22;187.09\n21.82;80.94;183.94\n21.84;63.76;182\n21.84;70.11;185.08\n21.85;83.4;180.11\n21.87;75.9;184.57\n22.23;81.34;195.88\n22.3;74.41;182\n22.31;93.31;191.17\n22.94;88.44;198.96\n23.05;70.98;189.1\n23.08;79.57;183\n23.15;90.05;190.6\n23.32;75.45;186.08\n23.43;69.95;185.01\n23.61;76.94;176.03\n23.71;88.73;187.02\n24.75;86.79;183\n24.76;109.01;180.99\n24.77;76.2;214.91\n25.67;88.97;192.93\n26.26;108.05;192.86\n",delim=';',col_types='ddd')
handball_short <- handball |> head(n = 5)
handball_short |>  
  kable(col.names=c('Velocity[m/s]','body mass[kg]','arm span[cm]'), 
        booktabs=T,
        linesep = '',
        digits = 1,
        )
```

Im Prinzip könnte der isolierte Einfluss der beiden Prädiktorvariablen Körpermasse und Armspannweite auf die Wurfgeschwindigkeit untersucht werden. Allerdings ist den meisten Fällen von größerem Interesse wie sich die beiden Variablen zusammen verhalten und ob durch die Kombination der beiden Variablen ein besseres Modell der Daten erstellt werden kann.

Aus dieser Problemstellung heraus ergibt sich die Notwendigkeit von der einfachen linearen Regression auf eine multiple multiple lineare Regression überzugehen. Formal, geschieht dies einfach dadurch, dass die Formel der einfachen Regression mit dem Prädiktor $x$ um eine zweite Variable erweitert wird.

Dementsprechend wird aus:

\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\end{equation}

die Formel für die multiple Regression mit:

\begin{equation}
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_K x_{Ki} + \epsilon_i
\end{equation}

Da bei der einfachen Regression nur eine einzige $x$-Variable in der Formel vorhanden war, ist kein zusätzlicher Index notwendig gewesen, bei der mutliplen Regression mit mehreren Prädiktorvariablen $x$ wird jeder $x$ Variabler ein zusätzlicher Index $j$ angehängt um die Variablen eindeutig zu identifizieren. Per Konvention, wobei diese leider nicht global eingehalten wird, wird die Anzahl der Prädiktorvaiablen mit $K$ bezeichnet. Der y-Achsenabschnitt erhält den Index $j=0$ und die weiteren Steigungskoeffzienten $\beta_1$ bis $\beta_K$ erhalten den Prädiktorvariablen $x_j$ entsprechden Index.

In welcher Reihenfolge die Prädiktorvariablen mit $j=1, j=2, \ldots, j=K$ verteilt werden hat zunächst keine Auswirkung auf das Modell und regelt lediglich die Bezeichnung. In unserem konkreten Fall der Handballwurfdaten wäre zum Beispiel eine mögliche Zuordnung, das $x_1$ die Körpermasse und $x_2$ die Armspannweite kodiert.

```{r}
handball_short |> 
  add_column(i = 1:5, .before=1) |> 
  kable(
        col.names=c('$i$', 'Velocity[m/s]','body mass[kg] $j=1$','arm span[cm] $j=2$'), 
        booktabs=T,
        linesep = '',
        digits = 1,
  )
```

Rein formal haben wir jetzt schon den Übergang zur multiple Regression vollzogen. Die Frage die sich natürlich direkt anschließt bezieht sich nun auf die Bedeutung der Koeffizienten $\beta_1, \ldots, \beta_k$. 

## Bedeutung der Koeffizienten bei der multiplen Regression 

Um die Bedeutung der Regressionskoeffzienten bei der multiple Regression besser zu verstehen ist es von Vorteil sich noch einmal die Bedeutung der Koeffizienten im einfachen Regressionsmodell zu vergegenwärtigen (siehe @fig-mlm-basics-simple).

```{r}
#| fig.cap: "Beispiel für eine einfache Regression und der resultierenden Regressiongeraden"
#| label: fig-mlm-basics-simple

set.seed(12)
simple <- tibble::tibble(x = 0:3,
                 y = 2 + 0.5 * x + rnorm(4,0,.5))
mod0 <- lm(y ~ x, simple)
simple$y_hat <- predict(mod0)
simple$epsilon <- paste0('hat(epsilon)[',1:4,']')
simple$ys <- paste0('list(x[',1:4,'],y[',1:4,'])')
simple$yshat <- paste('hat(y)[',1:4,']')
p_res <- ggplot(simple, aes(x,y,label = epsilon)) + 
  geom_point(size=3) +
  geom_line(aes(y = y_hat), size=2, color = 'red') +
  #geom_segment(aes(x = x, y = y_hat, xend = x, yend = y),
  #             color = 'black') +  
  lims(x = c(-.3,3.3), y = c(1,3.8)) 
print(p_res)
```

Bei der einfachen Regression haben mittels der Methode der kleinsten Quadrate eine Regressiongerade durch unsere Punktwolke gelegt. Dabei haben wir die Regressionsgerade so gewählt, dass die senkrechten Abstände der beobachteten Punkte von der Regressionsgerade minimiert werden bzw. die Abstände zwischen denen auf der Gerade liegenden, vorhergesagten Werte $\hat{y}_i$ und den beobachteten Wert $y_i$.

Wenn wir nun den Übergang von einer Prädiktorvariablenzum nächstkomplizierteren Fall nehmen mit zwei Prädiktorvariablen $x_1$ und $x_2$, dann wäre eine mögliche Darstellungsform der Daten eine Punktwolke im dreidimensionalen Raum (siehe @fig-mlm-basics-scatter-1). 


```{r}
#| label: fig-mlm-basics-scatter
#| layout-ncol: 2
#| fig.cap: "Punktwolken bei der multiple Regression"
#| fig-subcap:
#|   - "3D Punktwolke"
#|   - "3D Punktwolke mit gefitteter Ebene"

#par(mar=c(1,1,1,1), cex.axis = 0.5)
# x, y, z variables
x <- mtcars$wt
y <- mtcars$disp
z <- mtcars$mpg
# Compute the linear regression (z = ax + by + d)
fit <- lm(z ~ x + y)
# predict values on regular xy grid
grid.lines = 26
x.pred <- seq(min(x), max(x), length.out = grid.lines)
y.pred <- seq(min(y), max(y), length.out = grid.lines)
xy <- expand.grid( x = x.pred, y = y.pred)
z.pred <- matrix(predict(fit, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)
# fitted points for droplines to surface
fitpoints <- predict(fit)
# scatter plot 
scatter3D(x, y, z, col='black', pch=16, cex=1.2,
    theta = 45, phi = 20, ticktype = "detailed",
    xlab = "x1", ylab = "x2", zlab = "y",  
    surf = NULL, 
    main = "",
    bty="b2",
    colkey=F, d = 4)
# scatter plot with regression plane
scatter3D(x, y, z, col='black', pch=16, cex=1.2,
    theta = 45, phi = 20, ticktype = "detailed",
    xlab = "x1", ylab = "x2", zlab = "y",  
    surf = list(x = x.pred, y = y.pred, z = z.pred,  
    facets = NA, fit = fitpoints, col='red'), main = "", bty="b2",
    colkey=F, d = 4)
```

Da jetzt eine einzelne Gerade nicht mehr in der Lage ist die Daten zu fitten, ist die nächst Möglichkeit eine Ebene die in die Punktwolke gelegt wird (siehe @fig-mlm-basics-scatter-2). Dies ermöglicht dann genau die gleiche Herangehensweise wie bei der einfachen linearen Regression anzuwenden. Als Zielgröße wird aus den möglichen Ebenen diejenigen gesucht deren vorhergesagten, auf der Ebene liegenden Punkte $\hat{y}_i$ die geringsten senkrechten Abstand zu den beobachteten Punkten $y_i$ haben. Anders, wir suchen diejenigen Ebene durch die Punktwolke deren Summe der quadrierten Residuen $e_i = y_i - \hat{y}_i$ minimal ist.

Diese Herangehensweise hat den Vorteil, dass sie zum einem die einfache lineare Regression  als Spezialfall mit $K=1$ beinhaltet und sich beliebig erweitern lässt mit der Einschränkung, dass bei $K>2$ die dreidimenionale Darstellung mittels einer Grafik nicht mehr möglich ist. Das Prinzip der Minimierung der Abweichungen von $\hat{y}_i$ zu $y$ bleibt aber immer erhalten. Zusammenfassend hat dieser Ansatz somit die folgenden Vorteile:

- Die Berechnungen bleiben alle gleich
- Abweichungen $\hat{\epsilon_i}$ sind jetzt nicht mehr Abweichungen von einer Gerade sondern von einer $K$-dimensionalen Hyperebene. Die Eigenschaften der Residuen bleiben aber alle erhalten. 
- Die Modellannahmen bleiben gleich: Unabhängige $y_i$ und $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$ iid 
- Inferenz für die Koeffizienten mittels $t_k = \frac{\hat{\beta}_k}{s_k} \sim t(N-K-1)$ (Konfidenzintervall dito)
- Konzepte für die Vorhersage bleiben erhalten
- Modelldiagnosetools bleiben alle erhalten

Als nächster Schritt versuchen wir nun die Interpretation der Koeffizienten  im multiplen Regressionsmodell besser zu verstehen.

## Einfaches Beispiel 


\begin{align*}
y_i &= \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \epsilon_i \\
\beta_0 &= 1 ,\beta_1 = 3, \beta_2 = 0.7 \\
\epsilon_i &\sim N(0,\sigma = 0.5)
\end{align*}

```{r}
#| echo: true

N <- 50 # Anzahl Datenpunkte
beta_0 <- 1
beta_1 <- 3
beta_2 <- 0.7
sigma <- 0.5
set.seed(123)
df <- tibble(
  x1 = runif(N, -2, 2),
  x2 = runif(N, -2, 2),
  y = beta_0 + beta_1*x1 + beta_2*x2 + 
    rnorm(N, 0, sigma)) 
```

```{r}
#| fig.height=2.2,
#| fig.cap="Einfacher Zusammenhang y~x1"

ggplot(df, aes(x1,y)) + geom_point() 
```

```{r}
#| fig.height=2.2,
#| fig.cap="Einfacher Zusammenhang y~x2"

ggplot(df, aes(x2,y)) + geom_point()
```


## Wie sieht der Fit aus?

```{r}
mod <- lm(y ~ x1 + x2, df)
summary(mod)
```

## Was bedeuten die einzelnen Koeffizienten?

```{r}
lm_tbl_knitr(mod, long=F)
```

Der Unterschied in der abhängigen Variablen, wenn zwei Objekte sich in $x_i$ um eine Einheit unterscheiden und die paarweise gleichen Werte in den verbleibenden $x_j, j \neq i$ annehmen.

## Was bedeuten die Koeffizienten in Kombination?

### Full model
```{r}
#lm_coef_ex(mod)
lm_tbl_knitr(mod, long=F)
```

### um x2 bereinigt
```{r, echo=T}
mod_x1_x2 <- lm(x1 ~ x2, df)
res_mod_x1_x2 <- resid(mod_x1_x2)
mod_x1_res <- lm(y ~ res_mod_x1_x2, df)
```

```{r}
lm_coef_ex(mod_x1_res)
```

### um x1 bereinigt
```{r}
#| echo: true

mod_x2_x1 <- lm(x2 ~ x1, df)
res_mod_x2_x1 <- resid(mod_x2_x1)
mod_x2_res <- lm(y ~ res_mod_x2_x1, df)
```
```{r}
lm_coef_ex(mod_x2_res)
```

## Was bedeuten die Koeffizienten in Kombination?

- $\hat{\beta}_1$: Wenn ich $x_2$ weiß, welche zusätzlichen Informationen bekomme ich durch $x_1$ 
- $\hat{\beta}_2$: Wenn ich $x_1$ weiß, welche zusätzlichen Informationen bekomme ich durch $x_2$ 

In Beispiel nicht problematisch, weil nach Konstruktion $x_1$ und $x_2$ unabhängig voneinander sind:
```{r}
#| echo: true
round(cor(df),3)
```

## Added-variable plots
```{r}
#| fig.cap="Zusammenhang zwischen y und x2 bereinigt um den Einfluß von x1.",
#| fig.height=2,
#| fig.width=3

res_1 <- resid(lm(y ~ x1, df))
res_2 <- resid(lm(x2 ~ x1, df))
ggplot(tibble(x1_res = res_2, y_res = res_1),
       aes(x1_res,y_res)) + geom_point() + labs(x = 'x2 | x1', y = 'y | x1')
```


## Added-variable plots mit `car::avPlots()`
```{r}
#| echo=T,
#| eval=F

car::avPlots(mod, ~x2)
```

```{r}
#| fig.height=2,
#| fig.width=3,
#| fig.align='center'

par(mar=c(3,3,2,1))
car::avPlots(mod, ~x2)
mtext('x2 | others', 1, line=1.8)
mtext('y | others', 2, line=1.9)
```

## Was passiert wenn ich einen Prädiktor weg lasse?

```{r}
lm_tbl_knitr(mod, long=F)
```

```{r}
#| echo=T

coef(lm(y ~ x1, df))
coef(lm(y ~ x2, df))
```

In unserem Beispiel wieder nicht viel, da die Variablen unabhängig (orthogonal) voneinander sind.


## Was passiert wenn Prädiktoren stark miteinander korrelieren?

```{r}
#| fig.height=2

bodyfat <- readr::read_delim(
  "triceps;thigh;midarm;body_fat\n19.5;43.1;29.1;11.9\n24.7;49.8;28.2;22.8\n30.7;51.9;37;18.7\n29.8;54.3;31.1;20.1\n19.1;42.2;30.9;12.9\n25.6;53.9;23.7;21.7\n31.4;58.5;27.6;27.1\n27.9;52.1;30.6;25.4\n22.1;49.9;23.2;21.3\n25.5;53.5;24.8;19.3\n31.1;56.6;30;25.4\n30.4;56.7;28.3;27.2\n18.7;46.5;23;11.7\n19.7;44.2;28.6;17.8\n14.6;42.7;21.3;12.8\n29.5;54.4;30.1;23.9\n27.7;55.3;25.7;22.6\n30.2;58.6;24.6;25.4\n22.7;48.2;27.1;14.8\n25.2;51;27.5;21.1\n",
  delim = ';', col_types = 'dddd'
) 
knitr::kable(head(bodyfat),
             caption = 'Ausschnitt von Körperfettdaten',
             booktabs = TRUE,
             linesep = '')
```

^[Beispiel nach @kutner2005]


## Was passiert wenn Prädiktoren stark miteinander korrelieren?
```{r}
#| echo=T,
#| fig.cap="Korrelationsmatrize",
#| fig.height=2.5

GGally::ggpairs(bodyfat) + theme(text = element_text(size = 10))
```

## Was passiert wenn Prädiktoren stark miteinander korrelieren?

```{r}
#| echo=T

# Alle drei Prädiktoren
mod_full <- lm(body_fat ~ triceps + thigh + midarm, bodyfat)
# ohne Arm
mod_wo_midarm <- lm(body_fat ~ triceps + thigh, bodyfat)
# Ohne Oberschenkel
mod_wo_thigh <- lm(body_fat ~ triceps + midarm, bodyfat)
# Ohne Triceps
mod_wo_triceps <- lm(body_fat ~ thigh + midarm, bodyfat)
```

## Was passiert wenn Prädiktoren stark miteinander korrelieren?

```{r}
lm_tbl_knitr(mod_full, long=F, caption="full model")
```

```{r}
lm_tbl_knitr(mod_wo_midarm, long=F, caption='w/o midarm')
```

```{r}
lm_tbl_knitr(mod_wo_thigh, long=F, caption='w/o thigh')
```

```{r}
lm_tbl_knitr(mod_wo_triceps, long=F, caption='w/o triceps')
```

## Multikollinearität^[informell nach @kutner2005 [pp. 407]]

- Große Änderungen in den Koeffizienten wenn Prädiktoren ausgelassen/eingefügt werden
- Koeffizienten haben eine andere Richtung als erwartet
- Hohe (einfache) Korrelationen zwischen Prädiktoren
- Breite Konfidenzintervalle für "wichtige" Prädiktoren $b_j$ 

$$
\widehat{\text{Var}}(b_j) = \frac{\hat{\sigma}^2}{(n-1)s_j^2}\frac{1}{1-R_j^2} 
$$


$R_j^2$ = Multipler Korrelationskoeffizient der Prädiktoren auf Prädiktorvariable $j$.

## Variance Inflation Factor (VIF)

$$
\text{VIF}_j = \frac{1}{1-R_j^2}
$$

\vspace{1cm}

:::{.callout-tip}
Wenn VIF > 10 ist, dann deutet dies auf hohe Multikollinearität hin.
:::

^[Manchmal wird auch Tolerance = $\frac{1}{VIF}$ betrachtet.]


## Variance Inflation Factor (VIF)
```{r}
#| echo: true

car::vif(mod_full) 
```
^[car::vif berechnet generalized variance inflation factor wenn Prädiktoren Faktoren oder Polynome sind [@fox2011.] ]

Üblicherweise wird der größte Wert betrachtet um die Multikollinearität zu bewerten.


## Wenn Prädiktoren sich gegenseitig maskieren^[adaptiert nach @mcelreath2016]

```{r}
#| fig.height=2.5,
#| fig.cap="x_pos maskiert den Einfluss von x_neg"

N <- 100
set.seed(123)
rho <- 0.7
x_pos <- rnorm(N)
x_neg <- rnorm(N, rho*x_pos)
y <- rnorm(N, x_pos - x_neg)
df <- tibble(y, x_pos, x_neg)
GGally::ggpairs(df) + theme(text = element_text(size = 10))
```

## Wenn Prädiktoren sich gegenseitig maskieren

```{r}
co_1 <- lm(y ~ x_pos, df)
co_2 <- lm(y ~ x_neg, df)
lm_tbl_knitr(co_1, long=F)
```

```{r}
lm_tbl_knitr(co_2, long=F)
```

```{r}
co_3 <- lm(y ~ x_pos + x_neg, df)
lm_tbl_knitr(co_3, long=F)
```

## Multiple Regression

Aus der einfachen Regression

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

wird

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_K x_{Ki} + \epsilon_i
$$

mit K Prädiktorvariablen und Multikollinearität.


## Zum Nacharbeiten

@pos_multiple_regression \newline
@kutner2005 [p.278-288] \newline
@fox2011 [p.325-327]

