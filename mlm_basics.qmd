# Einführung 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

```{r defs_reg_04}
require(plot3D)
lm_coef_ex <- function(mod, max_co=3) round(summary(mod)$coefficients[,1:max_co],2)
ncol_theme <- theme(text = element_text(size=20))
```

In vielen Fällen in der Praxis liegt selten der einfache Fall vor, dass eine abhängige Variable mittels nur einer einzigen Variable erklärt bzw. vorhergesagt werden soll. Sondern meisten sind mehrere Variablen an dem Prozess beteiligt der modelliert werden soll. Ein einfaches Beispiel aus der Literatur ist der Zusammenhang zwischen der Wurfgeschwindigkeit beim Handball in Abhängigkeit vom Körpergewicht und der Armspannweite. In @tbl-debanne-2011 ist ein Ausschnitt aus einem möglichen Datensatz abgebildet.


```{r}
#| label: tbl-debanne-2011
#| tbl-cap: 'Datenausschnitt: Wurfgeschwindigkeit, Körpermasse und Armspannweite bei professionellen Handballern (angelehnt an Debanne \\& Laffaye, 2011).' 

handball <- readr::read_delim(
  "vel;body_mass;arm_span\n15.77;70.66;189.16\n17.22;63.68;182\n18.29;76.23;192.11\n18.43;64.94;171.07\n18.43;63;181.06\n18.55;67.24;168.05\n18.91;73.02;178.1\n19.25;63.23;179.05\n19.42;65.98;175.03\n19.73;68.5;170.06\n19.86;67.47;187.09\n20.15;71.42;171.07\n20.18;80.42;181.49\n20.19;63.86;182.06\n20.39;83.74;185.08\n21.12;74.06;185.08\n21.34;82.31;189.1\n21.44;88.27;190.1\n21.63;81.22;187.09\n21.82;80.94;183.94\n21.84;63.76;182\n21.84;70.11;185.08\n21.85;83.4;180.11\n21.87;75.9;184.57\n22.23;81.34;195.88\n22.3;74.41;182\n22.31;93.31;191.17\n22.94;88.44;198.96\n23.05;70.98;189.1\n23.08;79.57;183\n23.15;90.05;190.6\n23.32;75.45;186.08\n23.43;69.95;185.01\n23.61;76.94;176.03\n23.71;88.73;187.02\n24.75;86.79;183\n24.76;109.01;180.99\n24.77;76.2;214.91\n25.67;88.97;192.93\n26.26;108.05;192.86\n",delim=';',col_types='ddd')
handball_short <- handball |> head(n = 5)
handball_short |>  
  kable(col.names=c('Velocity[m/s]','body mass[kg]','arm span[cm]'), 
        booktabs=T,
        linesep = '',
        digits = 1,
        )
```

Im Prinzip könnte der isolierte Einfluss der beiden Prädiktorvariablen Körpermasse und Armspannweite auf die Wurfgeschwindigkeit jeweils einzeln untersucht werden. Allerdings ist den meisten Fällen von größerem Interesse wie sich die beiden Variablen zusammen verhalten und ob durch die Kombination der beiden Variablen ein besseres Modell der Daten erstellt werden kann. Aus dieser Problemstellung heraus ergibt sich die Notwendigkeit von der einfachen linearen Regression auf eine multiple multiple lineare Regression überzugehen. Formal geschieht dies einfach dadurch, dass die Formel der einfachen Regression mit dem Prädiktor $X$ um eine zweite oder mehr Variablen erweitert wird. Dementsprechend wird aus:

\begin{equation}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\end{equation}

die Formel für die multiple Regression mit:

\begin{equation}
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_K x_{Ki} + \epsilon_i
\end{equation}

Da bei der einfachen Regression nur eine einzige $X$-Variable in der Formel vorhanden war, ist kein Index notwendig gewesen. Bei der mutliplen Regression mit mehreren Prädiktorvariablen $X$ wird jeder $X$ Variabler ein zusätzlicher Index $j$ angehängt um die Variablen eindeutig zu identifizieren. Per Konvention, wobei diese leider nicht global eingehalten wird, wird die Anzahl der Prädiktorvaiablen mit $K$ bezeichnet. Der y-Achsenabschnitt erhält den Index $j=0$ und die weiteren Steigungskoeffzienten $\beta_1$ bis $\beta_K$ erhalten den Prädiktorvariablen $X_j$ entsprechden Index.

In welcher Reihenfolge die Prädiktorvariablen mit $j=1, j=2, \ldots, j=K$ verteilt werden hat zunächst keine Auswirkung auf das Modell und regelt lediglich die Bezeichnung. In unserem konkreten Fall der Handballwurfdaten wäre zum Beispiel eine mögliche Zuordnung, das $X_1$ die Körpermasse und $X_2$ die Armspannweite kodiert.

```{r}
handball_short |> 
  add_column(i = 1:5, .before=1) |> 
  kable(
        col.names=c('$i$', 'Velocity[m/s]','body mass[kg] $j=1$','arm span[cm] $j=2$'), 
        booktabs=T,
        linesep = '',
        digits = 1,
  )
```

Rein formal haben wir jetzt schon den Übergang zur multiple Regression vollzogen. Die Frage die sich natürlich direkt anschließt bezieht sich nun auf die Bedeutung der Koeffizienten $\beta_1, \ldots, \beta_k$ und wie sich die weiteren Schritte die vorher bei der einfachen Regression kennengelernt haben auf die multiple Regression übertragen.

## Bedeutung der Koeffizienten bei der multiplen Regression 

Um die Bedeutung der Regressionskoeffzienten bei der multiple Regression besser zu verstehen ist es von Vorteil sich noch einmal die Bedeutung der Koeffizienten im einfachen Regressionsmodell zu vergegenwärtigen (siehe @fig-mlm-basics-simple).

```{r}
#| fig.cap: "Beispiel für eine einfache Regression und der resultierenden Regressiongeraden"
#| label: fig-mlm-basics-simple

set.seed(12)
simple <- tibble::tibble(x = 0:3,
                 y = 2 + 0.5 * x + rnorm(4,0,.5))
mod0 <- lm(y ~ x, simple)
simple$y_hat <- predict(mod0)
simple$epsilon <- paste0('hat(epsilon)[',1:4,']')
simple$ys <- paste0('list(x[',1:4,'],y[',1:4,'])')
simple$yshat <- paste('hat(y)[',1:4,']')
p_res <- ggplot(simple, aes(x,y,label = epsilon)) + 
  geom_point(size=3) +
  geom_line(aes(y = y_hat), size=2, color = 'red') +
  #geom_segment(aes(x = x, y = y_hat, xend = x, yend = y),
  #             color = 'black') +  
  lims(x = c(-.3,3.3), y = c(1,3.8)) 
print(p_res)
```

Bei der einfachen Regression haben wir mittels der Methode der kleinsten Quadrate eine Regressiongerade durch die Punktwolke der Daten gelegt. Dabei haben wir die Regressionsgerade so gewählt, dass die senkrechten Abstände der beobachteten Punkte von der Regressionsgerade minimiert werden bzw. die Abstände zwischen denen auf der Gerade liegenden, vorhergesagten Werte $\hat{y}_i$ und den beobachteten Wert $Y_i$ (Residuen). Wenn wir nun den Übergang von einer Prädiktorvariablen zum nächstkomplizierteren Fall nehmen mit zwei Prädiktorvariablen $X_1$ und $X_2$, dann wäre eine mögliche Darstellungsform der Daten eine Punktwolke im dreidimensionalen Raum (siehe @fig-mlm-basics-scatter-1). 


```{r}
#| label: fig-mlm-basics-scatter
#| layout-ncol: 2
#| fig.cap: "Punktwolken bei der multiple Regression"
#| fig-height: 5
#| fig-subcap:
#|   - "3D Punktwolke"
#|   - "3D Punktwolke mit gefitteter Ebene"

par(mar=c(1,1,1,1), cex.axis = 0.5)
# x, y, z variables
x <- mtcars$wt
y <- mtcars$disp
z <- mtcars$mpg
# Compute the linear regression (z = ax + by + d)
fit <- lm(z ~ x + y)
# predict values on regular xy grid
grid.lines = 26
x.pred <- seq(min(x), max(x), length.out = grid.lines)
y.pred <- seq(min(y), max(y), length.out = grid.lines)
xy <- expand.grid( x = x.pred, y = y.pred)
z.pred <- matrix(predict(fit, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)
# fitted points for droplines to surface
fitpoints <- predict(fit)
# scatter plot 
scatter3D(x, y, z, col='black', pch=16, cex=1.2,
    theta = 45, phi = 20, ticktype = "detailed",
    xlab = "x1", ylab = "x2", zlab = "y",  
    surf = NULL, 
    main = "",
    bty="b2",
    colkey=F, d = 4)
# scatter plot with regression plane
scatter3D(x, y, z, col='black', pch=16, cex=1.2,
    theta = 45, phi = 20, ticktype = "detailed",
    xlab = "x1", ylab = "x2", zlab = "y",  
    surf = list(x = x.pred, y = y.pred, z = z.pred,  
    facets = NA, fit = fitpoints, col='red'), main = "", bty="b2",
    colkey=F, d = 4)
```

Da jetzt eine einzelne Gerade nicht mehr in der Lage ist die Daten zu fitten, ist die nächste Möglichkeit eine Ebene in die Punktwolke zu legen (siehe @fig-mlm-basics-scatter-2). Dies ermöglicht dann genau die gleiche Herangehensweise wie bei der einfachen linearen Regression anzuwenden. Als Zielgröße wird aus den möglichen Ebenen diejenigen gesucht deren vorhergesagten, auf der Ebene liegenden, Punkte $\hat{y}_i$ die geringsten senkrechten Abstand zu den beobachteten Punkten $y_i$ haben (wieder Residuen). Anders, wir suchen diejenigen Ebene durch die Punktwolke deren Summe der quadrierten Residuen $e_i = y_i - \hat{y}_i$ minimal ist.

Diese Herangehensweise hat den Vorteil, dass sie zum einem die einfache lineare Regression  als Spezialfall mit $K=1$ beinhaltet und sich beliebig erweitern lässt mit der Einschränkung, dass bei $K>2$ die dreidimenionale Darstellung mittels einer Grafik nicht mehr möglich ist. Das Prinzip der Minimierung der Abweichungen von $\hat{y}_i$ zu $y$ bleibt aber immer erhalten.

Zusammenfassend: 

- Die Berechnungen bleiben alle gleich
- Abweichungen $\hat{\epsilon_i}$ sind jetzt nicht mehr Abweichungen von einer Gerade sondern von einer $K$-dimensionalen Hyperebene. Die Eigenschaften der Residuen bleiben aber alle erhalten. 
- Die Modellannahmen bleiben gleich: Unabhängige $y_i$ und $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$ iid 
- Inferenz für die Koeffizienten mittels $t_k = \frac{\hat{\beta}_k}{s_k} \sim t(N-K-1)$ (Konfidenzintervall dito)
- Konzepte für die Vorhersage bleiben erhalten
- Modelldiagnosetools bleiben alle erhalten

Als nächster Schritt versuchen wir nun die Interpretation der Koeffizienten  im multiplen Regressionsmodell besser zu verstehen.

## Einfaches Beispiel 

Fangen wir mit einem einfachen Beispiel an. Wir gehen von einem DGP mit den folgenden Modellwerten aus.

\begin{align*}
y_i &= \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} + \epsilon_i \\
\beta_0 &= 1 ,\beta_1 = 3, \beta_2 = 0.7 \\
\epsilon_i &\sim N(0,\sigma = 0.5)
\end{align*}

Übersetzt in `R` code bedeutet dies, wenn wir $x_1$ und $x_2$ als Gleichverteilt aus $U(-2,2)$ ansehen.

```{r}
#| echo: true

N <- 50 # Anzahl Datenpunkte
beta_0 <- 1
beta_1 <- 3
beta_2 <- 0.7
sigma <- 0.5
set.seed(123)
df <- tibble(
  x1 = runif(N, -2, 2),
  x2 = runif(N, -2, 2),
  y = beta_0 + beta_1*x1 + beta_2*x2 + 
    rnorm(N, 0, sigma)) 
```

Wenn wir die Prädiktorvariblen $x_1$ und $x_2$ gegen $y$ jeweils in einem Streudiagramm abtragen erhalten wir die folgenden Abbildungen (siehe @fig-mlm-basics-example-01).

```{r}
#| label: fig-mlm-basics-example-01
#| layout-ncol: 2
#| fig-height: 4 
#| fig-cap: "Streudiagramme der einfachen Zusammenhänge für das Beispiel."
#| fig-subcap:
#|   - Streudiagramm $y$ gegen $x_1$
#|   - Streudiagramm $y$ gegen $x_2$

ggplot(df, aes(x1,y)) + geom_point() + ncol_theme 
ggplot(df, aes(x2,y)) + geom_point() + ncol_theme
```

Der Zusammenhang zwischen $x_1$ und $y$ ist ziemlich klar (siehe @fig-mlm-basics-example-01-1), während das Streudiagramm für $x_2$ gegen $y$ nicht ganz so eindeutig ist (siehe @fig-mlm-basics-example-01-2). Fitten wir das Modell mittels `lm()` in `R`

```{r}
#| echo: true
mod <- lm(y ~ x1 + x2, df)
```

Wir erhalten die folgenden Modellkoeffizienten.

```{r}
#| label: tbl-mlm-basics-example-01
#| tbl-cap: "Modellkoeffizienten für das Beispiel."
 
lm_tbl_knitr(mod, long=F, add_sigma=T)
```

In @tbl-mlm-basics-example-01 sehen wir, dass wir die Koeffizienten des DGP wieder sehr gut wiederherstellen können. Das sollte uns erst mal etwas beruhigen, da es zeigt, dass wir mit unserer multiplen Regression eigentlich ganz gut arbeiten können und das Modell in der Lage ist die korrekten Modellparamter im Rahmen der Stichprobenunsicherheit relativ gut abschätzen zu können. Die Interpretation von $\beta_1$ ist gleich derjenigen bei der einfachen linearen Regression. Zwei Objekte die sich in $x_1$ um eine Einheit voneinander unterscheiden, unterscheiden sich in der abhängigen Variablen $y$ um den Steigungskoeffizienten $\beta_1$. Die gleiche Interpretation trifft auch auf $\beta_2$ zu nur eben mit den umgekehrten Rollen von $x_2$ und $x_1$. Schauen wir uns nun an, wo die Koeffizienten herkommen?

Dazu machen wir erst einmal einen kleinen Umweg. Wir erstellen uns eine neue Variable $x_1^*$. Wir erhalten $x_1^*$ indem wir eine Regression von $x_1$ auf $x_2$ berechnen. Also das Modell: 

\begin{equation*}
x_{1i} = \beta_0 + \beta_1 x_{2i} + \epsilon_i
\end{equation*}

D.h. wir versuchen $x_1$ mittels $x_2$ vorherzusagen. Was soviel heißt wie, wie viele Information über $x_1$ sind schon in $x_2$ enthalten. Von diesem Modell nehmen wir jetzt die Residuen $e_i$.

\begin{equation}
e_i = x_{1i} - \beta_0 - \beta_1 x_{2i}
\label{eq-mlm-basics-coef-01}
\end{equation}

Dies Residuen stellen denjenigen Teil von $x_1$ der sich nicht linear durch $x_2$ darstellen lässt. Anders, derjenige Teil von $x_1$ der um den Teil von $x_2$ *bereinigt* wurde. Die Residuen $e_i$ aus Formel \eqref{eq-mlm-basics-coef-01} bezeichnen wir nun mit $x_1^*$. Im nächsten Schritt verwenden wir $x_1^*$ um eine einfache lineare Regression $y$ durchzuführen. 

\begin{equation*}
y_i = \beta_0 + \beta_1 x_{1i}^* 
\end{equation*}

In `R` entsprechend:

```{r}
#| echo: true
 
mod_x1_star<- lm(x1 ~ x2, df)
x1_star <- resid(mod_x1_star)
mod_y_x1_star <- lm(y ~ x1_star, df)
```

Schauen wir uns nun die Koeffizienten an (siehe @tbl-mlm-basics-example-02)

```{r}
#| label: tbl-mlm-basics-example-02
#| tbl-cap: "Modellkoefizienten von $y$ gegen $x_1^*$."

lm_tbl_knitr(mod_y_x1_star, long = F)
```

Der Steigungskoeffizient $\beta_1$ ist genau derjenige aus dem Modell mit beiden Variablen (siehe @tbl-mlm-basics-example-01).

Wenig überraschend, wenn wir das gleiche Verfahren auf $x_2$ anwenden. 

```{r}
#| echo: true

mod_x2_star <- lm(x2 ~ x1, df)
x2_star <- resid(mod_x2_star)
mod_y_x2_star <- lm(y ~ x2_star, df)
coef(mod_y_x2_star)[2]
```

Erhalten wir den Steigungskoeffizienten für $\beta_2$ aus dem Modell mit beiden Variablen.

Insgesamt können wird die Steigungskoeffizienten in multiplen Regressionsmodell dahingehend interpretieren, dass wenn wir den Steigungskoeffzienten $\beta_i$ betrachten. Welche zusätzlichen Information erhalten wir durch die Variable $x_i$ wenn wir schon alle anderen Variablen $x_j, j \neq i$ im Modell haben. D.h. das multiple Regressionsmodell kontrolliert automatisch immer den Einfluss der anderen $K-1$ Variablen bei der Betrachtung der Variablen $i$. Nochmal als Liste:

- $\hat{\beta}_1$: Wenn ich $x_2$ weiß, welche zusätzlichen Informationen bekomme ich durch $x_1$ 
- $\hat{\beta}_2$: Wenn ich $x_1$ weiß, welche zusätzlichen Informationen bekomme ich durch $x_2$ 

Bezogen auf die Werte der Steigungskoeffzienten $\beta_i$ ist die Bedeutung immernoch wie oben, wenn sich zwei Objekte auf der Variablen $x_i$ um eine Einheit voneinander unterscheiden, dann unterschieden sich die erwarteten (vorhergesagten) Werte $\hat{y}_i$ laut des Modells um $\beta_i$. Die anderen Prädiktorvariblen $x_j, j \neq i$ werden dabei konstant gehalten.

## Added-variable plots

Der letzte Satz, das die anderen Variablen konstant gehalten werden ist dabei auch das Problem wenn ich ein Streudiagramm bei einer multiplen linearen Regression von einer $Y$ gegen eine der Prädiktorvariablen erstelle wie in @fig-mlm-basics-example-01. In @fig-mlm-basics-example-01-1 können wir den Einfluss von $x_1$ sehr gut identifizieren, da der Steigungskoeffizient $\beta_1$ im Verhältnis zu $\sigma$ recht groß ist. Im Falle von @fig-mlm-basics-example-01-2 ist dies nicht der Fall. Eine Möglichkeit hier Abhilfe zu schaffen kommt durch die sogenannten added-variable plots. Die Idee dahinter geht von dem Trick von eben aus, den wir benutzt haben um die Bedeutung der Steigungskoeffizienten zu verstehen. Neben der Bereinigung von $x_2$ aus $x_1$ rechnen wir auch noch den Einfluss von $x_2$ aus $y$ raus. Schauen wir uns an wie wir einen solchen Plot erstellen könnten (siehe @fig-mlm-basics-avplot-01).

```{r}
#| echo: true
#| label: fig-mlm-basics-avplot-01
#| fig-cap: "Zusammenhang zwischen $y$ und $x_2$ bereinigt um den Einfluß von $x_1$."

x1_star <- resid(lm(y ~ x1, df))
y_star <- resid(lm(x2 ~ x1, df))
ggplot(tibble(x1_star, y_star),
       aes(x1_star,y_star)) +
  geom_point() + 
  labs(x = 'x2 | x1', y = 'y | x1')
```

Wir sehen jetzt viel deutlicher den Zusammenhang zwischen $x_2$ und $y$ als dies in @fig-mlm-basics-example-01-2 der Fall war. Im Paket `car` gibt es auch eine Funktion `avPlots()` mit der wir die added-variable plots auch direkt erstellen können.

```{r}
#| echo: true
#| fig-height: 3

car::avPlots(mod)
```

Im Falle von mehr als zwei Prädiktorvariablen werden immer alle $K-1$ Variablen aus $x_i$ und $y$ rausgerechnet.

## Was passiert wenn ich einen Prädiktor weg lasse?

```{r}
lm_tbl_knitr(mod, long=F)
```

```{r}
#| echo=T

coef(lm(y ~ x1, df))
coef(lm(y ~ x2, df))
```

Jetzt wenden wir aber gleich ein, das das in Beispiel auch nicht besonders problematisch war, weil nach Konstruktion $x_1$ und $x_2$ unabhängig voneinander sind:
```{r}
#| echo: true

round(cor(df),3)
```


In unserem Beispiel wieder nicht viel, da die Variablen unabhängig (orthogonal) voneinander sind.


## Was passiert wenn Prädiktoren stark miteinander korrelieren?

```{r}
#| fig.height=2

bodyfat <- readr::read_delim(
  "triceps;thigh;midarm;body_fat\n19.5;43.1;29.1;11.9\n24.7;49.8;28.2;22.8\n30.7;51.9;37;18.7\n29.8;54.3;31.1;20.1\n19.1;42.2;30.9;12.9\n25.6;53.9;23.7;21.7\n31.4;58.5;27.6;27.1\n27.9;52.1;30.6;25.4\n22.1;49.9;23.2;21.3\n25.5;53.5;24.8;19.3\n31.1;56.6;30;25.4\n30.4;56.7;28.3;27.2\n18.7;46.5;23;11.7\n19.7;44.2;28.6;17.8\n14.6;42.7;21.3;12.8\n29.5;54.4;30.1;23.9\n27.7;55.3;25.7;22.6\n30.2;58.6;24.6;25.4\n22.7;48.2;27.1;14.8\n25.2;51;27.5;21.1\n",
  delim = ';', col_types = 'dddd'
) 
knitr::kable(head(bodyfat),
             caption = 'Ausschnitt von Körperfettdaten',
             booktabs = TRUE,
             linesep = '')
```

^[Beispiel nach @kutner2005]


## Was passiert wenn Prädiktoren stark miteinander korrelieren?
```{r}
#| echo=T,
#| fig.cap="Korrelationsmatrize",
#| fig.height=2.5

GGally::ggpairs(bodyfat) + theme(text = element_text(size = 10))
```

## Was passiert wenn Prädiktoren stark miteinander korrelieren?

```{r}
#| echo=T

# Alle drei Prädiktoren
mod_full <- lm(body_fat ~ triceps + thigh + midarm, bodyfat)
# ohne Arm
mod_wo_midarm <- lm(body_fat ~ triceps + thigh, bodyfat)
# Ohne Oberschenkel
mod_wo_thigh <- lm(body_fat ~ triceps + midarm, bodyfat)
# Ohne Triceps
mod_wo_triceps <- lm(body_fat ~ thigh + midarm, bodyfat)
```

## Was passiert wenn Prädiktoren stark miteinander korrelieren?

```{r}
lm_tbl_knitr(mod_full, long=F, caption="full model")
```

```{r}
lm_tbl_knitr(mod_wo_midarm, long=F, caption='w/o midarm')
```

```{r}
lm_tbl_knitr(mod_wo_thigh, long=F, caption='w/o thigh')
```

```{r}
lm_tbl_knitr(mod_wo_triceps, long=F, caption='w/o triceps')
```

## Multikollinearität^[informell nach @kutner2005 [pp. 407]]

- Große Änderungen in den Koeffizienten wenn Prädiktoren ausgelassen/eingefügt werden
- Koeffizienten haben eine andere Richtung als erwartet
- Hohe (einfache) Korrelationen zwischen Prädiktoren
- Breite Konfidenzintervalle für "wichtige" Prädiktoren $b_j$ 

$$
\widehat{\text{Var}}(b_j) = \frac{\hat{\sigma}^2}{(n-1)s_j^2}\frac{1}{1-R_j^2} 
$$


$R_j^2$ = Multipler Korrelationskoeffizient der Prädiktoren auf Prädiktorvariable $j$.

## Variance Inflation Factor (VIF)

$$
\text{VIF}_j = \frac{1}{1-R_j^2}
$$

\vspace{1cm}

:::{.callout-tip}
Wenn VIF > 10 ist, dann deutet dies auf hohe Multikollinearität hin.
:::

^[Manchmal wird auch Tolerance = $\frac{1}{VIF}$ betrachtet.]


## Variance Inflation Factor (VIF)
```{r}
#| echo: true

car::vif(mod_full) 
```
^[car::vif berechnet generalized variance inflation factor wenn Prädiktoren Faktoren oder Polynome sind [@fox2011.] ]

Üblicherweise wird der größte Wert betrachtet um die Multikollinearität zu bewerten.


## Wenn Prädiktoren sich gegenseitig maskieren^[adaptiert nach @mcelreath2016]

```{r}
#| fig.height=2.5,
#| fig.cap="x_pos maskiert den Einfluss von x_neg"

N <- 100
set.seed(123)
rho <- 0.7
x_pos <- rnorm(N)
x_neg <- rnorm(N, rho*x_pos)
y <- rnorm(N, x_pos - x_neg)
df <- tibble(y, x_pos, x_neg)
GGally::ggpairs(df) + theme(text = element_text(size = 10))
```

## Wenn Prädiktoren sich gegenseitig maskieren

```{r}
co_1 <- lm(y ~ x_pos, df)
co_2 <- lm(y ~ x_neg, df)
lm_tbl_knitr(co_1, long=F)
```

```{r}
lm_tbl_knitr(co_2, long=F)
```

```{r}
co_3 <- lm(y ~ x_pos + x_neg, df)
lm_tbl_knitr(co_3, long=F)
```

## Multiple Regression

Aus der einfachen Regression

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$

wird

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_K x_{Ki} + \epsilon_i
$$

mit K Prädiktorvariablen und Multikollinearität.


## Zum Nacharbeiten

Die folgenden Texte eignen sich gut um die Inhalte noch einmal zu vertiefen. @pos_multiple_regression diskutiert noch mal eingehend was passiert wenn Variablen miteinander korrelieren und wie sich das auf die Schätzer für die Koeffizienten auswirkt. [@kutner2005, p.278-288] und [@fox2011, p.325-327] sind nochmal als Zusammenfassungen zu lesen.

