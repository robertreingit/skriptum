# Einfache Datenbearbeitung und Visualisierung in `R`  

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

```{r}
library(tibble)
```

Im Folgenden werden wir verschiedene Funktion zur Bearbeitung und Visualisierung von Daten kennenlernen. Die verwendeten Funktionen sind alle in einer großen Sammlung von Funktionen in der Paketsammlung `tidyverse` zusammengefasst. Pakete aus dem `tidyverse` folgen alle einer einheitlichen Syntax und Idee der Datenverarbeitung. Die Paket verfügen über eine ausgezeichnete Dokumentation. Daher werden die jeweiligen Funktionen hier nur kurz angeschnitten. Weitergehende Informationen und vor allem jede Menge Beispielanwendungen findet ihr in der `tidyerverse` [Dokumention](https://tidyvers.org).

## Daten in `R` einlesen

Um Daten von der Festplatte oder anderen Speichermedien in `R` einzulesen benötigen wir spezielle Funktionen. In Abhängigkeit von der Formatierung der Daten werden unterschiedliche, spezialisierte Funktionen verwendet. Daher müssen wir uns zunächst über die Dateiformatierung im Klaren sein, um die Daten erfolgreich in `R` einzulesen. In `R` decken drei Pakete den Großteil der in Frage kommenden Dateitypen ab. Die Pakete sind `readr` für Textdateien, `readxl` für Excel-Dateien und `haven` für SPSS- und weitere Binäredatenformate aus anderen Statistikprogrammen. 

### readr

Im Paket `readr` sind eine Reihe von Funktionen enthalten um rechteckige Textdateien einzulesen. Rechteckig bezieht sich in diesem Zusammenhang auf die Anordnung der Daten in der Datei ähnlich einer Tabelle. Um die Daten korrekt einzulesen, ist es notwendig die Trennzeichen (im engl. als delimiter bezeichnet) zwischen Datenwerten zu kennen. Oft verwendete Trennzeichen sind Kommas `,`, Semikolons `;`, Leerzeichen oder das Einrückungszeichen {{< kbd TAB >}}. Durch die Kombination von Daten die jeweils durch ein Trennzeichen voneinander getrennt sind und über mehrere Zeilen verteilt sind, kommt die rechteckige Anordnung zustande.

Die flexibelste Funktion um solche Textdateien einzulesen ist `read_delim()`.  `read_delim()` benötigt die Angabe des Trennzeichens zwischen den einzelnen Spalteneinträgen über den Parameter `delim`. Seien z.B. die folgenden Daten in einer Datei `example_01.txt` in dem Ordner `data` gespeichert.

| id	grp	value
| p1	CON	1
| p2	CON	2
| p3	TRT	3
| p4	TRT	4

Das Trennzeichen zwischen den Eintragen ist ein {{< kbd TAB >}}. Die Datei kann dann mittels des folgenden Befehls einlesen werden.

```{r}
#| echo: true

df <- readr::read_delim(
  file = "data/example_01.txt",
  delim = "\t"
)
```

Der Parameter `delim="\t"` spezifiziert das verwendete Trennzeichen während `file` den Pfad und den Namen zur Datei angibt. 

:::{.callout-caution}
Der Pfad zur Datei ist immer entweder relativ zum akutellen Arbeitsverzeichnis (`getwd()`) oder absolut z.B. `C:/X/Y/Z/example_01.txt` anzugeben. Das Arbeitsverzeichnis ist das Verzeichnis von dem `R` aus gerade arbeitet (In R-Studio mit {{< kbd CTRL+SHIFT+h >}} wechseln).

Befinden wir uns beispielsweise in einem Verzeichnis *stats* und müssen eine Hierarchiestufe nach oben gehen um dann im Verzeichnis *data* die Datei `example_01.txt` einzulesen. Dann können wir entweder den vollständigen Pfad benutzen oder einen relativen Pfad mit `../data/example_01.txt`. Dabei bedeuten die `..` eine Hierarchiestufe nach oben zu springen.

```{r}
knitr::include_graphics("pics/relative_pfad.png")
```

:::

`read_delim()` verwendet eine Reihe von Heuristiken um den jeweiligen Datentyp (numerisch, Zeichenkette, etc.) der Spalten zu bestimmen. Der Rückgabewert von `read_delim()` ist ein `tibble()` der Daten. In unserem Beispiel weisen wir dem `tibble()` den Variabllennamen `df` zu und können somit in der weiteren Analyse mit dem Bezeichner `df` auf die Daten zugreifen.

```{r}
#| echo: true

df
```

Im Beispieldaten sind die ersten beiden Spalten als Zeichenketten (`<char>`) erkannt worden, während die dritte Spalte als Zahl (`<dbl>`) erkannt wurde. Manchmal funktioniert die automatische Erkennung der Datentypen nicht korrekt. In dem Falle können mit dem Parameter `col_types` die Spaltentypen direkt angegeben werden. Wenn keine Kopfzeile in den Daten vorhanden ist, kann diese über den Parameter `col_names` spezifiziert werden. Mit `skip` können Zeilen zu Beginn der Datei übersprungen werden.

::: {#exm-readr-01}
Wollen wir zum Beispiel andere Spaltennamen haben, dann können wir die erste Zeile beim einlesen überspringen und andere Spaltennamen angeben.

```{r}
#| echo: true

readr::read_delim(
  file = 'data/example_01.txt',
  delim = '\t',
  skip = 1,
  col_names = c('ID', 'Gruppe','Wert'),
  col_types = 'ccd'
)
```

Wenn ihr das Paket `readr` vorher mit `library(readr)` geladen habt, ist die Qualifizierung der Funktion mit `readr::` nicht notwendig und ihr könnt direkt den Funktionsnamen verwenden. Hier überspringen wir mit `skip=1` die erste Zeile in der Datei und geben mit `col_names=c('ID','Gruppe','Wert')` eigene Spaltennamen an und spezifizieren direkt mit `col_types` die Dateitypen der Spalten mit `c` $=$ `character` und `d` = `double`.
:::

Die weiteren Funktionen in `readr`  wie `read_csv`, `read_tsv` usw. sind in den meisten Fällen Spezialversionen von `read_delim` bei denen der Parameter `delim` schon voreingestellt ist. Schaut euch etwas in der [Dokumentation](https://readr.tidyverse.org/reference/index.html) um, um einen Überblick über die verschiedenen Varianten und Funktionalitäten zu bekommen.

### readxl

Die gleichen Daten in einer Excel-Datei können wir mit der Funktion `read_xlsx()` aus dem Paket `readxl` in `R` laden. Wenn die Daten die folgende Form hat:

```{r}
#| label: fig-r-basics-excel 
#| fig-cap: "Beispieldatei in Excel"

knitr::include_graphics('pics/excel_example.png')
```

dann können wir die Daten mit dem folgenden Befehl laden:

```{r}
#| echo: true

df <- readxl::read_xlsx(
  path = 'data/example_01.xlsx',
  sheet = 'data',
  range = 'A1:C5'
)
df
```

Ähnlich wie bei `read_delim()` geben wir mit `path` den Pfad zur Datei an. Der Parameter `sheet` spezifiziert aus welchem Excelblatt die Daten einlesen werden sollen, während der Parameter `range` den Datenbereich auf dem Blatt definieren. Je nachdem wie kompliziert eure Exceldatei aussieht müssen `sheet` und `range` oft gar nicht angegeben werden da `read_xlsx()` auch über heuristische Regeln versucht zu erraten welche Daten ihr einlesen wollt. Allerdings ist dies eher eine fragile Annahme und daher im Sinne einer robusten Datenanalyse ist zu empfehlen beide Parameter immer anzugeben und sicherzugehen das auch wirklich die gewollten Daten eingelesen werden.

:::{.callout-note}
Bei Parameter `range` darauf achten, das dieser bei Veränderung der Excel-Datei, z.B. wenn neue Daten dazukommen entsprechend angepasst wird.
:::

### haven

Im Paket `haven` haben wir Funktionen um mit SPSS-Dateien (siehe @fig-r-basics-spss) zu arbeiten.

```{r}
#| label: fig-r-basics-spss
#| fig-cap: Beispieldatei in SPSS

knitr::include_graphics('pics/spss_example.png')
```

Die Funktionen funktionen zum Glück selbst wenn SPSS auf dem Rechner nicht installiert ist. Allerdings kann hier nicht nachkontrolliert werden ob das Einlesen korrekt stattgefunden hat, wenn keine Dokumentation zu den Daten vorhanden ist. In SPSS werden Datein üblicherweise in einem binären `sav`-Dateikontainer gespeichert. In `R` können die Daten mittels der Funktion `read_sav()` aus dem Paket `haven` eingelesen werden.

```{r}
#| echo: true

df <- haven::read_sav('data/example_01.sav')
df
```

Da SPSS eigene Datentypen, insbesondere im Zusammenhang mit nominalen bzw. ordinalen Variablen hat, sind im Paket `haven` spezielle Datentypen definiert. Im Beispiel hier ersichtlich am Datentyp für `grp` mit `<dbl+lbl>`. Es handelt sich hier um den Datentypen `labelled`. In der Dokumentation von `read_sav()` gibt es hierzu weiter Informationen. 

:::{.callout-tip}
Mit der Funktion `as_factor()` aus dem Paket `haven` können die `labelled` Daten in den `R` Dateityp `factor` umgewandelt werden.
:::

Wenn wir die Daten in `R` eingelesen haben, können wir nun als nächstes die Daten verarbeiten und so strukturieren wie wir sie für die weitere Analyse benötigen. Dazu stehen uns eine Reihe von weiteren Funktionen aus dem `tidyverse` zur verfügung.

## Daten in `R` prozessieren mit `tidyverse()`

Die Art der Programmierung im `tidyverse` verwendet oft eine eigene Herangehensweise die sich vonderjenigen die wir bisher kennengelernt haben etwas unterscheidet. Um den Code leserlich zu halten werden mehrere Funktionen in sogenannten *pipes* aneinandergehängt ohne zwischendrin temporäre Variablen zu erstellen. Dies führt zu Code der fast wie Prosa gelesen werden kann. Dazu müssenw wir aber zunächst einen neuen Operator kennenlernen.

### Der Pipe operator `|>`

In `R` gibt es den sogenannten pipe-operator `|>`. Der `|>` hat die Eigenschaft, dass ein vorangestellter Wert (dies kann auch der Rückgabewert einer Funktion sein) als das erste Argument an eine nachfolgende Funktion übergeben wird. Dies ermöglicht es Code zu schreiben der sich wie gesprochener Text liest. Schauen wir uns ein einfaches Beispiel an. Wir wollen den Mittelwert eines Zahlenvektors berechnen und anschließend das Ergebnis auf die zweite Nachkommastelle runden. Normalerweise würden wir das wie folgt formulieren wenn wir keine Zwischenvariablen definieren wollen.

```{r}
#| echo: true

vec <- c(1, 7, 3, -5.22, 5, 6.3)
round(mean(vec), 2)
```

D.h. wir haben ein Schachtelung (nesting) der Funktionen. Die `mean()` Funktion ist innerhalb der `round()` Funktion geschachtelt bzw. der Rückgabewert von `mean()` wird als erstes Argument an `round()` übergeben. Schauen wir uns nun an, wie wir das gleiche Programm mit dem pipe-operator `|>` formulieren.

```{r}
#| echo: true

vec |> mean() |> round(2)
```

Was ist hier passiert? Die erste pipe `|>` übergibt ihr links stehendes Argument, den Vektor `vec` and das erste Argument der rechts stehenden Funktion `mean()`. Aus `vec |> mean()` wird `mean(vec)`. `mean()` ist happy und berechnet den Mittelwert des Vektors der nun als Rückgabewert zur Verfügung steht. Der zweite pipe-Operator und nimmt nun wieder diesen *linke* Wert, der berechnete Mittelwert, und übergibt diesen an das erste Argument der nachfolgenden `round(2)`-Funktion. Wenn ihr euch die Hilfe von `round()` anschaut, dann seht ihr, dass dere erste Parmeter der zu rundende Wert ist. Was macht jetzt aber die `2` in `round(2)`. Nun, `|>` stellt den links stehenden Wert an die erste Stelle der rechts stehenden Funktion, dadurch rutscht die `2` an die zweite Argumentenstelle in `round()` und bestimmt somit die Anzahl der zu runden Stellen. 

Der Vorteil des pipe-Operators ist, dass ihr das Program nun einfach von links nach rechts lesen könnt. Nimm `vec`, stecke es in `mean()` und stecke das was rauskommt in `round(2)`. Bei dem ursprünglichen Programm musstet ihr euch von innen nach außen arbeiten und dabei immer im Blick behalten auf welcher Stufe ihr seid um die Parameterzuordnung richtig interpretieren zu können.

Noch ein Beispiel, wir wollen den Mittelwert auf den Absolutwerten des Vektors berechnen. Nach der Standardmethode.

```{r}
#| echo: true

round(mean(abs(vec)), 2)
```

Mit dem pipe-Operator

```{r}
#| echo: true

vec |> abs() |> mean() |> round(2)
```

Wenn die pipe Anfängt zu lang zu werden, dann hat es sich einbürgert nach der pipe eine neue Zeile anzufangen.

```{r}
#| echo: true

vec |> abs() |> 
  mean() |> round(2)
```


Der pipe-Operator `|>` ist so alltäglich, dass ihr in RStudio einen short-cut für ihn habt {{< kbd STRG+SHIFT+m >}}.

Bei der Vewendung der pipe dabei nicht vergessen das Ergebnis der pipe dann letztendlich doch wieder eine Variable zuzuweisen. Ansonsten berechnet `R` den Wert und schmeißt ihn gleich wieder weg.

```{r}
var_abs <- vec |> abs() |> 
  mean() |> round(2)
var_abs
```

Der pipe-Operator `|>` ist im alltäglichen Umgang mit Datenprozessierung im Zusammenhang mit `tidyverse` praktisch unabkömmlich und praktisch jegliche Codeschnipsel die ihr im Netz dazu findet verwenden pipes. Tatsächlich ist die dahinterliegende Idee in der Informatik [siehe @kernighan1984] schon relativ lange bekannt. Die darüberliegende Idee ist nämlich Anstatt große, komplizierte Funktionen zu schreiben die eine Vielzahl von Argumenten haben und mehrere unterschiedliche Aufgaben erledigen, werden lieber viele kleine, spezialisierte Programme zu erstellen. Die spezialisierten Programme können dann zusamengesetzt werden um komplizierte Aufgaben zu erfüllen. Der pipe-operator ist dabei zentral für diese Idee, da er es ermöglicht die spezialisierten, kleinen Programmen einfach aneinander zu hängen. Ähnlich wie zum Beispiel bei einen Kinderwasserspielzeug mit Rohren, Schaufeln und Filtern können durchumordnung der Einzelfunktion beliebig komplexe Mechanismen abgebildet werden. Unter Linux bash ist daher auch ein pipe-Operator `|` zu finden.  

Nach diesem Ausflug zurück zur Datenbearbeitung mit dem `tidyverse`. Um die Daten möglichst einfach mit dem `tidyverse` verarbeiten zu können, sollten die Daten im `tibble()` einer bestimmten Struktur folgen. Im `tidyverse` wird diese Anordnung als *tidy-Data* bezeichnet.

### tidy-Data

Zu tidy-Data gibt es in der einfachsten Form nur drei Regeln die zu beachten sind:

1. Jede Spalte ist eine Variable
2. Jede Zeile ist eine Beobachtung
3. Jede Zelle ist ein einzelner Eintrag

Schauen wir uns wieder ein einfaches, fiktives Beispiel mit Sprunghöhen an.

```{r}
#| echo: true

df <- tibble(
  time = rep(c('pre','post'), 4),
  gender = rep(c('m','f'), each=4),
  age = rep(round(runif(4, 20, 40)),each=2),
  cmj = round(rnorm(8, c(25,20), 2), 1)
)
df
```

Wir haben vier Spalten, `time`, `gender`, `age` und `cmj` die jeweils eine Variable darstellen. In jeder Zeile ist eine Beobachtung eine Sprunghöhe `cmj` einer Person eines Alters `age` und `gender` zu einem bestimmten Zeitpunkt `time`. Schaut also tidy aus.

Diese Darstellung ist aber wahrscheinlich unterschiedlich zu derjenigen wie ihr solche Daten schon öfter gesehen habt. Wahrscheinlich nämlich eher so.

```{r}
df |> pivot_wider(names_from=time, values_from=cmj)
```

Diese Darstellung ist zwar kompakter aber entspricht nicht mehr den tidy-Anforderungen, da wir nun nicht mehr nur eine Beobachtung pro Zeile haben. Wir haben für jede Person die Sprunghöhe zu zwei Zeitpunkten in einer Zeile. Die Daten sind in dieser Darstellung also *untidy*. Die *tidy*-Version ist etwas länger und enthält redundante Informationen aber wir werden im Folgenden sehen, dass diese Darstellung in der Verarbeitung zahlreiche Vorteile hat. Dazu werden wir auch Funktionen kennenlernen mit denen wir zwischen diesen beiden Formaten hin- und herwechseln können. Die *tidy*-Darstellung wird als long-Format bezeichnet, während die *untidy*-Darstellung als wide-Format bezeichnet wird.

### `filter()`

Lernen wir jezt unseren ersten `tidyverse()` Befehl zur Datenmanipulation kennen. Die Befehle werden als Verben bezeichnet und die Regel für die Funktionennahmen ist zum Glück ziemlich einfach:"was im Namen draufsteht ist auch in der Packung drin". Der Befehl `filter` filtert Daten, d.h. wir geben einen Sack Daten rein und wollen nur einen Teil wieder raus haben der bestimmte Eigenschaften hat. Dazu können wir einfache Regel mittels der Vergleichsoperatoren in `filter()` zusammen mit den Spaltennahmen im `tibble()` verwenden. Wollen wir aus unserem Datensatz nur alle weiblichen Datenpunkte herausfiltern und wir wissen, dass die Information über das Gender in der Spalte `gender` steht und mit `f` für weiblich kodiert ist, dann können wir mit dem Vergleich `gender == 'f' die Daten entsprechen filtern.

```{r}
#| echo: true

df |> filter(gender == 'f')
```

Wollen wir dagegen auf das Alter mit `age < 30` filtern, verwenden wir:

```{r}
#| echo: true

df |> filter(age < 30)
```

Wenn die Daten mehrere Konditionen erfüllen müssen, verwenden wir entsprechend mehrere Filteranweisungen. Die Vergleiche werden intern mittels einer Und-Operation zusammengesetzt.

```{r}
#| echo: true

df |> filter(age < 25, gender == 'f')
```

`filter()` gibt uns in diesem Bespiel also alle Datenpunkte zurück bei denen `age < 25` und `gender == 'f'` gilt. Durch die Kombination von mehreren Vergleichen können relativ einfach komplexe Bedingungen formuliert werden.

::: {.callout-tip}
Manchmal möchte wir nur alle vollständigen Zeilen aus einen Datensatz weiterverwenden. Dies könnten wir mit `filter()` erreichen, dazu müssten wir aber für jede Spalte einen Vergleich mit `is.na()` schreiben wenn die fehlenden Werte `NA` über mehrere Spalten verteilt sind. Schneller geht dies mit dem Befehlt `drop_na()`.

```{r}
#| echo: true

df_missing <- tibble(x = 1:4, y = c(11,NA,13,14), z = c(21,22,23,NA))
df_missing
df_missing |> drop_na()
```
:::

Insgesamt führt `filter()` in den meisten Fällen dazu, dass wir ein `tibble` mit weniger Zeilen als ursprünglich erhalten.

### `select()`

Das nächste Verb aus dem `tidyverse` ist `select()`. Mit `select()` können wir einzelne Variablen/Spalten aus einem `tibble()` selektieren. Wollen wir zum Beispiel aus unserem Datensatz nur die beiden Spalten `time` und `gender` auswählen, dann können wir dies mit `select()` wie folgt formulieren:

```{r}
#| echo: true

df |> select(time, gender)
```

Der Rückgabewert von `select()` ist entsprechend ein `tibble` das nur die selektierten Spalten enthält. Ähnlich wie das auch bei der Indexierung von Elementen in Vektoren funktioniert versteht `select()` auch eine Ausschlusssyntax mit `-`. Wollen wir z.B. alle Spalten aus `gender` formulieren wir dies wie folgt:

```{r}
#| echo: true

df |> select(-gender)
```

::: {#exm-select-01}
Da wir jetzt schon zwei Verben kennen, können wir auch direkt sehen wie wir durch die Kombination der Befehle komplexere Anweisungen relativ einfach in Code übersetzen können. Wollen wir zum Beispiel alle Datenpunkte von Frauen die älter als 28 sind verwenden und brauch dann nicht mehr die `gender` Variable könen wir dies wie folgt übersetzen:

```{r}
#| echo: true

df |> filter(gender == 'f', age > 27) |> select(-gender)
```

:::

Insgesamt führt `select()` dazu, dass wir ein `tibble` erhalten, dass weniger Spalten als ursprünglich enthält.

### `mutate()`

In den beiden vorhergehenden Fällen, haben wir die Daten nicht wirklich verändert sondern entweder nur bestimmte Fälle mit `filter()` oder bestimmte Variablen mit `select()` ausgewählt. Oft kommt es aber vor, dass wir aus den bestehenden Daten neue Daten erstellen wollen. Um neue Daten zu berechnen verwenden wir im `tidyverse` das Verb `mutate()`.

In unserem Datensatz liegen die Sprunhöhen in Zentimeter vor, wir benötigen aber für weitere Berechnungen die Sprunghöhen in Metern. Dazu wollen wir eine neue Spalte erstellen, die wir, um sie von der anderen Spalte abzugrenzen, mit `cmj_m` bezeichnen wollen. Dazu setzen wir `mutate()` wie folgt ein.

```{r}
#| echo: true

df |> mutate(cmj_m = cmj / 100)
```

Wie sehen, dass eine neue Spalte in dem `tibble` erstellt wurde mit eben der Namen `cmj_m`.

::: {.callout-warning}
`mutate()` verhindert nicht, Variablen mit neuen Werten zu überschreiben. Würden wir die Sprunghöhen in Zentimeter später nicht mehr benötigen, könnten wir die Spalte `cmj` auch überschreiben.

```{r}
#| echo: true

df |> mutate(cmj = cmj / 100)
```
:::

Der große Vorteil von `mutate()` ist hier auch wieder, das wir direkt auf die Spaltenahmen zugreifen können und nicht wie sonst notwendig den `$`-Operator.

Durch `mutate()` werden insgesamt also zusätzliche Spalten an das ursprüngliche `tibble` angehängt.

::: {#exm-mutate-01}
Aufbauend auf dem Beispiel von eben, wollen wir zum Beispiel die Spalte mit Sprunghöhen nicht beibehalten aber einen neuen Namen verwenden damit wir später nachvollziehen können, dass wir die Sprunghöhen bearbeitet haben, könnten wir wieder mit einer Kombination der Verben `mutate()` und `select()` in einer pipe vorgehen:

```{r}
#| echo: true

df |> mutate(cmj_m = cmj / 100) |> select(-cmj)
```
:::

### `summarize()`

Mit der `summarize()` Funktion werden alle Beobachtung (Zeilen) in eine Zeile zusammengefasst. Wollen wir zum Beispiel die durchschnittliche Sprunghöhe über alle Beobachtung berechnen.

```{r}
#| echo: true

df |> summarize(cmj_bar = mean(cmj))
```

`summarize()` kann auch mehrere Werte berechnen.

```{r}
#| echo: true

df |> summarize(cmj_bar = mean(cmj), age_bar = mean(age))
```


### `group_by()`

Die `summarize()` Funktion wird tatsächlich erst richtig mächtig im Zusammenhang mit der `group_by()` Funktion. Mit `group_by()` kann ein Datensatz anhand der Werte einer Variable in Untergruppen geschnitten werden. Wollen wir in unserem Beispiel die Mittelwerte für verschiedenen Zeitpunkte berechnen.

```{r}
#| echo: true

df |> group_by(time) |> summarize(cmj_bar = mean(cmj))
```

Das gruppieren funktioniert auch über mehrere Variablen.

```{r}
#| echo: true

df |> group_by(time, gender) |> summarize(cmj_bar = mean(cmj))
```

Die Kombination aus `filter()`, `group_by()` und `summarize()` deckt wahrscheinlich mehr als die Hälfte der Anwendungen bei der Datenanalyse ab. Die folgenden Befehle sind etwas spezialisiert.

### `separate()`

Mit dem `separate()` Befehle kann eine Variable in der mehrere Informationen gespeichert sind, in mehrere Variablen separiert werden.

```{r}
df_2 <- tibble(
  Kondition = c('pre_trt','post_trt','pre_con','post_con'),
  wert = 1:4
)
df_2
```

Die Variable Kondition hat zwei Variablen in einer kodiert (*untidy*). Mit `separate()` können wir Kondition aufteilen. `separate()` benötigt als Parameter neben dem Spaltennamen eine Vector der neuen Spaltennamen übergeben an das Funktionsargument `into`.

```{r}
#| echo: true

df_2 |> separate(Kondition, into=c('time','group'))
```

### `pivot_wider()`

Mit `pivot_wider()` können *tidy*-Datensätze von der long-Version in die wide-version umkodiert werden. Im einfachsten Fall benötigt `pivot_wider()` zwei Argumente. `names_from` wird der Variablenname übergeben, der einzelne `wide`-Variablen kodieren. `values_from` spezifiziert die einzutragenden Variablen.

```{r}
#| echo: true

df_w <- df |> pivot_wider(names_from=time, values_from=cmj)
df_w
```

### `pivot_longer()`

Die Umkehrfunktion von `pivot_wider()` ist `pivot_longer()`. Wieder im einfachsten Fall müssen die umzukodierenden Spalten angegeben werden, zusammen mit dem neuen Spaltennamen für die Indizierung (`names_to`) und dem neuen Spaltennamen für die Werte (`values_to`).

```{r}
#| echo: true

df_w |> pivot_longer(c(pre, post), names_to = "time", values_to = "cmj")
```

Zusammenfassend ist das hier nur eine schnelle Übersicht über die möglichen Befehle gewesen. Um die ins und outs der Funktionen zu verstehen, ist natürlich mehr Übung notwendig. Meistens hilft es aber schon zu wissen welche der Funktionen ungefähr helfen könnte um dann zusammen mit der Dokumentation eine Lösung zu finden. Eine exzellente Quelle für eine umfassendere Auseinandersetzung mit dem `tidyverse` ist das Buch *R for Data Science* von @wickham2023 das ihr auch kostenlos in der aktuellsten Form [online](https://r4ds.had.co.nz) findet.

### Joins

Manchmal möchten wir Informationen aus mehreren Tabellen zusammenfügen. Sei zum Beispiel der folgende Fall gegeben. Wir haben in einer Tabelle `anthro` anthropometrische Merkmale unserer Probanden und in einer Tabelle `df_exp` die Experimentaldaten.

```{r}
anthro <- tibble(pid = paste0('P',1:2), BMI = c(18,25), height=c(170,180))
anthro
df_exp <- tibble(pid = paste0('P', 1:3), speed = 11:13)
df_exp
```

Um Daten aus verschiedenen Tabellen zusammen zu fügen gibt es eine Familie von Funktion die unter `join` zusammengefasst sind (`left_join()`, `right_join()`, `full_join()`). Die Unterschied bestehen darin was mit Daten gemacht wird die in einer der Tabelle fehlen. Wir fokussieren uns hier auf `left_join()`, wenn das Prinzip allerdings verstanden wurde, dann ist die Herleitung der beiden anderen Varianten einfach (siehe Dokumentation dazu).

Um die Tabellen zusammen zu fügen, brauchen wir eine Variable die die einzelnen Fälle indiziert und in beiden Tabellen vorhanden ist. Oben haben wir dazu die Variable `pid` definiert. Damit können wir den join einfach durchführen

```{r}
df_exp |> left_join(anthro, by = 'pid')
```

Wir erhalten eine Tabelle bei der für jede Zeile aus der *linken* Tabelle `df_exp` die `pid` entsprechende Zeile aus `anthro` hinzugefügt wurde. Wenn der entsprechende Wert aus der *rechten* Tabelle fehlt, dann werden entsprechend fehlende Werte `NA` eingesetzt.

Manchmal kommt es vor, dass die Namen der Indexvariable in den beiden Tabellen unterschiedlich sind.  Mit der Funktion `join_by(x == y)` kann die Zusammenfügung trotzdem durchgeführt werden. 

```{r}
anthro_2 <- tibble(id = paste0('P',1:2), BMI = c(18,25))
df_exp |> left_join(anthro_2, by=join_by(pid == id))
```


### Spezialthemen

#### `rowid_to_column()` und `row_number()`

Manchmal ist es von Vorteil wenn eine Variable vorhanden ist, die die Zeilen eindeutige identiziert. Eine einfache Lösung ist die Funktion `rowid_to_column()` mit der ein fortlaufender Zähler an der ersten Stelle eingefügt wird.

```{r}
#| echo: true

df |> rowid_to_column(var = "rid")
```

Eine weitere Möglichkeit ist über die Funktion `row_number()` innerhalb von `mutate()`

```{r}
#| echo: true

df |> mutate(rid = row_number(), .before=1)
```

Dieser Ansatz hat auch noch den Vorteil, das wir eine eindeutige ID nach einem beliebigen Muster erstellen können. Würde zum Beispiel ein identifier für Probanden fehlen.

```{r}
#| echo: true

df |> mutate(pid = paste0('Proband_', row_number()), .before=1)
```

#### `pull()`

Manchmal möchte ich die Werte einer Variablen aus der Tabelle extrahieren. Dies kann mit dem Befehl `pull()` dirchgeführt werden. Möchte ich zum Beispiel nur die Einträge für das Alter extrahieren kann ich den folgenden Befehl benutzen:

```{r}
df |> pull(age)
```

Wir erhalten in diesem Fall einen Vektor mit den Einträgen.

## Daten in `R` visualisieren mit `ggplot2`

`ggplot2` ist eine spezielles package das Funktionen bereitstellt um Grafiken zu erstellen. `ggplot2` verfolgt einen ganz speziellen Ansatz um Grafiken zu beschreiben indem eine spezielle Sprache, eine sogenannten domänenspezifische Sprache, verwendet wird. Das hört sich im ersten Moment komplizierter an als es ist. Wie wir gleich sehen werden werden Grafiken erstellen indem verschiedene Ebenen übereinander gelegt werden. In der Syntax von `ggplot2` erfolgt dieses übereinanderlegen mittels des `+` operator im Zusammenhang mit speziellen Funktionen. Die Hauptidee besteht allerdings darin, das einzelne Variablen auf verschiedene Skalen in einer Grafik abgebildet werden und dann mittels geometrischer Elemente, sogenannte `geom`s, dargestellt werden.

`ggplot2` arbeitet damit mit einem zweidimensionalen Modell und ist daher nicht für 3D-Darstellungen geeignet. Starten wir mit einem einfachen Datensatz und schauen uns an wie wir mit `ggplot2` damit arbeiten würden.

```{r}
#| echo: true

df <- tibble(
  dv = 1:8,
  iv = 11:18,
  group = rep(c('a','b'), 4),
  team = rep(c('Nuggets','Lakers'), each=4)
)
```

```{r}
knitr::kable(df,
             booktabs = TRUE,
             linesep = '')
```

Als Grundgerüst brauchen wir immer die `ggplot()` Funktion, der wir als erstes Element das `tibble()` übergeben.

```{r}
#| echo: true
#| label: fig-r-basics-ggplot-01
#| fig-label: "Die einfachstmögliche Grafik."

ggplot(df)
```

In @fig-r-basics-ggplot-01 ist erst mal nichts beeindruckend passiert, außer das wir eine leere, graue Ebene erstellt haben. Definieren wir unsere erste Abbildung indem wir `ggplot` sagen, dass die Variable `iv` auf die $x$-Achse abbilden wollen.

```{r}
#| echo: true
#| label: fig-r-basics-ggplot-02
#| fig-label: "Die einfachstmögliche Grafik."

ggplot(df, mapping = aes(x = iv))
```

Die Grafik ist marginal interessanter geworden (siehe @fig-r-basics-ggplot-02). Allerdings haben wir jetzt eine vollständige $x$-Achse mit Beschriftung und Einheiten. Im Code haben wir dazu an das Argument `mapping` ein `aes()` Funktion übergeben, der wir wiederum das Argument `x` mit dem Namen der Variable belegt haben, das wir auf die $x$-Achse abbilden wollen. Wenn ihr euch die Hilfe für `?ggplot()` anschaut, dann seht ihr, das das zweite Argument sowieso `mapping` ist, daher können wir uns die Argumentenbezeichnung auch schenken. Wenn es in `aes()` ein `x` gibt, dann wird es wohl auch ein `y` geben. Bilden wir daher die Variable `dv` aus `df` auf die $y$-Achse ab.

```{r}
#| echo: true
#| label: fig-r-basics-ggplot-03
#| fig-label: "Die einfachstmögliche Grafik."

ggplot(df, aes(x = iv, y = dv))
```

In @fig-r-basics-ggplot-03 ist jetzt die zweite Achse erstellt worden. Nur sehen wir noch keine Daten. Wenn wir uns `?aes` anschauen, dann sehen wir das `x` und `y` sowieso Argumente 1 und 2 für `aes()` sind, daher können wir uns die Bezeichnung wieder sparen. Aber noch mal zurück zu den Abbildungen. Es gibt nicht nur die beiden Achsen, sondern auch zum Beispiel die Größe oder Farbe von Objekte. Diese werden aber erst interessant wenn wir Ebenen mit geometrische Objekte auf definieren.

### `geom_point()`

Fangen mit dem einfachsten geometrischen Objekt an, dem Punkt bzw. den Punkten. Punkte sind mindestens durch ihre $x$ und $y$-Position, die Größe und die Farbe gekennzeichnet. Schauen wir uns zunächst einmal nur die Position an, und lassen die Farbe und die Größe auf den voreingestellten Werten.

```{r}
#| echo: true
#| fig-cap: "Ein Streudiagramm"
#| label: fig-r-basics-scatterplot

ggplot(df, aes(x = dv, y = iv)) +
  geom_point()
```

In @fig-r-basics-scatterplot haben wir ein einfaches Streudiagramm der Daten erstellt. Schauen wir uns den Code etwas genauer an. Wir haben eine Funktion `geom_point()` verwendet und diese mittels eines `+` an `ggplot()` angehängt. Der erste Teil des Namens `geom` zeigt an, das es sich um ein geometrische Objekt handelt. Im Folgenden werden wir verschiedene Funktionen sehen, die alle mit dem Kürzel `geom` beginnen und entsprechend unterschiedliche Formen haben. Für jedes `geom` erstellt `ggplot()` eine eigene Ebene. Eine mentales Template könnte eine Folie wie früher bei den Overhead-Projektoren sein. `ggplot()` nimmt sich eine leere Folie, legt diese auf die Grafikfolie mit den Achsen und malt die Punkte an die entsprechende Stelle auf die leere Folie.

Wie vorhin schon erwähnt führt der folgende kürzere Code zum gleichen Ergebnis.

```{r}
#| echo: true
#| eval: false

ggplot(df, aes(dv, iv)) +
  geom_point()
```

Diese Schreibweise wird uns im Folgenden immer wieder begegnen. Wenn ihr euch die Hilfe für `geom_point()` anseht, dann stellt ihr fest, dass `geom_point()` tatsächlich eine ganz normale Wald-und-Wiesen Funktion ist. Das erste Argument an `geom_point()` ist ebenfalls `mapping`. Daher können wir das Streudiagramm auch folgendermaßen erstellen.

```{r}
#| echo: true

ggplot(df) +
  geom_point(aes(dv, iv))
```

Der Unterschied zwischen dem Argument `mapping` in `ggplot()` und in `geom_point()` besteht darin, dass die Abbildung im ersten Falle für alle `geom`s gilt die weiter angehängt werden, während im zweiten Fall, `mapping` in `geom_point()` die Abbildung nur für `geom_point()` gilt. Wenn wir gleich mehrere `geom` aneinanderreihen wird der Unterschied klarer.

Schauen wir uns als nächste zwei verschiedene Arten an, die Größe für die Punkte zu bestimmen. Hier gibt es auch wieder zwei Fälle zu unterscheiden. Einmal die Größe innerhalb von `aes()` zu bestimmen oder als Argument zu `geom_point()`. Im ersten Fall können wir dynamisch anhand der zugewiesenen Abbildungsvariable die Größe verändern, während im zweiten eine Größe für alle Punkte zugewiesen wird. Innerhalb von `aes()` können wir den Namen der Variable nehmen, während dies in `geom_point()` nicht möglich ist, hier müssen wir eine Zahl übergeben.

```{r}
#| echo: true
#| layout-ncol: 2
#| label: fig-r-basics-scattersize
#| fig-cap: Definition der Punktgröße auf zwei Arten
#| fig-height: 4
#| fig-subcap:
#|   - "Definition in `aes()`"
#|   - "Definition in `geom_point()`"

ggplot(df, aes(dv,iv, size=dv)) +
geom_point()
ggplot(df, aes(dv, iv)) +
geom_point(size=4)
```

In @fig-r-basics-scattersize-1 sehen wir, dass die Größe der Punkte variiert und `ggplot()` auch noch eine Legende der Größen angefügt hat. In @fig-r-basics-scattersize-2 haben alle Punkte die gleiche Größe.

Das gleiche Prinzip können wir auch auf die Farbe der Punkte anwenden.

```{r}
#| echo: true
#| layout-ncol: 2
#| label: fig-r-basics-scattercol
#| fig-cap: Definition der Punktfarbe auf zwei Arten
#| fig-height: 4
#| fig-subcap:
#|   - "Definition in `aes()`"
#|   - "Definition in `geom_point()`"

ggplot(df, aes(dv,iv, color = team)) +
geom_point()
ggplot(df, aes(dv, iv)) +
geom_point(color = 'red')
```

Wenn die Farbe in `aes()` definiert wird, erhalten wir eine Legende und die Farbe wird anhand der Variable bestimmt (siehe @fig-r-basics-scattercol-1), während bei der Definition als Argument zu `geom_point()` alle Punkte die gleiche Farbe bekommen (siehe @fig-r-basics-scattercol-2).

Natürlich können wir auch gleichzeitig die Farbei und die Größe bestimmen.

```{r}
#| echo: true
#| layout-ncol: 2
#| label: fig-r-basics-scattercolsize
#| fig-cap: Definition der Punktfarbe und Punktgröße auf zwei Arten
#| fig-height: 4
#| fig-subcap:
#|   - "Definition in `aes()`"
#|   - "Definition in `geom_point()`"

ggplot(df, aes(dv,iv, size = dv, color = team)) +
geom_point()
ggplot(df, aes(dv, iv)) +
geom_point(size = 4, color = 'red')
```

### `geom_line()`

Schauen wir uns als nächstes `geom_line()` an. Wie der Name vermuten lässt, können wir mit diesem `geom` Linien erstellen. Linen werden zwischen aufeinanderfolgenden Punkten die eine $(x,y)$-Position haben gezogen. Daher sind die gleichen Abbildungen wie bei den Punkten möglich.

```{r}
#| echo: true
#| label: fig-r-basics-line-01
#| fig-cap: "Linendiagramm"

ggplot(df, aes(iv,dv)) +
  geom_line()
```

Wenn wir zwei unterschiedlichen Linien für die Teams erstellen wollen, dann können wir das zum Beispiel über die Farbe steuern.

```{r}
#| echo: true
#| label: fig-r-basics-line-02
#| fig-cap: "Liniendiagramm unterteilt nach Team"

ggplot(df, aes(iv, dv, color = team)) +
  geom_line()
```

Jetzt fehlt in @fig-r-basics-line-02 das Verbindungsstück zwischen den beiden Punkten das in @fig-r-basics-line-01 noch vorhanden war, da `ggplot()` den Datensatz in zwei Teildatensätze unterteilt.

Wie oben schon angedeutet können wir mehrere `geom` miteinander kombinieren.

```{r}
#| echo: true
#| label: fig-r-basics-line-03
#| fig-cap: "Ein kombiniertes Linien und Streudiagramm"

ggplot(df, aes(iv, dv)) +
  geom_line() +
  geom_point()
```

Die Reihenfolge spielt dabei eine Rolle, `geom`s die später dazugefügt werden, liegen oberhalb von `geom`s die früher definiert wurden.

```{r}
#| echo: true
#| layout-ncol: 2
#| label: fig-r-basics-line-04
#| fig-cap: Einfluss der Reihenfolge von `geom`s 
#| fig-height: 4
#| fig-subcap:
#|   - "`geom_point()  vor `geom_line()`"
#|   - "`geom_line()  vor `geom_point()`"

ggplot(df, aes(iv, dv)) +
  geom_point(color = 'red', size = 4) +
  geom_line()
ggplot(df, aes(iv, dv)) +
  geom_line() + 
  geom_point(color = 'red', size = 4) 
```

In @fig-r-basics-line-04-1 sehen wir, dass die Linien die Punkte durchschneiden, da sie oberhalb liegen, während in @fig-r-basics-line-04-2 die Linien hinter den Punkten liegen.

Vielleicht ist es euch schon aufgefallen, das wir in den letzten beiden Beispielen den Mechanismus verwendet haben, dass die Abbilung in `ggplot()` definiert wurden und dann für bei beide `geom`s `geom_point()` und `geom_line()` angewendet wurden. Wenn wir die Abbildung nur ein `geom_point()` definiert hätten, dann würde `geom_point()` meckern, das es nicht weiß wo es die Punkte hinsetzen soll.

```{r}
#| echo: true
#| error: true

ggplot(df)  +
  geom_line(aes(iv, dv)) + 
  geom_point() 
```

Nachdem wir jetzt schon die Grundprinzipien kennengelernt haben, schauen wir uns die nächsten `geom`s etwas kürze an.

### `geom_boxplot()`

Der Boxplot als eine praktische Art der Visualisierung sollte natürlich auch nicht fehlen und hat daher auch ein eigenes `geom` spendiert bekommen. Hier ist zu beachten, dass die Abbildung auf die $x$-Achse üblichweise nicht numerisch sondern entweder nominal oder ordinal ist. In der Praxis können auch Zeichenketten verwendet werden, die dann als Faktor interpretiert werden.

```{r}
#| echo: true
#| label: fig-r-basics-boxplot
#| fig-cap: "Ein Boxplot mit `geom_boxplot()`."

ggplot(df, aes(team, dv)) +
  geom_boxplot()
```

Wir können wieder mehrere Abbildungen kombinieren und zum Beispiel getrennte Boxplots für die Teams und die Gruppen erstellen.

```{r}
#| echo: true
#| label: fig-r-basics-boxplot-2
#| fig-cap: "Getrennte Boxplots nach Team und Gruppe."

ggplot(df, aes(team, dv, fill=group)) +
  geom_boxplot()
```

### `geom_col()`

Die nächste Art von Visualisierung sind Säulendiagramme. Hier müssen wir allerdings etwas Vorarbeit leisten. Bei Säulendiagrammen wird in den meisten Fällen der Mittelwert dargestellt. Daher müssen wir den Mittelwert zunächste berechnen.

```{r}
#| echo: true

df_team <- df |> 
  group_by(team) |> 
  summarize(m = mean(dv), sd = sd(dv))
```

```{r}
df_team |> knitr::kable(booktabs=TRUE, digits = 2)
```

Wir haben gleich auch noch die Standardabweichungen berechnet, da wir die gleich benötigen werden. Säulendiagramme können mit `geom_col()` erstellt werden. Wie bei `geom_boxplot()` sollte die $x$-Skala nominal sein.

```{r}
#| echo: true
#| label: fig-r-basics-col-01
#| fig-cap: "Ein Säulendiagramm mit `geom_col()`."

ggplot(df_team, aes(team, m)) +
  geom_col()
```

So weit so gut, aber keine Darstellung des Mittelwerts ohne eine Darstellung der Streuung. Dazu benötigen wir allerdings ein weiteres `geom`.

### `geom_errorbar()`

Um die Streuung in einem Säulendiagramm darzustellen, verwenden wir `geom_errorbar()`. Hier kommen endlich einmal zwei neue Abbildungen ins Spiel. `ymin` bestimmt das untere Ende des Fehlerbalkens, während `ymax` das obere Ende bestimmt. D.h. die Fehlerbalken sind müssen nicht unbedingt in beide Richtungen gleich lang sein.

```{r}
#| echo: true
#| label: fig-r-basics-col-02
#| fig-cap: "Ein Säulendiagram mit `geom_col()`."

ggplot(df_team, aes(team, m)) +
  geom_col() +
  geom_errorbar(aes(ymin = m - sd, ymax = m + sd))
```

In @fig-r-basics-col-02 können wir nun die Fehlerbalken sehen, aber schön sehen sie noch nicht besonders aus. Mit dem Argument `width` können wir spezifieren wie breit die Hüte sein sollen, während `linewidth` die Linienbreite spezifiziert. Passen wir auch gleich noch die Farbe etwas an.


```{r}
#| echo: true
#| label: fig-r-basics-col-03
#| fig-cap: "Ein Säulendiagram mit `geom_col()`."

ggplot(df_team, aes(team, m)) +
  geom_col() +
  geom_errorbar(aes(ymin = m - sd, ymax = m + sd), width = 0.3, color = 'red',
                linewidth = 1.5)
```

Ein Problem das mit `geom_errorbar()` immer wieder auftaucht entsteht wenn es mehrere Werte für jeden $x$-Wert gibt. In der Standardeinstellung werden die Werte aufeinander gesetzt (stacked). Erstellen wir uns erst einmal einen passenden Datensatz.

```{r}
#| echo: true

df_tg <- df |> group_by(team, group) |> summarize(m = mean(dv), sd = sd(dv))
```
```{r}
df_tg |> knitr::kable(booktabs=T, digits = 3, linesep = '')
```

Schauen wir uns erst einmal nur die Säulen an.

```{r}
#| echo: true
#| label: fig-r-basics-col-04

ggplot(df_tg, aes(team, m, fill = group)) +
  geom_col()
```

In @fig-r-basics-col-04 sind die Mehrfachwerte aufeinandergestapelt. Dies ist in den seltensten Fällen die Darstellungsart die wir wirklich wollen. Wir können die Säulen nebeneinander setzen indem wir das Argument `position` in `geom_col()` verwenden.


```{r}
#| echo: true
#| label: fig-r-basics-col-05
#| fig-label: "Gruppierte Säulen"

ggplot(df_tg, aes(team, m, fill = group)) +
  geom_col(position = 'dodge')
```

@fig-r-basics-col-05 sind schon besser aus. Es entsteht nun ein Problem wenn wir die Fehlerbalken dazufügen.

```{r}
#| echo: true
#| label: fig-r-basics-col-06
#| fig-label: "Gruppierte Säulen mit Fehlerbalken"

ggplot(df_tg, aes(team, m, fill = group)) +
  geom_col(position = 'dodge') +
  geom_errorbar(aes(ymin = m - sd, ymax = m + sd))
```

Die Fehlerbalken in @fig-r-basics-col-06 sind beide in der Mitte des $x$-Wertes plaziert. Mit `dodge`

```{r}
#| echo: true
#| label: fig-r-basics-col-07
#| fig-label: "Gruppierte Säulen mit Fehlerbalken"

ggplot(df_tg, aes(team, m, fill = group)) +
  geom_col(position = 'dodge') +
  geom_errorbar(aes(ymin = m - sd, ymax = m + sd), position = 'dodge')
```

Passt es jetzt wieder (siehe @fig-r-basics-col-07). Allerdings nur solange wie nicht die Breite verändern.

```{r}
#| echo: true
#| label: fig-r-basics-col-08
#| fig-label: "Gruppierte Säulen mit falsch verschobenen Fehlerbalken"

ggplot(df_tg, aes(team, m, fill = group)) +
  geom_col(position = 'dodge') +
  geom_errorbar(aes(ymin = m - sd, ymax = m + sd), position = 'dodge', width=.7)
```

Was jetzt passiert, `ggplot()` berechnet die Breite des `geom`s und verschiebt es entsprechend der Breite nach links oder rechts, so dass keine Überlappung besteht. Die einfachste Lösung besteht darin die Position der Fehlerbalken der Funktion `position_dodge()` direkt anzugeben.

```{r}
#| echo: true
#| label: fig-r-basics-col-09
#| fig-label: "Gruppierte Säulen mit korrekten Fehlerbalken"

ggplot(df_tg, aes(team, m, fill = group)) +
  geom_col(position = 'dodge') +
  geom_errorbar(aes(ymin = m - sd, ymax = m + sd),
                position = position_dodge(width = .9), 
                width=.7)
```

Warum das in @fig-r-basics-col-09 funktioniert erscheint zunächst etwas undurchsichtig. Es hat was damit zu tun, dass die Säulen in `geom_col()` standardmäßig eine Breite von `0.9` in den Einheiten des Graphen haben. Entsprechend werden die Säulen um diesen Wert gegeneinander verschoben. Mit der Funktion `position_dodge()` sorgen wir dafür, dass die Fehlerbalken ebenfalls um diesen Wert verschoben werden und nicht um deren eigene Breite. Würden wir die Breite der Säulen selbst spezifieren, dann würde die Fehlerbalken wieder nicht an der richtigen Position sitzen (siehe @fig-r-basics-col-10).

```{r}
#| echo: true
#| label: fig-r-basics-col-10
#| fig-label: "Gruppierte Säulen mit korrekten Fehlerbalken"

ggplot(df_tg, aes(team, m, fill = group)) +
  geom_col(position = 'dodge', width = 0.5) +
  geom_errorbar(aes(ymin = m - sd, ymax = m + sd),
                position = position_dodge(width = .9), 
                width=.7)
```

### `geom_histogram()`

Schauen wir uns letztes `geom` nun noch `geom_histogram()` an, mit dem wir Histogramme erstellen können. Wir erstellen dazu aber erst noch einen neuen Datensatz, da `df` für ein Histogram etwas unterbesetzt ist.

```{r}
#| echo: true
#| fig-cap: "Beispiel für ein Histogram mit `geom_histogram()`"
#| label: fig-r-basics-hist

df_hi <- tibble(x = rnorm(100))
ggplot(df_hi, aes(x)) + 
  geom_histogram()
```

### `facet_grid()` und `facet_wrap()`

Als nächstes kommen zwei der praktischsten Funktion in `ggplot2` überhaupt. Mit `facet_grid()` können wir ein Raster an Grafiken erstellen. Hier zwei einfache Beispiel für ein horizontales und eine vertikales Raster.

```{r}
#| echo: true
#| label: fig-r-basics-facet-01
#| layout-ncol: 2
#| fig-cap: "Zwei einfacher Beispiel für Grafikraster."
#| fig-subcap:
#|   - "Horizontales Raster"
#|   - "Vertikales Raster"

ggplot(df, aes(iv, dv)) +
  geom_point() +
  facet_grid(~team)
ggplot(df, aes(iv, dv)) +
  geom_point() +
  facet_grid(team~.)
```

Das Raster wir in `facet_grid()` mittels einer Formel mit der `~` spezifiziert. Nach dem Muster `Vertikal~Horizontal`. Wenn wir ein vertikales Raster erstellen möchten müssen wir einen Punkt `.` in die Formel aus Syntaxgründen einfügen. Natürlich können wir auch beide gleichzeitig verwenden. Was bei unseren Datensatz nicht wirklich spannend aussieht, da wir zu wenig Daten dafür haben (siehe @fig-r-basics-facet-02).

```{r}
#| echo: true
#| label: fig-r-basics-facet-02
#| fig-cap: "Eine vollständige Rastergrafik"

ggplot(df, aes(iv, dv)) +
  geom_line() +
  geom_point() +
  facet_grid(group ~ team)
```

Mit `facet_wrap()` kann ein ähnliche Effekt erreicht werden, nur das die Untergrafiken von links oben nach rechts unten aufgebaut werden und vor allem nützlich sind wenn wir viele Gruppen auf einmal anschauen wollen.

```{r}
#| echo: true
#| label: fig-r-basics-facet-03
#| fig-cap: "Ein Beispiel für facet_wrap()"
#| fig-height: 5
 
df_fw <- tibble(id = paste0('P',1:10), Wert = 1:10, x = 1)
ggplot(df_fw, aes(x,Wert)) +
  geom_point(size=2) +
  facet_wrap(~id)
```

### `labs()` und `lims()`

Wenn wir die Achsenbeschriftungen ändern wollen können wir dafür die Funktion `labs()` benutzen, während `lims()` die minimalen (maximalen) Werte der Achsen bestimmt.

```{r}
#| echo: true
#| label: fig-r-basics-labs
#| fig-cap: "Beschriftungen und Achsenwerte mit `labs()` und `lims()` bestimmen."

ggplot(df, aes(iv, dv)) +
  geom_point() +
  labs(x = 'Die unabhängige Variable', y = 'Die abhängige Variable') +
  lims(x = c(0,30), y = c(-5,20))
```

Wenn ihr mehr Kontrolle über die Achsen benötigt gibt es die Funktionen `scale_x_continuous()` für eine kontinuierliche Variable auf der $x$-Achse und `scale_x_discrete()` für nominale Variablen (bzw. die gleichen Funktion mit `y` anstatt `x` für die $y$-Achse). Mit diesen Funktionen kann das Aussehen der Achsen vollständig angepasst werden.

### `theme()` 

Als letzten Punkt noch die Funktion `theme()` mit der alle möglichen Formatierungseinstellung von `ggplot` angesprochen und individuell angepasst werden können. Hier als einfachstes Beispiel die Schriftgröße gleichmäßig für alle Textkomponenten in der Grafik anpassen.

```{r}
#| echo: true
#| label: fig-r-basics-theme
#| fig-cap: "Textgrößen mit `theme()` anpassen."

ggplot(df, aes(iv,dv)) +
  geom_point() +
  theme(text = element_text(size = 24))
```

Wenn ihr mit `theme()` arbeiten wollt/müsst, schaut euch wie immer die umfangreiche [Dokumentation](https://ggplot2.tidyverse.org/reference/theme.html) an.

### Weiterführendes

Insgesamt haben wir wieder nur die Oberfläche von `ggplot` gekratzt. Allerdings sollte ihr mit diesen wenigen `geom`s schon eine Großzahl eurer Anwendungsfälle abbilden können. Im Netz gibt es eine Reihe von sehr guten freien Quellen um sich eingehender mit `ggplot` auseinander zu setzen. Das [Buch](https://r-graphics.org/) von @chang2018 gibt schnelle Lösungen für konkrete Probleme. @healy2018 gibt mehr Hilfestellungen zur effektiven Visualisierung mit [ggplot](https://socviz.co/), während @wickham2009 das definitive Nachschlagewerk vom Haupprogrammier von [ggplot2](https://ggplot2-book.org/) ist. Bei Problemen ist auch wie gesagt die extrem gute [Dokumentation](https://ggplot2.tidyverse.org/) zu konsultieren. Dazu kommen mittlerweile auch eine Reihe von Zusatzbibliotheken um praktisch jede erdenkliche Art von Visualisierung zu erstellen. Ein Paket das ihr auch auf jeden Fall anschauen solltes ist [`patchwork`](https://patchwork.data-imaginist.com/index.html).