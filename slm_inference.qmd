# Inferenz

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

Nachdem wir im vorhergehenden Kapitel gelehrnt haben, wie wir ein Regressionsgerade an einen Datensatz fitten. Stellt sich nun die Frage ob die Regressionsgerade tatsächlich einen relevanten Zusammenhang zwischen den beiden Variablen beschreibt. Da das einfache lineare Modelle zwei Parameter $\beta_0$ und $\beta_1$ beinhaltet kann diese Fragestellung auf beide Koeffizienten angewendet werden. D.h. wir fragen uns ob das Modell einen statistisch signifikanten Zusammenhang zwischen den beiden Variablen beschreibt. Bezogen auf die beiden Parameter, ist der Parameter $\hat{\beta}_0$ statistisch signifikant und ist der Parameter $\hat{\beta}_1$ statistisch signifikant? Um unseren Werkzeugsatz zu statistischer Signifikanz anwenden zu können brauchen wir aber erst einmal wieder eine Verteilung bei der wir kritische Bereiche markieren können um zu entscheiden ob eine beobachtete Statistik statistisch signifikant ist. Wie behalten dabei im Hinterkopf das statistische Signifikanz nicht das Gleiche ist wie praktische Relevanz.

## Statistische Überprüfung von $\beta_1$ und $\beta_0$

Der erste Schritt um eine Verteilung zu bekommen ist allerdings, dass wir zunächst einmal eine Zufallsvariable benötigen. Bisher haben wir den Zusammenhang zwischen Variablen über die Formel

$$
y_i = \beta_0 + \beta_1 \cdot x_i
$$

beschrieben. In dieser Form ist allerdings noch kein zufälliges Element vorhanden. Für ein gegebenes $x_i$ bekommen wir ein genau spezifiziertes $y_i$. Allerdings haben wir bei der Herleitung gesehen, dass die Daten in den seltensten Fällen genau auf der Gerade liegen, sondern wir die Parameter $\hat{\beta}_0$ und $\hat{beta}_1$ so gewählt haben, dass die quadrierten Abweichungen, die Residuen $\epsilon_i$ minimal werden. Dies Residuen verwenden wir jetzt um eine zufälliges Element in unsere Regression rein zu bekommen. Ein mögliche Annahme ist, das die Residuen beispielsweise Normalverteilt sind.

Warum könnte dies Sinn machen. In dem vorhergehenden Weitsprungbeispiel haben wir informell hergeleitet, dass die Weitsprungleistung von unzähligen weiteren Faktoren beeinflusst werden kann, welche dazu führen, dass für eine gegebene Anlaufgeschwindigkeit nicht immer die gleiche Weitsprungweite erzielt wird. Generell, ist diese Art der Begründung bei biologischen System meistens plausibel. In vorhergehenden Abschnitt haben wir dazu aber auch noch gesehen, dass die Normalverteilung eben gut geeignet ist, um solche Prozesse, bei denen viele kleine additive Effekt auftreten. Dieser Argumentation folgend ist es plausibel diese Einflüsse auch beim Regressionsfall mittels einer Normalverteilung zu modellieren. Dazu führen wir noch eine weitere Annahme an, nämlich dass diese Einflüsse im Mittel in gleichen Maßen die Werte nach nach oben wie auch nach unten ablenken. D.h. die Werte nach oben und unten von der Regressionsgerade abweichen. Dies erlaubt uns jetzt die Annahme genau zu spezifizieren.

$$
\epsilon_i \sim \mathcal{N}(0, \sigma^2)
$$

D.h also, wir gehen davon aus, dass die Residuen normalverteilt sind, mit einem Mittelwert von $\mu = 0$ und einer noch näher zu spezifizierenden Varianz $\sigma^2$. Das führt dann zu der folgenden Formulierung des Regressionsmodells.

$$
Y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma^2)
$$ {#eq-simple-regression-model}

$Y$ wird jetzt groß geschrieben, da es sich um eine Zufallsvariable handelt. Dies führt jetzt dazu, das das Regressionsmodell in zwei Teile unterschieden werden kann. Einmal eine deterministischen Teil $\beta_0 + \beta_1 \cdot x$ und einen stochastischen Teil $\epsilon_i$. Dies führt dazu, dass $Y_i$ ebenfalls stochastisch ist und zu einer Zufallsvariable wird.

Schauen wir uns weiter an, wie sich $Y_i$ verhält, wenn wir $x_i$ als Konstante $x$ mit ein bestimmten Wert annehmen. Dann wird aus @eq-simple-regression-model $Y_i = \beta_0 + \beta_i \cdot x + \epsilon_i$. Folglich bleibt der deterministische Teil immer gleich, wird zu einer Konstante. Da $\epsilon_i$ normalverteilt ist ist $Y_i$ ebenfalls normalverteilt. Der Mittelwert der Normalverteilung von $Y_i$ $\mu_{Y_i}$ ist allerdings nicht gleich Null, sondern die Normalverteilung von $\epsilon_i$ wird um die Konstante $\beta_0 + \beta_1 \cdot x$ verschoben (siehe @fig-slm-inf-epsilon-1). Das führt dazu, dass $Y_i$ der Verteilung $\mathcal{N}(\beta_0 + \beta_1 x)$ folgt.

```{r}
#| label: fig-slm-inf-epsilon-1
#| layout-ncol: 2
#| fig.height: 3
#| fig.cap: Relation der Lageparameter von $e_i$ und $Y_i$
#| fig-subcap: 
#|   - "Verteilung von $\\epsilon_i$"
#|   - "Verteilung von $Y_i$"
 
ggplot(tibble(x = seq(-3,3,length.out=100), y = dnorm(x)), aes(x,ymin=0,ymax=y)) +
  geom_ribbon(fill='red', alpha=.5) +
  geom_line(aes(x,y)) +
  scale_x_continuous(expression(epsilon[i]), breaks = 0) +
  scale_y_continuous('Dichte', breaks = NULL)

ggplot(tibble(x = seq(-3,3,length.out=100), y = dnorm(x)), aes(x,ymin=0,ymax=y)) +
  geom_ribbon(fill='red', alpha=.5) +
  geom_line(aes(x,y)) +
  scale_x_continuous(expression(Y[i]), breaks = 0, labels=expression(beta[0]+beta[1]~x)) +
  scale_y_continuous('Dichte', breaks = NULL)
```

Daraus folgt jetzt aber zusätzlich, dass für jedes gegebenes $X$ die $Y$-Werte einer Normalverteilung folgen. Lediglich die Verschiebung des Mittelwert der jeweiligen $Y$-Normalverteilung hängt von $X$ über die Formel $\beta_0 + \beta_1 \cdot X$ zusammen. Formal:

$$
Y|X \sim N(\beta_0+ \beta_1 X,\sigma^2)
$$

Die Schreibweise $|X$ wird übersetzt für gegenbenes $X$ und sagt aus, dass die Verteilung von $Y$ von $X$ abhängt. Es handelt sich dabei um eine bedingte Wahrscheinlichkeit. Die Varianz der jeweiligen $Y$-Werte ist dabei die zuvor angenommen Varianz der $\epsilon_i$ also $\sigma^2$. Eine wichtige Annahme die noch mal betont werden sollte, wir gehen davon aus, dass die einzelnen Punkte unabhängig voneinander sind. Im Weitsprungbeispiel würde dies bedeuten, dass jeder Sprung von einem anderen Athleten kommen muss.

Wenn wir die Verteilungen von $Y$ graphisch führ beispielweise drei verschiedene $X$-Wert darstellen, dann folgt daraus die folgende Abbildung (siehe @fig-slm-inf-epsilon-2). D.h. für jeden $X$-Wert werden mehrere $Y$-Werte beobachtet, die jeweils einer Normalverteilung folgen.

```{r}
#| fig.cap: Verteilung der Daten für verschiedene $x$-Werte
#| label: fig-slm-inf-epsilon-2

include_graphics('pics/Stats_Figures.png')
```

In @fig-slm-inf-epsilon-2 ist klar zu sehen, wie für jeden der drei Punkte von $X$ die beobachteten $Y$-Werte einer Normalverteilung. Die Breite der Verteilung ist an jedem Punkte gleich, nämlich $=\sigma^2$ während der Mittelwert der Gleichung $\beta_0 + \beta_1 X$ folgend entlang der Regressionsgerade verschoben ist.

Wenn wir uns zurück an die Ausführungen zur statistischen Signifikanz erinnern, dann haben wir in dem Zusammenhang vom einem datengenerierenden Prozess gesprochen (@def-dgp) (DGP). In unserem jetzigen Modell können wir dementsprechend zwei Komponenten als Teile des DGP identizifieren. Entsprechend @eq-simple-regression-model besteht der DGP aus dem deterministischen Teil $\beta_0 + \beta_1 X$ und dem stochastischen Teil $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$. Diese Einsicht können wir verwenden um die Eigenschaften dieses Modells bezüglich Aussagen über statistische Signifikanz zu untersuchen.

Wir fokussieren uns jetzt auf ein vereinfachtes Modell bei dem wir zusätzlich noch $\beta_0 = 0$ setzen, und wir uns erst mal nur für die Eigenschaften von $\beta_1$ interessieren. Gehen wir nun davon aus, dass zwischen $X$ und $Y$ der Zusammenhang $\beta_1 = 1$ besteht. D.h. wenn $X$ um eine Einheit vergrößert wird, dann wird $Y$ ebenfalls um eine Einheit größer.

$$
Y = 0 + 1 \cdot X + \epsilon, \quad \epsilon\sim\mathcal{N}(0,\sigma^2)
$$ {#eq-slm-inf-sim-mod-1}

Jetzt müssen wir noch einen Wert für $\sigma^2$ festlegen. Sei dieser einfach einmal $\sigma = \frac{1}{2}$. Jetzt können wir `R` benutzen um *Experimente*, also Beobachtungen, anhand dieses DGP zu simulieren. Der Einfachheit halber legen wir ein übersichtliches $N = 12$ fest und nehmen uns jeweils drei $X$-Werte z.B.  mit $X \in \{-1, 0, 1\}$, d.h. wir ziehen für jeden $X$-Wert vier $Y$-Werte.

```{r}
#| echo: true

N <- 12
beta_0 <- 0
beta_1 <- 1
sigma <- 1/2
dat_sim_1 <- tibble(
  x_i = rep(-1:1, each=4),
  y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma)
)
```

Wenn wir uns die generierten Daten anschauen, dann sehen wir wenig überraschend 12 verschiedene Werte für $y_i$ und jeweils $3 \times 4$ verschiedene Werte für $x_i$ (siehe @tbl-slm-inf-sim-1).

```{r}
#| label: tbl-slm-inf-sim-1
#| tbl-cap: Eine Simulation des Modells @eq-slm-inf-sim-mod-1
 
kable(dat_sim_1,
      booktabs = TRUE,
      linesep = '',
      digits = 2)
```

Wenn wir die Daten graphisch darstellen erhalten wir (@fig-slm-inf-sim-1):

```{r}
#| echo: true
#| label: fig-slm-inf-sim-1
#| fig.cap: Streudiagramm der Daten aus @tbl-slm-inf-sim-1

ggplot(dat_sim_1, aes(x_i, y_i)) + 
  geom_point()
```

Ebenfalls wenig überraschend, die Punkte sind auf den $x$-Werten $-1, 0$ und $1$ zentriert und liegen nicht alle aufeinander, da sie einer Zufallsstichprobe aus $\mathcal{N}(0, \frac{1}{4})$ entspringen.

Jetzt kann ich natürlich für diese Daten unsere Normalengleichungen anwenden und Werte für $\hat{\beta}_0$ und $\hat{\beta}_1$ berechnen. Oder eben direkt in `R`.

```{r}
#| echo: true

mod_sim_1 <- lm(y_i ~ x_i, dat_sim_1)
coef(mod_sim_1)
```

Wir sehen, dass die berechneten Werte für $\beta_0$ und $\beta_1$ schon in der Nähe der tatsächlichen Werte liegen (siehe @eq-slm-inf-mod-1), aber auf Grund der Stichprobenvariabilität eben nicht genau auf diesen Werten. Was passiert denn jetzt, wenn ich das Ganze noch einmal durchlaufen lassen?

```{r}
#| echo: true

dat_sim_2 <- tibble(
  x_i = rep(-1:1, each=4),
  y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma)
)
mod_sim_2 <- lm(y_i ~ x_i, dat_sim_2)
coef(mod_sim_2)
```

Wieder wenig überraschend, da jedes Mal wenn ich `rnom()` eine neue Ziehung aus der Normalverteilung generiert wird, erhalte ich neue Werte für $y_i$ und dementsprechend andere Werte für $\hat{\beta}_0$ und $\hat{\beta}_1$. Nochmal, warum? **Stichprobenvariabilität**! Jetzt sind wir wieder bei dem gleichen Prinzip, das wir im Rahmen der kleinen Welt ausgiebig behandelt haben. Schauen wir uns jetzt doch einfach mal was passiert wenn wir die Simulation nicht $2\times$ sondern z.B. $1000\times$ durchführen.

```{r}
set.seed(123)
```

```{r}
#| echo: true

N_sim <- 1000
beta_1_s <- numeric(N_sim)
x_i <- rep(-1:1, each=4)
for (i in 1:N_sim) {
  daten_temporaer <- tibble(x_i,
                            y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma))
  model_temporaer <- lm(y_i ~ x_i, daten_temporaer)
  beta_1_s[i] <- coef(model_temporaer)[2]
}
```

Wir erhalten jetzt einen Vektor `beta_1_s` mit $`r N_sim`$ beobachteten $\hat{\beta}_1$. Da das etwas viele Werte sind um die uns einzeln anzuschauen, erstellen ein Histogramm der $\hat{\beta}_1$s. (@fig-slm-inf-hist-sim-1).

```{r}
#| echo: true
#| fig.cap: Histogram der auf den simulierten Daten berechneten $\hat{\beta}_1$. Wahrer Wert von $\beta_1$ rot eingezeichnet.
#| label: fig-slm-inf-hist-sim-1
#| fig.height: 4

hist(beta_1_s, xlab = expression(hat(beta)[1]), main='')
abline(v = beta_1, col='red', lty=2)
```

In @fig-slm-inf-hist-sim-1 begegnet uns zunächst einmal wieder unsere altbekannte Glockenkurve. Schön ist, dass deren Mittelwert im Bereich des wahren Werts von $\beta_1$ liegt und Werte mit größer werdender Abweichung vom wahren Wert in ihrer Häufigkeit abnehmen. Aber die Häufigkeit ist nicht Null, sondern eben nur geringer. Werte in der Nähe von $\beta_1$ weisen dagegen eine größere Häufigkeit aufweisen. Das sollte uns jetzt auch irgendwie zufrieden stimmen, denn dies bedeutet, dass wir in der Lage sind mit unserem Regressionsmodell im Mittel tatsächlich den korrekten Wert abzuschätzen. Allerdings, wie immer, bei einer einzelnen Durchführung des Experiments können wir alles von perfekt spot-on bis komplett danebenliegen und würden es nicht wissen.

Wir können jetzt aber auch wieder ganz parallel zu unseren Herleitungen in der kleinen Welt einen Entscheidungsprozess spezifizieren. Wenn @fig-slm-inf-hist-sim-1 den DGP beschreibt und das die Verteilung der zu erwartenden $\hat{\beta}_1$ unter dem Modell sind. Bei der Dürchführung eines neuen Experiments, dann würden wir sagen, dass wenn unserer beobachteter Wert in den Rändern der Verteilung von @fig-slm-inf-hist-sim-1 liegt, das wir eher nicht davon ausgehen, dass unserer neues Experiment den gleichen DGP zugrundeliegen hat. D.h wir definieren uns jetzt Grenzen am oberen und am unteren Rand der Verteilung. Wenn jetzt ein neuer beobachteter Wert entweder unterhalb der unteren Grenze oder oberhalb der oberen Grenze liegt, dann sagen wir: *Wir sind jetzt aber sehr überrascht diesen Wert zu sehen, wenn der dem gleichen datengenerierenden Prozess entstammen soll. Daher glauben wir nicht, dass dieses Experiment den gleichen DGP besitzt.*

Um diese Entscheidung treffen zu können, müssen wir also Grenzen definieren. Dazu können wir zunächst einmal einfach die Quantilen der Verteilung nehmen und schneiden z.B. unten $2.5\%$ und oben $2.5\%$ ab. So kommen wir dann insgesamt auf $5\%$, um auf die übliche Irrtumswahrscheinlichkeit von $\alpha = 0.05$ zu kommen. Dazu benutzen wir `R` und zwar `quantile()`-Funktion^[Im folgenden Snippet werden die Werte auf zwei Kommastellen mit `round()` der besseren Darstellung wegen gerundet).

```{r}
crit_vals <- round(quantile(beta_1_s, probs = c(0.025, 0.975)), 2)
crit_vals
```

Mittels dieser Werte können wir zwei disjunkte Wertmenge definieren, einmal die Werte innerhalb von $\hat{\beta}_1 \in [`r paste(crit_vals,collapse=',')`]$ bei denen wir nicht überrascht sind, und die unter der Annahme $\beta_1 = 1$ erwartbar sind und die Werte $\hat{\beta}_1 \notin [`r paste(crit_vals,collapse=',')`]$ diejenigen Werte die uns überraschen würden unter der Annahme. Ins Histogramm übertragen (siehe @fig-slm-inf-hist-sim-2).

```{r}
#| fig.cap: Histogram der auf den simulierten Daten berechneten $\hat{\beta}_1$. Wahrer Wert von $\beta_1$ rot eingezeichnet und kritische Werte grün.
#| label: fig-slm-inf-hist-sim-2
#| fig.height: 4

hist(beta_1_s, xlab = expression(hat(beta)[1]), main='')
abline(v = beta_1, col='red', lty=2)
abline(v = crit_vals, col='green', lty=2)
```
```{r}
set.seed(22411)
daten_neu <- tibble(x_i,
                    y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma))
model_neu <- lm(y_i ~ x_i, daten_neu)
beta_neu <- round(coef(model_neu)[2] + 0.1, 2)
```

Führen wir nun ein Experiment noch einmal durch. Wir beobachten einen Wert für $\hat{\beta}_1$ von 
$`r beta_neu`$. Dieser Wert liegt außerhalb unseres definierten Intervalls $[`r round(crit_vals[1],2)`, `r round(crit_vals[2], 2)`]$, daher sehen wir diesen Wert als derart unwahrscheinlich unter dem angenommenen DGP, das wir sagen: *Wir glauben nicht, dass diesem Experiment nicht der angenommene DGP zugrunde liegt*. Graphisch wieder dargestellt (siehe @fig-slm-inf-hist-sim-3).


```{r}
#| fig.cap: Histogram der auf den simulierten Daten berechneten $\hat{\beta}_1$. Wahrer Wert von $\beta_1$ rot eingezeichnet und kritische Werte grün und der beobachtete Wert als roter Punkt.
#| label: fig-slm-inf-hist-sim-3
#| fig.height: 4

hist(beta_1_s, xlab = expression(hat(beta)[1]), main='')
abline(v = beta_1, col='red', lty=2)
abline(v = crit_vals, col='green', lty=2)
points(beta_neu, 0, col='red', lwd=4)
```

Daher würden wir diesen Wert als statisisch signifikant bezeichnen und würden unsere Annahme ablehnen.

Jetzt sind wir aber etwas hin und her zwischen Experiment, Annahmen und Schlussfolgerungen gesprungen. Normalerweise kennen wir die Stichprobenverteilung nicht vor dem Experiment, sondern, wir sind am dem Wert $\beta_1$ interessiert. Wenn wir den Wert schon wissen würden, dann müssten wir ja gar kein Experiment mehr durchführen. D.h. wir haben eigentlich noch keinen klaren Vorkenntnisse. Mit welcher Annahme gehen wir dann in das Experiment rein? Nun, wir schon bei kleinen Welt Beispiel, starten wir mit der Annahme das zwischen den beiden Variablen kein Zusammenhang besteht. Übertragen auf die Modellparameter also, dass kein linearer Zusammenhang zwischen den beiden Variablen besteht.

\begin{align*}
H_0: \beta_1 &= 0 \\
H_1: \beta_1 &\neq 0
\end{align*} 

Um die Stichprobenverteilung unter der $H_0$ formal Herleitung zu können, ist der Erwartungswert von $\hat{\beta}_1$ und dessen Standardfehler notwendig. Es lässt sich zeigen, dass die folgenden Zusammenhänge unter den gesetzten Annahmen bestehen:

$$
E[\hat{\beta}_0] = \beta_0
$$

Also der Schätzer von $\beta_1$ ist erwartungstreu (biased) und der Standardfehler des Schätzer lässt sich wie folgt bestimmen.

$$
\sigma_{\beta_1} = \sqrt{\frac{\sigma^2}{\sum{(X_i - \bar{X})^2}}}
$$ {#eq-slm-beta1-se}

Hier taucht jetzt zum ersten Mal der Parameter $\sigma^2$ formal auf. Wo kommt diese Variance her? Sie gehört zu unserer Annahme der Verteilung der $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$. Bisher haben wir aber noch gar keine Möglichkeit kennen gelerntm, diese abzuschätzen. Wieder nach etwas motivierten Starren auf die verschiedenen Formeln, könnte heuristisch plausibel sein, dass die Varianz, also die Streuung der $\epsilon_i$ mit der Streuung unserer Werte um die Regressionsgerade zusammenhängen könnten. Formal hatten wir diese als Residuen bezeichnet und mit $e_i = \hat{y}_i - y_i$ bezeichnet. Vormals hatten wir diese Abweichungen als Fehler bezeichnet, aber unter den jetzt eingeführten Annahmen, handelt es sich nicht wirklich um Fehler, sondern die Abweichungen sind eine Folge davon, dass $Y_i$ für jeden Wert von $X_i$ nicht nur einen einzigen Wert hat, sondern eben einer Verteilung folgt $Y_i|X_i \sim \mathcal{N}(\beta_0 + beta_1, \sigma^2)$ deren Form über die $\epsilon_i$ bestimmt wird.

Die $e_i$ sind tatsächlich die Schätzer für die *wahren* $\epsilon_i$ also $e_i = \hat{\epsilon_i} = \hat{y}_i - y_i$. Es lässt sich nun wieder zeigen, dass mittels dieser $e_i$ ein erwartungstreuer Schätzer für $\sigma^2$ erzeugen lässt. Nämlich die mittleren quadrierten Abweichungen (MSE).

$$
\hat{\sigma} = \frac{\sqrt{\sum_{i=1}^N e_i^2}}{N-2} = \frac{\text{SSE}}{N-2} = \text{MSE}
$$ {#eq-slm-sigma}

Da das später immer wieder auftauchen wird, hier auch noch mal in die zwei Komponenten zerlegt. Der Zähler wird als Summe der quadrierten Abweichungen (SSE) bezeichnet und durch den Term $N-2$, der als Freiheitsgerade bezeichnet wird, geteilt. Dann mit die Formel und deren Bezeichnung *mittlere* Abweichung zusammenpasst, wäre es schöner wenn die Summe durch die Anzahl $N$ der Terme geteilt wird, allerdings verhält sich das in diesem Fall ähnlich wie bei der Varianz einer Stichprobe wo die Summe auch durch $N-1$ geteilt wird (zur Erinnerung $s = \frac{\sum_{i=1}^N (x_i - \bar{x})^2}{N-1}$). Jetzt wird dementsprechend nicht durch $N-1$ sondern durch $N-2$ geteilt.

Für unser Problem der Stichprobenverteilung ist jetzt aber wichtiger, dass wir mittels @eq-slm-sigma den Standardfehler von $\hat{\beta}_1$ bestimmen können, indem wir für $\sigma^2$ das mittels der Daten ermittelte $\hat{\sigma}^2$ einsetzen.

$$
\hat{\sigma}_{\beta_1} = \sqrt{\frac{\hat{\sigma}^2}{\sum{(X_i - \bar{X})^2}}}
$$ {#eq-slm-hatbeta1-se}

Dies erlaubt uns jetzt nach unserem bereits bekannten Muster eine Teststatistik für die $H_0$ herzuleiten:

$$
t = \frac{\hat{\beta}_1 - \beta_1}{\hat{\sigma}_{\beta_1}}
$$

Unter der $H_0$ mit $\beta_1 = 0$ wird daraus

$$
t = \frac{\hat{\beta}_1}{\hat{\sigma}_{\beta_1}}
$$ {#eq-slm-beta1-statistic}

Diese Teststatistik folgt einer t-Verteilung mit $N-2$ Freiheitsgeraden. Da diese Formel wieder etwas aus der Luft gegriffen erscheint, hier noch mal eine Simulation zusammen mit der theoretischen Testverteilung.

```{r}
#| echo: true
#| fig.cap: Verteilung von t bei 1000 Simulationen unter der Annahme der $H_0$ und die theoretische Verteilung von t (rot). 
#| label: fig-slm-inf-beta1-dist

N <- 45
n_sim <- 1000
x <- runif(N, -1, 1)
sigma <- 1
experiment <- function() {
  y <- rnorm(N, mean = 0, sd = sigma)
  mod <- lm(y~x)
  b <- coef(mod)[2]
  c(beta_0 = coef(mod)[1],
    beta_1 = coef(mod)[2],
    sigma = sigma(mod))
}
betas <- t(replicate(n_sim, experiment()))
betas <- tibble(beta_0 = betas[,1],
                beta_1 = betas[,2],
                sigma = betas[,3]) |> 
  mutate(
   s_e_beta_1 = sqrt(sigma**2/sum( (x - mean(x))**2)),
   t = beta_1 / s_e_beta_1)
t_theoretical <- tibble(
  t = seq(-3, 3, length.out = 150),
  p = dt(t, N - 2)
)

ggplot(betas, aes(t)) +
  geom_histogram(aes(y = ..density..), bins = 20) +
  geom_line(data = t_theoretical, aes(t, p), color = 'red') +
  labs(x = "t", y = 'Relative Häufigkeit') 
```

In @fig-slm-inf-beta1-dist können wir sehen, dass die theoretische Verteilung in rot die beobachtete Verteilung sehr gut abschätzt. 

In `R` kann der Wert $\hat{\sigma}^2$ über die Funktion `sigma()` aus dem gefitteten `lm()`-Modell extrahiert werden.

```{r defs_reg_02}
jump <- readr::read_delim('data/running_jump.csv',
                          delim=';',
                          col_types = 'dd')
mod <- lm(jump_m ~ v_ms, jump)
```

```{r}
#| echo: true

sigma(mod)
```

Schauen wir uns die Stichprobenverteilung von $\hat{\sigma}^2$ anhand unserer Simulation an. Es ist wieder zu beobachten, das im Mittel der korrekte, im Modell definierte, Wert von $\sigma = 1$ beobachtet wird (siehe @fig-slm-inf-sigma-dist).

```{r}
#| fig.cap: "Verteilung von $\\hat{\\sigma}$ in der Simulation und der wahre Wert in rot eingezeichnet"
#| label: fig-slm-inf-sigma-dist

ggplot(betas, aes(sigma)) +
  geom_histogram(aes(y = ..density..), bins = 20) +
  geom_vline(xintercept = 1, col = 'red', linetype = 'dashed') +
  labs(x = expression(hat(sigma)[i]), y = 'Relative Häufigkeit') 
```


Aber wie immer, leider steht uns bei einem realen Experiment diese Information nicht zur Verfügung und wir haben nur einen einzelnen Wert, der alles von komplett daneben bis ziemlich perfekt sein kann.

Schauen wir uns noch einmal die Ausgabe zu unserem Weitsprungmodell mittels `summary()` an. Unter Residual Standard Error sehen wir, dass hier $\hat{\sigma}$ angegeben wird. Dieser Wert wird auch als mittlerer Schätzfehler bezeichnet und kann als Maß verwendet werden, welche Abweichung das Modell im Mittel hat. Die Einheit sind wieder in den Einheiten der abhängigen Variable, so kann auch schon abgeschätzt werden mit welcher Präzision das Modell die Daten fittet.

```{r}
#| echo: true

summary(mod)
```

In unserem Fall beobachten wir $`r round(sigma(mod), 2)`m$. Diesen Wert muss jetzt unsere Trainerin im Sinne der Weitsprungleistung der deren Varianz interpretieren und ein Abschätzung treffen zu können.

Nach der Herleitung der Teststatistik für $\beta_1$, können wir jetzt auch weitere Teil der Ausgabe von `summary()` interpretieren. In der Tabelle stehen entsprechend die Standardfehler für $\hat{\beta}_1$ und $\hat{\beta}_0$. Für $\beta_0$ wird genau die gleiche Vorgehensweise wie auch bei $\beta_1$ angewendet. Die Nullhypothese $H_0$ ist hier ebenfalls das der Parameter standardmäßig als Null angesetzt wird. Der Standardfehler von $\beta_0$ errechnet sich nach:

\begin{equation}
\sigma^2[\beta_0] = \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}\right)
\label{eq-slm-inf-beta0-se}
\end{equation}

An Formel \eqref{eq-slm-inf-beta-0-se} ist zu erkennen, dass wenn die $X$-Werte den Mittelwert $0$ haben, dass $\sigma^2[\beta_0]$ gleich dem Standardfehler für den Mittelwert SEM wird. Was auch wiederum Sinn macht, da in diesem Fall $\beta_0 = \bar{y}$ gilt.

Dies führt dies zu den beiden zu überprüfenden Hypothesen für $\beta_0$:

\begin{align*}
H_0: \beta_0 &= 0 \\
H_1: \beta_0 &\neq 0
\end{align*}

Dementsprechend überprüft die Hypothesentestung ob der $y$-Achsenabschnitt gleich Null ist. Hier sollte berücksichtigt werden, dass diese Hypothese in den seltensten Fällen tatsächlich auch von Interesse ist und lediglich besagt, dass entweder der $y$-Achsenabschnitt durch den Nullpunkt geht, oder dass wenn tatsächlich $\beta_1 = 0$ gilt, der Mittelwert von $y$ gleich Null ist, was ebenfalls in den seltensten Fällen von Interesse ist.

Die Spalten 3 und 4 in `summary()` unter `Coefficients:` können jetzt interpretiert werden, da es sich hierbei um die $t$-Teststatistik handelt und den entsprechenden p-Wert unter der jeweiligen $H_0$. Die Hypothesen sind ungerichtet.

## Herleitung der Eigenschaften von $\hat{\beta}_1$

Um den Schätzer $\hat{\beta}_1$ für $\beta_1$ formal herzuleiten. Beginnen wir zunächst mit der folgenden Formel, wobei wir im folgenden den Schätzer mit $b_1$ bezeichnen.

$$
b_1 = \sum k_i Y_i
$$ {#eq-b1-k1Yi}

D.h. wir zeigen zunächst, dass $b_1$ durch eine lineare Kombination der $Y_i$-Werte berechnet werden kann. Die Koeffizienten $k_i$ der Summe sind dabei wie folgt definiert: 

$$
k_i = \frac{X_i - \bar{X}}{\sum(X_i - \bar{X})^2}
$$ {#eq-defn-ki}

Der Grund für diese zunächst etwas uneinsichtige Definition wird im weiteren klarer werden. Zunächst haben die $k_i$ verschieldene Eigenschaften die wir uns im Späteren zunutze machen wollen. Zunächst erst einmal noch ein paar Identitäten die wir später auch noch verwenden.

Die erste Identität bezieht sich auf das Kreuzprodukt der Abweichungen von $X_i$ und $Y_i$ von ihren jeweiligen Mittelwerten.

\begin{align*}
\sum(X_i-\bar{X})(Y_i-\bar{Y}) &= \sum(X_i - \bar{X})Y_i  -\underbrace{\sum(X_i - \bar{X})}_{=0}\bar{Y}  \\
&= \sum(X_i - \bar{X})Y_i
\end{align*}

Wenn wir in der Formel $(Y_i-\bar{Y})$ durch $(X_i-\bar{X})$ austauschen, folgt noch eine weitere nützliche Identität:

$$
\sum(X_i-\bar{X})^2 = \sum(X_i - \bar{X})X_i
$$

Werden die jeweiligen $k_i$ mit den dazugehörigen $X_i$ multipliziert und die Definition der $k_i$ (siehe @eq-defn-ki) beachten, erhalten wir:

$$
\sum k_i X_i = \frac{\sum(X_i - \bar{X})X_i}{\sum(X_i-\bar{X})^2} = \frac{\sum(X_i-\bar{X})^2}{\sum(X_i-\bar{X})^2} = 1
$$

D.h. Die Summe der $k_i X_i$ ist gleich $1$. Aus der Definition @eq-defn-ki folgt weiterhin.

$$
\sum k_i = \sum \left(\frac{X_i-\bar{X}}{\sum(X_i-\bar{X})^2}\right)= \frac{\sum(X_i-\bar{X})}{\sum(X_i-\bar{X})^2} = \frac{0}{\sum(X_i-\bar{X})^2} = 0
$$

D.h. die Summe der $k_i$ ist gleich Null.

Wenn wir jetzt wieder die Definition unseres Schätzer für $\beta_1$ verwenden (siehe @eq-slm-basics-norm1). Dann erhalten unter der Verwendung der Identität der Kreuzprodukte den gewünschten Zusammenhang zwischen $b_1$ und $Y_i$.

\begin{align*}
b_1 &= \frac{\sum(X_i - \bar{X})(Y_i - \bar{Y})}{\sum(X_i - \bar{X})^2} \\
&= \frac{\sum(X_i - \bar{X})Y_i}{\sum(X_i - \bar{X})^2} = \sum k_i Y_i\\
\end{align*}

Wenden wir jetzt den Erwartungswert auf $Y_i$ an, dann werden die $k_i$ als konstant angesehen und nur die $Y_i$ sind Zufallsvariablen. Da aber $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ gilt und in dieser Formel wiederum nur $\epsilon_i$ eine Zufallsvariable mit $\beta_0$ und $\beta_1 X_i$ konstant ist und zudem die $\epsilon \sim \mathcal{N}(0,\sigma^2)$ also $E[\epsilon_i] = 0$ laut der Annahme gilt, folgt:

\begin{align*}
    E[b_1] &= E\left[\sum k_i Y_i\right] = \sum k_i E[Y_i] = \sum k_i (\beta_0 + \beta_1 X_i) \\
    &= \beta_0 \sum k_i + \beta_1 \sum k_i X_i = \beta_1
\end{align*}

D.h. @eq-slm-basics-norm1 ist ein erwartungstreuer Schätzer für $\beta_1$. Das gleiche gilt auch für den Schätzer $b_0$ für $\beta_0$.

Leiten wir noch eine weitere Identität über die Summe der $k_i^2$ her:

$$
\sum k_i^2 = \sum \left[\frac{X_i-\bar{X}}{\sum(X_i-\bar{X})^2}\right]^2 = \frac{\sum(X_i-\bar{X})^2}{\left[\sum(X_i-\bar{X})^2\right]^2} = \frac{1}{\sum(X_i-\bar{X})^2}
$$
Können wir auch noch die Varianz bzw. den Standardfehler unseres Schätzers für $\beta_1$ herleiten. Es gilt nämlich:

\begin{align*}
    \sigma^2[b_1] &= \sigma^2\left[\sum k_i Y_i\right] = \sum k_i^2 \sigma^2[Y_i] \\
    &= \sum k_i^2 \sigma^2 = \sigma^2 \sum k_i^2 \\
    &= \sigma^2 \frac{1}{\sum(X_i-\bar{X})^2}
    \label{eq-slm-inf-beta1-deriv}
\end{align*}

Wir erhalten die bereits eingeführte Formel. Wiederum eine Einsicht aus der Herleitung der Formel folgt, dass die Varianz $\sigma^2$ als konstant angesehen wird, d.h. $\sigma_i^2 = \sigma^2$. Dies hat uns erlaubt im zweiten Schritt $\sigma^2$ aus der Summe heraus zu ziehen. Wenn die Varianz $\sigma^2$ nicht konstant ist, dann ist der berechnete Standardfehler für $\hat{\beta}_1 = b_1$ nicht korrekt.

Eine interessante Eigenschaft des Standardfehler von $\hat{\beta}_1$ ist in Formel \eqref{eq-slm-inf-beta1-deriv} zu sehen. Im Nenner stehen die Abweichungen der $X$-Werte vom Mittelwert $\hat{X}$. D.h. wenn die $X$-Werte weiter auseinander sind, dann führt dies dazu, dass der Standardfehler $\sigma^2[b_1]$ kleiner wird. Intuitive macht dies auch Sinn, wenn ich eine Gerade bestimmen will, dann ist es einfacher die Gerade anhand weit auseinander liegenden Stütztwerten zu bestimmen im Vergleich zu wenn ich eng beinander liegende $X$-Werte verwende.

## Maximum-likelihood Methode bei der einfachen linearen Regression

Ein anderer Herleitung für $\beta_0$ und $\beta_1$ kann über die sogenannten Maximum Likelihood durchgeführt werden. Dabei gehen direkt die Verteilungsannahmen direkt ein.

Für eine gegebene Zufallsvariable die jeweilige Dichte eines gegebenen Wertes über die Dichtefunktion berechnet werden. Wenn ein Zufallsvariable $X$ einer Normalverteilung folgt, dann wird die Verteilung von $X$ nach der bereits kennengelernte Dichtefunktion der Normalverteilung beschrieben.

$$
f(X|\mu,\sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2}\frac{(X - \mu)^2}{\sigma^2}\right)
$$

Hier wird die Dichte von $X$ als eine Funktion von $\mu$ und $\sigma^2$ aufgefasst. Es ist aber auch möglich, die Zufallsvariable $X$ als gegeben anzusehen und die Dichte für verschiedene Werte von $\mu$ und $\sigma^2$ abzutragen. Der Einfachheit halber gehen wir davon aus, dass $\sigma^2$ gegeben sei und wir $\mu$ nicht kennen. Eine mögliche Fragestellung ist jetzt, für einen beobachteten Wert $x$, welcher Wert von $\mu$ ist am plausibelsten?

Tragen wir dazu verschiedene Dichtewerte für ein gegebenes $x$ in Abhängigkeit von verschiedenen $\mu$ ab.

D.h. wir interpretieren die Funktion als:

$$
f(\mu|x,\sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2}\frac{(X - \mu)^2}{\sigma^2}\right)
$$

Diese Funktion wird als die likelihood-Funktion bezeichnet. Das Maximum dieser Funktion kann als derjenige Wert interpretiert werden bei dem derjenige Wert von $\mu$ die maximal mögliche Dichte einnimmt.

Die Likelihood-Funktion ist eine Funktion, die die Wahrscheinlichkeit beschreibt, mit der eine gegebene Stichprobe, in Abhängikeit  von den Parametern aus einer bestimmten Verteilung stammt. Die Likelihood-Funktion gibt also an, wie gut die beobachteten Daten zu einem bestimmten Satz von Parametern passen.

Formal wird die Likelihood-Funktion als die gemeinsame Wahrscheinlichkeitsdichte der Stichprobe beschrieben, betrachtet als Funktion der Parameter. Dabei werden die beobachteten Werte als festgelegt und die Parameter als Variablen betrachtet. Die Likelihood-Funktion ist also eine Funktion der Parameter, die die Wahrscheinlichkeit der beobachteten Daten als Funktion dieser Parameter beschreibt. Die Likelihood-Funktion ist dabei keine Dichtefunktion und beschreibt somit keine Wahrscheinlichkeiten. Dementsprechend ist gilt für das Integral der Likelihood-Funktion $\int L(\mu|X,\sigma^2) d\mu \neq 1$ bzw. ist $=1$ per *Zufall*.

In unserem Regressionsfall nimmt die Likelihood-Funktion für einen einzelnen Wert die folgende Form an:

$$
L(\beta_0, \beta_1, \sigma^2|y_i) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2\sigma^2}\right)
$$

Bei unserer Regressionsanalyse haben wir jedoch nicht nur einen einzigen beobachteten Wert $(y_i, x_i)$ sondern $N$ beobachtete Werte. Da die Werte unabhängig voneinander sind (laut der Annahmen), werden die jeweiligen likelihoods miteinander multipliziert. Die resultierende Likelihood-Funktion nimmt dann die folgenden Form an:

\begin{align*}
L(\beta_0, \beta_1, \sigma^2) &= \prod_{i=1}^{N} f(y_i | x_i; \beta_0, \beta_1, \sigma^2) \\
&= \prod_{i=1}^{N} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2\sigma^2}\right) \\
&= \left(\frac{1}{\sqrt{2\pi \sigma^2}}\right)^N \exp\left(\sum_{i=1}^N \frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2}\right) \\
&= \left(\frac{1}{2\pi \sigma^2}\right)^{N/2} \exp\left(\sum_{i=1}^N \frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2}\right) \\
\end{align*}

Die Idee ist jetzt wieder die Gleiche. Wir versuchen das Maximum dieser Funktion zu finden, da die Werte $\beta_0, \beta_1$ und $\sigma^2$ dann so gewählt sind, dass sie die höchste likelihood haben. Der Ansatz erfolgt wieder mechanisch ,indem wie bei der Herleitung der Normalengleichungen, die partiellen Ableitungen berechnet werden, diese gleich Null gesetzt werden und das resultierende Gleichungssystem gelöst wird. Zu beachten hierbei, wir haben in jedem Produktterm die gleichen Parameter $\beta_0, \beta-1$ und $\sigma^2$ und die jeweiligen beobachteten $(y_i, x_i)$ Tuple werden als gegeben angesehen. 

Um die Berechnungn zu vereinfachen, bietet sich bei der Likelihoo-Funktion ein Trick an. Es wird nicht Likelihood-Funktion abgeleitet, sondern der Logarithmus der Likelihood-Funktion. D.h. die Funktion wird transformiert. Bei der Logarithmus-Funktion handelt es sich um eine sogenannte bijektive Funktion. Eine bijektive Funktion ist eine Funktion die jedem Element in der Ursprungsmenge genau ein Element in der Zielmenge zuordnet und ebenfalls umgekehrt. Dadurch kommt es zu keinen Kollisionen oder Auslassungen. Einfach gesagt, wenn die Funktion $y = f(x) = log(x)$ ist, dann wird jedem $x$ genau ein $y$ zugeordnet. Bzw. anders herum, wenn ich $y$ kenne, dann kenne ich auch den Wert von $x$ mit $f(x) = y$ bzw. $x = f^{-1}(y) = \exp(y)$. Dadurch, das die Logarithmus-Funktion bijektiv ist, führt dies dazu, dass das Maximum der ursprünglichen Funktion $L(\beta_0, \beta_1, \sigma^2)$ an der gleichen Stelle auftritt wie bei der transformierten Funktion $\ln L(\beta_0, \beta_1, \sigma^2)$.

Wenn jetzt die Eigenschaften der Logarithmusfunktion, speziell des natürlichen Logarithmus, beachtet werden, dann wird auch klar, warum es Sinn machen könnte die Likelihood-Funktion mit dem Logarithmus zu transformieren, da aus den Produkten Summen werden mit denen einfacher umgegangen werden kann:

\begin{align*}
\log(xy) &= \log(x) + \log(y) \\
\log\left(\frac{x}{y}\right) &= \log(x) - \log(y) \\
\log(x^n) &= n\log(x) \\
\log(\exp(x)) &= x \\
\log(1) &= 0
\end{align*}

Der Logarithmus angewendet auf $L(\beta_0, \beta_1, \sigma^2)$ resultiert dann in der folgenden Funktion:

\begin{align*}
\ell(\beta_0, \beta_1, \sigma^2) &= \ln L(\beta_0, \beta_1, \sigma^2) \\
&= \ln \left[\left(\frac{1}{2\pi \sigma^2}\right)^{N/2} \exp\left(-\sum_{i=1}^N \frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2}\right)\right] \\
&= \ln \left[\left(\frac{1}{2\pi \sigma^2}\right)^{N/2} \right] + \ln \left[\exp\left(-\sum_{i=1}^N \frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2}\right)\right] \\
&= \frac{N}{2} \ln \left[\left(\frac{1}{2\pi \sigma^2}\right) \right] -\sum_{i=1}^N \frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2} \\
&= -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i - \beta_0 - \beta_1 x_i)^2
\end{align*}

Wir die Funktion $\ell(\beta_0, \beta_1, \sigma^2)$ wieder partiell nach $\beta_0$ und $\beta_1$ abgeleitet und gleich Null gesetzt erhalten wir das gleiche Gleichungssystem wie bei den vorhergehenden Herleitungen über die Abweichungen von der Regressionsgeraden. z.B.

\begin{align*}
\frac{\partial \ell(\beta_0, \beta_1, \sigma^2)}{\partial \beta_0} &= \frac{\partial}{\partial \beta_0} -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i - \beta_0 - \beta_1 x_i)^2 \\
&= \frac{2}{2\sigma^2}\sum_{i=1}^N (y_i - \beta_0 - \beta_1 x_i)
\end{align*}
Wenn dieser Ausdruck gleich Null gesetzt erhalten wir den gleichen Ausdruck wie unter

## Konfidenzintervalle für die Koeffizienten

Wie wir im oberen Abschnitt gesehen haben, sind unsere Schätzer für die Koeffizienten $\beta_0$ und $\beta_1$ mit Unsicherheiten behaftet die sich in Form der Standardfehler ausdrücken. Wir können nun, diese standardfehler wiederum verwenden um Konfidenzintervalle für die Koeffizienten zu bestimmen.

\begin{equation}
\hat{\beta_j} \pm q_{t_{\alpha/2,df=N-2}} \times \hat{\sigma}_{\beta_j}
\label{eq-slm-inf-conf-0}
\end{equation}

Wie in Formel\eqref{eq-slm-inf-conf-0} zu sehen berechnet sich das Konfidenzintervall nach dem üblichen Muster: Schätzer $\pm$ Quantile $\times$ Standardfehler. Im vorliegenden Falle wird die Quantile aus der $t$-Verteilung mit $N-2$ Freiheitsgarden bestimmt. Wie vorher bereits betont, das Konfidenzintervall erlaubt keine Aussage über die Wahrscheinlichkeit mit der der wahre Koeffizient in dem Intervall liegt, sondern gibt an welche $H_0$-Hypothesen mit den Daten kompatibel sind. Daher soll in der Ergebnisdokumentation das Konfidenzintervall angegeben und spätenstens in der Diskussion die obere und die untere Schranke diskutiert werden.

In `R` kann das Konfidenzintervall mit der Funktion `confint()` berechnet und ausgegeben werden.

```{r}
#| echo: true

confint(mod)
```

Wie die Koeffizienten haben die Konfidenzintervall die gleiche Einheit wie die abhängige Variable und können daher direkt interpretiert werden. Im vorliegenden Fall sollte daher besprochen werden welche Bedeutung ein Koeffizient von $\beta_1 = `r round(confint(mod)[2,1],1)`$  bzw. von $\beta_1 = `r round(confint(mod)[2,2],1)`$ für die Interpretation des Modell hat.

Noch einmal zu erwähnen ist, dass die beiden Parameter $\hat{\beta}_0$ und $\hat{\beta}_1$ welche die Regressionsgerade beschreiben, Schätzer für die Parameter aus einer Population sind der die beiden Parameter $\beta_0$ und $\beta_1$ den zugrundeliegenden Zusammenhang zwischen den beiden Variablen beschreiben. Diese betrachtung ist parallel zu derjenigen, wenn wir z.B. anhand des Mittelwerts $\bar{x}$ den währenen Populationsmittelwert $\mu$ versuchen zu schätzen. D.h. wir haben eine Populationsregressionsgerade, die wir mit Hilfe der Daten versuchen zu schätzen. Die wahre Regressionsgerade werden wird aber niemals mit 100%-iger Sicherheit bestimmen, eben genausowenig wie wir den Populationsmittelwert $\mu$ nicht mittels $\bar{x}$ bestimmen können.

## Weiteres Material 

@pos_simple_regression und @kutner2005 [p.40-48]

