# Inferenz

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

Nachdem das Modell gefittet wurde stellt sich die Frage ob tatsächlich ein Zusammenhang zwischen der Prädiktorvariable und der abhängigen Variable besteht. Da das einfache lineare Modelle zwei Parameter $\beta_0$ und $\beta_1$ beinhaltet (streng genommen ist $\sigma^2$ ein dritter Parameter) kann diese Frage auf beide Koeffizienten angewendet werden.

Eine kurze Überlegung zeigt, dass wenn zwischen der Prädiktorvariablen und $y$ kein Zusammenhang besteht, dann sollte der Steigungskoeffizient $\beta_1$ gleich Null sein bzw. auf Grund von Stichprobenvariabilität in der Nähe von Null sein. Daher ist eine plausible Hypothese die sich statistisch Überprüfung lässt:

$$
H_0: \beta_1 = 0
$$


```{r defs_reg_02}
jump <- readr::read_delim('data/running_jump.csv',
                          delim=';',
                          col_types = 'dd')
mod <- lm(jump_m ~ v_ms, jump)
```


## Inferenz

### Modellannahmen

\begin{align*}
y_i &= \beta_0 + \beta_1 x_i + \epsilon_i \quad i=1,\ldots,N \\
\epsilon_i &\sim N(0,\sigma^2) \quad \textrm{identisch, unabhängig verteilt}
\end{align*}

## Modellannahmen - Verteilung der Werte für gegebene x-Werte

$$
Y|X \sim N(\beta_0+ \beta_1 X,\sigma^2)
$$

```{r}
#| fig.cap: Verteilung der Daten für verschiedene $x$-Werte

include_graphics('pics/Stats_Figures.png')
```


## Statistische Hypothesen

### Ungerichtet

\begin{gather*}
H_0: \beta_1 = 0  \\
H_1: \beta_1 \neq 0
\end{gather*}

### Gerichtet

\begin{gather*}
H_0: \beta_1 \leq 0  \\
H_1: \beta_1 > 0
\end{gather*}

## Teststatistik informell herleiten

### Simulation unter der $H_0$

\begin{align*}
N &= 45 \\
x &\sim \mathcal{U}(-1,1) \\
y &\sim \mathcal{N}(0,\sigma) \\
\sigma &= 1 \\
H_0: & \beta_1 = 0
\end{align*}

## Teststatistik informell herleiten

```{r}
#| fig.cap="Acht Zufallsziehung unter der $H_0$",
 
set.seed(123)
N <- 45 
n <- 8
r_dat <- data.frame(
        x = rep(runif(N, -1, 1), n),
        y = rnorm(n*N),
        i = rep(1:n, each=N)
      ) 
ggplot(r_dat, aes(x,y)) + geom_point(size=.3) + 
  geom_smooth(method='lm', formula = y ~ x, se=F, col='red') +
  scale_x_continuous(breaks = c(-0.5,0.5)) +
  facet_wrap(~i, labeller = labeller(.cols = function(i) { paste('Ziehung', i)}), ncol=4) 
```

## Stichprobenverteilung von $\beta_1$ unter der Annahme $\beta_1 = 0$ 

```{r}
#| fig.cap="Verteilung der $\\beta_1$s - 1000 Simulationen unter der Annahme der $H_0$.",
#| fig.height=2.5

set.seed(123)
n <- 45
N <- 1000
x <- runif(n, -1, 1)
sigma <- 1
foo <- function() {
  y <- rnorm(n, mean = 0, sd = sigma)
  mod <- lm(y~x)
  b <- coef(mod)[2]
  c(b, sigma(mod))
}

betas <- replicate(N, foo())
ggplot(data.frame(b = betas[1,]), aes(b)) +
  geom_histogram(aes(y = ..density..), bins = 20) +
  labs(x = expression(beta[1]), y = 'Relative Häufigkeit') 
```

## Verteilung der Statistik unter der $H_0$


:::: columns
::: column

#### Standardfehler von $\beta_1$ 
$$ \sigma_{\beta_1} = \sqrt{\frac{\sigma^2}{\sum{(X_i - \bar{X})^2}}}$$

$\sigma$ lässt sich abschätzen mit:

$$
\hat{\sigma} = \sqrt{\sum_{i=1}^N e_i^2/(N-K)}
$$
:::
::: column

### in `R`

```{r, echo=T}
sigma(mod)
```


:::
::::

## Verteilung der Statistik unter der $H_0$

Unter den Annahmen des Regressionsmodells und der $H_0$ gilt:

$$
\frac{\beta_1}{\sigma_{\beta_1}} \sim t_{N-2}
$$

Mittels $\alpha$ lässt sich daher wieder ein kritischer Wert bestimmen ab dem die $H_0$ verworfen wird.



## Teststatistik


```{r}
#| fig.cap="Verteilung von $\\frac{\\beta_1}{s_{\\beta_1}}$,Dichtefunktion der t-Verteilung (rot) mit $df = n - 2$",
#| fig.height=2.5
#| 
s_e_beta_1 <- sqrt(betas[2,]**2/sum( (x - mean(x))**2))
df_2 <- tibble::tibble(
  b = seq(-4,4,length.out=100),
  y = dt(b, 23)
)
ggplot(data.frame(b = betas[1,]/s_e_beta_1), aes(b)) +
  geom_histogram(aes(y = ..density..), bins = 20) +
  geom_line(data = df_2, aes(b,y), color='red') +
  labs(x = expression(frac(hat(beta)[1],s[beta[1]])), y = 'Relative Häufigkeit') 
```

## Verteilung der $\hat{\sigma} = \sqrt{\sum_{i=1}^N e_i^2/(N-K)}$

```{r}
#| fig.cap="Verteilung von $\\hat{\\sigma}$",
#| fig.height=2.5

ggplot(data.frame(b = betas[2,]), aes(b)) +
  geom_histogram(aes(y = ..density..), bins = 20) +
  geom_vline(xintercept = 1, col = 'red', linetype = 'dashed') +
  labs(x = expression(hat(sigma)[i]), y = 'Relative Häufigkeit') 
```


## Nochmal `summary()`
\tiny
```{r, echo=T}
summary(mod)
```

## Konfidenzintervalle für die Koeffizienten

### Formel

$$
\hat{\beta_j} \pm q_{t_{\alpha/2,df=N-2}} \times \hat{\sigma}_{\beta_j}
$$

### In R

```{r, echo=T}
confint(mod)
```

## Zum Nacharbeiten

@pos_simple_regression und @kutner2005 [p.40-48]

