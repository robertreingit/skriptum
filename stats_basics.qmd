# Eine kleine Welt der Unsicherheit 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```


```{r}
n <- 20
set.seed(123)
world <- tibble(ID = paste0('P',stringr::str_pad(1:n, width=2, pad="0")),
                Kraft = sample(2000:2500, n))
world$Kraft[13] <- 1800
world$Kraft[17] <- 3200
d_gr <- 100
d_x <- 2
```

Starten wir mit einer übersichtlichen kleinen Welt, um das notwendige statistische Modell so einfach wie möglich zu halten. Die Welt, über die wir eine Aussage treffen wollen, besteht nur aus insgesamt 20 Personen. In @fig-small-world sind die Bewohner dieser Welt einzeln zu sehen. Die Gesamtheit aller Personen (allgemein Objekte), über die wir eine Aussage treffen wollen, bezeichnen wir als die Population\index{Population}.

```{r}
#| fig.cap: "Eine kleine Welt"
#| label: fig-small-world
#| fig.height: 4

include_graphics('pics/small_world.png')
```

::: {#def-population}

## Population

Die Gesamtheit aller Objekte/Dinge/Personen, über die eine Aussage getroffen werden soll, wird als Population oder Grundgesamtheit bezeichnet.
:::

## Ein Experiment

Wir wollen nun ein Experiment, eine Krafttrainingsstudie, durchführen, um zu überprüfen, ob ein bestimmtes Training dazu führt, dass sich die Beinkraft erhöht. Allerdings haben wir nur sehr wenige Ressourcen zur Verfügung (bzw. wir sind faul) und können daher nur sechs Messungen durchführen. Aus einem kürzlich durchgeführten Zensus haben wir die Beinkraftwerte der gesamten Population. Eine einfache Möglichkeit, die Kraft darzustellen, ist eine Tabelle (siehe @tbl-sts-basics-world).

```{r}
#| tbl-cap: "Kraftwerte (in Newton) der Lummerländer an der einbeinigen Beinpresse"
#| label: tbl-sts-basics-world

#world_wide <- cbind(world[1:10,],world[11:20,]) 
world |> 
  kable(
    booktabs=TRUE, 
    col.names = c('ID', 'Kraft[N]')) #,'ID','Kraft[N]')) 
```

Selbst bei 20 Werten ist die Darstellung mittels einer Tabelle allerdings leider wenig übersichtlich. Wir müssen Zeile für Zeile die Tabelle durchgehen und uns spezifische Kennwerte notieren, um Vergleiche zwischen den Werten durchführen zu können. Beispielsweise könnten wir notieren, dass der Maximalwert der Beinkraft bei $`r max(world$Kraft)`$N für `r world$ID[which.max(world$Kraft)]` und der Minimalwert von `r world$ID[which.min(world$Kraft)]` bei $`r min(world$Kraft)`$N liegt. Aber wirklich übersichtlich ist die Darstellung in Form einer Tabelle nicht. Für solche univariaten Daten (uni = eins) kann eine übersichtlichere Darstellung mittels eines sogenannten Dotplots erreicht werden (siehe @fig-sts-basics-lummer-dotplot).

```{r}
#| fig.cap: "Dotplot der Lummerlandkraftdaten"
#| label: fig-sts-basics-lummer-dotplot
#| fig.height: 1.5

ggplot(world,
       aes(Kraft, 1)) +
  geom_point(size=3) +
  scale_x_continuous('Kraftwerte[N]', breaks = seq(1800, 3200, 200)) +
  scale_y_continuous('', breaks = NULL)
```

Mittels eines Dotplots kann nun deutlich schneller abgelesen werden, welchen Wert das Minimum bzw. das Maximum annimmt. Die grafische Darstellung erlaubt weiterhin direkt abzuschätzen, in welchem Wertebereich der Großteil der Daten liegt. Allerdings wird durch diese Art der Darstellung die Information darüber, welche Person die jeweiligen Werte besitzt, nicht mehr dargestellt. Dies stellt jedoch nicht zwingend ein Problem dar, da wir in den meisten Fällen sowieso Aussagen über die Gruppe und weniger über einzelne Personen machen wollen. Ein Dotplot hat gleichzeitig den Vorteil, dass wir die Verteilung der Werte abschätzen können. In welchem Bereich liegen die meisten Datenwerte? Liegen die Werte eng beieinander oder streuen die Werte sehr stark? Gibt es einzelne Werte, die sehr unterschiedlich von den anderen Werten sind? Dies sind alles Fragen, die notwendig sind, um einen Datensatz und dessen Eigenschaften beurteilen zu können.

Kommen wir zurück zu unserem Kraftexperiment. Wir wollen den Gesundheitsstatus unserer Lummerländer verbessern und führen dazu ein Krafttraining für die Beine durch. Da wir evidenzbasiert arbeiten wollen, möchten wir überprüfen, ob das Training wirklich zu einer Verbesserung der Beinkraft geführt hat. Um das Experiment zu vereinfachen, und da es sich mehr um ein Gedankenexperiment handelt, gehen wir von einem perfekten Krafttraining aus. Das heißt, wir führen eine perfekte Intervention durch, die zu der gleichen Verbesserung bei allen Teilnehmerinnen und Teilnehmern führt (Warum ist dies eine unrealistische Annahme?).

Das Beinkrafttraining sei also perfekt und verbessert die Kraftleistung um genau $+`r d_gr`$N. Dieser Kraftzuwachs ist unabhängig davon, welche Person aus unserer Population das Training durchführt. Um die Effektivität des Trainings abzuschätzen, vergleichen wir zwei Gruppen miteinander: eine Interventionsgruppe, die das Krafttraining durchläuft, und eine Kontrollgruppe. In beiden Gruppen sollen jeweils $n_{\text{TRT}} = n_{\text{CON}} = 3$ Teilnehmerinnen bzw. Teilnehmer einbezogen werden. Um die spätere Diskussion zu vereinfachen, führen wir nun etwas Terminologie ein.

::: {#def-dep-var}
## Abhängige Variable \index{abhängige Variable}

Die abhängige Variable ist diejenige Variable, die in einer Studie beobachtet, gemessen oder analysiert wird. Die abhängige Variable wird oft als "Effekt" betrachtet.
:::

::: {#def-indep-var}
## Unabhängige Variable \index{unabhängige Variable}

Die unabhängige Variable ist die Variable, die in einer Studie oder einem Experiment manipuliert oder kontrolliert wird. Die unabhängige Variable wird oft als "Ursache" betrachtet, da sie den potenziellen Einfluss auf die abhängige Variable repräsentiert.
:::

In unserem Experiment ist die Gruppenzugehörigkeit die unabhängige Variable und die Beinkraft die abhängige Variable. Wir untersuchen den Effekt der Gruppenzugehörigkeit auf die Beinkraft. Die Gruppe ist die Ursache für mögliche Effekte auf die Beinkraft.

Wir tun jetzt so, als ob wir die Daten aus dem Zensus nicht vorliegen hätten. Dies kommt der Durchführung eines tatsächlichen Experiments näher, da dort üblicherweise auch nicht vorher bekannt ist, welche Performance die Teilnehmerinnen und Teilnehmer vor dem Experiment haben. Es stellen sich nun zwei Fragen: 1) Wie wählen wir die sechs Personen aus unserer Population aus, und 2) wie teilen wir die sechs Personen in die beiden Gruppen auf?

Wir könnten zum Beispiel die ersten drei Personen in die Interventionsgruppe und die letzten drei in die Kontrollgruppe stecken. Allerdings, wenn die Personen in irgendeiner Form nach der Beinkraft vorsortiert sind, z.B. allgemeiner Gesundheitsstatus, Arbeitstätigkeit usw., dann würde sich diese Sortierung auf die Gruppen übertragen. Das heißt, wir hätten eine sogenannte Störvariable\index{Störvariable}, die unser Ergebnis verfälschen würde.

::: {#def-confounder}
## Störvariable 

Eine Störvariable ist eine Variable, die einen Einfluss auf die abhängige Variable hat, deren Einfluss jedoch nicht kontrolliert wurde bzw. die Variable ist nicht von Hauptinteresse für die Untersuchung.
:::

Im Zweifelsfall kann davon ausgegangen werden, dass es immer eine ganze Reihe von Störvariablen gibt. Unglücklicherweise auch Störvariablen, die zum Teil gar nicht bekannt sind. Das heißt, es wird ein Mechanismus benötigt, der Teilnehmerinnen und Teilnehmer auswählt und gleichzeitig dafür sorgt, dass Variablen, die gar nicht bekannt sind, möglichst gleichmäßig ausgewählt werden. Der Mechanismus, der dies sicherstellen kann, ist eine sogenannte Zufallsstichprobe (Warum?).

::: {#def-sample}
## Stichprobe

Eine Stichprobe\index{Stichprobe} ist eine Teilmenge der Objekte aus der Population.
:::

::: {#def-random-sample}
## Zufallsstichprobe

Eine Zufallsstichprobe\index{Zufallsstichprobe} ist eine Teilmenge der Objekte aus der Population, die *zufällig* ausgewählt wurde.
:::

Bei der Zufallsstichprobe haben alle Personen in der Population vor dem Experiment die gleiche Wahrscheinlichkeit, gezogen zu werden. Dadurch kann sichergestellt werden, dass in der Stichprobe unterliegende Störvariablen, ob messbar oder nicht messbar, ebenso in der Stichprobe verteilt sind.

::: {exm-random-sample}

Schauen wir uns ein einfaches Beispiel an. Sei eine Population der Größe $N = 100$ gegeben, in der $30$ Objekte das Merkmal A haben und $70$ Objekte das Merkmal B. Es werden nun wiederholt Zufallsstichproben der Größe 20 aus dieser Population gezogen und für jede Stichprobe wird der Anteil der As berechnet.

```{r}
set.seed(25)
```

```{r}
#| echo: true

pop <- rep(c("A","B"), c(30,70))
n_sam <- 20
res <- numeric(n_sam)
for (i in 1:n_sam) {
  sam <- sample(pop, 20)
  res[i] <- mean(sam == 'A')
}
res
```

Wir sehen, dass tatsächlich in den meisten Fällen der Anteil von As in der Stichprobe in der Nähe der $30\%$ aus der Population liegt. Nicht in allen Fällen, es gibt Schwankungen um diesen Wert herum, aber im Großen und Ganzen spiegelt die Stichprobe in Bezug auf die Verteilung von Merkmalen A und B die Population gut wider. Das heißt, es handelt sich um eine *repräsentative* Stichprobe.
:::

Nachdem jetzt geklärt ist, wie die Stichprobe aus der Population ermittelt wird, ist nun die nächste Frage, wie die Objekte aus der Stichprobe auf die beiden Gruppen verteilt werden. Die gleichen Überlegungen wie diejenigen zur Ermittlung der Stichprobe führen dazu, dass die Objekte zufällig in die beiden Gruppen verteilt werden müssen. In diesem Fall der Aufteilung wird allerdings von einer sogenannten Randomisierung gesprochen.

::: {#def-randomisierung}
## Randomisierung \index{Randomisierung}

Mit Randomisierung wird der Prozess der zufälligen Zuweisung von Probanden oder Elementen zu verschiedenen Gruppen oder Bedingungen in einem Experiment bezeichnet. Die Randomisierung wird verwendet, um sicherzustellen, dass die Auswahl und Zuordnung der Elemente frei von systematischer Beeinflussung erfolgt. 
:::

```{r}
id_s1 <- c(8, 9, 3, 7, 10, 20)
```

Um jetzt für unser Experiment eine Stichprobe zu ermitteln, haben wir die Population durchnummeriert und mittels eines Zufallszahlengenerators die Zahlen $i = \{`r paste(sort(id_s1), collapse=',')`\}$ ermittelt. Die entsprechenden Personen werden aus der Population anhand der ID ausgewählt. Anschließend teilt wieder ein Zufallszahlengenerator die sechs Personen in die beiden Gruppen auf (siehe @fig-stats-basics-sample-01).

```{mermaid}
%%| fig-cap: "Ablaufdiagramm der Gruppenzuweisung."
%%| label: fig-stats-basics-sample-01

flowchart TD
    A{Population} --> B(Zufallszahlengenerator)
    B --> C[Stichprobe]
    C --> D(Zufallszahlengenerator)
    D --> E[Kontrollgruppe]
    D --> F[Interventionsgruppe]
```

Dieser Prozess der zufälligen Ziehung und Zuteilung ist extrem wichtig, um das Ergebnis des Experiments eindeutig zuordnen zu können und eine Generalisierung über die bestehenden Objekte hinaus durchführen zu können. Leider ist der erste Schritt, die zufällige Ziehung von Objekten aus der Population, in der Realität nur sehr schwer realisierbar.

In @tbl-sts-basics-experiment-1 ist die Stichprobe und Zuteilung in die Gruppen zu sehen.

```{r}
#| tbl-cap: "Zufällig ausgewählte Stichprobe der Größe $N=6$ und die Zuteilung in Kontroll- (CON) und Interventionsgruppe (TRT)."
#| label: tbl-sts-basics-experiment-1

world_ex_1 <- world[id_s1,] |> mutate(Gruppe=rep(c('CON','TRT'),each=3))
world_ex_1 |> 
  kable(
    booktabs=TRUE, 
    col.names = c('ID', 'Kraft[N]','Gruppe'))
```

Mit diesen sechs Personen führen wir jetzt unser Experiment durch. Die drei Personen aus der Kontrollgruppe durchlaufen im Interventionszeitraum nur ein Stretchtraining, während die Interventionsgruppe zweimal die Woche für 12 Wochen unser perfektes Krafttraining durchführt. Nach diesem Zeitraum messen wir die Beinkraft aller Personen aus beiden Gruppen. Wir erhalten das folgende Ergebnis (siehe @tbl-sts-basics-sample-1). Nochmal zur Erinnerung, wir nehmen an, dass wir die Werte aus dem Census nicht kennen.

```{r}
#| label: tbl-sts-basics-sample-1
#| tbl-cap: "Ergebnis der Intervention in Experiment 1 für die Kontroll- und die Interventionsgruppe."
#| tbl-subcap:
#|   - "Kontrollgruppe"
#|   - "Interventionsgruppe"
#| layout-ncol: 2

world_ex_1 <- world_ex_1 |>
  rows_update(world_ex_1 |> filter(Gruppe == 'TRT') |>
                mutate(Kraft = Kraft + d_gr),
              by = 'ID')

dat_kon <- world[id_s1[1:3],]
dat_int <- world[id_s1[4:6],] |> 
  mutate(Kraft = Kraft + d_gr)

add_mean <- function(tib) {
  if (dim(tib)[2] == 2) {
    tib |> add_row(
      tibble(ID = '$\\bar{K}$', Kraft = round(mean(tib$Kraft)))
    )
  }
  else {
    tib |> add_row(
      tibble(ID = '$\\bar{K}$', Kraft = NA, Kraft_2 = round(mean(tib$Kraft_2)))
    )
  }
}

kable(dat_kon |> add_mean(),
  booktabs=T,
  col.names=c('ID','Kraft[N]'))

kable(dat_int |> add_mean(),
  booktabs=T,
  col.names=c('ID','Kraft[N]'))
```

Für beide Gruppen ist in @tbl-sts-basics-sample-1 jeweils noch der Mittelwert\index{Mittelwert} $\bar{K}$ dokumentiert, um die Gruppen leichter miteinander vergleichen zu können. Später werden wir noch weitere Maße kennenlernen, die es ermöglichen, zwei Mengen von Werten miteinander zu vergleichen. Der Mittelwert ist ein Maß, das uns schon bekannt sein sollte.

::: {#def-Mittelwert}
## Mittelwert

Der Mittelwert $\bar{x}$ über $n$ Werte berechnet sich nach der Formel:

$$
\bar{x} = \frac{\sum_{i=1}^n x_i}{n}
$$ {#eq-mean}

Der Mittelwert wird mit einem Strich über der Variable dargestellt.
:::

Gleichzeitig haben wir das zentrale und namensgebende Konzept aus der Statistik kennengelernt, nämlich das der Statistik\index{Statistik}. Ein Wert, der mittels der Werte aus einer Stichprobe berechnet wird, wird als Statistik bezeichnet.

::: {#def-Statistik}
## Statistik

Ein auf einer Stichprobe berechneter Wert wird als Statistik bezeichnet.
:::

Der Definition folgend, ist somit der Mittelwert $\bar{X}$ einer Stichprobe eine Statistik. Das Gleiche würde gelten, wenn anstatt des Mittelwerts der Maximalwert oder der Minimalwert einer Stichprobe ermittelt wird. Beide Werte würden ebenfalls eine Statistik darstellen.

Um nun den Unterschied zwischen den beiden Gruppen zu untersuchen, berechnen wir die Differenz $D$ zwischen den beiden Mittelwerten $D = \bar{K}_{\text{TRT}} - \bar{K}_{\text{CON}}$. Die Differenz kann natürlich auch in die andere Richtung berechnet werden und es würde sich das Vorzeichen ändern. Hier gibt es keine Vorgaben, sondern die Richtung kann frei bestimmt werden. Wenn bekannt ist, in welcher Richtung der Unterschied berechnet wird, stellt dies kein Problem dar. Im vorliegenden Fall ziehen wir die Interventionsgruppe von der Kontrollgruppe ab, da wir davon ausgehen, dass die Intervention zu einer Krafterhöhung führt und wir dadurch einen positiven Unterschied erhalten, wenn das Training tatsächlich zu einer Steigerung der Kraftleistung führt (vgl. @eq-sts-basics-ex1-d).

```{r}
world_ex_1_hat <- world_ex_1 |> group_by(Gruppe) |> summarize(Kraft = round(mean(Kraft)))
D_1 <- (world_ex_1_hat |> summarize(D = diff(Kraft)))$D
```

$$
D = `r world_ex_1_hat$Kraft[2]`N - `r world_ex_1_hat$Kraft[1]`N = `r D_1` N
$$ {#eq-sts-basics-ex1-d}

Da der Wert D wiederum auf den Daten der Stichprobe berechnet wird, handelt es sich ebenfalls um eine Statistik.

```{r}
#| fig.cap: "Dotplot der beiden Stichproben. Senkrechte Striche zeigen die jeweiligen Mittelwerte an."
#| label: fig-sts-basics-ex-1-dotplot
#| fig.height: 1.5

diff_dotplot(world_ex_1, world_ex_1_hat)
```

In @fig-sts-basics-ex-1-dotplot sind die Werte der beiden Gruppen, deren Mittelwerte $\bar{K}_{\text{CON}}$ und $\bar{K}_{\text{TRT}}$ und der Unterschied $D$ zwischen diesen abgebildet. Wie erwartet zeigt die Interventionsgruppe den höheren Kraftwert im Vergleich zu der Kontrollgruppe. Allerdings ist der Wert mit $D = `r D_1`$ größer als der tatsächliche Zuwachs von $\Delta_{\text{Training}} = `r d_gr`$ (Warum ist das so?).

Der Unterschied zwischen den beiden Gruppen ist natürlich auch zum Teil auf die Unterschiede, die zwischen den beiden Gruppen vor der Intervention bestanden haben, zurückzuführen. Was wäre denn passiert, wenn wir eine andere Stichprobe gezogen hätten?

```{r}
id_s2 <- c(12,2,19,4,8,16)
```

Sei $i = \{`r paste(id_s2, collapse=',')`\}$ eine zweite Stichprobe. Dies würde zu den folgenden Werten nach der Intervention führen.

```{r}
#| label: tbl-sts-basics-sample-2
#| tbl-cap: "Ergebnis der Intervention in Experiment 2 für die Kontroll- und die Interventionsgruppe."

world_ex_2 <- world[id_s2,] |> mutate(Gruppe=rep(c('CON','TRT'),each=3))
world_ex_2 <- world_ex_2 |>
  rows_update(world_ex_2 |> filter(Gruppe == 'TRT') |>
                mutate(Kraft = Kraft + d_gr),
              by = 'ID')
world_ex_2_hat <- world_ex_2 |> group_by(Gruppe) |> summarize(Kraft = mean(Kraft))
D_2 <- round(diff(world_ex_2_hat$Kraft))

world_ex_2 |> 
  kable(
    booktabs=TRUE, 
    col.names = c('ID', 'Kraft[N]','Gruppe'))
```

```{r}
#| fig.cap: "Dotplot der beiden Stichproben in Experiment 2. Senkrechte Striche zeigen die jeweiligen Mittelwerte an."
#| label: fig-sts-basics-ex-2-dotplot
#| fig.height: 1.5

diff_dotplot(world_ex_2, world_ex_2_hat)
```
In @fig-sts-basics-ex-2-dotplot sind wieder die Datenpunkte, Mittelwerte und der Unterschied in den Mittelwerten zwischen den beiden Gruppen abgetragen. In diesem Fall ist allerdings die Differenz zwischen den beiden Gruppen genau in der anderen Richtung $D = `r D_2`$. Daher würden wir dieses Ergebnis genau in der anderen Richtung interpretieren. Das Krafttraining führt nicht nur zu keiner Verbesserung in der Kraftfähigkeit, sondern sogar zu einer Verschlechterung!

```{r}
id_s3 <- c(6,5,7,20,14,16) 
```

Es hätte aber auch sein können, dass wir noch eine andere Stichprobe gezogen hätten, z.B. $i = \{`r paste(id_s3, collapse=',')`\}$. Mit dieser Stichprobe würden wir zu folgendem Ergebnis kommen (siehe @tbl-sts-basics-ex-3). 

```{r}
#| tbl-cap: "Mittelwertsdaten aus Experiment 3 und der Unterschied $D$ zwischen den beiden Gruppenmittelwerten"
#| label: tbl-sts-basics-ex-3

world_ex_3 <- world[id_s3,] |>
  mutate(Gruppe=rep(c('CON','TRT'),each=3)) 
world_ex_3_hat <-  world_ex_3 |> 
  rows_update(world_ex_3 |> filter(Gruppe == 'TRT') |>
                mutate(Kraft = Kraft + d_gr),
              by = 'ID')  |> 
  group_by(Gruppe) |> summarize(Kraft = mean(Kraft)) 
D_3 <- round(diff(world_ex_3_hat$Kraft))

world_ex_3_hat |> 
  bind_rows(world_ex_3_hat |> summarize(Gruppe = '$D$', Kraft = diff(Kraft))) |> 
  kable(
    booktabs=T,
    digits = 0,
    col.names = c('Gruppe', 'Kraft[N]')
  ) #|> 
  #kableExtra::row_spec(2, extra_latex_after = "\\cmidrule{1-2}")
  
```

In diesem Fall haben wir zwar wieder einen positiven Unterschied zwischen den beiden Gruppen in der zu erwartenden Richtung gefunden. Der Unterschied von $D = `r D_3`$ ist allerdings deutlich kleiner als der tatsächliche Unterschied $\Delta = `r d_gr`$. Daher würden wir möglicherweise das Ergebnis so interpretieren, dass wir das Krafttraining als ineffektiv bewerten und keine Empfehlung für das Training aussprechen.

Zu beachten ist, dass keines der Ergebnisse 100% korrekt ist. Entweder ist der Unterschied zwischen den beiden Gruppen deutlich zu groß, in der falschen Richtung oder deutlich zu klein. Das Ergebnis des Experiments hängt ursächlich damit zusammen, welche Zufallsstichprobe gezogen wird. Dieses Phänomen gilt generell für jedes Ergebnis eines Experiments. Das Phänomen, dass der Wert der berechneten Statistik zwischen Wiederholungen des Experiments schwankt, wird als Stichprobenvariabilität bezeichnet.  

::: {#def-sample-variability}
## Stichprobenvariabilität

Durch die Anwendung von Zufallsstichproben variiert eine auf den Daten berechnete Statistik. Diese Variabilität wird als Stichprobenvariabilität\index{Stichprobenvariabilität} bezeichnet.
:::

Streng genommen führt die Stichprobenvariabilität allein noch nicht dazu, dass sich die Statistik zwischen Wiederholungen des Experiments verändert, sondern die zu untersuchenden Werte in der Population müssen selbst auch eine Streuung aufweisen. Wenn wir eine Population untersuchen würden, bei der alle Personen die gleiche Beinkraft hätten, würden unterschiedliche Stichproben immer den gleichen Mittelwert haben und wiederholte Durchführungen des Experiments würden immer wieder zu demselben Ergebnis führen. Dieser Fall ist in der Realität jedoch praktisch nie gegeben, und sämtliche Parameter, für die wir uns interessieren, zeigen immer eine natürliche Streuung in der Population. Diese Streuung in der Population führt daher zu dem besagten Effekt, dass das gleiche Experiment mehrmals wiederholt zu unterschiedlichen Zufallsstichproben führt und dementsprechend immer zu unterschiedlichen Ergebnissen führt. Das Ergebnis ist inhärent variabel bzw. unsicher.

Daher ist eine der zentralen Aufgaben der Statistik, mit dieser Variabilität umzugehen und Forscher in die Lage zu versetzen, trotzdem rationale Entscheidungen zu treffen. Eine implizite Kernannahme dabei ist, dass wir mit Hilfe von Daten überhaupt etwas über die Welt lernen können. Das heißt, dass uns die Erhebung von Daten auch in die Lage versetzt, rationale Entscheidungen zu treffen. Entscheidungen wie ein spezialisiertes Krafttraining mit einer klinischen Population durchzuführen oder eine bestimmte taktische Variante mit meiner Mannschaft zu trainieren, um die Gegner besser auszuspielen. Alle diese Entscheidungen sollten rational vor dem Hintergrund von Variabilität und Unsicherheit getroffen werden und auch möglichst oft zu korrekten Entscheidungen führen. Wie wir sehen werden, kann uns die Statistik leider nicht garantieren, immer die korrekte Entscheidung zu treffen. Nochmals auf den Punkt gebracht nach @wild2000 [p.28]:

> The subject matter of statistics is the process of finding out more about the real world by collecting and then making sense of data.

Untersuchen wir jedoch zunächst das Phänomen weiter, dass Wiederholungen desselben Experiments zu unterschiedlichen Ergebnissen führen. In unserem Lummerlandbeispiel haben wir nämlich den Vorteil, dass uns die Wahrheit bekannt ist. Diesen Umstand können wir uns zunutze machen.

In @fig-sts-basics-d-dist-1 ist die Verteilung unserer bisherigen drei $D$s abgetragen.

```{r}
#| fig-cap: "Bisherige Verteilung der Unterschiede $D$"
#| label: fig-sts-basics-d-dist-1
#| fig-height: 1.8

ggplot(tibble(D = c(D_1, D_2, D_3)), aes(x = D)) +
  geom_histogram(bins = 30) +
  scale_y_continuous('Anzahl', breaks = c(0, 1)) +
  scale_x_continuous('D[N]')
```


Die drei Werte liegen relativ weit auseinander. Eine Anschlussfrage könnte daher sein: "*Welche weiteren Werte sind denn überhaupt mit der vorliegenden Population möglich?*".

## Die Stichprobenverteilung

Wir können einfach mal das Experiment weiter wiederholen. In @fig-sts-basics-sample-combination sind 15 verschiedene Stichproben abgetragen. Wir haben in jeder Zeile jeweils sechs TeilnehmerInnen gezogen. Drei für die Kontrollgruppe und drei für die Interventionsgruppe. Für jede dieser Zeilen können wir jeweils den Gruppenmittelwert berechnen und den Unterschied $D$ bestimmen.

```{r}
#| fig.cap: "Beispiele für verschiedene Möglichkeiten, zwei Stichproben mit jeweils $n_i = 3$ aus der Population zu ziehen." 
#| label: fig-sts-basics-sample-combination
#| fig.height: 3

foo <- function(id, n=20, k=3) {
  x <- 1:n
  y <- rep(id,n)
  t <- rep('ungezogen',n)
  id <- sample(20, 2*k)
  t[id[1:k]] <- 'Kontrol'
  t[id[(k+1):(2*k)]] <- 'Intervention'
  tibble(x,y,t)
}
n_rep <- 15
dat <- purrr::map_dfr(1:n_rep,foo)
ggplot(dat, aes(x,y,color=t,pch=t)) +
  geom_hline(yintercept = 1:n_rep) +
  geom_point(size=4) +
  scale_x_continuous('Probanden ID', breaks=1:20) +
  scale_y_continuous('Mögliche Kombination[#]', breaks=1:n_rep) +
  scale_color_discrete('Kategorie') +
  guides(pch = "none") +
  theme_minimal()
```

Warum eigentlich bei 15 aufhören? Wir haben ja den Vorteil, dass unsere Population relativ übersichtlich ist. Vielleicht können wir uns ja noch aus unserer Schulzeit an Kombinatorik erinnern. Da haben wir den Binomialkoeffizienten kennengelernt. Die Anzahl der möglichen Kombinationen von $k$ Elementen aus einer Menge von $n$ Elementen berechnet sich nach:

$$
\text{Anzahl} = \binom{n}{k} = \frac{n!}{k!(n-k)!}
$$ {#eq-binom-coef}

In unserem Fall wollen wir zunächst sechs Elemente aus $N = `r n`$ auswählen und dann drei Elemente aus den sechs gezogenen Elementen auswählen, um diese entweder der Interventionsgruppe oder der Kontrollgruppe zuzuweisen (Warum brauchen wir uns nur eine Gruppe anzuschauen?). Die Anzahl der möglichen Stichprobenkombinationen ist folglich:

```{r}
count_all_exp <- choose(20,6)*choose(6,3)
```


$$
\text{Anzahl} = \binom{20}{6}\binom{6}{3} = `r count_all_exp`
$$ {#eq-count-experiment}

Das sind jetzt natürlich selbst bei dieser kleinen Population eine große Menge an einzelnen Experimenten, aber dafür sind Computer da: Die können all diese Experimente in kurzer Zeit durchführen. In @fig-sts-basics-all-combinations-d100 ist die Verteilung aller möglichen Experimentausgänge, d.h. alle Differenzen $D$ zwischen der Interventions- und der Kontrollgruppe, abgebildet.


```{r}
#| fig.cap: "Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe bei einer Intervention mit $\\Delta = 100$ (im Graphen mittels der roten Linie angezeigt)."
#| label: fig-sts-basics-all-combinations-d100
#| fig.height: 3

differences <- readr::read_csv('data/combinations_differences.csv')
ggplot(differences |> dplyr::mutate(d = d + d_gr), aes(d)) +
  geom_histogram(aes(y=after_stat(density)), bins=50) +
  geom_vline(xintercept = d_gr, color='red', linetype='dashed') +
  labs(x = 'Differenzen D[N]', y = 'relative Häufigkeit') +
  lims(x = c(-800, 800)) 
```

Auf der x-Achse sind die möglichen Differenzen $D$ abgetragen, während auf der y-Achse die relative Häufigkeit, d.h. die Häufigkeit für einen bestimmten $D$-Wert geteilt durch die Anzahl $`r count_all_exp`$ aller möglichen Werte. Die Verteilung der $D$s wird als Stichprobenverteilung bezeichnet.

::: {#def-sample-distribution}

Die Stichprobenverteilung\index{Stichprobenverteilung} einer Statistik beschreibt die Verteilung der Statistik. Beispielsweise wenn die Statistik der Mittelwert $\bar{x}$ ist, dann beschreibt die Stichprobenverteilung die Verteilung der möglichen Mittelwerte.
:::

@fig-sts-basics-all-combinations-d100 zeigt, dass die überwiegende Anzahl der Ausgänge tatsächlich auch im Bereich von $\Delta = `r d_gr`$ liegt. Noch präziser: Das Maximum der Verteilung, also die höchste relative Häufigkeit, liegt genau auf der roten Linie. Dies sollte uns etwas beruhigen, denn es zeigt, dass unsere Art der Herangehensweise mittels zweier Stichproben auch tatsächlich in den meisten Fällen einen nahezu korrekten Wert ermittelt. Allerdings zeigt die Stichprobenverteilung auch, dass Werte am rechten Ende, die deutlich zu hoch sind, wie auch Werte am linken Ende der Verteilung, die deutlich in der falschen Richtung liegen, möglich sind. Das bedeutet, wenn wir das Experiment nur einmal durchführen, können wir uns eigentlich nie sicher sein, welches dieser vielen Experimente wir durchgeführt haben. Es ist zwar wahrscheinlicher, dass wir eines aus der Mitte der Verteilung durchgeführt haben, einfach da die Anzahl größer ist, aber wir haben keine 100%-ige Garantie, dass wir nicht *Pech* gehabt haben und das Experiment ganz links mit $D = -500$ oder aber das Experiment ganz rechts mit $D = 700$ durchgeführt haben. Diese Unsicherheit wird leider keine Art von Experiment vollständig auflösen können. Eine weitere Eigenschaft der Verteilung ist ihre Symmetrie bezüglich des Maximums mit abnehmenden relativen Häufigkeiten, je weiter von Maximum $D$ entfernt ist (Warum macht das heuristisch Sinn?).

Die Darstellungsform von @fig-sts-basics-all-combinations-d100 wird als Histogramm bezeichnet und eignet sich vor allem dazu, die Verteilung einer Variablen z.B. $x$ darzustellen. Dazu wird der Wertebereich von $x$ zwischen dem Minimalwert $x_{\text{min}}$ und dem Maximalwert $x_{\text{max}}$ in $k$ gleich große Intervalle unterteilt, und die Anzahl der Werte innerhalb jedes Intervalls wird abgezählt und durch die Anzahl der Gesamtwerte geteilt, um die relative Häufigkeit zu erhalten.

```{r}
hist_ex <- tibble(
  x_i = c(1, 1.5, 1.8, 2.1, 2.2, 2.7, 2.8, 3.5, 4),
  y_i = rep(c(1.5, 2.5, 3.5), c(3,4,2)),
  c_i = unlist(purrr::map(c(3,4,2), ~1:.x))
)
```

Zum Beispiel für die Werte:

$$
x_i \in \{`r paste(hist_ex$x_i, collapse=',')` \}
$$
könnte das Histogram ermittelt werden, indem der Bereich von $x_{\text{min}} = `r min(hist_ex$x_i)`$ bis $x_{\text{max}} = `r max(hist_ex$x_i)`$ in vier Intervalle unterteilt wird und dann die Anzahl der Werte in den jewiligen Intervallen ermittelt wird (siehe @fig-sts-basics-hist-example). Die ermittelte Anzahl würde dann noch durch die Gesamtanzahl $`r length(hist_ex$x_i)`$ der Elemente geteilt um die relative Häufigkeit zu berechnen.

```{r}
#| label: fig-sts-basics-hist-example
#| fig.cap: "Beispiel für die Darstellung eines Histogramms für die Daten $x_i$."
#| fig.width: 3

ggplot(hist_ex, aes(y_i, c_i)) +
  geom_point(size=4) +
  scale_x_continuous('x-Werte',
                     limits = c(1,4),
                     breaks = 1:4,
                     minor_breaks = NULL) +
  scale_y_continuous('Anzahl',
                     minor_breaks = NULL) +
  theme(
    panel.grid.major.x = element_line(color = 'red', linetype = 'dashed')
  )
```

Die Form des Histogramms hängt davon ab, wie viele Intervalle verwendet werden. Die Auflösung wird mit mehr Intervallen besser, aber gleichzeitig verringert sich die Anzahl pro Intervall. Andersherum wird die Auflösung mit weniger Intervallen geringer, aber die Anzahl der Elemente pro Intervall wird größer und somit stabiler. Daher sollte in den meisten praktischen Fällen die Anzahl variiert werden, um sicherzugehen, dass nicht nur zufällig eine spezielle Darstellung gefunden wurde.

Zurück zu unserer Verteilung von $D$ unter $\Delta = `r d_gr`$N in @fig-sts-basics-all-combinations-d100. Wie schon besprochen, sind alle Werte zwischen etwa $D = -500N$ und $D = 700$N plausibel bzw. möglich. Schauen wir uns doch einmal an, was passiert, wenn das Training überhaupt nichts bringt und es keine Verbesserung gibt, also $\Delta = 0$.

```{r}
#| fig.cap: "Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe, wenn $\\Delta = 0$ (rote Linie)."
#| label: fig-sts-basics-all-combinations-d0
#| fig.height: 3

ggplot(differences, aes(d)) +
  geom_histogram(aes(y=..density..), bins=50) +
  geom_vline(xintercept = 0, color='red', linetype='dashed') +
  labs(x = 'Differenzen D[N]', y = 'relative Häufigkeit') +
  lims(x = c(-800, 800)) 
```

Die Verteilung in @fig-sts-basics-all-combinations-d0 sieht praktisch genau gleich aus wie diejenige für $\Delta = `r d_gr`$. Der einzige Unterschied ist lediglich, dass sie nach links verschoben ist, und zwar scheinbar genau um die $100$N Unterschied zwischen den beiden $\Delta$s. Dies ist letztendlich auch nicht weiter verwunderlich, bei der Berechnung des Unterschieds $D$ zwischen den beiden Gruppen kommen in beiden Fällen genau die gleichen Kombinationen vor. Bei $\Delta = 100$ wird aber zu der Interventionsgruppe das $\Delta$ addiert, bevor die Differenz der Mittelwerte berechnet wird. Da jedoch gilt:

$$
D = \frac{1}{3}\sum_{i=1}^3 x_{\text{KON}i} - \frac{1}{3}\sum_{j=1}^3 (x_{\text{TRT}j} + \Delta) = \bar{x}_{\text{KON}} - \bar{x}_{\text{TRT}} + \Delta
$$

Daher bleibt die Form der Verteilung genau gleich und wird lediglich um den Wert $\Delta$ im Vergleich zur Nullintervention nach rechts verschoben. Mit Nullintervention ist umgangssprachlich die Intervention gemeint, bei der nichts passiert, also $\Delta = 0$ gilt.

Als Zwischenfazit sollten wir jetzt verstanden haben, dass jede Statistik, die wir auf einer Stichprobe berechnen, inherent unsicher ist. In der Realität haben wir nicht nur die Variabilität aufgrund der Randomisierung, sondern auch viele andere Einflussgrößen, die das Ergebnis eines Experiments bei Wiederholungen beeinflussen können. Mithilfe der Statistik versuchen wir, die Unsicherheit zu quantifizieren, und lassen dies später in unsere Entscheidungsprozesse einfließen.

## Unsicherheit in Lummerland

Spielen wir das Spiel mit den beiden Stichprobenverteilungen weiter. Gehen wir davon aus, dass nur eine dieser beiden Annahmen korrekt ist. Entweder ist die Intervention effektiv $\Delta = `r d_gr`$ oder sie bringt nichts, also $\Delta = 0$. Wenn wir diese beiden Verteilungen übereinanderlegen, erhalten wir @fig-sts-basics-all-combinations-both. Wir haben die Darstellung jetzt etwas verändert und eine Kurve durch die relativen Häufigkeiten gelegt. Dieser Graph wird jetzt nicht mehr als Histogramm, sondern als Dichtegraph\index{Dichtegraph} bezeichnet.

```{r}
#| fig.cap: "Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe bei $\\Delta = 0$ und $\\Delta = 100$."
#| label: fig-sts-basics-all-combinations-both

n_sim <- dim(differences)[1]
dat <- tibble(
  di = c(differences$d + d_gr, differences$d),
  hypo = rep(c('H100','H0'), c(n_sim,n_sim))
)
p_both <- ggplot(dat, aes(di)) +
  geom_density(aes(fill=hypo), alpha=0.5) +
  geom_vline(xintercept = c(0, d_gr), color = 'red', linetype = 'dashed') +
  scale_x_continuous('Differenzen D[N]', breaks = c(-500, 0, 100, 500)) +
  scale_y_continuous('relative Häufigkeit')  +
  scale_fill_discrete("Annahme") 
print(p_both)
```

In @fig-sts-basics-all-combinations-both ist klar zu sehen, dass die beiden Graphen zu großen Teilen überlappen, und das auch noch in einem Bereich, in dem beide Ergebnisse ihre höchsten relativen Häufigkeiten haben, also auch die größte Wahrscheinlichkeit, unter den jeweiligen Annahmen aufzutreten. Unser Problem besteht darin, dass wir in der Realität nicht wissen, welchen Effekt unser Training auf die Stichprobe hat. Wenn wir dies wüssten, müssten wir das Experiment ja gar nicht durchführen. Normalerweise haben wir nur ein einziges Ergebnis, nämlich den Ausgang unseres einen Experiments.

```{r}
#| label: fig-sts-basics-all-combinations-decision
#| fig.cap: "Zuweisung eines beobachteten Unterschieds $D$ nach einem Experiment"

p_both + 
  annotate("segment", x = 50, y = 0, xend = 0, yend = 0.0018, color = 'black', 
           arrow=arrow(length=unit(3,"mm"), angle=20)) +
  annotate("segment", x = 50, y = 0, xend = 100, yend = 0.0018, color = 'black',
           arrow=arrow(length=unit(3,'mm'), angle=20)) +
  geom_point(data = tibble(di = 50, y = 0, hypo = 'H0'), aes(y=y), color = 'red', size=4) 
```

Wenn wir jetzt unser Experiment einmal durchgeführt haben und ein einziges Ergebnis für $D$ erhalten, zum Beispiel $D = 50$, dann haben wir ein Zuweisungsproblem (siehe @fig-sts-basics-all-combinations-decision). Wie weisen wir unser Ergebnis den beiden möglichen Realitäten zu? Einmal kann es sein, dass das Krafttraining nichts gebracht hat und wir lediglich eine der vielen möglichen Stichprobenkombinationen beobachtet haben, die zu einem positiven Wert für $D$ führen. Oder aber das Krafttraining ist effektiv gewesen und hat zu einer Verbesserung von $\Delta = 100$N geführt, und wir haben lediglich eine Stichprobenkombination gezogen, die zu einem Ergebnis von $D = 50$ führt. In der Realität wissen wir jedoch nicht, welche der beiden Annahmen korrekt ist, und wir können es auch nie vollständig wissen. Denn egal, wie viele Experimente wir durchführen, es bleibt immer die Möglichkeit, dass wir zufällig nur Werte aus dem linken oder rechten Teil der Verteilung beobachten. Diese Unsicherheit lässt sich leider durch keine Art von Experiment vollständig auflösen.

Die Methoden der Statistik liefern uns nun Werkzeuge an die Hand, um trotzdem rational zu entscheiden, welche der beiden Annahmen möglicherweise wahrscheinlicher ist. Gleichzeitig ermöglicht uns die Statistik, abzuschätzen bzw. zu berechnen, wie groß die Unsicherheit dieser Entscheidung ist. Die Statistik sagt dabei immer nur etwas über die beobachteten Daten aus, jedoch nichts über die zugrundeliegenden wissenschaftlichen Theorien.

## Things to know

- Population
- (Zufalls-)Stichprobe
- Randomisierung
- Statistik
- Stichprobenverteilung
- Abhängige und unabhängige Variable
