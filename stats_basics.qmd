# Eine kleine Welt der Unsicherheit 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```


```{r}
n <- 20
set.seed(123)
world <- tibble(ID = paste0('P',stringr::str_pad(1:n, width=2, pad="0")),
                Kraft = sample(2000:2500, n))
world$Kraft[13] <- 1800
world$Kraft[17] <- 3200
d_gr <- 100
d_x <- 2
```

Starten wir mit einer übersichtlichen kleinen Welt um das notwendige statistische Modell so einfach wie möglich zu halten. Die Welt über die wir eine Aussage treffen wollen besteht nur aus insgesamt 20 Personen. In @fig-small-world sind die Bewohner dieser Welt einzeln zu sehen. Die Gesamtheit aller Personen (allgemein Objekte) über die wir eine Aussage treffen wollen bezeichnen wir als die Population\index{Population}.

```{r}
#| fig.cap: "Eine kleine Welt"
#| label: fig-small-world
#| fig.height: 4

include_graphics('pics/small_world.png')
```

::: {#def-population}

## Population

Die Gesamtheit aller Objekte/Dinge/Personen über die eine Aussage getroffen werden soll wird als Population oder Grundgesamtheit bezeichnet.
:::

## Ein Experiment

Wir wollen nun ein Experiment, eine Krafttrainingsstudie, durchführen um zu überprüfen ob ein bestimmtes Training dazu führt, dass sich die Beinkraft erhöht. Allerdings haben wir nur sehr wenige Ressourcen zur Verfügung (bzw. wir sind faul) und können daher nur sechs Messungen durchführen. Aus einem kürzlich durchgeführten Zensus haben wir die Beinkraftwerte der ganzen Population. Eine einfache Möglichkeit die Kraft darzustellen ist eine Tabelle (siehe @tbl-sts-basics-world)

```{r}
#| tbl-cap: "Kraftwerte (in Newton) der Lummerländer an der einbeinigen Beinpresse"
#| label: tbl-sts-basics-world

#world_wide <- cbind(world[1:10,],world[11:20,]) 
world |> 
  kable(
    booktabs=TRUE, 
    col.names = c('ID', 'Kraft[N]')) #,'ID','Kraft[N]')) 
```

Selbst bei 20 Werten ist die Darstellung mittels einer Tabelle allerdings leider wenig übersichtlich. Wir müssen Zeile für Zeile die Tabelle durchgehen und uns spezifische Kennwerte notieren und Vergleiche zwischen den Werten durchführen zu können. Beispielsweise könnten wir notieren, dass der Maximalwert der Beinkraft bei $`r max(world$Kraft)`$N für `r world$ID[which.max(world$Kraft)]` und der Minimalwert von `r world$ID[which.min(world$Kraft)]` bei $`r min(world$Kraft)`$N liegt. Aber wirklich übersichtlich ist die Darstellung in Form einer Tabelle nicht. Für solche univariaten Daten (uni = eins) kann eine übersichtlichere Darstellung mittels eines sogenannten Dotplots erreicht werden (siehe @fig-sts-basics-lummer-dotplot).

```{r}
#| fig.cap: "Dotplot der Lummerlandkraftdaten"
#| label: fig-sts-basics-lummer-dotplot
#| fig.height: 1.5

ggplot(world,
       aes(Kraft, 1)) +
  geom_point(size=3) +
  scale_x_continuous('Kraftwerte[N]', breaks = seq(1800, 3200, 200)) +
  scale_y_continuous('', breaks = NULL)
```

Mittels eines Dotplots kann nun deutlich schneller abgelesen werden welchen Wert das Minimum bzw. das Maximum annimmt. Die graphische Darstellung erlaubt weiterhin direkt abzuschätzen in welchem Wertebereich der Großteil der Daten liegt. Allerdings wird durch diese Art der Darstellung die Information über welche Person die jeweiligen Werte besitzt nicht mehr dargestellt. Dies stellt allerdings nicht zwingend ein Problem dar, da wir in den meisten Fällen sowieso aussagen über die Gruppe und weniger über einzelne Personen machen wollen. Ein Dotplot hat gleichzeitig den Vorteil, dass wir die Verteilung der Werte abschätzen können. In welchem Bereich liegen die meisten Datenwerte? Liegen die Werte eng beieinander oder streuen die Werte sehr stark? Gibt es einzelne Werte die sehr unterschiedlich sind von den anderen Werten? Dies sind alles Fragen die notwendig sind um einen Datensatz und dessen Eigenschaften beurteilen zu können.

Kommen wir zurück zu unsererm Kraftexperiment. Wir wollen den Gesundheitsstatus unserer Lummerländer verbessern und führen dazu ein Krafttraining für die Beine durch. Da wir evidenzbasiert arbeiten wollen, möchten wir überprüfen ob das Training wirklich zu einer Verbesserung der Beinkraft geführt hat. Um das Experiment zu vereinfachen, und da es sich mehr um eine Gedankenexperiment handelt, gehen wir von einem perfekten Krafttraining aus. D.h wir führen eine perfekte Intervention durch die zu der gleichen Verbesserung bei allen Teilnehmerinnen und Teilnehmern führt (Warum ist dies eine unrealistische Annahme?).

Das Beinkrafttraining sei also perfekt und verbessert die Kraftleistung um genau $+`r d_gr`$N. Dieser Kraftzuwachs ist unabhängig davon welche Person aus unserer Population das Training durchführt. Um die Effektivität des Training abzuschätzen vergleichen wir zwei Gruppen miteinander. Eine Interventionsgruppe, die das Krafttraining durchläuft, und eine Kontrollgruppe. In beiden Gruppen sollen jeweils $n_{\text{TRT}} = n_{\text{CON}} = 3$ TeilnehmerInnen bzw. Teilnehmer einbezogen werden. Um die spätere Diskussion zu vereinfachen, führen wir nun etwas Terminologie ein.

::: {#def-dep-var}
## Abhängige Variablen \index{abhängige Variable}

Die abhängige Variable ist diejenige Variable, die in einer Studie beobachtet, gemessen oder analysiert wird. Die abhängige Variable wird oft als "Effekt" betrachtet.
:::

::: {#def-indep-var}
## Unabhängige Variable \index{unabhängige Variable}

Die unabhängige Variable ist die Variable, die in einer Studie oder einem Experiment manipuliert oder kontrolliert wird. Die unabhängige Variable wird oft als "Ursache" betrachtet, da sie den potenziellen Einfluss auf die abhängige Variable repräsentiert.
:::

In unserem Experiment ist die Gruppenzugehörigkeit die unabhängige Variable und die Beinkraft die abhängige Variable. Wir untersuchen den Effekt der Gruppenzugehörigkeit auf die Beinkraft. Die Gruppe ist die Ursache für mögliche Effekte auf die Beinkraft.

Wir tuhen jetzt so, als ob wir die Daten aus dem Census nicht vorliegen haben. Dies kommt Durchführung eines tatsächlichen Experiments näher, da dort üblicherweise auch nicht vorher bekannt ist, welche Performance welche die Teilnehmerinnen und Teilnehmer vor dem Experiment haben. Es stellen sich nun zwei Fragen: 1) Wie wählen wir die sechs Personen aus unserer Population aus und 2) wie teilen wir die sechs Personen in die beiden Gruppen auf? 

Wir könnten zum Beispiel die ersten drei Personen in die Interventionsgruppe und die letzten drei in die Kontrollgruppe stecken. Allerdings wenn wenn die Personen in irgendeiner Form nach der Beinkraft vorsortiert sind, z.B. allgemeiner Gesundheitsstatus, Arbeitstätigkeit usw. dann würde sich diese Sortierung auf die Gruppen übertragen. D.h. wir hätten eine sogenannte Störvariablen \index{Störvariable} die unser Ergebnis verfälschen würden.

::: {#def-confounder}
## Störvariable 

Eine Störvariable ist eine Variable die einen Einfluss auf die unabhängige Variable hat, deren Einfluss jedoch nicht kontrolliert wurde bzw. die Variable ist nicht Hauptinteresse der Untersuchung.
:::

Im Zweifelsfall kann davon ausgegangen werden, dass es immer eine ganze Reihe von Störvariablen gibt. Unglücklicherweise auch Störvariablen die zum Teil gar nicht bekannt sind. D.h. es wird ein Mechanismus benötigt der Teilnehmerinnen und Teilnehmer auswählt und gleichzeitig dafür sorgt, das Variablen die gar nicht bekannt sind möglichst gleichmäßig ausgewählt werden. Der Mechanismus der dies sicherstellen kann ist eine sogenannte Zufallsstrichprobe (Warum?). 

::: {#def-sample}
## Stichprobe

Eine Stichprobe\index{Stichprobe} ist eine Teilmenge der Objekte aus der Population.
:::

::: {#def-random-sample}
## Zufallsstichprobe

Eine Zufallsstichprobe\index{Zufallsstichprobe} ist eine Teilmenge der Objekte aus der Population die *zufällig* ausgewählt wurde.
:::

Bei der Zufallsstichprobe haben alle Personen in der Population vor dem Experiment die gleiche Wahrscheinlichkeit gezogen zu werden. Dadurch kann sichergestellt werden, dass in der Stichprobe unterliegende Störvariablen, ob messbar oder nicht messbar, ebenso in der Stichprobe verteilt sind.

::: {exm-random-sample}

Schauen wir uns ein einfaches Beispiel an. Sei eine Population der Größe $N = 100$ geben in der $30$ Objekte das Merkmal A haben und $70$ Objekte das Merkmal B. Es werden nun wiederholt Zufallsstichproben der Größe 20 aus dieser Stichprobe gezogen und für jede Stichprobe wird der Anteil der As berechnet.

```{r}
set.seed(25)
```

```{r}
#| echo: true

pop <- rep(c("A","B"), c(30,70))
n_sam <- 20
res <- numeric(n_sam)
for (i in 1:n_sam) {
  sam <- sample(pop, 20)
  res[i] <- mean(sam == 'A')
}
res
```

Wir sehen, dass tatsächlich in den meisten Fällen der Anteil von As in Stichprobe in der Nähe der $30\%$ aus der Population liegt. Nicht in allen Fällen, es gibt Schwankungen um diesen Wert herum, aber im Großen und Ganzen spiegelt die Stichprobe in Bezug auf die Verteilung von Merkmalen A und B die Population gut wieder. D.h es handelt sich um eine *repräsentative* Stichprobe.
:::

Nachdem jetzt geklärt ist, wie die Stichprobe aus der Population ermittelt wird ist nun die nächste Frage, wie die Objekte aus der Stichprobe auf die beiden Gruppen verteilt wird. Die gleichen Überlegungen wie diejenigen zur Ermittlung der Stichprobe führen dazu, dass die Objekte zufällig in die beiden Gruppen verteilt werden müssen. In diesem Fall der Aufteilung wird allerdings von einer sogenannten Randomisierung gesprochen.

::: {#def-randomisierung}
## Randomisierung \index{Randomisierung}

Mit Randomisierung wird der Prozess der zufälligen Zuweisung von Probanden oder Elementen zu verschiedenen Gruppen oder Bedingungen in einem Experiment bezeichnet. Die Randomisierung wird verwendet, um sicherzustellen, dass die Auswahl und Zuordnung der Elemente frei von systematischer Beeinflussung erfolgt. 
:::

```{r}
id_s1 <- c(8, 9, 3, 7, 10, 20)
```

Um jetzt für unser Experiment eine Stichprobe zu ermitteln, haben wir die Population durchnummeriert und mittels eines Zufallszahlengeneratoren die Zahlen $i = \{`r paste(sort(id_s1), collapse=',')`\}$ ermittelt. Die entsprechenden Personen werden aus der Population anhand der ID ausgewählt. Anschließend teilt wieder ein Zufallszahlengenerator die sechs Personen in die beiden Gruppen auf (siehe @fig-stats-basics-sample-01).

```{mermaid}
%%| fig-cap: "Ablaufdiagramm der Gruppenzuweisung."
%%| label: fig-stats-basics-sample-01

flowchart TD
    A{Population} --> B(Zufallszahlengenerator)
    B --> C[Stichprobe]
    C --> D(Zufallszahlengenerator)
    D --> E[Kontrollgruppe]
    D --> F[Interventionsgruppe]
```

Dieser Prozess der zufälligen Ziehung und Zuteilung ist extrem wichtig um das Ergebnis des Experiment eindeutig zuordnen zu können und eine Generalisierung über die bestehenden Objekte hinaus durchführen zu können. Leider ist der erste Schritt, die zufällige Ziehung von Objekten aus der Population, in der Realität nur sehr schwer realisierbar.

In @tbl-sts-basics-experiment-1 ist die Stichprobe und Zuteilung in die Gruppen zu sehen.

```{r}
#| tbl-cap: "Zufällig ausgewählte Stichprobe der Größe $N=6$ und die Zuteilung in Kontroll- (CON) und Interventionsgruppe (TRT)."
#| label: tbl-sts-basics-experiment-1

world_ex_1 <- world[id_s1,] |> mutate(Gruppe=rep(c('CON','TRT'),each=3))
world_ex_1 |> 
  kable(
    booktabs=TRUE, 
    col.names = c('ID', 'Kraft[N]','Gruppe'))
```

Mit diesen sechs Personen führen wir jetzt unser Experiment durch. Die drei Personen aus der Kontrollgruppe, unterlaufen im Interventionszeitraum nur ein Stretchtraining während die Interventionsgruppe zweimal die Woche für 12 Wochen unser perfektes Krafttraining durchführt. Nach diesem Zeitraum messen wir die Beinkraft aller Personen aus beiden Gruppen. Wir erhalten das folgende Ergebnis (siehe @tbl-sts-basics-sample-1). Nochmal zur Erinnerung, wir nehmen an, dass wir die Werte aus dem Census nicht kennen.

```{r}
#| label: tbl-sts-basics-sample-1
#| tbl-cap: "Ergebnis der Intervention in Experiment 1 für die Kontroll- und die Interventionsgruppe."
#| tbl-subcap:
#|   - "Kontrollgruppe"
#|   - "Interventionsgruppe"
#| layout-ncol: 2

world_ex_1 <- world_ex_1 |>
  rows_update(world_ex_1 |> filter(Gruppe == 'TRT') |>
                mutate(Kraft = Kraft + d_gr),
              by = 'ID')

dat_kon <- world[id_s1[1:3],]
dat_int <- world[id_s1[4:6],] |> 
  mutate(Kraft = Kraft + d_gr)

add_mean <- function(tib) {
  if (dim(tib)[2] == 2) {
    tib |> add_row(
      tibble(ID = '$\\bar{K}$', Kraft = round(mean(tib$Kraft)))
    )
  }
  else {
    tib |> add_row(
      tibble(ID = '$\\bar{K}$', Kraft = NA, Kraft_2 = round(mean(tib$Kraft_2)))
    )
  }
}

kable(dat_kon |> add_mean(),
  booktabs=T,
  col.names=c('ID','Kraft[N]'))

kable(dat_int |> add_mean(),
  booktabs=T,
  col.names=c('ID','Kraft[N]'))
```

Für beide Gruppen ist in @tbl-sts-basics-sample-1 jeweils noch der Mittelwert\index{Mittelwert} $\bar{K}$ dokumentiert, um die Gruppen leichter miteinander vergleichen zu können. Später werden wir noch weitere Maße kennenlernen die es ermöglichen zwei Mengen von Werten miteinander zu vergleichen. Der Mittelwert ist ein Maß, dass uns schon bekannt sein sollte.

::: {#def-Mittelwert}
## Mittelwert

Der Mittelwert $\bar{x}$ über $n$ Werte berechnet sich nach der Formel:

$$
\bar{x} = \frac{\sum_{i=1}^n x_i}{n}
$$ {#eq-mean}

Der Mittelwert wird mit einem Strich über der Variable dargestellt.
:::

Gleichzeitig haben wir das zentrale und namensgebende Konzept aus der Statistik kennengelernt. Nämlich das der Statistik\index{Statistik}. Ein Wert der mittels der Werte aus einer Stichprobe berechnet wird, wird als Statistik bezeichnet.

::: {#def-Statistik}
## Statistik

Ein auf einer Stichprobe berechnet Wert, wird als Statistik bezeichnet.
:::

Der Definition folgend, ist somit der Mittelwert $\bar{X}$ einer Stichprobe eine Statistik. Das gleiche würde gelten wenn anstatt der Mittelwert der Maximalwert oder der Minimalwert einer Stichprobe ermittelt wird. Beide Werte würden ebenfalls eine Statistik darstellen.

Um nun den Unterschied zwischen den beiden Gruppen zu untersuchen berechnen wir die Differenz $D$ zwischen den beiden Mittelwerten $D = \bar{K}_{\text{TRT}} - \bar{K}_{\text{CON}}$. Die Differenz kann natürlich auch in die andere Richtung berechnet werden und es würde sich das Vorzeichen ändern. Hier gibt es keine Vorgaben, sondern die Richtung kann frei bestimmt werden. Wenn bekannt ist in welcher Richtung der Unterschied berechnet wird, dann stellt dies keine Problem dar. Im vorliegenden Fall ziehen wir die Interventionsgruppe von der Kontrollgruppe ab, da wir davon ausgehen, dass die Intervention zu einer Krafterhöhung führt und wir dadurch einen positiven Unterschied erhalten wenn das Training tatsächlich zu einer Steigerung der Kraftleistung führt (vgl. @eq-sts-basics-ex1-d)

```{r}
world_ex_1_hat <- world_ex_1 |> group_by(Gruppe) |> summarize(Kraft = round(mean(Kraft)))
D_1 <- (world_ex_1_hat |> summarize(D = diff(Kraft)))$D
```

$$
D = `r world_ex_1_hat$Kraft[2]`N - `r world_ex_1_hat$Kraft[1]`N = `r D_1` N
$$ {#eq-sts-basics-ex1-d}

Da der Wert D, wiederum auf den Daten der Stichprobe berechnet wird, handelt es sich ebenfalls um eine Statistik.

```{r}
#| fig.cap: "Dotplot der beiden Stichproben. Senkrechte Striche zeigen die jeweiligen Mittelwerte an."
#| label: fig-sts-basics-ex-1-dotplot
#| fig.height: 1.5

diff_dotplot(world_ex_1, world_ex_1_hat)
```

In @fig-sts-basics-ex-1-dotplot sind die Werte der beiden Gruppen, deren Mittelwerte $\bar{K}_{\text{CON}}$ und $\bar{K}_{\text{TRT}}$ und der Unterschied $D$ zwischen diesen abgebildet. Wie erwartet zeigt die Interventionsgruppen den höheren Kraftwert im Vergleich zu der Kontrollgruppe. Allerdings ist der Wert mit $D = `r D_1`$ größer als der tatsächliche Zuwachs von $\Delta_{\text{Training}} = `r d_gr`$ (Warum ist das so?).

Der Unterschied zwischen den beiden Gruppen ist natürlich auch zum Teil auf die Unterschiede die zwischen den beiden Gruppen vor der Intervention bestanden haben zurück zu führen. Was wäre denn passiert, wenn wir eine andere Stichprobe gezogen hätten?

```{r}
id_s2 <- c(12,2,19,4,8,16)
```

Sei $i = \{`r paste(id_s2, collapse=',')`\}$ eine zweite Stichprobe. Dies würde zu den folgenden Werten nach der Intervention führen.

```{r}
#| label: tbl-sts-basics-sample-2
#| tbl-cap: "Ergebnis der Intervention in Experiment 2 für die Kontroll- und die Interventionsgruppe."

world_ex_2 <- world[id_s2,] |> mutate(Gruppe=rep(c('CON','TRT'),each=3))
world_ex_2 <- world_ex_2 |>
  rows_update(world_ex_2 |> filter(Gruppe == 'TRT') |>
                mutate(Kraft = Kraft + d_gr),
              by = 'ID')
world_ex_2_hat <- world_ex_2 |> group_by(Gruppe) |> summarize(Kraft = mean(Kraft))
D_2 <- round(diff(world_ex_2_hat$Kraft))

world_ex_2 |> 
  kable(
    booktabs=TRUE, 
    col.names = c('ID', 'Kraft[N]','Gruppe'))
```

```{r}
#| fig.cap: "Dotplot der beiden Stichproben in Experiment 2. Senkrechte Striche zeigen die jeweiligen Mittelwerte an."
#| label: fig-sts-basics-ex-2-dotplot
#| fig.height: 1.5

diff_dotplot(world_ex_2, world_ex_2_hat)
```

In @fig-sts-basics-ex-2-dotplot sind wieder die Datenpunkte, Mittelwerte und der Unterschied in den Mittelwerten zwischen den beiden Gruppen abgetragen. In diesem Fall ist allerdings die Differenz zwischen den beiden Gruppen genau in der anderen Richtung $D = `r D_2`$. Daher würden wir dieses Ergebnis genau in der anderen Richtung interpretieren. Das Krafttraining führt nicht nur zu keiner Verbesserung in der Kraftfähigkeit, sondern sogar zu einer Verschlechterung!

```{r}
id_s3 <- c(6,5,7,20,14,16) 
```

Es hätte aber auch sein können, dass wir noch eine andere Stichprobe gezogen hätten, z.B. $i = \{`r paste(id_s3, collapse=',')`\}$. Mit dieser Stichprobe würden wir zu folgenden Ergebnis kommen (siehe @tbl-sts-basics-ex-3). 

```{r}
#| tbl-cap: "Mittelwertsdaten aus Experiment 3 und der Unterschied $D$ zwischen den beiden Gruppenmittelwerten"
#| label: tbl-sts-basics-ex-3

world_ex_3 <- world[id_s3,] |>
  mutate(Gruppe=rep(c('CON','TRT'),each=3)) 
world_ex_3_hat <-  world_ex_3 |> 
  rows_update(world_ex_3 |> filter(Gruppe == 'TRT') |>
                mutate(Kraft = Kraft + d_gr),
              by = 'ID')  |> 
  group_by(Gruppe) |> summarize(Kraft = mean(Kraft)) 
D_3 <- round(diff(world_ex_3_hat$Kraft))

world_ex_3_hat |> 
  bind_rows(world_ex_3_hat |> summarize(Gruppe = '$D$', Kraft = diff(Kraft))) |> 
  kable(
    booktabs=T,
    digits = 0,
    col.names = c('Gruppe', 'Kraft[N]')
  ) #|> 
  #kableExtra::row_spec(2, extra_latex_after = "\\cmidrule{1-2}")
  
```

In diesem Fall haben wird zwar wieder einen positiven Unterschied zwischen den beiden Gruppen in der zu erwartenden Richtung gefunden. Der Unterschied von $D = `r D_3`$ ist allerdings deutlich kleiner als der tatsächliche Unterschied $\Delta = `r d_gr`$. Daher würden wir möglicherweise das Ergebnis so interpretieren, dass wir das Krafttraining als ineffektiv bewerten und keine Empfehlung für das Training aussprechen.

Zu beachten ist, dass keines der Ergebnisse 100% korrekt ist. Entweder ist der Unterschied zwischen den beiden Gruppen deutlich zu groß, oder in der falschen Richtung oder deutlich zu klein. Das Ergebnis des Experiments hängt ursächlich damit zusammen, welche Zufallsstichprobe gezogen wird. Diese Phänomen gilt in jedem Fall generell für jedes Ergebnis eines Experiments. Das Phänomen, das der Wert der berechneten Statistik zwischen Wiederholungen des Experiments schwankt wird als Stichprobenvariabilität bezeichnet.  

::: {#def-sample-variability}
## Stichprobenvariabilität

Durch die Anwendung von Zufallsstichproben, variiert eine auf den Daten berechnete Statistik. Die Variabilität wird als Stichprobenvariabilität\index{Stichprobenvariabilität} bezeichnet.
:::

Streng genommen, führt die Stichprobenvariabilität für sich genommen noch nicht dazu, das sich die Statistik zwischen Wiederholungen des Experiments verändert, sondern die zu untersuchenden Werte in der Population müssen selbst auch noch eine Streuung aufweisen. Wenn wir eine Population untersuchen würden, bei der alle Personen die gleiche Beinkraft hätten, würden unterschiedliche Stichproben immer den gleichen Mittelwert haben und wiederholte Durchführungen des Experiment würden immer wieder zu dem selben Ergebnis führen. Dieser Fall ist in der Realität aber praktisch nie gegeben und sämtliche Parameter für die wir uns interessieren zeigen immer eine natürlich Streuung in der Population. Diese Streuung in der Population führt daher zu dem besagten Effekt, das das gleiche Experiment mehrmals wiederholt zu unterschiedlichen Zufallsstichproben führt und dementsprechend immer zu unterschiedlichen Ergebnissen führt. Das Ergebnis ist inherent variable bzw. unsicher.

Daher ist eine der zentrale Aufgabe der Statistik mit dieser Variabilität umzugehen und Forscherinnen in die Lage zu versetzen trotzdem rationale Entscheidungen zu treffen. Eine implizite Kernannahme dabei ist, dass wir mit Hilfe von Daten überhaupt etwas über die Welt lernen können. D.h. das uns die Erhebung von Daten auch in die Lage versetzt rationale Entscheidungen zu treffen. Entscheidungen wie ein spezialisiertes Krafttraining mit einer klinischen Population durchzführen oder eine bestimmte taktische Variante mit meiner Mannschaft zu trainieren um die Gegner besser auszuspielen. Alle diese Entscheidungen sollten rational vor dem Hintergrund von Variabilität und Unsicherheit getroffen werden und auch möglichst oft zu korrekten Entscheidungen führen. Wie wir sehen werden, kann uns die Statistik leider nicht garantieren immer die korrekte Entscheidungen zu treffen. Nochmal auf den Punkt gebracht nach @wild2000 [p.28]

> The subject matter of statistics is the process of finding out more about the real world by collecting and then making sense of data.

Untersuchen wir jedoch zunächst das Phänomen weiter, das Wiederholungen des gleichen Experiments zu unterschiedlichen Ergebnissen führen. In unserem Lummerlandbeispiel haben wir nämlich den Vorteil, dass uns die Wahrheit bekannt ist. Diesen Umstand können wir uns zu Nutze machen.

In @fig-sts-basics-d-dist-1 ist die Verteilung unsere bisherigen drei $D$s abgetragen.

```{r}
#| fig-cap: "Bisherige Verteilung der Unterschiede $D$"
#| label: fig-sts-basics-d-dist-1
#| fig-height: 1.8

ggplot(tibble(D = c(D_1, D_2, D_3)), aes(x = D)) +
  geom_histogram(bins = 30) +
  scale_y_continuous('Anzahl', breaks = c(0, 1)) +
  scale_x_continuous('D[N]')
```


Die drei Werte liegen relativ weit auseinander. Eine Anschlussfrage könnte daher sein: "*Welche weiteren Werte sind denn überhaupt mit der vorliegenden Population möglich ?*".

## Die Stichprobenverteilung

Wir können einfach mal das Experiment weiter wiederholen. In @fig-sts-basics-sample-combination sind 15 verschiedene Stichproben abgetragen. Wir haben in jeder Zeile jeweils sechs TeilnehmerInnen gezogen. Drei für die Kontrollgruppe und drei für die Inervationsgruppe. Für jede dieser Zeilen können wir jeweils den Gruppenmittelwert berechnen und den Unterschied $D$ bestimmen.

```{r}
#| fig.cap: "Beispiele für verschiedene Möglichkeiten zwei Stichproben mit jeweils $n_i = 3$ aus der Population zu ziehen" 
#| label: fig-sts-basics-sample-combination
#| fig.height: 3

foo <- function(id, n=20, k=3) {
  x <- 1:n
  y <- rep(id,n)
  t <- rep('ungezogen',n)
  id <- sample(20, 2*k)
  t[id[1:k]] <- 'Kontrol'
  t[id[(k+1):(2*k)]] <- 'Intervention'
  tibble(x,y,t)
}
n_rep <- 15
dat <- purrr::map_dfr(1:n_rep,foo)
ggplot(dat, aes(x,y,color=t,pch=t)) +
  geom_hline(yintercept = 1:n_rep) +
  geom_point(size=4) +
  scale_x_continuous('Probanden ID', breaks=1:20) +
  scale_y_continuous('Mögliche Kombination[#]', breaks=1:n_rep) +
  scale_color_discrete('Kategorie') +
  guides(pch = "none") +
  theme_minimal()
```

Warum eigentlich bei 15 aufhören. Wir haben ja den Vorteil, das unsere Population relativ übersichtlich ist. Vielleicht können wir uns ja noch aus unserer Schulezeit an Kombinatorik erinnern. Da haben wir den Binomialkoeffizienten kennengelernt. Die Anzahl der möglichken Kombination von $k$ Elementen aus einer Menge von $n$ Elementen berechnet sich nach:

$$
\text{Anzahl} = \binom{n}{k} = \frac{n!}{k!(n-k)!}
$$ {#eq-binom-coef}

In unserem Fall wollen wir zunächst sechs Elemente aus $N = `r n`$ auswählen und dann drei Elemente aus den sechs gezogenen Elementen auswählen um diese entweder der Interventionsgruppe oder der Kontrollgruppe zu zuweisen (Warum brauchen wir uns nur eine Gruppe anzuschauen?). Die Anzahl der möglichen Stichprobenkombinationen ist folglich:

```{r}
count_all_exp <- choose(20,6)*choose(6,3)
```


$$
\text{Anzahl} = \binom{20}{6}\binom{6}{3} = `r count_all_exp`
$$ {#eq-count-experiment}

Das sind jetzt natürlich selbst bei dieser kleinen Population ein große Menge von einzelnen Experimenten, aber dafür sind Computer da, die können alle diese Experiment in kurzer Zeit durchführen. In @fig-sts-basics-all-combinations-d100 ist die Verteilung aller möglichen Experimentausgänge, d.h. alle Differenzen $D$ zwischen der Interventions- und der Kontrollgruppe, abgebildet.


```{r}
#| fig.cap: "Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe bei einer Intervention mit $\\Delta = 100$ (im Graphen mittels der roten Linie angezeigt)."
#| label: fig-sts-basics-all-combinations-d100
#| fig.height: 3

differences <- readr::read_csv('data/combinations_differences.csv')
ggplot(differences |> dplyr::mutate(d = d + d_gr), aes(d)) +
  geom_histogram(aes(y=after_stat(density)), bins=50) +
  geom_vline(xintercept = d_gr, color='red', linetype='dashed') +
  labs(x = 'Differenzen D[N]', y = 'relative Häufigkeit') +
  lims(x = c(-800, 800)) 
```

Auf der x-Achse sind die möglichen Differenzen $D$ abgetragen, während auf der y-Achse die relative Häufigkeit, d.h. die Häufigkeit für einen bestimmten $D$-Wert geteilt durch die Anzahl $`r count_all_exp`$ aller möglichen Werte. Die Verteilung der D's wird als Stichprobenverteilung bezeichnet.

::: {#def-sample-distribution}

Die Stichprobenverteilung\index{Stichprobenverteilung} einer Statistik beschreibt die Verteilung der Statistik. Beispielsweise wenn die Statistik der Mittelwert $\bar{x}$ ist, dann beschreibt die Stichprobenverteilung die Verteilung der möglichen Mittelwerte.
:::

@fig-sts-basics-all-combinations-d100 zeigt, dass die überwiegende Anzahl der Ausgänge tatsächlich auch im Bereich von $\Delta = `r d_gr`$ liegen. Noch präziser das Maximum der Verteilung, also die höchste relative Häufigkeit liegt genau auf der roten Linie. Dies sollte uns etwas beruhigen, denn es zeigt, das unsere Art der Herangehensweise mittels zweier Stichproben auch tatsächlich in den meisten Fällen einen nahezu korrekten Wert ermittelt. Allerdings zeigt die Stichprobenverteilung auch das Werte am rechten Ende die deutlich zu hoch sind wie auch Werte am linken Ende der Verteilung die deutlich in der falschen Richtung möglich sind. Das bedeutet, wenn wir das Experiment nur einmal durchführen wir uns eigentlich nie sich sein können, welches dieser vielen Experimente wir durchgeführt haben. Es ist zwar warscheinlicher, dass wir eins aus der Mitte der Verteilung durchgeführt haben, einfach da die Anzahl größer ist, aber wir haben keine 100% Versicherung, das wir nicht *Pech* gehabt haben und das Experiment ganz links mit $D = -500$ oder aber das Experiment ganz rechts mit $D = 700$ durchgeführt haben. Diese Unsicherheit wird leider keine Art von Experiment vollständig auflösen können. Eine weitere Eigenschaft der Verteilung ist ihre Symmetrie bezüglich des Maximums mit abnehmenden relativen Häufigkeiten umso weiter von Maximum $D$ entfernt ist (Warum macht das heuristisch Sinn?).

Die Darstellungsform von @fig-sts-basics-all-combinations-d100 wird als Histogramm bezeichnet und eignet sich vor allem dazu die Verteilung einer Variablen z.B. $x$ darzustellen. Dazu wird der Wertebereich von $x$ zwischen dem Minimalwert $x_{\text{min}}$ und dem Maximalwert $x_{\text{max}}$ in $k$ gleich große Intervalle unterteilt und die Anzahl der Werte innerhalb jedes Intervalls wird abgezählt und durch die Anzahl der Gesamtwerte geteilt um die relative Häufigkeit zu erhalten.

```{r}
hist_ex <- tibble(
  x_i = c(1, 1.5, 1.8, 2.1, 2.2, 2.7, 2.8, 3.5, 4),
  y_i = rep(c(1.5, 2.5, 3.5), c(3,4,2)),
  c_i = unlist(purrr::map(c(3,4,2), ~1:.x))
)
```

Zum Beispiel für die Werte:

$$
x_i \in \{`r paste(hist_ex$x_i, collapse=',')` \}
$$
könnte das Histogram ermittelt werden, indem der Bereich von $x_{\text{min}} = `r min(hist_ex$x_i)`$ bis $x_{\text{max}} = `r max(hist_ex$x_i)`$ in vier Intervalle unterteilt wird und dann die Anzahl der Werte in den jewiligen Intervallen ermittelt wird (siehe @fig-sts-basics-hist-example). Die ermittelte Anzahl würde dann noch durch die Gesamtanzahl $`r length(hist_ex$x_i)`$ der Elemente geteilt um die relative Häufigkeit zu berechnen.

```{r}
#| label: fig-sts-basics-hist-example
#| fig.cap: "Beispiel für die Darstellung eines Histogramms für die Daten $x_i$."
#| fig.width: 3

ggplot(hist_ex, aes(y_i, c_i)) +
  geom_point(size=4) +
  scale_x_continuous('x-Werte',
                     limits = c(1,4),
                     breaks = 1:4,
                     minor_breaks = NULL) +
  scale_y_continuous('Anzahl',
                     minor_breaks = NULL) +
  theme(
    panel.grid.major.x = element_line(color = 'red', linetype = 'dashed')
  )
```

Die Form des Histogramms hängt davon ab wie viele Intervalle verwendet werden. Die Auflösung wird mit mehr Intervallen besser, aber gleichzeitig verringert sich die Anzahl pro Intervall. Andersherum wird die Auflösung mit weniger Intervallen geringer aber die Anzahl der Elemente pro Intervall wird größer und somit stabiler. Daher sollte in den meisten praktischen Fällen die Anzahl variiert werden um sicher zu gehen, das nicht nur zufällig eine spezielle Darstellung gefunden wurde.

Zurück zu unserer Verteilung von $D$ unter $\Delta = `r d_gr`$N in @fig-sts-basics-all-combinations-d100. Wie schon besprochen sind alle Werte zwischen etwa $D = -500N$ und $D = 700$N plausibel bzw. möglich. Schauen wir uns doch einmal an, was passiert wenn das Training überhaupt nichts bringen würde und es keine Verbesserung gibt, also $\Delta = 0$.

```{r}
#| fig.cap: "Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe wenn $\\Delta = 0$ (rote Linie)."
#| label: fig-sts-basics-all-combinations-d0
#| fig.height: 3

ggplot(differences, aes(d)) +
  geom_histogram(aes(y=..density..), bins=50) +
  geom_vline(xintercept = 0, color='red', linetype='dashed') +
  labs(x = 'Differenzen D[N]', y = 'relative Häufigkeit') +
  lims(x = c(-800, 800)) 
```

Die Verteilung in @fig-sts-basics-all-combinations-d0 sieht praktisch genau gleich aus, wie diejenige für $\Delta = `r d_gr`$. Der einzige Unterschied ist lediglich das sie nach links verschoben ist und zwar scheinbar genau um die $100$N Unterschied zwischen den beiden $\Delta$s. Dies ist letztendlich auch nicht weiter verwunderlich, bei der Berechnung des Unterschied $D$ zwischen den beiden Gruppen kommen in beiden Fällen genau die gleichen Kombination vor. Bei $\Delta = 100$ wird aber zu der Interventionsgruppe das $\Delta$ addiert bevor die Differenz der Mittelwerte berechnet wird. Da aber gilt:

$$
D = \frac{1}{3}\sum_{i=1}^3 x_{\text{KON}i} - \frac{1}{3}\sum_{j=1}^3 (x_{\text{TRT}j} + \Delta) = \bar{x}_{\text{KON}} - \bar{x}_{\text{TRT}} + \Delta
$$

Daher bleibt die Form der Verteilung genau gleich und wird lediglich um den Wert $\Delta$ im Vergleich zur Nullintervention nach rechts verschoben. Wobei mit Nullintervention Umgangssprachlich die Intervention bezeichnet, bei der nichts passiert also $\Delta = 0$ gilt.

Als Zwischenfazit sollten wir jetzt verstanden haben, dass die jede Statistik die wir auf einer Stichprobe berechnen inherent unsicher ist. In der Realität haben wir nicht die Variabilität auf Grund der Randomisierung, sondern wir haben meist noch alle anderen möglichen Einflussgrößen die das Ergebnis meines Experiments bei Wiederholungen beeinflussen können. Mit Hilfe der Statistik versuchen wir die Unsicherheit zu quantifzieren und lassen dies später in unsere Entscheidungsprozesse einfließen.

## Unsicherheit in Lummerland

Spielen wir das Spiel mit den beiden Stichprobenverteilungen weiter. Gehen wir einmal davon aus, dass nur eine dieser beiden Annahmen korrekt ist. Entweder ist die Intervention effektiv $\Delta = `r d_gr`$ oder die Intervention bringt nichts bringt $\Delta = 0$. Wenn wir diese beiden Verteilungen übereinander legen erhalten wir @fig-sts-basics-all-combinations-both. Wir haben die Darstellung jetzt etwas verändert und eine Kurve durch die relativen Häufigkeiten gelegt. Dieser Graphen wird jetzt nicht mehr als Histogramm sondern als Dichtegraph\index{Dichtegraph} bezeichnet.

```{r}
#| fig.cap: "Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe wenn $\\Delta = 0$ und $\\Delta = 100$."
#| label: fig-sts-basics-all-combinations-both

n_sim <- dim(differences)[1]
dat <- tibble(
  di = c(differences$d + d_gr, differences$d),
  hypo = rep(c('H100','H0'), c(n_sim,n_sim))
)
p_both <- ggplot(dat, aes(di)) +
  geom_density(aes(fill=hypo), alpha=0.5) +
  geom_vline(xintercept = c(0, d_gr), color = 'red', linetype = 'dashed') +
  scale_x_continuous('Differenzen D[N]', breaks = c(-500, 0, 100, 500)) +
  scale_y_continuous('relative Häufigkeit')  +
  scale_fill_discrete("Annahme") 
print(p_both)
```

In @fig-sts-basics-all-combinations-both ist klar zu sehen, dass die beiden Graphen zu großen Teilen überlappen und dazu noch in einem Bereich wo beide Ergebnisse ihrer höchsten relativen Häufigkeiten, also auch die größte Wahrscheinlichkeit haben unter den jeweiligen Annahmen aufzutreten. Unser Problem besteht darin, dass wir in der Realität nicht die Information haben welchen Effekt unser Training auf die Stichprobe hat. Wenn wir dies wüssten, dann müssten wir das Experiment ja gar nicht durchführen. Wir haben im Normalfall nur ein einziges Ergebnis, nämlich den Ausgang unseres einen Experiments.

```{r}
#| label: fig-sts-basics-all-combinations-decision
#| fig.cap: "Zuweisung eines beobachteten Unterschieds $D$ nach einem Experiment"

p_both + 
  annotate("segment", x = 50, y = 0, xend = 0, yend = 0.0018, color = 'black', 
           arrow=arrow(length=unit(3,"mm"), angle=20)) +
  annotate("segment", x = 50, y = 0, xend = 100, yend = 0.0018, color = 'black',
           arrow=arrow(length=unit(3,'mm'), angle=20)) +
  geom_point(data = tibble(di = 50, y = 0, hypo = 'H0'), aes(y=y), color = 'red', size=4) 
```

Wenn wir jetzt unser Experiment einmal durchgeführt haben und ein einziges Ergebnis für $D$ erhalten haben, sei zum Beispiel $D = 50$ dann haben wir ein Zuweisungsproblem (siehe @fig-sts-basics-all-combinations-decision). Wie weisen wir unser Ergebnis jetzt den beiden möglichen Realität zu? Einmal kann es sein, das das Krafttraining aber auch gar nichts gebracht hat und wir haben lediglich eine der vielen möglichen Stichprobenkombination beobachtet haben die zu einem positiven Wert für $D$ führt. Oder aber das Krafttraining ist effektiv gewesen und hat zu einer Verbesserung von $\Delta = 100$N geführt und wir haben lediglich ein Stichprobenkombination aus den vielen möglichen Stichprobenkombination gezogen die zu einem Ergebnis von $D = 50$ führt. Noch mal, in der Realität wissen wir nicht welche der beiden Annahmen korrekt ist und können es auch nie vollständig wissen. Denn egal wie viele Experimente wir machen, wir können immer den zwar unwahrscheinlichen aber nicht unmöglichen Fall haben, das wir nur Werte beispielsweise aus dem linken Teil der Verteilung beobachten. Wir haben fundamental immer mit der Ungewissheit zu kämpfen. Wir können nicht im Sinne eines Beweises zeigen, das das Training effektiv ist. 

Die Methoden der Statistik liefern uns nun Werkzeuge an die Hand um trotzdem rational zu Entscheiden welche der beiden Annahmen möglicherweise wahrscheinlicher ist. Gleichzeitig ermöglicht uns die Statistik abzuschätzen respektive zu berechnen wie groß die Unsicherheit dieser Entscheidung ist. Die Statistik sagt dabei immer nur etwas über die beobachteten Daten aus. Die Statistik sagt jedoch nichts über die zugrundeliegenden wissenschaftlichen Theorien aus.

## Things to know

- Population
- (Zufalls-)Stichprobe
- Randomisierung
- Statistik
- Stichprobenverteilung
- Abhängige und unabhängige Variable
