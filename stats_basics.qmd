# Eine kleine Welt der Unsicherheit 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```


```{r}
n <- 20
set.seed(123)
world <- tibble(ID = paste0('P',stringr::str_pad(1:n, width=2, pad="0")),
                Kraft = sample(2000:2500, n))
world$Kraft[13] <- 1800
world$Kraft[17] <- 3200
d_gr <- 100
d_x <- 2
```

Beginnen wir mit eine einfachen Modell. Dazu nehmen wir eine kleine Welt die nur aus 20 Personen besteht. In @fig-small-world können wir alle Personen einzeln sehen. Die Gesamtheit aller Personen (allgmeine Objekte) über die wir eine Aussage treffen woll bezeichnen wir als eine Population\index{Population}.

```{r}
#| fig.cap: "Eine kleine Welt"
#| label: fig-small-world
#| fig.height: 4

include_graphics('pics/small_world.png')

```


::: {#def-population}

## Population

Eine Population oder auch die Grundgesamtheit ist Gesamtheit aller Objekte/Dinge/Personen über die eine Aussage getroffen werden soll.

:::

## Ein Experiment

Wir wollen nun eine Krafttrainingsstudie durchführen um die Beinkraft zu erhöhen. Wir haben allerdings nur sehr wenige Ressourcen (bzw. wir sind faul) und können insgesamt nur sechs Messungen durchführen. Aus einem kürzlich durchgeführten Census haben wir aber die Kraftwerte der ganzen Population. Wir stellen die Kraftwerte zunächst mittels einer Tabelle dar (siehe @tbl-sts-basics-world)

```{r}
#| tbl-cap: "Kraftwerte (in Newton) der Lummerländer an der einbeinigen Beinpresse"
#| label: tbl-sts-basics-world

world_wide <- cbind(world[1:10,],world[11:20,]) 
world_wide |> 
  kable(
    booktabs=TRUE, 
    col.names = c('ID', 'Kraft[N]','ID','Kraft[N]')) 
```

Selbst bei 20 Werten ist diese Darstellung wenig übersichtlich. Wir könnten zwar Zeile für Zeile durchgehen und nach etwas notieren und suchen würden wir sehen das der Maximalwert bei $3200$N für P17 und der Minimalwert von Person P13 bei $1800$N liegt. Aber wirklich einfach ist diese Darstellung nicht. Für solche univariaten Daten (uni = eins) kann eine übersichtlichere Darstellung mittels eines sogenannten Dotplots erreicht werden (siehe @fig-sts-basics-lummer-dotplot).

```{r}
#| fig.cap: "Dotplot der Lummerlandkraftdaten"
#| label: fig-sts-basics-lummer-dotplot
#| fig.height: 1.5

ggplot(world,
       aes(Kraft, 1)) +
  geom_point(size=3) +
  scale_x_continuous('Kraftwerte[N]', breaks = seq(1800, 3200, 200)) +
  scale_y_continuous('', breaks = NULL)
```

Hier kann deutlich schneller abgelesen werden was das Minimum und das Maximum der Daten ist, sowie es kann auch direkt abgeschätzt werden in welchem Bereich sich der Großteil der Daten befindet. Allerdings wird durch diese Art der Darstellung die Information über welche Person die jeweiligen Werte besitzt nicht mehr dargestellt. Dies stellt in den meisten Fällen allerdings kein Problem dar, da wir in den meisten Fällen aussagen über die Gruppe und weniger über einzelne Personen machen wollen.

Gehen wir jetzt von der folgenden Fragestellung aus. Wir wollen den Gesundheitsstatus unserer Lummerländer verbessern und wollen dazu ein Krafttraining durchführen. Da evidenzbasiert arbeiten wollen, möchten wir überprüfen ob wirklich ein Verbesserung der Kraft durch das Training stattgefunden hat. Da es sich aber gleichzeitig um unsere selbst geschaffene Welt handelt führen wir natürlich ein perfektes Krafttraining, eine perfekte Intervention, durch. D.h wir stellen uns immer wieder als unwissend da und geben vor das wir gar nicht wissen, das das Training perfekt effektiv ist.

D.h. wir führen gleichzeitig ein Gedankenexperiment durch. Wir führen ein Krafttraining für die Beine durch. Das Training ist perfekt und verbessert die Kraftleistung um genau $+`r d_gr`$N. Dieser Kraftzuwachs unabhängig davon welche Person aus unserer Population das Training durchführt (Warum ist das keine realistische Annahme?). Wir wollen zwei Gruppen miteinander vergleichen eine Interventionsgruppe und eine Kontrollgruppe. In beiden Gruppen sollen jeweils $n_{\text{TRT}} = n_{\text{CON}} = 3$ TeilnehmerInnen bzw. Teilnehmer einbezogen werden da wir nicht mehr Ressourcen für mehr ProbandInnen haben.

Die erste Frage die sich nun stellt ist wie wählen wir die sechs Personen aus unserer Population aus und wie teilen wir die sechs Personen in die beiden Gruppen? Nach etwas überlegen kommen wir darauf, dass wir am besten eine zufällige Stichprobe ziehen sollten (Warum?).

::: {#def-sample}
## Stichprobe

Eine Stichprobe\index{Stichprobe} ist eine Teilmenge der Objekte aus der Population.
:::

::: {#def-random-sample}
## Zufallsstichprobe

Eine Zufallsstichprobe\index{Zufallsstichprobe} ist eine Teilmenge der Objekte aus der Population die *zufällig* ausgewählt wurde.
:::

Diese sechs Personen, unsere Stichprobe, wird dann wiederum zufällig auf die beiden Gruppen aufgeteilt.

```{r}
id_s1 <- c(8, 9, 3, 7, 10, 20)
```

Ein Zufallszahlengenerator hat die Zahlen $i = \{`r paste(sort(id_s1), collapse=',')`\}$ gezogen. Die entsprechenden Personen werden aus der Population ausgewählt und wiederum zufällig in die beiden Gruppen aufgeteilt (siehe @tbl-sts-basics-experiment-1).

```{r}
#| tbl-cap: "Zufällig ausgewählte Stichprobe der Kontrollgruppe (CON) und der Interventionsgruppe (TRT)."
#| label: tbl-sts-basics-experiment-1

world_ex_1 <- world[id_s1,] |> mutate(Gruppe=rep(c('CON','TRT'),each=3))
world_ex_1 |> 
  kable(
    booktabs=TRUE, 
    col.names = c('ID', 'Kraft[N]','Gruppe'))
```

Mit diesen sechs Personen führen wir jetzt unser Experiment durch. Die drei Personen aus der Kontrollgruppe, unterlaufen im Interventionszeitraum nur ein Stretchtraining während die Interventionsgruppe zweimal die Woche für 12 Wochen unser perfektes Krafttraining durchführt. Nach diesem Zeitraum messen wir alle Personen aus beiden Gruppen und erhalten das folgende Ergebnis (siehe @tbl-sts-basics-sample-1).

```{r}
#| label: tbl-sts-basics-sample-1
#| tbl-cap: "Ergebnis der Intervention in Experiment 1 für die Kontroll- und die Interventionsgruppe."
#| tbl-subcap:
#|   - "Kontrollgruppe"
#|   - "Interventionsgruppe"
#| layout-ncol: 2

world_ex_1 <- world_ex_1 |>
  rows_update(world_ex_1 |> filter(Gruppe == 'TRT') |>
                mutate(Kraft = Kraft + d_gr),
              by = 'ID')

dat_kon <- world[id_s1[1:3],]
dat_int <- world[id_s1[4:6],] |> 
  mutate(Kraft = Kraft + d_gr)

add_mean <- function(tib) {
  if (dim(tib)[2] == 2) {
    tib |> add_row(
      tibble(ID = '$\\bar{K}$', Kraft = round(mean(tib$Kraft)))
    )
  }
  else {
    tib |> add_row(
      tibble(ID = '$\\bar{K}$', Kraft = NA, Kraft_2 = round(mean(tib$Kraft_2)))
    )
  }
}

kable(dat_kon |> add_mean(),
  booktabs=T,
  col.names=c('ID','Kraft[N]'))

kable(dat_int |> add_mean(),
  booktabs=T,
  col.names=c('ID','Kraft[N]'))
```

Für beide Gruppen ist jeweils der Mittelwert\index{Mittelwert} berechnet worden, um die Wert miteinander vergleichen zu können. Später werden wir noch weitere Maße kennenlernen die es ermöglichen zwei Mengen von Werten miteinander zu vergleichen.

::: {#def-Mittelwert}
## Mittelwert

Der Mittelwert über $n$ Werte berechnet sich nach der Formel:

$$
\bar{x} = \frac{\sum_{i=1}^n x_i}{n}
$$ {#eq-mean}

Der Mittelwert wird mit einem Strich über der Variable dargestellt.

:::

Damit lernen wir direkt auch ein neues Konzept kennen. Nämlich das der Statistik\index{Statistik}. Ein Wert der auf der erhobenen Stichprobe berechnet wird, wird als Statistik bezeichnet.

::: {#def-Statistik}
## Statistik

Ein auf einer Stichprobe berechnet Wert, wird als Statistik bezeichnet.

:::

Um jetzt Unterschied zwischen den beiden Gruppen zu untersuchen berechnen wir die Differenz D zwischen den beiden Mittelwerten $D = \bar{K}_{\text{TRT}} - \bar{K}_{\text{CON}}$. Die Differenz kann natürlich auch in die andere Richtung berechnet werden und es würde sich das Vorzeichen ändern. Hier gibt es keine Vorgaben, sondern die Richtung kann frei bestimmt werden. Wenn bekannt ist in welcher Richtung der Unterschied berechnet wird, dann stellt dies keine Problem dar. Im vorliegenden Fall ziehen wir die Interventionsgruppe von der Kontrollgruppe ab, da wir davon ausgehen, dass die Intervention zu einer Krafterhöhung führt und wir dadurch einen positiven Unterschied erhalten (vgl. @eq-sts-basics-ex1-d)

```{r}
world_ex_1_hat <- world_ex_1 |> group_by(Gruppe) |> summarize(Kraft = round(mean(Kraft)))
D_1 <- (world_ex_1_hat |> summarize(D = diff(Kraft)))$D
```

$$
D = `r world_ex_1_hat$Kraft[2]`N - `r world_ex_1_hat$Kraft[1]`N = `r D_1` N
$$ {#eq-sts-basics-ex1-d}

Da der Wert D, wiederum auf den Daten der Stichprobe berechnet wird, handelt es sich ebenfalls um eine Statistik.

```{r}
#| fig.cap: "Dotplot der beiden Stichproben. Senkrechte Striche zeigen die jeweiligen Mittelwerte an."
#| label: fig-sts-basics-ex-1-dotplot
#| fig.height: 1.5

diff_dotplot <- function(df_1, df_2) {
  # Plots a dotplot with two groups.
  # Args:
  #   df_1: point data
  #   df_2: average data
  ggplot(df_1,
         aes(Kraft, 1, color = Gruppe)) +
    geom_point(size=3) +
    geom_segment(data = df_2, aes(xend = Kraft, y=0.99, yend=1.01), size=1.3) +
    annotate("segment",
             x = df_2$Kraft[1],
             xend = df_2$Kraft[2],
             y = 1.01,
             yend = 1.01,
             arrow = arrow(ends = "both", angle = 45, length = unit(.2,"cm"))) +
    annotate("text",
             x = mean(df_2$Kraft), y = 1.01, label = 'D', vjust=-.5) +
    scale_x_continuous('Kraftwerte[N]', breaks = seq(1800, 3200, 200), limits=c(1800, 3200)) +
    scale_y_continuous('', breaks = NULL, limits=c(0.975, 1.025))
}

diff_dotplot(world_ex_1, world_ex_1_hat)
```

In @fig-sts-basics-ex-1-dotplot sind die Werte der beiden Gruppen, deren Mittelwerte $\bar{K}_{\text{CON}}$ und $\bar{K}_{\text{TRT}}$ und der Unterschied $D$ zwischen diesen abgebildet. Wie erwartet zeigt die Interventionsgruppen den höheren Kraftwert im Vergleich zu der Kontrollgruppe. Allerdings ist der Wert mit $D = `r D_1`$ größer als der tatsächliche Zuwachs von $\Delta_{\text{Training}} = `r d_gr`$ (Warum ist das so?).

Der Unterschied zwischen den beiden Gruppen ist natürlich auch zum Teil auf die Unterschiede die zwischen den beiden Gruppen vor der Intervention bestanden haben zurück zu führen. Was wäre denn passiert, wenn wir eine andere Stichprobe gezogen hätten?

```{r}
id_s2 <- c(12,2,19,4,8,16)
```

Sei $i = \{`r paste(id_s2, collapse=',')`\}$ eine zweite Stichprobe. Dies würde zu den folgenden Werten führen nach der Intervention führen.

```{r}
#| label: tbl-sts-basics-sample-2
#| tbl-cap: "Ergebnis der Intervention in Experiment 2 für die Kontroll- und die Interventionsgruppe."

world_ex_2 <- world[id_s2,] |> mutate(Gruppe=rep(c('CON','TRT'),each=3))
world_ex_2 <- world_ex_2 |>
  rows_update(world_ex_2 |> filter(Gruppe == 'TRT') |>
                mutate(Kraft = Kraft + d_gr),
              by = 'ID')
world_ex_2_hat <- world_ex_2 |> group_by(Gruppe) |> summarize(Kraft = mean(Kraft))
D_2 <- round(diff(world_ex_2_hat$Kraft))

world_ex_1 |> 
  kable(
    booktabs=TRUE, 
    col.names = c('ID', 'Kraft[N]','Gruppe'))
```

```{r}
#| fig.cap: "Dotplot der beiden Stichproben in Experiment 2. Senkrechte Striche zeigen die jeweiligen Mittelwerte an."
#| label: fig-sts-basics-ex-2-dotplot
#| fig.height: 1.5

diff_dotplot(world_ex_2, world_ex_2_hat)
```

In @fig-sts-basics-ex-2-dotplot sind wiederum die Datenpunkte, Mittelwerte und der Unterschied abgetragen. In diesem Fall ist allerdings die Differenz zwischen den beiden Gruppen genau in der anderen Richtung $D = `r D_2`$, so dass die Interpretation des Ergebnisses genau in der anderen Richtung wäre. Nämlich, nicht nur hat das Krafttraining zu keiner Verbesserung in der Kraftfähigkeit geführt, sondern zu einer Verschlechterung!

```{r}
id_s3 <- c(6,5,7,20,14,16) 
```

Es hätte aber auch sein können, das wir noch eine andere Stichprobe gezogen hätten, z.B. $i = \{`r paste(id_s3, collapse=',')`\}$. Dies würde zu dem folgenden Ergebnis führen (siehe @tbl-sts-basics-ex-3). 

```{r}
#| tbl-cap: "Mittelwertsdaten aus Experiment 3 und der Unterschied $D$ zwischen den beiden Gruppenmittelwerten"
#| label: tbl-sts-basics-ex-3

world_ex_3 <- world[id_s3,] |>
  mutate(Gruppe=rep(c('CON','TRT'),each=3)) 
world_ex_3_hat <-  world_ex_3 |> 
  rows_update(world_ex_3 |> filter(Gruppe == 'TRT') |>
                mutate(Kraft = Kraft + d_gr),
              by = 'ID')  |> 
  group_by(Gruppe) |> summarize(Kraft = mean(Kraft)) 
D_3 <- round(diff(world_ex_3_hat$Kraft))

world_ex_3_hat |> 
  bind_rows(world_ex_3_hat |> summarize(Gruppe = '$D$', Kraft = diff(Kraft))) |> 
  kable(
    booktabs=T,
    digits = 0,
    col.names = c('Gruppe', 'Kraft[N]')
  ) #|> 
  #kableExtra::row_spec(2, extra_latex_after = "\\cmidrule{1-2}")
  
```

In diesem Fall haben wird zwar wieder einen positiven Unterschied zwischen den beiden Gruppen in der zu erwartenden Richtung gefunden. Der Unterschied von $D = `r D_3`$ ist allerdings deutlich kleiner als das tatsächlichen $\Delta = `r d_gr`$. Daher würden wir möglicherweise das Ergebnis so interpretieren, führen, dass wir das Krafttraining als ineffektiv bewerten würden und keine Empfehlung ausprechen.

Zusammengenommen, ist keines der Ergebnisse 100% korrekt. Entweder der Unterschied zwischen den beiden Gruppen ist deutlich zu groß, oder in der anderen Richtung oder deutlich zu klein. Das Ergebnis des Experiments hängt ursächlich damit zusammen, welche Stichprobe gezogen wird. Diese Einsicht gilt in jedem Fall generell für jedes Ergebnis eines Experiments.

Das Phänomen, das der Wert der berechneten Statistik zwischen Wiederholungen des Experiments schwankt wird als Stichprobenvariabilität bezeichnet.  

::: {#def-sample-variability}
## Stichprobenvariabilität

Durch die Anwendung von Zufallsstichproben, variiert eine auf den Daten berechnete Statistik. Die Variabilität wird als Stichprobenvariabilität\index{Stichprobenvariabilität} bezeichnet.

:::

Streng genommen, führt die Stichprobenvariabilität für sich genommen noch nicht dazu, das sich die Statistik zwischen Wiederholungen des Experiments verändert, sondern die zu untersuchenden Werte in der Population müssen selbst auch noch eine Streuung aufweisen. Wenn wir eine Population untersuchen würden, bei der alle Personen die gleiche Beinkraft hätten, würden unterschiedliche Stichproben immer den gleichen Mittelwert haben und wiederholte Durchführung des Experiment würden immer wieder zu dem selben Ergebnis führen. Dieser Fall ist in der Realität aber praktisch nie gegeben und sämtlich Parameter für die wir uns hier interessieren zeigen immer eine natürlich Streuung in der Population. Diese Streuung in der Population führt daher zu dem besagten Ergebnis, das das gleiche Experiment mehrmals wiederholt zu unterschiedlichen Zufallsstichproben führt und dementsprechend immer zu unterschiedlichen Ergebnissen führt. 

Daher ist eine der zentrale Aufgabe der Statistik mit dieser Variabilität umzugehen und die Forscherin trotzdem in die Lage zu versetzen rationale Entscheidungen zu treffen. Eine implizite Kernannahme dabei ist, das wir mit Hilfe von Daten überhaupt etwas über die Welt lernen können. D.h. das uns die Erhebung von Daten überhaupt auch in die Lage versetzt rationale Entscheidungen zu treffen. Entscheidungen wie ein spezialisiertes Krafttraining mit einer klinischen Population durchzführen oder eine bestimmte taktische Variante mit meiner Mannschaft zu trainieren um die Gegner besser auszuspielen. Alle diese Entscheidungen sollten rational vor dem Hintergrund von Variabilität getroffen werden und auch möglichst oft korrekte Entscheidungen zu treffen. Wie wir sehen werden, kann uns die Statistik leider nicht garantieren immer die korrekte Entscheidungen zu treffen. Nochmal auf den Punkt gebracht nach @wild2000 [p.28]

> The subject matter of statistics is the process of finding out more about the real world by collecting and then making sense of data.

Untersuchen wir jedoch zunächst unsere Einsicht, das Wiederholungen des gleichen Experiments zu unterschiedlichen Ergebnissen führt, weiter. In unserem Beispiel aus Lummerland haben wir nämlich den Vorteil, das uns die Wahrheit bekannt ist. In @fig-sts-basics-d-dist-1 ist die Verteilung unsere bisheringen drei $D$s abgetragen.

```{r}
#| fig.cap: "Bisherige Verteilung der Unterschiede $D$"
#| label: fig-sts-basics-d-dist-1
#| fig.height: 1.8

ggplot(tibble(D = c(D_1, D_2, D_3)), aes(x = D)) +
  geom_histogram(bins = 30) +
  scale_y_continuous('Anzahl', breaks = c(0, 1)) +
  scale_x_continuous('D[N]')
```


Die drei Werte liegen ja relativ weiter auseiander. Eien Anschlussfrage könnte jetzt sein: "*Welche weiteren Werte sind denn überhaupt möglich mit der vorliegenden Population?*".

## Die Stichprobenverteilung

Wir können jetzt ja einfach mal das Experiment anfangen zu wiederholen. In @fig-sts-basics-sample-combination sind mal 15 verschiedene Stichproben abgetragen. Wir haben in jeder Zeile jeweils sechs TeilnehmerInnen gezogen. Drei für die Kontrollgruppe und drei für die Inervationsgruppe. Für jede dieser Zeilen können wir jeweils den Gruppenmittelwert berechnen und den Unterschied $D$ bestimmen.

```{r}
#| fig.cap: "Beispiele für verschiedene Möglichkeiten zwei Stichproben mit jeweils $n_i = 3$ aus der Population zu ziehen" 
#| label: fig-sts-basics-sample-combination
#| fig.height: 3

foo <- function(id, n=20, k=3) {
  x <- 1:n
  y <- rep(id,n)
  t <- rep('ungezogen',n)
  id <- sample(20, 2*k)
  t[id[1:k]] <- 'Kontrol'
  t[id[(k+1):(2*k)]] <- 'Intervention'
  tibble(x,y,t)
}
n_rep <- 15
dat <- purrr::map_dfr(1:n_rep,foo)
ggplot(dat, aes(x,y,color=t,pch=t)) +
  geom_hline(yintercept = 1:n_rep) +
  geom_point(size=4) +
  scale_x_continuous('Probanden ID', breaks=1:20) +
  scale_y_continuous('Mögliche Kombination[#]', breaks=1:n_rep) +
  scale_color_discrete('Kategorie') +
  guides(pch = "none") +
  theme_minimal()
```

Warum eigentlich bei 15 aufhören. Wir haben ja den Vorteil, das unsere Population relativ übersichtlich ist. Vielleicht können wir uns ja noch aus unserer Schulezeit an Kombinatorik erinnern. Da haben wir den Binomialkoeffizienten kennengelernt. Die Anzahl der möglichken Kombination von $k$ Elementen aus einer Menge von $n$ Elementen berechnet sich nach:

$$
\text{Anzahl} = \binom{n}{k} = \frac{n!}{k!(n-k)!}
$$ {#eq-binom-coef}

In unserem Fall wollen wir zunächst sechs Elemente aus $N = `r n`$ auswählen und dann drei Elemente aus den sechs gezogenen Elementen auswählen um diese entweder der Interventionsgruppe oder der Kontrollgruppe zu zuweisen (Warum brauchen wir uns nur eine Gruppe anzuschauen?). Die Anzahl der möglichen Stichprobenkombinationen ist folglich:

```{r}
count_all_exp <- choose(20,6)*choose(6,3)
```


$$
\text{Anzahl} = \binom{20}{6}\binom{6}{3} = `r count_all_exp`
$$ {#eq-count-experiment}

Das sind jetzt natürlich selbst bei dieser kleinen Population ein große Menge von einzelnen Experimenten, aber dafür sind Computer da, die können alle diese Experiment in kurzer Zeit durchführen. In @fig-sts-basics-all-combinations-d100 ist die Verteilung aller möglichen Experimentausgänge, d.h. alle Differenzen $D$ zwischen der Interventions- und der Kontrollgruppe, abgebildet.


```{r}
#| fig.cap: "Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe bei einer Intervention mit $\\Delta = 100$ (im Graphen mittels der roten Linie angezeigt)."
#| label: fig-sts-basics-all-combinations-d100
#| fig.height: 3

differences <- readr::read_csv('data/combinations_differences.csv')
ggplot(differences |> dplyr::mutate(d = d + d_gr), aes(d)) +
  geom_histogram(aes(y=..density..), bins=50) +
  geom_vline(xintercept = d_gr, color='red', linetype='dashed') +
  labs(x = 'Differenzen D[N]', y = 'relative Häufigkeit') +
  lims(x = c(-800, 800)) 
```

Auf der x-Achse sind die möglichen Differenzen $D$ abgetragen, während auf der y-Achse die relative Häufigkeit, d.h. die Häufigkeit für einen bestimmten $D$-Wert geteilt durch die Anzahl $`r count_all_exp`$ aller möglichen Werte. Die Verteilung der D's wird als Stichprobenverteilung bezeichnet.

::: {#def-sample-distribution}

Die Stichprobenverteilung\index{Stichprobenverteilung} kennzeichnet die Verteilung der beobachteten Statistik.
:::

Die @fig-sts-basics-all-combinations-d100 zeigt, dass die überwiegende Anzahl der Ausgänge tatsächlich auch im Bereich von $\Delta = `r d_gr`$ liegen. Noch präziser das Maximum der Verteilung, also die höchste relative Häufigkeit liegt genau auf der roten Linie. Dies sollte uns etwas beruhigen, denn es zeigt, das unsere Art der Herangehensweise mittels zweier Stichproben auch tatsächlich in den meisten Fällen einen nahezu korrekten Wert ermittelt. Allerdings zeigt die Stichprobenverteilung auch das Werte am rechten Ende die deutlich zu hoch sind wie auch Werte am linken Ende der Verteilung die deutlich in der falschen Richtung möglich sind. Das bedeutet, wenn wir das Experiment nur einmal durchführen wir uns eigentlich nie sich sein können, welches dieser vielen Experimente wir durchgeführt haben. Es ist zwar warscheinlicher, dass wir eins aus der Mitte der Verteilung durchgeführt haben, einfach da die Anzahl größer ist, aber wir haben keine 100% Versicherung, das wir nicht *Pech* gehabt haben und das Experiment ganz links mit $D = -500$ oder aber das Experiment ganz rechts mit $D = 700$ durchgeführt haben. Diese Unsicherheit wird leider keine Art von Experiment vollständig auflösen können. Eine weitere Eigenschaft der Verteilung ist ihre Symmetrie bezüglich des Maximums mit abnehmenden relativen Häufigkeiten umso weiter von Maximum $D$ entfernt ist (Warum macht das heuristisch Sinn?).

Die Darstellungsform von @fig-sts-basics-all-combinations-d100 wird als Histogramm bezeichnet und eignet sich vor allem dazu die Verteilung einer Variablen z.B. $x$ darzustellen. Dazu wird der Wertebereich von $x$ zwischen dem Minimalwert $x_{\text{min}}$ und dem Maximalwert $x_{\text{max}}$ in $k$ gleich große Intervalle unterteilt und die Anzahl der Werte innerhalb jedes Intervalls wird abgezählt und durch die Anzahl der Gesamtwerte geteilt um die relative Häufigkeit zu erhalten.

```{r}
hist_ex <- tibble(
  x_i = c(1, 1.5, 1.8, 2.1, 2.2, 2.7, 2.8, 3.5, 4),
  y_i = rep(c(1.5, 2.5, 3.5), c(3,4,2)),
  c_i = unlist(purrr::map(c(3,4,2), ~1:.x))
)
```

Zum Beispiel für die Werte:

$$
x_i \in \{`r paste(hist_ex$x_i, collapse=',')` \}
$$
könnte das Histogram ermittelt werden, indem der Bereich von $x_{\text{min}} = `r min(hist_ex$x_i)`$ bis $x_{\text{max}} = `r max(hist_ex$x_i)`$ in vier Intervalle unterteilt wird und dann die Anzahl der Werte in den jewiligen Intervallen ermittelt wird (siehe @fig-sts-basics-hist-example). Die ermittelte Anzahl würde dann noch durch die Gesamtanzahl $`r length(hist_ex$x_i)`$ der Elemente geteilt um die relative Häufigkeit zu berechnen.

```{r}
#| label: fig-sts-basics-hist-example
#| fig.cap: "Beispiel für die Darstellung eines Histogramms für die Daten $x_i$."
#| fig.width: 3

ggplot(hist_ex, aes(y_i, c_i)) +
  geom_point(size=4) +
  scale_x_continuous('x-Werte',
                     limits = c(1,4),
                     breaks = 1:4,
                     minor_breaks = NULL) +
  scale_y_continuous('Anzahl',
                     minor_breaks = NULL) +
  theme(
    panel.grid.major.x = element_line(color = 'red', linetype = 'dashed')
  )
```

Die Form des Histogramms hängt davon ab wie viele Intervalle verwendet werden, so wird die Auflösung mit mehr Intervallen besser, aber es die Anzahl wird geringer und andersherum wird die Auflösung mit weniger Intervallen geringer aber die Anzahl der Elemente pro Intervall wird größer und somit stabiler. Daher sollte in den meisten praktischen Fällen die Anzahl variiert werden um sicher zu gehen, das nicht nur zufällig eine spezielle Darstellung verwendet wird.

Zurück zu unserer Verteilung von $D$ unter $\Delta = `r d_gr`$N in @fig-sts-basics-all-combinations-d100. Wie schon besprochen sind alle Werte zwischen etwa $D = -500N$ und $D = 700$N plausibel bzw. möglich. Schauen wir uns doch einmal an, was passiert wenn das Training überhaupt nichts bringen würde und es keine Verbesserung gibt, also $\Delta = 0$.

```{r}
#| fig.cap: "Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe wenn $\\Delta = 0$ (rote Linie)."
#| label: fig-sts-basics-all-combinations-d0
#| fig.height: 3

ggplot(differences, aes(d)) +
  geom_histogram(aes(y=..density..), bins=50) +
  geom_vline(xintercept = 0, color='red', linetype='dashed') +
  labs(x = 'Differenzen D[N]', y = 'relative Häufigkeit') +
  lims(x = c(-800, 800)) 
```

Die Verteilung in @fig-sts-basics-all-combinations-d0 sieht praktisch genau gleich aus, wie diejenige für $\Delta = `r d_gr`$. Der einzige Unterschied ist lediglich das sie nach links verschoben ist und zwar scheinbar genau um die $100$N Unterschied zwischen den beiden $\Delta$s. Dies ist letztendlich auch nicht weiter verwunderlich, bei der Berechnung des Unterschied $D$ zwischen den beiden Gruppen kommen in beiden Fällen genau die gleichen Kombination vor. Bei $\Delta = 100$ wird aber zu der Interventionsgruppe das $\Delta$ dazuaddiert bevor die Differenz der Mittelwerte berechnet wird. Da aber gilt:

$$
D = \frac{1}{3}\sum_{i=1}^3 x_{\text{KON}i} - \frac{1}{3}\sum_{j=1}^3 (x_{\text{TRT}j} + \Delta) = \bar{x}_{\text{KON}} - \bar{x}_{\text{TRT}} + \Delta
$$

Daher bleibt die Form der Verteilung immer genau gleich und wird lediglich um den Wert $\Delta$ im Vergleich zur Nullintervention verschoben. Wobei mit Nullintervention Umgangssprachlich die Intervention bezeichnet, bei der nichts passiert also $\Delta = 0$ gilt.

## Unsicherheit in Lummerland

Das führt jetzt aber zu einem Problem für uns. Gehen wir jetzt nämlich von diesen beiden Annahmen aus, das entweder die Intervention effektiv ist $\Delta = `r d_gr`$ gilt oder das die Intervention nichts bringt also $\Delta = 0$ gilt. Wenn wir diese beiden Verteilungen übereinander legen erhalten wir @fig-sts-basics-all-combinations-both. Wir haben die Darstellung jetzt etwas verändert und eine Kurve durch die relativen Häufigkeiten gelegt. Dieser Graphen wird jetzt nicht mehr als Histogramm sondern als Dichtegraph\index{Dichtegraph} bezeichnet.

```{r}
#| fig.cap: "Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe wenn $\\Delta = 0$ und $\\Delta = 100$."
#| label: fig-sts-basics-all-combinations-both

n_sim <- dim(differences)[1]
dat <- tibble(
  di = c(differences$d + d_gr, differences$d),
  hypo = rep(c('H100','H0'), c(n_sim,n_sim))
)
p_both <- ggplot(dat, aes(di)) +
  geom_density(aes(fill=hypo), alpha=0.5) +
  geom_vline(xintercept = c(0, d_gr), color = 'red', linetype = 'dashed') +
  scale_x_continuous('Differenzen D[N]', breaks = c(-500, 0, 100, 500)) +
  scale_y_continuous('relative Häufigkeit')  +
  scale_fill_discrete("Annahme") 
print(p_both)
```

In @fig-sts-basics-all-combinations-both ist klar zu sehen, dass die beiden Graphen zu großen Teilen überlappen und dazu noch in einem Bereich wo beide Ergebnisse ihrer höchsten relativen Häufigkeiten, also auch die größte Wahrscheinlichkeit haben unter den jeweiligen Annahmen aufzutreten. Unser Problem besteht jetzt darin, dass wir in der Realität gar nicht diese Information haben welchen Effekt unser Training auf die Stichprobe ausführt. Wenn wir dies wüssten, dann müssten wir das Experiment ja gar nicht durchführen. Wir haben im Normalfall nur ein einziges Ergebnis, nämlich den Ausgang unseres einen Experiments.

```{r}
#| label: fig-sts-basics-all-combinations-decision
#| fig.cap: "Zuweisung eines beobachteten Unterschieds $D$ nach einem Experiment"

p_both + 
  annotate("segment", x = 50, y = 0, xend = 0, yend = 0.0018, color = 'black', 
           arrow=arrow(length=unit(3,"mm"), angle=20)) +
  annotate("segment", x = 50, y = 0, xend = 100, yend = 0.0018, color = 'black',
           arrow=arrow(length=unit(3,'mm'), angle=20)) +
  geom_point(data = tibble(di = 50, y = 0, hypo = 'H0'), aes(y=y), color = 'red', size=4) 
```

Wenn wir jetzt unser Experiment einmal durchgeführt haben und ein einziges Ergebnis für $D$ erhalten haben, sei zum Beispiel $D = 50$ dann haben wir ein Zuweisungsproblem (siehe @fig-sts-basics-all-combinations-decision). Wie weisen wir unser Ergebnis jetzt den beiden möglichen Realität zu? Einmal kann es sein, das das Krafttraining aber auch gar nichts gebracht hat und wir haben lediglich eine der vielen möglichen Stichprobenkombination beobachtet haben die zu einem positiven Wert für $D$ führt. Oder aber das Krafttraining ist effektiv gewesen und hat zu einer Verbesserung von $\Delta = 100$N geführt und wir haben lediglich ein Stichprobenkombination aus den vielen möglichen Stichprobenkombination gezogen die zu einem Ergebnis von $D = 50$ führt. Noch mal, in der Realität wissen wir nicht welche der beiden Annahmen korrekt ist und können es auch nie vollständig wissen. Denn egal wie viele Experimente wir machen, wir können immer den zwar unwahrscheinlichen aber nicht unmöglichen Fall haben, das wir nur Werte beispielsweise aus dem linken Teil der Verteilung beobachten. Das heißt wir haben immer mit einer Ungewissheit zu kämpfen. Wir können nicht im Sinne eines Beweises zeigen, das das Training effektiv ist. 

Die Methoden der Statistik liefern uns nun Werkzeuge an die Hand um trotzdem rational zu Entscheiden welche der beiden Annahmen möglicherweise wahrscheinlicher ist. Gleichzeitig ermöglicht uns die Statistik abzuschätzen respektive zu berechnen wie groß die Unsicherheit in dieser Entscheidung ist. Die Statistik sagt dabei immer nur etwas über die beobachteten Daten aus. Die Statistik sagt jedoch nichts über die zugrundeliegenden wissenschaftlichen Theorien aus.

Schauen wir uns jetzt als vorläufig letzten Punkt an welche Entscheidungsmöglichkeiten wir haben.

## Eine Entscheidung treffen

Wir hatten im Beispiel zwei verschiedene Annahmen, einmal das das Training nichts bringt und keine Verbesserung der Kraftfähigkeit folgt $\Delta = 0N$. Andererseits hatten wir das Beispiel gestartet damit, dass die Kraftfähigkeit um $100N$ zunimmt, also $\Delta = 100N$. Wie bezeichnen jetzt diese beiden Annahmen als Hypothesen und bezeichnen $\Delta = 0N$ als die Nullhypothese $H_0$ und $\Delta = 100N$ als die Alternativhypothese $H_1$.

Wenn wir jetzt das Experiment durchgeführt haben, können wir uns also entweder für die $H_0$ oder die $H_1$ entscheiden. Aus Gründen der Symmetrie ist dies gleichbedeutend wenn wir uns nur auf die $H_0$ fokussieren und entweder die $H_0$ annehmen bzw. beibehalten oder verwerfen also uns gegen $H_0$ entscheiden.

```{r}
#| label: tbl-sts-basics-decisions
#| tbl-cap: "Entscheidungsmöglichkeiten wenn entweder $H_0$ oder $H_{1}$ zutrifft." 

tibble(
  hypo = c('$H_0$', '$H_1$'),
  h_0 = c('korrekt', '$\\alpha$'),
  h_1 = c('$\\beta$', 'korrekt')
) |> 
  kable(
    booktabs = TRUE,
    col.names = c('', '$H_0$', '$H_1$'),
    escape = F
  ) |> 
  kableExtra::add_header_above(c(" " = 1, "Realität" = 2))  
```

In @tbl-sts-basics-decisions sind die verschiedenen Entscheidungsmöglichkeiten abgetragen. In der Realität gehen wir, wie gesagt, von zwei Fällen aus. Entweder trifft die $H_0$ oder die $H_1$ zu. Wenn die $H_=$ zutrifft und wir uns für die $H_0$ entscheiden, dann haben wir eine korrekte Entscheidung getroffen. Wenn $H_0$ zutrifft und wir allerdings die $H_0$ ablehnen, also uns für die $H_1$ entscheiden ist unsere Entscheidung falsch und wir begehen einen Fehler. Dieser Fehler wird als Fehler 1. Art bzw. $\alpha$-Fehler bezeichnet. Trifft in der Realität dagegen die $H_1$ zu und wir entscheiden uns gegen die $H_0$ und für die $H_1$, dann haben wir wiederum eine korrekte Entscheidung getroffen. Zuletzt, wenn die $H_1$ zutrifft und wir uns aber für die $H_0$ entscheiden, also die $H_0$ beibehalten bzw. uns gegen die $H_1$ entscheiden, treffen wir wieder eine falsche Entscheidung. Dieser Fehler wird als Fehler 2. Art, bzw. $\beta$-Fehler bezeichnet.

::: {#def-alpha-fehler}
Wenn eine Entscheidung gegen die $H_0$ getroffen wird, obwohl die $H_0$ korrekt ist, wird dies als $\alpha$-Fehler\index{$\alpha$-Fehler} bezeichnet.
:::

::: {#def-beta-fehler}
Wenn eine Entscheidung gegen die $H_1$ getroffen wird, obwohl die $H_1$ korrekt ist, wird dies als $\beta$-Fehler\index{$\beta$-Fehler} bezeichnet.
:::
