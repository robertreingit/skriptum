# Reliabilität

Diese Kapitel beschäftigt sich mit der Güte von Messinstrumenten. Trotzdem eigentlich ein zentrales Thema bei der Erhebung von Messdaten wird es in den meisten Fällen gar nicht betrachtet.

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```


```{r reliability_defs}
lil <- readr::read_delim('data/lilquist_2021_tbl_05.txt',
                         delim = ' ')
lil_w <- lil |> tidyr::pivot_longer(-Subject,
                                    names_to = 'day',
                                    values_to = 'emg')
```

## Genauigkeit versus Präzision

Die ersten beiden wichtigen Eigenschaften eines Messinstruments sind die Genauigkeit und die Präzision. Beide sind Eigenschaften sind zentrale zur Beurteilung der Qualität eines Messinstruments. **Genauigkeit** beschreibt, wie nahe eine Messung am tatsächlichen oder wahren Wert liegt. Ein Messinstrument mit hoher Genauigkeit liefert Werte, die nur eine geringe Abweichung vom *realen Wert* aufweisen. **Präzision** hingegen bezieht sich auf die Wiederholbarkeit von Messungen: Ein präzises Instrument liefert bei mehrfacher Messung ähnliche oder identische Werte, unabhängig davon, ob diese dem wahren Wert nahekommen. Ein Messinstrument kann also präzise, aber ungenau sein, wenn die Werte zwar eng beieinanderliegen, jedoch systematisch vom tatsächlichen Wert abweichen. Idealerweise sollte ein Messinstrument sowohl genau als auch präzise sein, um verlässliche Ergebnisse zu liefern.

In @fig-ed-rel-precision sind Präzision und Genauigkeit anhang einer Zielscheibe noch einmal verdeutlicht.

```{r}
#| fig-cap: "Genauigkeit versus Präzision"
#| label: fig-ed-rel-precision

plot_precision_accuracy <- function() {
  bull <- function(x=0,y=0,r=1,n=100,n_rings=3) {
    theta <- seq(0, 2*pi, length.out=n)
    tibble(
      x = x + sin(theta) * rep(seq(1, r/(n_rings+1), length.out=3), each=n),
      y = y + cos(theta) * rep(seq(1, r/(n_rings+1), length.out=3), each=n),
      g = rep(1:n_rings, each=n)
    )
  }
  pts <- function(x=0,y=0,s=0.1,n=10) {
    tibble(
      x = rnorm(n,x,s),
      y = rnorm(n,y,s)
    )
  }
  set.seed(1)
  ggplot() +
    geom_path(data = bull(), aes(x,y,group=g)) +
    geom_path(data = bull(0,3), aes(x,y,group=g)) +
    geom_path(data = bull(3,0), aes(x,y,group=g)) +
    geom_path(data = bull(3,3), aes(x,y,group=g)) +
    geom_point(data = pts(0,3), aes(x,y), size=2, color='red') +
    geom_point(data = pts(3.5,3.5), aes(x,y), size=2, color='red') +
    geom_point(data = pts(0,0,.3), aes(x,y), size=2, color='red') +
    geom_point(data = pts(3.5,0.5,.3), aes(x,y), size=2, color='red') +
    geom_text(data = tibble(x = c(-1,-1,2,2),y = c(4,1,4,1)+0.3,
                             label=c('genau und präzise','genau und unpräzise',
                                     'ungenau und präzise','ungenau und unpräzise')), 
              aes(x,y,label=label), hjust='left') +
    coord_equal(xlim=c(-1,5)) +
    theme_void()
}
plot_precision_accuracy()
```

Bevor eine Messung durchgeführt werden soll, ist es daher notwendig über die Genauigkeit und die Präzision eines Messgeräts Bescheid zu wissen. In manchen Fällen kann beispielsweise eine geringere Genauigkeit eines Messgeräts weniger problematisch sein als eine geringere Präzision. Wenn zum Beispiel nur Unterschiede gemessen werden sollen, dann ist in bestimmten Fällen die absolute Genauigkeit weniger von Bedeutung da die Abweichungen vom *realen* Wert durch den Vergleich subtrahiert werden. Wird zum Beispiel eine Waage verwendet um die Veränderungen im Gewicht an Untersuchungspersonen über die Zeit verglichen werden sollen. Die Waage sei aber nicht korrekt geeicht und zeigt immer $2$ kg zu viel an. Dadurch ist der absolute, angezeigte Wert verfälscht. Werden aber die beobachteten Gewichte über die Messzeitpunkte voneinander abgezogen, dann ist der *Unterschied* zwischen den Messzeitpunkten dennoch korrekt. Die Annahme dabei ist allerdings, dass sich die Genauigkeit über den Beobachtungszeitung nicht verändert. Die Eigenschaften des *Messinstruments* können anhand zweier Eigenschaften bestimmt werden. Dies sind der Messfehlers und die Reliabilität.

## Übereinstimmung versus Reliabilität

Ein Problem in der Literatur ist, dass die Eigenschaften von Messinstrumenten in verschiedenen Disziplinen entwickelt wurden und dadurch Überschneidungen in der Verwendung der Terminologie auftreten (@devet2006). Der folgende Text ist daher ein Versuch eine Struktur zu erzeugen.

Die erste Frage die sich bei der Behandlung von Messinstrumenten stellt ist die Frage ob ein gegebenes Instrument bezüglich der Übereinstimmung (engl. agreement) oder bezüglich der Reliabilität (engl. reliability) untersucht werden soll. Unter agreement stellt sich die Frage: Wie nahe beieinander sind wiederholte Messungen am gleichen Objekt? Im Gegensatz dazu beschäftigt sich die Reliabilität mit der Frage: Wie gut können Objekte voneinander unterschieden werden. 

In @fig-ed-reliability-terminology ist ein schematischer Datensatz abgebildet an dem der Unterschied zwischen agreement und reliability veranschaulicht werden kann.

```{r}
#| fig-height: 1.5 
#| label: fig-ed-reliability-terminology
#| fig-cap: "Drei wiederholte Messungen an drei Versuchspersonen."

df <- tibble(
  x = c(0.9,1,1.1, 1.9,2.0,2.1,2.05,2.15,2.25),
  y = 1,
  VP = rep(c('A','B','C'), each=3)
)
ggplot(df, aes(x,y,color=VP)) +
  geom_point(aes(shape=VP), size=4) +
  scale_x_continuous("", labels=NULL)  +
  scale_y_continuous("", breaks=NULL) +
  theme_minimal() +
  theme(legend.position = 'bottom')
```

Es wurden drei Personen dreimal gemessen. Zwischen den Personen sind Unterschiede bezüglich der Messwerte zu erkennen aber auch innerhalb einer Person sind Schwankungen zu sehen. Die Übereinstimmung (agreement) der Messung wird durch die Schwankung innerhalb einer Person beschrieben. Dagegen beschreibt die Reliabilität das Verhältnis der Schwankung innerhalb einer Person zur Schwankung zwischen den Personen. D.h. die Reliabilität beschreibt wie gut das Messinstrument zwischen den Personen diskriminieren kann.

Die Diskriminierungsfähigkeit ist daher abhängig davon wie groß die Streuung zwischen den Personen ist. Beispielsweise ist das Messinstrument sehr gut zwischen den Versuchspersonen A und B bzw.A und C zu unterscheiden. Dies ist dagegen für den Unterschied zwischen den Personen B und C nicht mehr so ganz der Fall, da die Streuung der Messwerte für eine Person in der Größenordnung liegen der Unterschiede zwischen den beiden Personen. Bei der Beurteilung eines Messinstruments sollten daher möglichst beide Komponenten bekannt sein. Im folgenden wird zunächst die Reliabilität untersucht.

## Reliabilität

Um die Herleitung der Reliabilität zu veranschaulichen soll dies an einem konkreten Beispiel stattfinden. In @tbl-ed-rel-emg ist dazu ein Ausschnitt von EMG-Daten aus @liljequist2019 abgetragen.

```{r}
#| tbl-cap: "Beispieldaten von Lijequist (2019), EMG Median Frequenz (Hz), linke lumbale Rückenmuskulator über drei Tage"
#| label: tbl-ed-rel-emg

lil |> head() |> knitr::kable(
             linesep = '',
             col.names = c('Subject','Day 1', 'Day 2', 'Day 3'))
```

Insgesamt wurde bei $N = 10$ Personen die Muskelaktivierung in der Rückenmuskulatur über drei Tage jeweils an der gleichen Stelle gemessen. Ziel war eine Abschätzung zu erhalten, wie stabil bzw. ähnlich die EMG-Messungen über die verschiedenen Tage sind. In @fig-ed-rel-emg sind die Daten noch einmal graphisch dargestellt.

```{r}
#| fig-cap: "EMG Daten nach Liljequist et al. (2019). Die roten Punkte sind die Mittelwerte mit Standardabweichungen, schwarze gestrichtelte Linien sind die einzelnen Personen."
#| label: fig-ed-rel-emg

lil_hat <- lil_w |> dplyr::group_by(day) |> 
  dplyr::summarize(m = mean(emg),
            s = sd(emg),
            Subject = 11)
ggplot(lil_w, aes(day, emg, group=Subject)) +
  geom_pointrange(data = lil_hat,
                  aes(y = m, ymin = m - s, ymax = m + s),
                  size = 1.4, col = 'red') +
  geom_line(position = position_jitter(seed = 123, width=.05), linetype='dashed') +
  geom_point(position = position_jitter(seed = 123, width=.05), size=3) +
  scale_x_discrete('Messzeitpunkt', labels = paste('Tag', 1:3)) +
  labs(y = 'Median EMG Frequenz') 
```

Wie in @fig-ed-rel-emg zu sehen ist, schwanken die Werte über die drei Messzeitpunkte. Einmal sind innerhalb der Personen Unterschiede zu erkennen, aber auch die Mittelwerte für die Messtage über die zehn Personen hinweg verändern sich über die Messzeitpunkte. D.h. die gewählte Messmethodik weist selbst Schwankungen auf. Die Größe der Schwankungen müssen bei der Erstellung eines Untersuchungsdesigns berücksichtigt werden. Sollen beispielsweise Unterschiede zwischen zwei Gruppen mit Hilfe dieser Messmethodik untersucht werden, aber wenn sich die zu erwartenden Unterschiede zwischen den Gruppen in der Größenordnung der Schwankungen oder möglicherweise sogar darunter, dann wird es sehr schwierig bis unmöglich werden diese Unterschiede in einem Experiment nachweisen zu können.

Um sich der Reliabilität inhaltlich zu nähern ist zunächst die Erstellung eines theoretischen Modells erforderlich. Soll ein *Wert* bestimmt werden, der die Eigenschaft eines Objekts beschreibt, dann ist eine plausible Annahme das es einen *wahren* Wert gibt. Zum Beispiel ist direkt einsichtig, dass ein gegebener Holzstab eine bestimmte Länge hat. Oder eine Hantelscheibe hat ein bestimmtes Gewicht. Solange keine Manipulation an der Scheibe durchgeführt wird, sollte sich dieses Gewicht nicht ändern. Dieser Wert soll nun als *true-score* bezeichnet werden und der true-score erhält das Zeichen $\tau$. Während $\tau$ für eine Hantelscheibe noch relativ direkt einsichtig ist, ist dies nicht mehr ganz so einfach sobald biologische Objekte ins Spiel kommen. Soll beispielsweise die Körpergröße einer Person bestimmt werden, dann ist klar das die Körpergröße keine feststehende Größe ist, sondern im Verlauf eines Tages einer Schwankung unterliegt [@tyrrell1985]. In @fig-ed-rel-height ist beispielsweise die Veränderung der Körperhöhe im Verlauf eines Tages abgetragen.

```{r}
#| fig-cap: "Veränderung der Körpergröße über den Tag (adaptiert nach Tyrrell et al. (1985))."
#| label: fig-ed-rel-height

height <- tibble(
  time = c(0, 3.2, 7, 8, 9, 10, 12, 15.5, 20, 24),
  height = c(3.5, 16.8, 22, 13.9, 11.6, 8.6, 5.86, 4.4, 3.94, 3.9)
)
ggplot(height, aes(time,height)) +
  geom_line() + 
  geom_point() +
  scale_x_continuous('Zeit nach Mitternacht [Stunden]', breaks=seq(0,24,2)) +
  scale_y_continuous('Höhenveränderung\nzu Mitternacht [mm]')
```

Es soll nun aber davon ausgegangen werden, dass der wahre Wert $\tau$ wohldefiniert ist. Dann besteht aber nach wie vor das Problem, selbst bei der Hantelscheibe, dass dieser Wert mit Hilfe eines Messmethodik bestimmt werden muss. Zum Beispiel im Fall der Hantelscheibe muss eine Waage verändert werden um das Gewicht zu bestimmen. Diese Waage ist aber selbst unweigerlich mit *Messfehlern* behaftet. Vielleicht ist bei der Eichung was schief gegangen, vielleicht ist die Feder mit Zeit ausgeleiert, oder die Umgebungstemperatur entspricht nicht den Vorgaben usw. und sofort. Das führt dazu, dass der *gemessene* Wert nicht gleich dem wahren Wert $\tau$ entspricht. Um den gemessenen Wert vom wahren Wert zu unterscheiden wird ein weiteres Symbol $Y$ verwendet. $Y$ ist dementsprechend der *beobachtete* Wert. Die Messfehler werden weiterhin mit dem Symbol $\epsilon$ bezeichnet. Dies führt insgesamt zu folgendem Modell:

\begin{equation}
Y = \tau + \epsilon
\label{eq-ed-reliability-model}
\end{equation}

D.h. der beobachtete Wert $Y$ setzt sich zusammen aus dem wahren Wert $\tau$ und dem Messfehler $\epsilon$. Der Messfehler geht in diesem Modell additiv in den Wert $Y$ ein. Eine weitere Annahme soll nun dahin gehend bestehen, dass die Messfehler zufällig sind und bei wiederholter Messung im *Mittel* gleich $0$ sind. Formal ist der Erwartungswert $E[\epsilon_0] = 0$, wie das auch schon im Rahmen der Regression verwendet wurde. Weiterhin besteht kein Zusammenhang zwischen der Größe des Messfehlers und der Größe von $\tau$. Beispielsweise bei der Messung der Körpergröße verändert sich die Größe des Messfehlers nicht mit der Größer der Probandinnen sondern bleibt konstant. Formal führt dies zu $\text{Cov}(\epsilon,\tau) = 0$.

Werden nun mehrere bzw. $N$ Messungen durchgeführt, dann wird entsprechend nicht nur ein wiederholter, konstanter Wert beobachtet sondern eine Menge von Werten. Um diese zu unterscheiden wird entsprechend ein Index $i$ eingeführt.

\begin{equation}
Y_i = \tau_i + \epsilon_i, \quad i \in [1,\ldots,N]
\end{equation}

Sobald nun mehrere Werte zur Verfügung stehen, kann für diese beobachteten Wert $Y_i$ eine Varianz berechnet werden und, zumindest theoretisch, auch eine Varianz für die $\tau_i$s berechnet werden. Dies ermöglicht nun eine formale Definition der Reliabilität unter dem gegebenen Modell.

::: {#def-reliability}
## Reliabilität - Definition 

Die Reliabilität ist definiert als das Verhältnis der Varianz der *True-Score-Variablen* $\sigma_{\tau}$ zur *Gesamtvarianz* der Testvariablen $\sigma_{Y}$. [@moosbrugger2020, p.282]

\begin{equation}
\text{Rel} = \frac{\sigma_{\tau}^2}{\sigma_{Y}^2} = \frac{\sigma_{\tau}^2}{\sigma_{\tau}^2+\sigma_{\epsilon}^2}
\end{equation}

Der Wertebereich der Reliabilität liegt in $\text{Rel} \in [0,1]$ wobei eine Wert $0$ gar keine Reliabilität anzeigt, während ein Wert von $\text{Rel} = 1$ auf eine perfekte Reliabilität hindeutet.
:::

Veranschaulicht, wenn der Messfehler gleich Null ist, also $\sigma_{\epsilon}^2 = 0$ gilt, dann ist $\text{Rel} = 1$, während der Messfehler sehr groß im Verhältnis zu $\sigma_{\tau}^2$, dann geht der $\text{Rel}$ gegen Null. Diese Herleitung unterstreicht noch einmal die Abhängigkeit der Reliabilität von der Varianz der Stichprobe. Für einen gegebenen Messfehler wird die Reliabilität besser wenn die Varianz der Messobjekte $\sigma_{\tau}^2$ größer wird. Andersherum, für eine gegebene Varianz $\sigma_{\tau}^2$ zwischen den Objekten wird die Varianz besser wenn der Messfehler $\sigma_{\epsilon}^2$ kleiner wird.

Für die Interpretation der Reliabilität hat sich in der Literatur eine Kategorisierung entlang der Werte in @tbl-ed-rel-interpretation etabliert.

| Bereich | Interpretation |
| --- | --- |
| $<0.5$ | poor |
| $0.5-0.75$ | moderate |
| $0.75-0.9$ | good |
| $>0.9$ | excellent |

: Interpretation der Reliablität nach @koo2016 {#tbl-ed-rel-interpretation}

Es gibt unterschiedliche Arten wie die Reliabilität bestimmt werden kann (@baumgartner1989). Die **Test-Retest-Reliabilität** beschreibt die Stabilität eines Tests über die Zeit, indem die gleichen Objekte zu unterschiedlichen Zeitpunkten getestet werden. Die **Interrater-Reliabilität** bezieht sich auf die Übereinstimmung der Bewertungen durch verschiedene Beobachter oder Rater der gleichen Objekte und wird auch als Objektivität im Rahmen der Testgütekriterien bezeichnet. Die **Paralleltest-Reliabilität** vergleicht die Ergebnisse zweier gleichwertiger Testformen, um die Konsistenz zu überprüfen. 

### Intraclass correlation coefficient

Ein Maß das in der Literatur zur Bestimmung der Reliabilität sehr oft angewendet wird, ist der Intraklassen-Korrelationskoefficient (intraclass correlation coefficient) ICC. In der Literatur gibt es ein ganze Reihe von unterschiedlichen Arten den ICC zu berchnen. Es werden allerdings in den meisten Fällen einer von drei verschiedene Typen angewendet. **ICC(1)** basiert auf einem einfachen Modell mit nur einen zufälligen Faktor. Unter diesem Modell wird jede Bewertungseinheit von unterschiedlichen, zufällig ausgewählten Beurteilern bewertet. ICC(1) ist für die Betrachtungen hier nicht weiter von großem Interesse. **ICC(2)** verwendet ein komplizierteres Modell bei dem alle Beurteiler alle Bewertungsobjekte bewerten. Dabei sind sowohl Beurteiler als auch die Objekte zufällig und werden als Stichproben aus einer größeren Stichprobe angesehen. D.h. die Ergebnisse sind geeignet um über die Beobachtungsobjekte und Bewerter hinaus zu verallgemeinern. **ICC(3)** nutzt ebenfalls ein Zweiweg-Modell. Unter ICC(3) werden die Beurteiler als feste Objekte betrachtet. Daher kann nicht auf andere Beurteiler generalisiert werden. Der Hauptunterschied zwischen ICC(2) und ICC(3) liegt also in der Annahme, ob die Beurteiler zufällig oder fest sind. Daraus resultiert die folgende Systematik der ICCs (siehe @fig-ed-reliability-icc-sys)

```{r}
#| out-width: "80%"
#| label: fig-ed-reliability-icc-sys
#| fig-cap: "Adaptiert nach Liljequist et al. (2019)"

knitr::include_graphics('pics/liljequist_2019.png')
```

Tatsächlich unterscheided sich die Berechnung unter Modell 2 und Modell 3 nicht voneinander. Beide Modelle gibt es in zwei unterschiedlichen Typen: Konsistenz und Agreement. Die Agreementmodelle modellieren die Varianz zwischen den Konsistenz im Gegensatz zu den Konsistenzmodellen. Das agreement ist dabei nicht dasjenige das oben unter Agreement verwendet wurde. Beide Modelle sind nach wie vor Maße für die Reliabilität und nicht für die Übereinstimmung.

Das Datenmodell für die Berechnung des ICC sieht folgendermaßen aus (siehe @tbl-ed-reliability-data).

| Bewertungsobjekt | Bewerter 1 | Bewerter 2 | $\cdots$ | Bewerter k | 
| --- | --- | --- | --- | --- | 
| 1 | $x_{11}$ | $x_{12}$ | $\cdots$ | $x_{1k}$  |
| 2 | $x_{21}$ | $x_{22}$ | $\cdots$ | $x_{2k}$  |
| $\vdots$ |$\vdots$ |$\vdots$ | |$\vdots$ |
| n | $x_{n1}$ | $x_{n2}$ | $\cdots$ | $x_{nk}$ $ |
| Mean | $M_1$ | $M_2$ | $\cdots$ | $M_k$ | 

: Datenmodell für die Berechnung des ICC {#tbl-ed-reliability-data}

D.h. für jedes Bewertungsobjekt werden mehrere Bewertungen abgegeben. Die Bewerter können dabei auch unterschiedliche Messapparaturen sein und müssen nicht unbedingt Personen sein. Generell ergibt sich eine Aufteilung in $r$ Zeilen und $c$ Spalten. Daher kann auch die Streuung entlang dieser beiden Dimensionen ermittelt werden. Eine Streuung zwischen den Bewertern entlang der Zeilen $\sigma^2_r$ und eine Streuung entlang der Spalten zwischen den Objekten $\sigma^2_c$. Daraus können die Modelle wie folgt voneinander unterschieden werden (siehe @tbl-ed-reliability-models-var}.

| Bezeichnung | Modell | 
| --- | -------- | 
| Model 1 | $x_{ij} = \mu + \underbrace{r_i + e_{ij}}_{\text{zufällig}}$| 
| Model 2 | $x_{ij} = \mu + \underbrace{r_i + \overbrace{c_j}^{bias} + e_{ij}}_{\text{zufällig}}$  | 
| Model 3 | $x_{ij} = \mu + \underbrace{r_i}_{\text{zufällig}} + \overbrace{c_j}^{\text{fixed}} + \underbrace{e_{ij}}_{\text{zufällig}}$ |

: Übersicht über die Modelle {#tbl-ed-reliability-models-var}

Die Unterschied zwischen den Bewertern wird dabei als **Bias** bezeichnet. D.h. die Tendenz eines Bewerters eher *zu hohe* bzw. eher *zu niedrige* Werte zu geben. Dies Unterschiede können entweder zufällig sein, wenn die Bewerter als Stichprobe aus einer *Population von Bewertern* angesehen werden, oder als feste Effekte, wenn nur die Unterschiede der beobachteten Bewerter von Bedeutung sind. Daraus resultiert eine Unterschiedliche Berechnung der ICC in Abhängigkeit vom Modell und ob die Konsistenz oder die Übereinstimmung berechnet werden soll.


| Modell |   Modell 2 | Modell 3 |
| --- | --- | --- |
| Übereinstimmung | $\rho_{2A} = \frac{\sigma_r^2}{\sigma_r^2+\sigma_c^2 + \sigma_e^2}$  | $\rho_{3A} = \frac{\sigma_r^2}{\sigma_r^2 + \theta_c^2 + \sigma_e^2}$  |
| Konsistenz  | $\rho_{2C} = \frac{\sigma_r^2}{\sigma_r^2 + \sigma_e^2}$  | $\rho_{3C} = \frac{\sigma_r^2}{\sigma_r^2 + \sigma_e^2}$ |

: Berechnung des ICC $\rho$ für Modell 2 und Modell 3 {#tbl-ed-reliab-icc-calc}

Je nach Modellannahmen und intendierter Zielgröße ergeben sich somit unterschiedliche Berechnungsarten. Insbesondere sind die ICC $\rho$-Werte für die Berechnung der Konsistenz immer mindestens so groß wie diejenigen für die Übereinstimmung, da hier die Varianz $\sigma_r^2$ zwischen den Bewertern noch durch den zusätzlichen Faktor $\sigma_c^2$ unter Modell 2 bzw. $\Theta_c^2$ unter Modell drei geteilt wird.

Es sei noch einmal betont, dass der ICC ein Reliabilitätsmaß ist und somit von der Streuung in der Stichprobe abhängt. Die folgende Abbildung verdeutlicht dies noch einmal (siehe @fig-ed-reliability-icc-change).

```{r}
#| label: fig-ed-reliability-icc-change
#| fig-cap: "Veränderung der ICC-Werte für verschiedene Zusammenhängen zwischen Bewertung und für zwei verschiedene Bereiche."

icc_tostr <- function(icc) {
  paste(paste(icc$type, '=', round(icc$icc,3)), collapse=', ')
}

x_1 <- 1:5
x_2 <- seq(1,10,2)

df <- tibble(
  x = c(rep(x_1, 3), rep(x_2, 3)),
  y = c(x_1, x_1 + 3, 2*x_1, x_2, x_2+ 3, 2*x_2),
  var = rep(c('x=1-5','x=1-9'), each=15),
  type = factor(rep(rep(c('y=x','y=x+3','y=2x'), each=5), 2),
                levels=c('y=x', 'y=x+3', 'y=2x'))
)

df_labs <- df |> 
  group_by(var, type) |> 
  summarize(label = icc_tostr(icc(cbind(x,y))))

ggplot(df, aes(x,y)) + 
  geom_point() +
  geom_label(data = df_labs, aes(x=1, y=15, label = label), size=3, 
             vjust=1, hjust=0) +
  facet_grid(type ~ var) +
  scale_x_continuous("X", breaks = 1:9) +
  scale_y_continuous("Y", limits=c(0,15))
```

### Spearman-Brown prophecy formula

$$
\rho_{xx'}^* = \frac{K \cdot \rho_{xx'}}{1 + (K-1) \cdot \rho_{xx'}}
$$

```{r}
#| fig-cap: "Einfluss der Wiederholungen auf die Reliabilität für verschiedene $\\rho_{xx'}$"

foo <- function(K,r) { K*r/(1+((K-1)*r)) }
K_r <- tidyr::expand_grid(K = 1:10, r = seq(0.1, 0.9, 0.1)) |> 
  dplyr::mutate(r_star = foo(K,r), r_f = ordered(r))
ggplot(K_r, aes(K,r_star, group=r_f)) +
  geom_line() +
  geom_label(data = K_r |> dplyr::filter(K == 1), aes(label = r), size=2) +
  scale_x_continuous('Anzahl der Wiederholungen K', breaks = 1:10) +
  scale_y_continuous(expression(rho[xx*minute]^"*")) +
  scale_color_discrete(expression(rho[xx*minute])) 
```

### Intraclass Correlation in `R`

In `R` kann der ICC relativ komfortable über das Package `psych` berechnet werden, welches die Funtion `ICC()` zur Verfügung stellt.

```{r}
#| echo: true

icc_hat <- psych::ICC(lil[,-1])
icc_hat
```

siehe @liljequist2019, @qin2019


## Beispiel aus @guyatt1987 

```{r}
#| tbl-cap: "Messwerte für acht Patienten"
guyatt_1 <- readxl::read_xlsx('data/reliability_01.xlsx',
                              sheet='Instrument AB')
knitr::kable(guyatt_1,
             booktabs = TRUE,
             linesep = "") |> 
  kableExtra::kable_styling(font_size = 7)
```



## Agreement 

### Standard Error of Measurement 

\begin{align*}
SEM &= \sqrt{\sigma_{error}} \\
SEM_1 &= s_y \sqrt{1 - ICC} \\
SEM_H   &= \frac{s_{d_i}}{\sqrt{2}} 
\end{align*}^[$SEM_H$ = nach @hopkins2000, $s_y$ = Standardabweichung der Messung]

### Limits of Agreement

$\text{loa} = \bar{d}\pm 1.96\ s_d$

```{r}
#| fig-cap: "Bland-Altman Plot der EMG-Frequenzdaten"

lil <- lil |> dplyr::mutate(d = Day_1 - Day_2, m = (Day_1+Day_2)/2)
lil_loa <- dplyr::summarize(lil, d_hat = mean(d), s_d = sd(d), n = dplyr::n(),
                            loa_u = d_hat + 1.96 * s_d,
                            loa_l = d_hat - 1.96 * s_d)
p_loa <- ggplot(lil, aes(m,d)) + 
  geom_point() +
  geom_hline(yintercept = 0, color = 'gray') +
  geom_hline(yintercept = lil_loa$d_hat, linetype = 'dashed') + 
  geom_hline(yintercept = c(lil_loa$loa_u, lil_loa$loa_l),
            linetype = 'dotted') +
  labs(x = 'Mittlere Median EMG Frequenz\n(Day 1 + Day 2)/2',
       y = 'Differenz\nDay 1 - Day 2\nder EMG Frequenzen') 
print(p_loa)
```


Konfidenzintervall bias

$$
CI95\%_{\text{bias}} = \bar{d} \pm 1.96\frac{s_d}{\sqrt{n}}
$$

Konfidenzintervall loa

\begin{align*}
\text{loa} &= \bar{d}\pm 1.96\ s_d \\
Var(\bar{d}\pm1.96\ s_d) &= \left(\frac{1}{n}+\frac{1.96^2}{2(n-1)}\right)s_d^2\approx1.71^2\frac{s_d^2}{n} \\
CI95\%_{\text{loa}} &= \text{loa}\pm q_{t,\alpha,df=n-1}\times 1.71\frac{s_d}{\sqrt{n}}
\end{align*}
^[$s_d$ = Standard deviation of differences]


```{r}
#| fig-cap: "Bland-Altman Plot mit Konfidenzintervallen (bias:rot, loa:grün)"

lil_loa <- lil_loa |> dplyr::mutate(bias_c = 1.96*s_d/sqrt(n),
                         loa_c = qt(0.975, n-1)*1.71*s_d/sqrt(n))
with(lil_loa, 
p_loa + 
  geom_ribbon(aes(ymin=d_hat - bias_c, ymax=d_hat + bias_c),
              alpha=0.1, col = 'red', fill='red') +
  geom_ribbon(aes(ymin=loa_u - loa_c, ymax=loa_u + loa_c),
              alpha=0.1, col = 'green', fill='green') +
  geom_ribbon(aes(ymin=loa_l - loa_c, ymax=loa_l + loa_c),
              alpha=0.1, col = 'green', fill='green'))
```


### Smallest Worthwhile Change (SWC)

```{r}
#| fig-cap: "Pre-Pre-Post Design um Limits of Agreement zu bestimmen."

{
s_1 <- 123
set.seed(s_1)
n <- 30
mu_s <- c(10, 10, 11)
R <- matrix(c(1,0.5,0.5,0.5,1,0.5,0.5,0.5,1), nr=3)
S <- diag(rep(0.5,3))
sigma <- S %*% R %*% S
y_i <- MASS::mvrnorm(n, mu_s, sigma)
sigma[3,1] <- sigma[1,3] <- 8;
dat <- tibble::tibble(
  time = rep(1:3, each=n),
  y = c(y_i[,1], y_i[,2], y_i[,3]),
  id = rep(paste('S',1:n), 3)
) |> dplyr::arrange(id)

loa_dat <- dat |> dplyr::filter(time != 3) |> dplyr::group_by(id) |> 
  dplyr::summarize(d_i = diff(y)) |> 
  dplyr::summarize(s_d = sd(d_i), d_hat = mean(d_i))
y_hat_pre_1 <- dat |> dplyr::filter(time == 1) |> dplyr::summarize(m = mean(y))
loa_l <- y_hat_pre_1$m - 1.96 * loa_dat$s_d
loa_u <- y_hat_pre_1$m + 1.96 * loa_dat$s_d
loa_tibl <- tibble::tibble(time = 2:3,
                           ymin = c(loa_l, loa_l),
                           ymax = c(loa_u, loa_u),
                           y = 0)
pos <- position_jitter(seed = s_1, width=.1)
ggplot(dat, aes(time, y)) +
  geom_boxplot(aes(group=time)) +
  geom_ribbon(data = loa_tibl, aes(ymin = ymin, ymax = ymax),
              alpha = 0.1, col = 'red', fill = 'red') +
  geom_point(aes(group=id), position = pos) +
  geom_line(aes(group=id), position=pos, linetype='dotted') + 
  scale_x_continuous('Zeit', breaks=1:3, label=c('Pre_0','Pre','Post')) +
  scale_y_continuous('Abhängige Variable', labels=NULL) 
}
```

### Analytical goal

## Zum Nachlesen 

| Übersicht: @atkinson1998, @baumgartner1989
| Limits of agreement: @bland1999, @atkinson2000
| Analytical goal: @brown2007, @atkinson1998, @fraser1990
| SEM: @denegar1993

