{
  "hash": "fa0cb47189b1edc124e0c67fcdee36df",
  "result": {
    "engine": "knitr",
    "markdown": "# Einführung \n\n\n::: {.cell}\n\n:::\n\n\n## Back to school\n\nWir beginnen mit einem uns schon bekannten Konzept, der Punkt-Steigungsform aus der Schule (siehe Formel \\eqref{eq-slm-psform-1}).\n\n\\begin{equation}\ny = m x + b\n\\label{eq-slm-psform-1}\n\\end{equation}\n\nWir haben eine abhängige Variable $y$ und eine lineare Formel $mx + b$ die den funktionalen Zusammenhang zwischen den Variablen $y$ und $x$ beschreibt. Um das Ganze einmal konkret zu machen setzen wir $m = 2$ und $b = 3$ fest. Die Formel \\eqref{eq-slm-psform-1} wird dann zu:\n\n\n::: {.cell}\n\n:::\n\n\n\\begin{equation}\ny = 2 x + 3\n\\label{eq-slm-psform-2}\n\\end{equation}\n\nUm ein paar Werte für $y$ zu erhalten setzen wir jetzt verschiedene Wert für $x$ ein indem wir $x$ in Einserschritten zwischen $[0, \\ldots, 5]$ erhöhen. Um die Werte darzustellen verwenden wir zunächst eine Tabelle (vgl. @tbl-slm-psform)\n\n\n::: {#tbl-slm-psform .cell tbl-cap='Tabelle der Daten'}\n::: {.cell-output-display}\n\n\n|  x|  y|\n|--:|--:|\n|  0|  3|\n|  1|  5|\n|  2|  7|\n|  3|  9|\n|  4| 11|\n|  5| 13|\n\n\n:::\n:::\n\n\nWenig überraschend nimmt $y$ für den Wert $x = 0$ den Wert $3$ an und z.B. für den Wert $x = 3$ nimmt $y$ den Wert $2 \\cdot 3 + 3 = 9$ an. \n\nEine weitere Darstellungsform ist die graphische Darstellung in dem wir die Werte von $y$ gegen $x$ in einem Graphen abtragen (siehe @fig-slm-psform-1).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Graphische Darstellung der Daten aus @tbl-slm-psform](slm_basics_files/figure-html/fig-slm-psform-1-1.png){#fig-slm-psform-1 width=672}\n:::\n:::\n\n\nWiederum wenig überraschen sehen wir einen linearen Zuwachs der $y$-Wert mit den größerwerdenden $x$-Werten. Da in der Definition der Formel \\eqref{eq-slm-psform-2} nirgends festgelegt wurde, dass diese nur für ganzzahlige $x$-Werte gilt, haben wir direkt eine Gerade durch die Punkte gelegt. Somit sollte auch die Bedeutung von $m$ und $b$ direkt klar. Die Variable $m$ bestimmt die Steigung der Gleichung während $b$ den y-Achsenabschnitt beschreibt. \n\n::: {#def-slm-basics-y-intercept}\n## $y$-Achsenabschnitt\n\nDer y-Achsenabschnitt\\index{y-Achsenabschnitt} ist der Wert den $y$ einnimmt wenn $x$ den Wert $0$ annimmt. Sei $y$ durch eine lineare Gleichung $y = mx + b$ definiert, dann wird der y-Achsenabschnitt durch den Wert $b$ bestimmt.\n:::\n\nDie Variable $m$ dahingehend bestimmt die Steigung der Gerade. \n\n::: {#def-slm-basics-slope}\n## Steigungskoeffizient\n\nWenn $y$ durch eine lineare Gleichung $y = mx + b$ definiert ist, dann bestimmt die Variable $m$ die Steiung der dazugehörenden Gerade. D.h. wenn sich die Variable $x$ um einen Einheit vergrößert (verkleinert) wird der Wert von $y$ um $m$ Einheiten größer (kleiner). Gilt $m < 0$ dann umgekehrt.\\index{Steigungskoeffizient}\n:::\n\nDiese beiden trivialen Konzepte mit eigenen Definitionen zu versehen erscheint im ersten Moment vielleicht etwas übertrieben. Wie sich allerdings später zeigen wird, sind diese beiden Einsichten immer wieder zentral wenn es um die Interpretation von linearen statistischen Modellen geht.\n\nSoweit so gut. Führen wir direkt ein paar Symbole ein, die uns später noch behilflich sein werden. Sei jetzt die Menge der $x$-Werte geben $x = [0, 1, 2, 3, 4, 5]$. Strenggenommen handelt es sich wieder um ein Tupel, da wir jetzt die Reihenfolge nicht mehr ändern. Wir führen nun einen Index $i$ ein, um einzelne Werte in dem Tupel über ihre Position zu bestimmen und wir hängen diesen Index $i$ an $x$ an. Dann wird aus $x$, $x_i$.\n\n\n::: {#tbl-slm-basic-index .cell tbl-cap='$x$-Werte und ihr Index $i$'}\n::: {.cell-output-display}\n\n\n| Index $i$| $x$-Wert|\n|---------:|--------:|\n|         1|        0|\n|         2|        1|\n|         3|        2|\n|         4|        3|\n|         5|        4|\n|         6|        5|\n\n\n:::\n:::\n\n\nDamit können wir jetzt einen speziellen Wert zum Beispiel den dritten Wert mit $x_3 = 2$ bestimmen. Wenden wir unseren Index auf unsere Formel \\eqref{eq-slm-psform-1} an, folgt daraus, dass $y$ jetzt auch einen Index $i$ erhält.\n\n$$\ny_i = m x_i + b \\qquad i \\text{ in } [1,2,3,4,5,6]\n$$\n\nWir bezeichnen die beiden Variablen $m$, die Steigung, und $b$, den y-Achsenabschnitt, mit neuen Variablen die ebenfalls einen Index erhalten. Aus $m$ wird $\\beta_1$ und aus $b$ wird $\\beta_0$. Damit wird der y-Achsenabschnitt mit $\\beta_0$ bezeichnet und die Steigung wird mit $\\beta_1$ bezeichnet. Dann wir aus unserer Gleichung:\n\n\\begin{equation}\ny_i = \\beta_0 + \\beta_1 x_i\n\\label{eq-slm-psform-beta}\n\\end{equation}\n\nFormel \\eqref{eq-slm-psform-beta} ist immer noch unsere einfache Punkt-Steigungsform, wir haben lediglich den Index $i$ eingeführt um unterschiedliche $y-x$-Wertepaare zu bezeichnen und wir haben den $y$-Achsenabschnitt und die Steigung mit neuen Symbolen versehen. Wenn wir uns später mit der multiplen linearen Regression beschäftigen und nicht mehr nur eine einzige $x$-Variable haben, dann vereinfacht die Benutzung von $\\beta$s die Schreibweise, da wir einfach zusätzliche $\\beta$s anhängen und mit einem fortlaufenden Index (z.B. $\\beta_2, \\beta_3, \\ldots$) versehen.\n\n## Funktionaler versus stochastischer Zusammenhang zwischen zwei Variablen\n\nBei dem bisherigen Zusammenhang handelt es sich um einen funktionalen Zusammenhang\\index{Funktionaler Zusammenhang} zwischen den beiden Variablen $x$ und $y$. Funktional deswegen, weil wir ein definiertes mathematisches Modell angeben können, d.h. wir haben eine präzise mathematische Funktion welche die Beziehung zwischen den beiden Variablen beschreibt. Wenn wir den Wert für $x$ kennen, dann können wir einen, einzigen Wert für $y$ ausreichnen indem wir den Wert $x$ in Formel \\eqref{eq-slm-psform-1} einsetzen. Aus der Schule kennen wir noch die Darstellung $y = f(x)$. Streng genommen ist diese Darstellung für Formel \\eqref{eq-slm-psform-1} nicht ausreichend, denn um den Wert für $y$ auszurechnen benötigen wir auch noch Kenntnis über die Werte $m$ und $b$, bzw. in unsere weiteren Darstellung $\\beta_0$ und $\\beta_1$. Daher sollte der Zusammenhang eigentlich mit $y = f(x, \\beta_0, \\beta_1)$ bezeichnet werden. Es gilt aber immernoch, für gegebene $x, \\beta_0$ und $\\beta_1$ ist der Wert für $y$ fest determiniert.\n\nWenn wir mit realen Daten arbeiten, dann funktioniert dieser Ansatz leider nicht direkt. Selbst wenn wir ein Experiment mehrmals genau gleich durchführen, werden wir immer etwas unterschiedliche Werte im Sinne der Messungenauigkeit messen. Wenn wir biologische Systeme messen, kommt dazu das diese in den seltensten Fällen zeitstabil sind, sondern immer bestimmte Veränderungen von einem Zeitpunkt zum nächsten auftauchen.\n\nIn @fig-slm-basics-jump-1 ist ein realer Datensatz abgetragen. Auf der $y$-Achse sind die Sprungweiten von mehreren Weitspringerinnen gegen die Anlaufgeschwindigkeit auf der $x$-Achse abgetragen. Bei der Betrachtung der Daten erscheint ein linearer Zusammenhang zwischen diesen beiden Variablen durchaus als plausibel. \n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite beim Weitsprung](slm_basics_files/figure-html/fig-slm-basics-jump-1-1.png){#fig-slm-basics-jump-1 width=672}\n:::\n:::\n\n\nIn @fig-slm-basics-jump-1 sind zwei Punkte rot markiert. Die beiden Werte haben praktisch den gleichen $x$-Wert unterscheiden sich allerdings bezüglich ihrer $y$-Werte deutlich voneinander. Und dies sind nicht die einzigen Beispielpaare bei denen die $x$-Werte nahe zusammen liegen, während die $y$-Werte klar voneinander getrennt sind. Dies ist im Unterschied zu einen funktionalen Zusammenhang nach Formel \\eqref{eq-slm-psform-1}. Bei einem rein funktionalen Zusammenhang wird jedem $x$-Wert genau ein $y$-Wert zugeordnet.\n\nDie Abweichungen kommen durch zufällige Einflussfaktoren wie eben zum Beispiel die Veränderungen angesprochener biologischer Faktoren und Messunsicherheiten. Beim Beispiels des Weitsprungs spielen auch noch externe Einflüsse, wie beispielsweise Windverhältnisse, eine Rolle. Vielleicht wenn es sich um den gleichen Springer handelt, hat er auch beim zweiten Mal keine Lust mehr gehabt. Wenn die Punkte zwei unterschiedliche Springer sind, dann kommt auch dazu, dass zwei Weitspringer bei identischer Anlaufgeschwindigkeit unterschiedliche Sprungfähigkeiten haben oder auch technisch nicht gleich gesprungen sind und so weiter und so fort. Insgesamt führen alle diese Einflüsse dazu, dass wir nicht mehr einen streng funktionalen Zusammenhang zwischen unseren beiden Variablen $x$ der Anlaufgeschwindigkeit und $y$ der Sprungweite vorfinden. Es handelt sich um einen stochastischen Zusammenhang zwischen den beiden Variablen. Wie wir mit diesen zufälligen Einflüssen umgehen ist das zentrale Thema des nächsten Abschnitts und markiert auch unseren Eingang zur einfachen linearen Regression.\n\n## Die einfache lineare Regression\n\nBleiben wir bei unserem Beispiel aus @fig-slm-basics-jump-1. Wir versetzen uns in die Lage einer Weitsprungtrainerin und die vor der Aufgabe steht ihr Training zu optimieren um die Weitsprungleistung ihrer Athleten zu verbessern. Wir haben uns dazu entschlossen am Anlauf etwas zu verbessern wissen jetzt aber nicht ob das wirklich lohnenswert ist. Von einer befreundeten Trainerin haben wir einen Datensatz bekommen mit Anlaufgeschwindigkeiten und den dazugehörigen Sprungweiten. Schauen wir uns zunächst die einmal die Struktur der Daten an.\n\n\n::: {#tbl-slm-basics-jump-1 .cell tbl-cap='Ausschnitt der Sprungdaten'}\n::: {.cell-output-display}\n\n\n| jump_m| v_ms|\n|------:|----:|\n|   4.36| 6.13|\n|   4.31| 6.39|\n|   4.56| 6.56|\n|   4.75| 6.44|\n|   5.52| 7.30|\n|   5.63| 7.19|\n|   5.70| 7.30|\n\n\n:::\n:::\n\n\nIn @tbl-slm-basics-jump-1 sind die ersten $7$ Zeilen der Sprungdaten abgebildet. Die Daten zeigen eine einfache Struktur mit zwei Spalten. `jump_m` bezeichnet die Sprungweiten und `v_ms` die Anlaufgeschwindigkeiten. Damit wir die Datenpaare voneinander unterscheiden bzw. identifizieren können führen wir unseren bereits besprochenen Index $i$ und können so einzelne Paare ansprechen.\n\n\n::: {#tbl-slm-basics-jump-2 .cell tbl-cap='Ausschnitt der Sprungdaten'}\n::: {.cell-output-display}\n\n\n|  i| jump_m| v_ms|\n|--:|------:|----:|\n|  1|   4.36| 6.13|\n|  2|   4.31| 6.39|\n|  3|   4.56| 6.56|\n|  4|   4.75| 6.44|\n|  5|   5.52| 7.30|\n|  6|   5.63| 7.19|\n|  7|   5.70| 7.30|\n\n\n:::\n:::\n\n\nDaher hat zum Beispiel der dritte Datenpunkt $i = 3$  die Datenwerte $\\text{jump} = 4.31$ und $\\text{v}_3 = 6.56$.\n\nDas waren bisher aber nur Formalitäten. Wir wollen jetzt denn Zusammenhang zwischen den beiden Variablen modellieren. Wir könnten wahrscheinlich auch einfach Pi-mal-Daumen abschätzen wie groß der Zusammenhang ist. Wenn wir jetzt aber einen unserer Läufer haben, der z.B. etwa $9m/s$ anläuft, welchen Vergleichswerte nehmen wir dann aus Formel \\eqref{fig-slm-basics-jump-1}. Den unteren oder den oberen der beiden roten Werte? Oder vielleicht den Mittelwert? Welchen Wert nehmen wir wenn unser Athlete mit $v=9.7m/s$ anläuft. Da haben wir leider keinen Vergleichswert in unserer Tabelle. Daher wäre es schon ganz praktisch eine Formel nach dem Muster von Formel \\eqref{eq-slm-psform-beta} zu haben. Wie wir allerdings schon festgestellt haben, geht dies nicht so einfach da wir eben das Problem mit den Einflussfaktoren haben, die dazu führen, dass die Werte nicht präzise auf eine Gerade liegen. Somit liegt die Herausforderung nun darin, eine Gerade zu finden die möglichst *genau* die Daten wiederspiegelt.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Mögliche Geraden um den Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite zu modellieren](slm_basics_files/figure-html/fig-slm-basics-line-1-1.png){#fig-slm-basics-line-1 width=672}\n:::\n:::\n\n\nIn @fig-slm-basics-line-1 sind die Daten zusammen mit verschiedenen möglichen Geraden abgebildet. Eine kurze Überlegung macht schnell klar, dass es im Prinzip unendlich viele unterschiedliche Geraden gibt die durch die Datenpunkte gelegt werden können. Da jede Gerade durch die beiden Parameter $\\beta_0$ und $\\beta_1$ spezifiziert ist, gibt es somit auch unendlich viele Kombinationen von $\\beta_0$ und $\\beta_1$, die die jeweiligen Geraden definieren. Um nun eine spezielle Gerade aus den unendlich vielen Geraden auswählen zu können, benötigen wir ein Kriterium welches definiert was eine besser passende versus eine schlechter passende Gerade durch die gegebenen Punkte ist. D.h. wir suchen eine Geraden die im Sinne eines Kriteriums *optimal* ist. Tatsächlich gibt es dort auch verschiedene Möglichkeiten Kriterien zu entwickeln. Dasjenige Kriterium das am weitesten verbreitet ist und auch intuitiv nachvollziehbar ist basiert auf den quadrierten Abweichungen der Gerade von den Datenpunkten (engl. least squares). Versuchen wir daher nun die Herleitung der least squares schrittweise nachzuvollziehen.\n\n## Methode der kleinsten Quadrate\n\nFangen wir dazu zunächst einmal nur mit den einfachen Abweichung, also nicht den quadrierten Abweichungen an. In @fig-slm-basics-line-2 ist zur Übersicht nur ein Ausschnitt der Daten zusammen mit einer möglichen Gerade eingezeichnet. Die senkrechten Abweichungen der Geraden zu den jeweiligen Datenpunkten sind rot eingezeichnet. Es ist ersichtlich, dass für diese Wahl der Geraden zwei Punkte vorhanden sind, die ziemlich genau auf der Geraden liegen während die anderen Punkte zum Teil oberhalb bzw. unterhalb der Geraden liegen. Ein mögliches Kriterium könnte dementsprechend sein, diejenige Geraden aus den unendlich vielen zu finden, bei der die Abweichung (die roten Linien) ein Minimum annimmt.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Abweichungen der Gerade von der Datenpunkten für die Daten mit eine Anlaufgeschwindigkeit zwischen $8m/s$ und $10m/s$.](slm_basics_files/figure-html/fig-slm-basics-line-2-1.png){#fig-slm-basics-line-2 width=672}\n:::\n:::\n\n\nUm die Abweichungen (rote Linien) berechnen zu können benötigen wir für jeden beobachteten Wert den dazugehörenden Punkt auf der Geraden. Wenn wir die Gerade schon hätten, dann wäre das kein Problem. Bisher haben wir allerdings nur die Formel der Geraden. Die Punkte auf der Geraden sind mit Hilfe der Formel nun genau diejenigen $y$-Werte die für die jeweiligen $x$-Werte mittels der $\\beta_0 + \\beta_1 x_i$ berechnet werden. Die Abweichung für einen Punkt $y_i$ von der Geraden lässt sich somit wie folgt berechnen:\n\n\\begin{equation*}\n\\text{Abweichung}_i = y_i - (\\beta_0 + \\beta_1 x_i)\n\\end{equation*}\n\nEntsprechend führt unser Kriterium zu Summation der Abweichungen über alle Punkte zu folgender Formel:\n\n\\begin{equation*}\n\\text{min}\\sum_{i=1}^n y_i - (\\beta_0 + \\beta_1 x_i) = \\sum_{i=1}^n y_i - \\beta_0 - \\beta_1 x_i\n\\end{equation*}\n\nWir verschärfen das Kriterium um eine optimale Gerade zu finden nun noch etwas indem wir größere Abweichungen stärker gewichten wollen als kleinere Abweichungen. D.h. große Abweichungen zwischen der Gerade und den Datenpunkten sollen stärker berücksichtigt werden, als kleine Abweichungen. Dies können wir erreichen indem wir die Abweichungen quadrieren. Dementsprechend erhalten wir die folgende Gleichung:\n\n\\begin{equation}\nE(\\beta_0, \\beta_1) = \\sum_{i=1}^n(y_i - (\\beta_0 + \\beta_1 x_i))^2\n\\label{eq-slm-basics-rms-1}\n\\end{equation}\n\nMit $E(\\beta_0, \\beta_1)$ wird ausgedrückt, dass die Summe von der Wahl der beiden Parametern $\\beta_0$ und $\\beta_1$ abhängt. $E$ ist eine Funktion von $\\beta_0$ und $\\beta_1$. Um unsere Gerade zu bestimmen suchen wir daher nun das Minimum der Funktion $E(\\beta_0, \\beta_1)$ ($E$ nach englisch error).\n\nDie Abweichungen der Punkte auf der Gerade von den tatsächlichen Datenpunkten spielen in der weiteren Betrachtung immer wieder eine wichtige Rolle und haben daher einen eigenen Term. Die Abweichungen werden als Residuen $e_i$ bezeichnet. Dementsprechend kann die Minimierungsgleichung auch mit:\n\n$$\n\\text{min} \\sum_{i=1}^n e_i^2\n$$\ndargestellt werden. Es gilt $e_i := y_i - (\\beta_0 + \\beta_1 x_i)$. Insgesamt:\n\n\\begin{equation*}\nE(\\beta_0, \\beta_1) = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2\n\\end{equation*}\n\nDas Minimum läßt sich nun bestimmen, indem wir die partiellen Ableitungen von $E$ nach $\\beta_0$ und $\\beta_1$ berechnet werden und, wie wir es aus der Schule kennen, diese gleich Null setzen.\n\n\\begin{align*}\n\\frac{\\partial E(\\beta_0, \\beta_1)}{\\partial \\beta_0} &= -2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) = 0 \\\\\n\\frac{\\partial E(\\beta_0, \\beta_1)}{\\partial \\beta_1} &= -2 \\sum_{i=1}^n x_i (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\end{align*}\n\nDies Gleichungen bilden ein Gleichungssystem mit zwei Unbekannten $\\beta_0$ und $\\beta_1$ und können mittels etwas Algebra nach $\\beta_0$ und $\\beta_1$ aufgelöst werden (siehe @sec-step-by-step-normal). Dies führt zu den folgenden Lösungen für $\\beta_0$ und $\\beta_1$:\n\n\\begin{align}\n\\label{eq-slm-basics-norm-1} \n\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n\\label{eq-slm-basics-norm-2}\n\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1} \\bar{x} \n\\end{align} \n\n$\\bar{x}$ und $\\bar{y}$ sind die Mittelwerte von $x_i$ und $y_i$. Diese beiden Gleichungen werden als die Normalengleichungen \\index{Normalengleichungen} bezeichnet.\n\nWir führen noch einen weiteren Term ein, den vorhergesagten Wert $\\hat{y}_i$ von $y_i$ anhand der Geradengleichung. Das Hütchen über $y_i$ ist dabei immer das Signal dafür, das es sich um einen abgeschätzten Wert handelt. Wenn wir $\\beta_0$ und $\\beta_1$ anhand der Normalengleichung bestimmen, dann sind das mit großer Wahrscheinlichkeit nicht die *wahren* Werte aus der Population, sondern wir haben sie nur anhand der Daten abgeschätzt. Daher bekommen die berechneten Werte ebenfalls ein Hütchen $\\hat{\\beta}_0$ und $\\hat{\\beta}_1$. Insgesamt nimmt die lineare Geradengleichung dann die folgende Form an:\n\n$$\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i\n$$\n\nD.h. die Residuen können auch mittels $e_i = y_i - \\hat{y}_i$ dargestellt werden. Graphisch sind die $\\hat{y}_i$s die Werte auf der Geraden für die gegebenen $x_i$-Werte.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Die vorhergesaten Werte $\\hat{y}_i$ auf der Gerade.](slm_basics_files/figure-html/fig-slm-basics-line-3-1.png){#fig-slm-basics-line-3 width=672}\n:::\n:::\n\n\nGehen wir zurück zu unseren Weitsprungdaten. Wenn wir die Datenpunkte in die Normalengleichungen Formeln \\eqref{eq-slm-basics-norm-1} und \\eqref{eq-slm-basics-norm-2} einsetzen, dann erhalten wir für die Koeffizienten die Werte $\\hat{\\beta}_0 = -0.14$ und $\\hat{\\beta}_1 = 0.76$. Somit folgt für die Geradengleichung:\n\n$$\n\\hat{y}_i = -0.14 + 0.76 \\cdot x_i\n$$\n\nWir erhalten die graphische Darstellung der Geradengleichung indem die $x_i$-Werte eingesetzt werden und eine Gerade durch die Punkte gezogen wird. Oder auch einfacher für den größten und den kleinsten $x_i$-Wert.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Die Regressionsgerade der Sprungdaten.](slm_basics_files/figure-html/fig-slm-basics-line-4-1.png){#fig-slm-basics-line-4 width=672}\n:::\n:::\n\n\nUm uns auch zu vergewissern, dass unsere Berechnungen korrekt sind, schauen wir uns noch einmal an, wie sich $E$ verhält, wenn wir unterschiedliche Kombinationen von Werten für $\\beta_0$ und $\\beta_1$ in die lineare Gleichung einsetzen.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Heatmap von $log(E)$ für verschiedene Werte von $\\beta_0$ und $\\beta_1$](slm_basics_files/figure-html/fig-slm-basics-gradient-1.png){#fig-slm-basics-gradient width=384}\n:::\n:::\n\n\nIn @fig-slm-basics-gradient sind verschiedene Werte für $E$ in Form einer heatmap dargestellt. Die Abweichungen wurden $log$-transformiert (d.h. der Logarithmus der $E$-Werte wurde berechnet), da sonst die Unterschiede in der diagnaolen Bildrichtung zu schnell wachsen und die Unterschiede nicht mehr so einfach zu erkennen sind. Werte näher an Weiß bedeuten kleine Werte und Werte näher an Rot bedeuten größere Werte von $E$. Das berechnete Paar für $(\\hat{\\beta}_0, \\hat{\\beta}_1)$ mit $\\hat{\\beta}_0 = -0.14$ und $\\hat{\\beta}_1 = 0.76$ ist schwarz eingezeichnet. Die Abbildung zeigt, dass dieses Wertepaar tatsächlich ein Minimum bezüglich der Funktion $E$ ist, da in alle Richtung weg von dem schwarzen Punkt die Werte für $E$ zunehmen. Da wir nur einen Ausschnitt der möglichen Werte sehen, handelt es sich zunächst um eine lokales Minimum aber es lässt sich zeigen, dass es sich dabei auch um ein globales Minimum handelt. Diese Eigenschaft hängt mit der Form der Funktion $E$ zusammen. In @tbl-slm-basics-gradient sind beispielhaft ein paar Werte für $log(E)$ für Paare von $\\beta_0$ und $\\beta_1$ angezeigt, die in @fig-slm-basics-gradient gelb eingezeichnet sind.\n\n\n::: {#tbl-slm-basics-gradient .cell tbl-cap='Werte von $log(E)$ für verschiedenen Kombinationen von $\\beta_0$ und $\\beta_1$.'}\n::: {.cell-output-display}\n\n\n| $\\beta_0$| $\\beta_1$| $log(E)$|\n|---------:|---------:|--------:|\n|     -0.48|      0.67|    70.34|\n|     -0.44|      0.68|    51.85|\n|     -0.38|      0.72|    22.04|\n|     -0.30|      0.75|     6.46|\n|     -0.22|      0.75|     3.77|\n|     -0.14|      0.76|     2.41|\n\n\n:::\n:::\n\n\nWir können in @tbl-slm-basics-gradient erkennen, das das Wertepaar für $\\beta_0$ und $\\beta_1$, welches wir mit Hilfe der Normalengleichung bestimmt haben, denn kleinesten Abweichungswert hat. Also im Sinne unseres Kriteriums ein Optimum darstellt.\n\n\n## Was bedeuten die Koeffizienten?\n\nGehen wir zurück nun zu unseren Ausgangsproblem der Weitspringer, was haben wir jetzt durch die Berechnung der Gerade eigentlich gewonnen? Dazu müssen wir erst einmal verstehen was die beiden Koeffizienten $\\hat{\\beta}_0$ und $\\hat{\\beta}_1$ bedeuten. Wenn wir zurück zu Formel \\eqref{eq-slm-psform-1} gehen, haben die beiden Koeffzienten den $y$-Achsenabschnitt und die Steigung der Geraden beschrieben. In unserem Beispiel haben wir anhand der Daten einen $y$-Achsenabschnitt $\\hat{\\beta}_0$ von $-0.14$ berechnet. D.h ein Weitspringer der mit einer Anlaufgeschwindigkeit von $x = 0$ anläuft, landet $14$cm hinter der Sprunglinie. Dies macht offensichtlich nicht viel Sinn (warum?). Der Grund warum hier ein offensichtlich unrealistischere Wert berechnet wurde, werden wir später noch genauer betrachten. Wir können trotzdem zwei Eigenschaften von $\\hat{\\beta}_0$ beobachten. 1) der Koeffizient hat eine Einheit, nämlich die gleiche Einheit wie die Variable $y$. 2) Ob der Wert zu interpretieren ist, hängt von der Verteilung der Daten ab. Schauen wir uns nun den Steigungskoeffizienten $\\hat{\\beta}_0$ an. Der Steigungskoeffizient in Formel \\eqref{eq-slm-psform-1} zeigt an, wie sich der $y$-Wert verändert, wenn sich der $x$-Wert um einen Einheit verändert. In unserem Fall welcher Unterschied zu erwarten ist zwischen zwei Weitspringern die sich in der Anlaufgeschwindigkeit um eine $m/s$ unterscheiden. D.h. der Steigungskoeffizient ist ebenfalls in der Einheit der $y$-Variable zu interpretieren.\n\nUnsere Trainerin kann jetzt die berechnete Gerade dazu nehmen um zu überprüfen ob es sich lohnen würde Trainingszeit in den Anlauf zu stecken und welche Verbesserung dort zu erwarten sind. Allerdings fehlt dazu noch etwas, wir wissen nämlich noch nicht ob die berechnete Gerade auch wirklich die Daten gut wiederspiegelt. Im Beispiel erscheint dies anhand der Grafik als relativ plausibel. Das muss aber nicht immer so sein. Wir können nämlich für alle möglichen Daten eine Gerade berechnen ohne das diese Gerade die Daten wirklich auch nur annährend korrekt wiedergibt. In den Formeln \\eqref{eq-slm-basics-norm-1} und \\eqref{eq-slm-basics-norm-2} steht nirgends für welche Daten die Berechnung nur erlaubt ist.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Gefittete Gerade durch die Daten einer Funktion $f(x) = x^3$.](slm_basics_files/figure-html/fig-slm-basics-cubepoly-1.png){#fig-slm-basics-cubepoly width=672}\n:::\n:::\n\n\nIn @fig-slm-basics-cubepoly sind synthetische Daten der Funktion $f(x) = x^3$ abgebildet und die mittels \\eqref{eq-slm-basics-norm-1} berechneten Gerade eingezeichnet. Die Gerade ist zwar in der Lage die ansteigenden Werte zu modellieren aber eben nicht Schwingungen die durch die kubische Abhängigkeit zustande kommen. Aber, nichts verhindert die Anwendung der Formel auf die Daten!\n\nDer gleiche Effekt ist auch in @fig-slm-basics-sinus wieder zu beobachten. Hier besteht eine sinusförmige Abhängigkeit zwischen $y$ und $x$. Wir können wieder die Normalengleichungen anwenden und erhalten wieder ein Ergebnis für $\\hat{\\beta}_0$ und $\\hat{\\beta}_1$. Allerdings repräsentiert die Gerade in keinster Weise den tatsächlichen Zusammenhang zwischen den Daten wie in @fig-slm-basics-sinus zu beobachten ist.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Gefittete Gerade durch die Daten einer Funktion $f(x) = sin(x) + 3$.](slm_basics_files/figure-html/fig-slm-basics-sinus-1.png){#fig-slm-basics-sinus width=672}\n:::\n:::\n\n\nIm nächsten Kapitel werden wir uns daher damit beschäftigen die Repräsentation der Daten näher zu betrachten und zu präzisieren.\n\nWir nehmen noch eine weitere Eigenschaft der Gerade mit, die zunächst nichts mit der Interpretation der Koeffizienten zu tun hat, aber später noch mal von Interesse sein wird. Die Gerade hat nämlich die Eigenschaft durch den Punkt ($\\bar{x}$, $\\bar{y}$) zu gehen. Dies kann daran gesehen werden wenn in die Gleichung $\\bar{x}$ für $x_i$ eingesetzt wird. Anhand der Formel \\eqref{eq-slm-basics-norm-2} der Normalgleichungen kann die Geradengleichung in der folgenden Form dargestellt werden.\n\n$$\ny_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i = \\underbrace{\\bar{y} - \\hat{\\beta}_1 \\bar{x}}_{\\text{Def. }\\hat{\\beta}_0} + \\hat{\\beta}_1 \\cdot x_i\n$$\n\nWird jetzt für $x_i$ der Mittelwer der $x$-Werte $\\bar{x}$ eingesetzt folgt daraus.\n\n$$\ny_i = \\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 \\bar{x} = \\bar{y}\n$$\n\nD.h. für den Wert $\\bar{x}$ nimmt die Geradengleichung der Wert $\\bar{y}$ an. Für die Sprungdaten ist diese Eigenschaft auch noch mal in @fig-slm-basics-hatxy graphisch dargestellt.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Regressionsgerade der Sprungdaten und der Punkt $(\\bar{x}, \\bar{y})$](slm_basics_files/figure-html/fig-slm-basics-hatxy-1.png){#fig-slm-basics-hatxy width=672}\n:::\n:::\n\n\nEine weitere Eigenschaft die in weiteren Behandlung der Regression immer wieder auftaucht bezieht sich auf die $x$-Werte. Bei der Regression wird im Allgemeinen davon ausgegangen, dass die beobachteten $x$-Werte fixiert sind. D.h. trotzdem die $x$-Werte bei einem Experiment möglicherweise zufällige Werte angenommen haben, werden diese in den nachfolgenden Analyseschritten als fixiert bzw. gegeben angesehen.\n\n## Die einfache lineare Regression in `R`\n\nIn `R` wird eine Regression mit der Funktion `lm()` berechnet. Die für uns zunächst wichtigsten Parameter von `lm()` sind der erste Parameter `formula` und der zweite Parameter `data`. Mit der Formel wird der Zusammenhang zwischen den Variablen beschrieben, dabei können die Namen bzw. Bezeichner aus dem `tibble()` benutzt werden, die an den zweiten Parameter `data` übergeben werden. D.h. die Spaltennamen aus dem `tibble()` werden in `formula` verwendet.\n\nIn unserem Weitsprungbeispiel konnten wir in @tbl-slm-basics-jump-1 sehen, das das `tibble()` zwei Spalten mit den Namen `v_ms`, den Anlaufgeschwindigkeiten, und `jump_m`, den Weitsprungweiten enthielt. Dementsprechend, müssen wir diese beiden Bezeichner in `formula` verwenden, um unser Regressionsmodell zu beschreiben. Die Form der Modellbeschreibung folgt, dabei einer bestimmten Syntax die wir uns zunächst anschauen müssen. Zentrales Element der Syntax ist das Tilde Zeichen `~` (WIN: {{< kbd ALTGR-+ >}}, MAC: {{< kbd ALT-n >}}),  welches interpretiert wird als *modelliert mit*. Der Term der auf der linken Seite steht bezeichnet die abhängige Variable während die Terme auf der rechten Seite der Tilde stehen die unabhängige Variablen spezifizieren. Dementsprechend kann der Satz *\"Y wird mittels X modelliert\"* in die Formelsyntax mit `Y ~ X` übersetzt. Die komplette Syntax orientiert sich an eine Arbeit von @wilkinson1973.\n\nWenn ein konstanter in der Syntax benötigt wird, dann wird dieser mit einer $1$ bezeichnet. Also zum Beispiel wenn wir die Formel \\eqref{eq-slm-psform-beta} modellieren wollen benutzen wir die Syntax `y ~ 1 + x`. Die beiden Koeffizienten $\\beta_0$ und $\\beta_1$ brauchen wir nicht explizit anzugeben, sondern `R` generiert uns automatisch anhand der Bezeichner Koeffizienten, die allerdings die Namen der Bezeichner bekommen. Dazu kommt noch eine Besonderheit, dass `R` bei einer Regressionsgleichung automatisch davon ausgeht, dass ein konstanter Term verwendet werden soll, d.h. der Term `+1` wird automatisch dazugefüht. Wenn wir ein Modell ohne einen $y$-Achsenabschnitt fitten wollen, dann müssen wir dies `R` explizit mitteilen, indem wir `-1` der linken Seite hinzufügen, also z.B. `y ~ x - 1`. Die Syntax generalisiert dann später einfach, wenn zusätzliche Terme in der multiplen Regression benötigt werden, in dem weitere unabhängige Variablen durch `+` dazugefügt werden. Dementsprechend würde sich die Formel `y ~ x_1 + x_1` übersetzen in *die abhängige Variable $y$ wird mittels der unabhängigen Variablen `x_1` und `x_2` und einem konstaten Term modelliert*.\n\nIn @tbl-slm-basics-formelsyntax sind weitere Beispiele für die Syntax von Formeln für `lm()` gezeigt.\n\nTable: Formelsyntaxbeispiele für `lm()` (y-Ab = y-Achsenabschnitt, StKoef = Steigungskoeffizient) {#tbl-slm-basics-formelsyntax}\n\n| Modell | Formel |  Erklärung |\n|---|---|-----|\n| $y=\\beta_0$ | `y ~ 1` | y-Achsenabschnitt\n| $y=\\beta_0+\\beta_1 \\times x_1$ | `y ~ x` | y-Ab und StKoef |\n| $y=\\beta_0+\\beta_1 \\times x_1+\\beta_2 \\times x_2$ | `y ~ x1 + x2` | y-Ab und 2 StKoe |\n\nUm unsere Weitsprungdaten mittels `lm()` zu modellieren, müssen wir den folgenden Befehl verwenden.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(jump_m ~ v_ms, data = jump)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = jump_m ~ v_ms, data = jump)\n\nCoefficients:\n(Intercept)         v_ms  \n    -0.1385       0.7611  \n```\n\n\n:::\n:::\n\n\nDer Rückgabewert von `lm()` ist in der Form nicht besonders hilfreich und es werden nur die beiden berechneten Koeffizienten ausgegeben. Dabei bezeichnet der Term `(Intercept)` den automatisch dazugefügten konstanten Term in der Formel, sprich den $y$-Achsenabschnitt $\\hat{\\beta}_0$ und mit `v_ms` den Steigungskoeffizienten $\\hat{\\beta}_1$.\n\nUm aus `lm()` mehr Informationen heraus zu bekommen, ist es sinnvoll das Ergebnis von `lm()` einer Variable zuzuweisen. In dem vorliegenden Arbeit wird dazu in den meisten Fällen eine Variante des Bezeichners `mod` benutzt, als Kurzform vom *model*. Diese Bezeichnung ist aber wie alle Bezeichner in `R` vollkommen willkürlich und entspringt nur der Tippfaulheit des Autors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(jump_m ~ v_ms, data = jump)\n```\n:::\n\n\nUm mehr Informationen aus dem gefitteten `lm()`-Objekt zu bekommen kann nun eine der zahlreichen Helferfunktionen verwendet werden. Die wichtigste Funktion ist die `summary()`-Funktion (`?summary.lm`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = jump_m ~ v_ms, data = jump)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.44314 -0.22564  0.02678  0.19638  0.42148 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.13854    0.23261  -0.596    0.555    \nv_ms         0.76110    0.02479  30.702   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2369 on 43 degrees of freedom\nMultiple R-squared:  0.9564,\tAdjusted R-squared:  0.9554 \nF-statistic: 942.6 on 1 and 43 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nHier bekommen wir schon deutlich mehr Informationen mitgeteilt. Ganz oben, als erstes wird die `lm()` übergebene Formel angezeigt. Es folgt ein Abschnitt über die Residuen, gefolgt von den Koeffzienten (`Coefficients`) und im unteren Abschnitt noch weitere Statistiken. Wir konzentrieren uns zunächst einmal nur auf die Tabelle im Abschnitt `Coefficients`. Hier begegnen uns wieder in der ersten Spalte die Bezeichner für die beiden $\\beta$s in Form von $\\beta_0$ `(Intercept)` und $\\beta_1$ `v_ms`. In der zweiten Spalte daneben stehen die berechneten Koeffizienten die wir jetzt schon mehrmals gesehen haben. Die weiteren Spalten ignorieren wir zunächst. Im Laufe der folgenden Kapitel werden wir uns die weiteren Statistiken anschauen und deren Bedeutung erarbeiten.\n\nBei der Benutzung von `lm()` werden uns noch weitere Helferfunktionen begegnen, die den Umgang mit dem gefitteten Modell vereinfachen. Wollen wir zum Beispiel die beiden Koeffiziente aus dem Modell extrahieren können wir dazu die Funktion `coefficients()` oder auch nur kurz `coef()` verwenden.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)        v_ms \n -0.1385361   0.7611019 \n```\n\n\n:::\n:::\n\n\nDie Funktion `coef()` gibt einen benannten Vektor zurück der entweder über die Bezeichner oder einfach über die Position der Koeffizienten angesprochen werden kann. Möchte ich zum Beispiel den Steigungskoeffizienten extrahieren, kann ich die folgende Syntax verwenden:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mod)[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n -0.1385361 \n```\n\n\n:::\n:::\n\n\noder \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mod)['v_ms']\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     v_ms \n0.7611019 \n```\n\n\n:::\n:::\n\n\nEin etwas übersichtlichere Arbeitsweise ist wieder zunächst einmal das Ergebnis von `coef()` einer Variablen zuweisen und diese dann weiter benutzen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njump_betas <- coef(mod)\njump_betas[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n -0.1385361 \n```\n\n\n:::\n:::\n\n\nDie Koeffizienten kann ich zum Beispiel benutzen um die Regressionsgerade in ein Streudiagramm hinzuzufügen (Das `tibble()` mit den Sprungdaten hat den Bezeichner `jump`). Entweder mit dem `ggplot2()` Grafiksystem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(jump,\n       aes(x = v_ms, y = jump_m)) +\n  geom_abline(intercept = jump_betas[1],\n              slope = jump_betas[2],\n              color = 'red') +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](slm_basics_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nOder mit den Standard `R`-Grafiksystem. Hier kann der Funktion `abline()` das gefittete `lm()`-Objekt direkt übergeben werden und die Koeffizienten werden automatisch extrahiert.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(jump_m ~ v_ms, data = jump)\nabline(mod, col = 'red')\n```\n\n::: {.cell-output-display}\n![](slm_basics_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nSchauen wir uns noch mal ein ganz einfaches Beispiel, bei dem wir tatsächlich wissen welcher Zusammenhang zwischen den beiden Variablen. Wir halten das Beispiel ganz einfache und nehmen vier verschiedene $x$-Werte mit $x_i = i$. Wir setzen $\\beta_0 = 1$ und $\\beta_1 = 0.5$. Wir generieren die vier Werte mit `R`, speichern diese in einem `tibble()` mit dem Bezeichner `data` und berechnen die resultierenden Koeffizienten mittels `lm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- tibble(\n  x = 1:4,\n  y = 1 + 0.5 * x\n) \nmod <- lm(y ~ x, data)\ncoef(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           x \n        1.0         0.5 \n```\n\n\n:::\n:::\n\n\nUnd tatsächlich können wir die korrekten Koeffizienten mittels der einfachen linearen Regression wiedergewinnen. Diesen Ansatz mittels synthetisch generierten Daten die eingeführten Konzepte und Ansätze zu überprüfen werden wir im weiteren Verlauf des Skripts immer wieder anwenden, da er die Möglichkeit bietet relativ einfach und nachvollziehbar das Verhalten verschiedener Ansätze auszutesten. \n\nZusammenfassend lässt sich sagen, das wir jetzt gelernt haben wie wir ein einfaches Regressionmodell der Form Formel \\eqref{eq-slm-psform-beta} an einen beliebigen Datensatz fitten können. Die Berechnung der beiden Koeffizienten $\\beta_0$ und $\\beta_1$ erfolgt mittels Formeln \\eqref{eq-slm-basics-norm-1} und \\eqref{eq-slm-basics-norm-2}. Dabei berechnen wir die Koeffizienten nicht von Hand sondern lassen die von `R` mittels der `lm()` durchführen. Die Berechnung ist dabei vollkommen mechanisch und die Koeffizienten per-se sagen nichts darüber aus, ob das lineare Modell die Daten tatsächlich auch widerspiegelt. Dazu müssen wir noch etwas mehr Theorie aufbauen um Aussagen darüber zu treffen ob das Modell adäquat ist. Dies gehen wir in den folgenden Abschnitten und Kapiteln an.\n\n## Schritt-für-Schritt Herleitung der Normalengleichungen (advanced) {#sec-step-by-step-normal}\n\nUm die Herleitung der Normalengleichungen Schritt-für-Schritt nachvollziehen zu können benötigen wir zunächst einmal ein paar algebraische Tricks.\n\nFür den Mittelwert gilt:\n$$\n\\bar{x} = \\frac{1}{n}\\sum x_i \\Leftrightarrow \\sum x_i = n \\bar{x}\n$$\n\nBei Summen und konstanten $a$ konstant gilt:\n\\begin{align}\n    \\sum a &= n a \\\\\n    \\sum a x_i &= a \\sum x_i \\\\\n    \\sum (x_i + y_i) &= \\sum x_i + \\sum y_i\n\\end{align}\n\nWenn eine Summe abgeleitet wird, kann in die Ableitung in die Summe reingezogen werden.\n$$\n\\frac{d}{d x}\\sum f(x) = \\sum\\frac{d}{d x} f(x)\n$$\n\nHier ein zwei Umformungen bei Summen und dem Kreuzprodukt bzw. dem Quadrat.\n\\begin{alignat}{2}\n && \\sum(x_i-\\bar{x})(y_i-\\bar{y}) \\\\\n \\Leftrightarrow\\mkern40mu && \\sum (x_iy_i-\\bar{x}y_i-x_i\\bar{y}+\\bar{x}\\bar{y}) \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_i y_i - \\sum\\bar{x}y_i - \\sum x_i \\bar{y} + \\sum \\bar{x} \\bar{y} \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu&&  \\sum x_iy_i - n\\bar{x}\\bar{y}-n\\bar{x}\\bar{y}+n\\bar{x}\\bar{y} \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_iy_i - n\\bar{x}\\bar{y} \\nonumber\n\\end{alignat}\n\\begin{alignat}{2}\n&& \\sum(x_i - \\bar{x})^2 \\\\\n \\Leftrightarrow\\mkern40mu && \\sum(x_i^2 - 2 x_i \\bar{x} + \\bar{x}^2) \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum\\bar{x}^2 \\nonumber\\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_i^2 - 2\\bar{x}n\\bar{x} + n\\bar{x}^2 \\nonumber \\\\\n \\Leftrightarrow\\mkern40mu && \\sum x_i^2 - n \\bar{x}^2 \\nonumber\n\\end{alignat}\n\\subsection{Herleitung}\n\nZurück zu unserem Problem. Es gilt $E$ zu minimieren:\n\n\\begin{alignat}{2}\n&& E = \\sum e_i^2 = \\sum (y_i - \\hat{y}_i)^2 \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2 \\nonumber\n\\end{alignat}\n\nDie Gleichung hängt von zwei Variablen $\\beta_0$ und $\\beta_1$. Um das Minimum der Gleichung zu erhalten, verfährt man wie in der Schule, indem man die Ableitung gleich Null setzt. Der vorliegenden Fall ist jedoch etwas komplizierter, da die Gleichung von zwei Variablen abhängt. Daher müssen wir die partiellen Ableitungen $\\frac{\\partial}{\\partial \\beta_0}$ und $\\frac{\\partial}{\\partial \\beta_1}$ verwendet. Wir erhalten dadurch ein Gleichungssystem mit zwei Gleichungen (die jeweiligen Ableitungen) in zwei Unbekannten ($\\beta_0$ und $\\beta_1$). Die Lösung erfolgt, indem zuerst eine Gleichung nach der einen Unbekannten umgestellt wird und das Ergebnis dann in die andere Gleichung eingesetzt wird.\n\nWir beginnen mit der partiellen Ableitung nach $\\beta_0$ für den y-Achsenabschnitt.\n(Zurück an die Schule erinnern: Äußere Ableitung mal innere Ableitung)\n\n\\begin{alignat}{2}\n&& \\frac{\\partial \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2}{\\partial \\beta_0} \\\\\n\\Leftrightarrow\\mkern40mu && \\sum\\frac{\\partial}{\\partial \\beta_0}(y_i - \\beta_0- \\beta_1 x_i)^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum 2(y_i - \\beta_0- \\beta_1 x_i) (-1) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && -2 \\sum (y_i - \\beta_0- \\beta_1 x_i) \\nonumber\n\\end{alignat}\nZum minimieren gleich Null setzen.\n\\begin{alignat}{2}\n&& -2 \\sum (y_i - \\beta_0- \\beta_1 x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0- \\beta_1 x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i - \\sum \\beta_0- \\sum \\beta_1 x_i = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && n \\bar{y} - n \\beta_0- \\beta_1 n \\bar{x} = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\bar{y} - \\beta_0- \\beta_1 \\bar{x} = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\bar{y} - \\beta_1 \\bar{x} = \\beta_0\\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\beta_0= \\bar{y} - \\beta_1 \\bar{x} \n\\end{alignat} \n\nEs folgt nach dem gleichen Prinzip die Herleitung für die Steigung $\\beta_1$ und indem die Lösung für $\\beta_0$ eingesetzt wird.\n\n\\begin{alignat}{2}\n&& \\frac{\\partial \\sum (y_i - \\beta_0 - \\beta_1x_i)^2}{\\partial \\beta_1} \\\\\n\\Leftrightarrow\\mkern40mu && \\sum\\frac{\\partial}{\\partial b}(y_i - \\beta_0 - \\beta_1x_i)^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum2(y_i - \\beta_0 - \\beta_1x_i) -x_i \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && -2 \\sum(y_i - \\beta_0 - \\beta_1x_i)x_i\n\\end{alignat}\nWiederum gleich Null setzen.\n\\begin{alignat}{2}\n&& -2 \\sum(y_i - \\beta_0 - \\beta_1x_i)x_i = 0 \\nonumber\\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0 - \\beta_1x_i)x_i = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i x_i - \\beta_0 x_i - \\beta_1x_i x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - \\beta_0 \\sum x_i - b\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n \\beta_0 \\bar{x} - \\beta_1\\sum x_i^2 = 0 \\nonumber\n\\end{alignat}\nEinsetzen der Lösung für $\\beta_0$ führt zu:\n\\begin{alignat}{2}\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n (\\bar{y} - \\beta_1 \\bar{x}) \\bar{x} - \\beta_1\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n\\bar{y}\\bar{x} + n \\beta_1\\bar{x}^2 - \\beta_1\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n\\bar{y}\\bar{x} = \\beta_1 \\sum x_i^2 - \\beta_1n \\bar{x}^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (x_i-\\bar{x})(y_i-\\bar{y}) = \\beta_1 (\\sum x_i^2 - n\\bar{x}^2) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum x_i^2 - n\\bar{x}^2} = \\beta_1\\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\beta_1= \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2} \\nonumber\n\\end{alignat}\n\nSomit erhält man die beiden Normalengleichungen der Regression.\n\nÜber diese beiden Gleichungen erhalten wir die gewünschten Koeffizienten $\\hat{\\beta_0}$ und $\\hat{\\beta_1}$. Die Methode wird als die als die Methode der kleinsten Quadrate\\index{Methode der kleinsten Quadrate} bezeichnet oder im Englischen Root-Mean-Square (RMS)\\index{RMS}.\n\n",
    "supporting": [
      "slm_basics_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}