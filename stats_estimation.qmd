# Hypothesentests

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

```{r stats_significance_defs}
n <- 20
set.seed(123)
world <- tibble(ID = paste0('P',stringr::str_pad(1:n, width=2, pad="0")),
                Kraft = sample(2000:2500, n))
world$Kraft[13] <- 1800
world$Kraft[17] <- 3200
d_gr <- 100
d_x <- 2 
mu_lummer <- 0
sd_lummer <- 230
sample_k9 <- readr::read_csv('data/sample_k9.csv')
```

## Irttumswahrscheinlichkeiten

Im vorhergehenden Kapitel wurden das Konzept eines signifikanten Ergebnis entwickelt. Unter diesem Aspekt war die Erstellung einer $H_0$-Hypothese und deren Ablehnung oder Beibehaltung zentral. Im Folgenden wird nun ein Ansatz entwickelt, bei dem die Möglichkeit sich zu irren zentral ist. Der Ansatz geht auf die Arbeiten von @neyman1933 zurück, die in etwa zur gleichen Zeit wie diejenigen von Fisher entwickelt wurden. 

Um zu entscheiden ob ein statistische signifikantes Ergebnis vorliegt, wird eine $H_0$-Hypothese angesetzt und zu dieser Hypothese wird basierend auf dem beobachteten Wert ein p-Wert berechnet. Wenn der p-Wert klein ist, dann wird dies als Evidenz gegen die $H_0$ angesehen und diese wird abgelehnt. Nun kann es aber sein, dass selbst unter der $H_0$, d.h. wenn die $H_0$ korrekt ist, ein p-Wert beobachtet wird der unter der Schwelle von $0.05 = 5\%$ liegt. Wenn das Experiment $100$-mal wiederholt wird und die $H_0$ zutrifft, dann würden etwa $5$ Fälle eintreten, bei denen der beobachtete p-Wert kleiner als $0.05$ ist. Das heißt, in diesen $5$ Fällen würde man sich irren. Der Ansatz von @neyman1933 benutzt diese Einsicht nun um einen **Entscheidungsregel** zu entwickeln, welche die Wahrscheinlichkeit sich über mehrfache Wiederholungen eines Experiments zu irren kontrolliert. Es wird versucht die **Irrtumswahrscheinlichkeit** zu minimieren bzw. unter einem festgesetzten Niveau zu halten. Die Irrtumswahrscheinlichkeit wird mit dem Symbol $\alpha$ gekennzeichnet.

::: {#def-alpha-err}
## Irrtumswahrscheinlichkeit $\alpha$ \index{Irrtumswahrscheinlichkeit}

Die Wahrscheinlichkeit, mit der fälschlicherweise eine korrekte $H_0$-Hypothese abgelehnt wird, wird als Irrtumswahrscheinlichkeit bezeichnet. Die Irrtumswahrscheinlichkeit wird mit dem Symbol $\alpha$ bezeichnet und auch als **Fehler I. Art** bezeichnet.
:::

Wenn die $H_0$ nun nicht zutrifft, dann muss zwangsläufig eine andere Hypothese zutreffen. Ohne das Vorhandensein einer anderen Hypothese ist ein irren ja streng genommen auch gar nicht möglich. Diese *alternative* Hypothese wird üblicherweise als die $H_1$ Hypothese oder auch Alternativhypothese bezeichnet. Die Alternativhypothese ist dabei unter @neyman1933 genauso eine spezifisch, definierte Hypothese wie die $H_0$. Spezifisch definiert bedeutet, dass **eine** definierte Stichprobenverteilung unter der $H_1$ angegeben werden kann.

Unter dem Neyman-Pearson-Ansatz wird nun für die Stichprobenverteilung unter der $H_0$ ein *kritischer* Bereich angegeben. Wenn der beobachtete Wert in diesen Bereich fällt, dann wird dies als Evidenz gegen die Annahme der $H_0$ angesehen, ähnlich wie das auch für den Signifikanztest unter Fisher interpretiert wird. Allerdings liegt dem kritischen Bereich keine Metrik zugrunde. D.h. ein Werte weiter reichts bzw. weiter linke im kritischen Bereich bedeutet keine größere Evidenz gegen die $H_0$. Der beobachtete Wert ist entweder im kritischen Bereich oder er ist es nicht. Der p-Wert beim Signifikanztest dagegen kann *geordnet* werden, d.h. ein p-Wert von $p = 0.001$ hat einen höheren Evidenzwert gegen die $H_0$ als ein p-WErt mit $p = 0.04$.

In @fig-stats-sig-power-01 ist neben der Stichprobenverteilung unter der $H_0$ für das kleine Welt Beispiel eine weitere Verteilung unter einer Alternativhypothese $H_1$ abgetragen. In diesem Fall ist unter der $H_1$ der wahre Unterschied zwischen den beiden Verteilungen $\Delta = 500N$, also $H_1: \Delta = 500N$ gegenüber $H_0: \Delta = 0$.

```{r}
#| fig-cap: "Differenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von $\\alpha$, wenn $H_0$ zutrifft, und unter einer Alternativhypothese $H_1: \\Delta = 500N$."
#| label: fig-stats-sig-power-01

differences <- readr::read_csv('data/combinations_differences.csv')
n_sim <- dim(differences)[1]
sigma <- sd(differences$d)
xx <- -750:1250
n_pts <- length(xx)
q_crit <- qnorm(0.975, 0, sd = sigma)
dat_power <- tibble(
  x = rep(xx,2),
  y = c(dnorm(xx,0,sigma), dnorm(xx,500,sigma)),
  hypo = rep(c("H0","H500"), c(n_pts, n_pts))
)
low <- tibble(x = seq(-750,-q_crit), y = dnorm(x, 0, sigma), hypo='H50')
up <- tibble(x = seq(q_crit, 750), y = dnorm(x, 0, sigma), hypo='H50')
ggplot(dat_power, aes(x,y,fill=hypo,ymin=0,ymax=y)) +
  geom_ribbon(alpha=.5) +
  geom_area(data = up, fill='red', alpha=0.8) +
  geom_area(data = low, fill='red', alpha=.8) +
  geom_line() +
  scale_fill_discrete('Hypothese') +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeit') 
```

In @fig-stats-sig-power-01 ist zu sehen, dass der rechte kritische Bereich unter der $H_0$ etwa bei $500$ N anfängt. D.h. wenn ein Wert rechts von $500$N beobachtet wird, dann wird die $H_0$ abgelehnt. Allerdings ist in der Abbiludng auch zu erkennen, dass  sich die beiden Verteilungen überschneiden. Daher ist der Bereich zwischen etwa $100-400N$ unter beiden Verteilungen relativ *wahrscheinlich*. Das heißt, wenn ein Wert in diesem Bereich beobachtet wird, könnten nicht wirklich trennscharf argumentiert werden, aus welcher Stichprobenverteilung der Wert tatsächlich stammt. Dennoch wird ein Bereich in diesem Bereich als Evidenz für die $H_0$ angesehen, da er nicht im kritischen Bereich liegt und dementsprechend nicht zur Ablehung von $H_0$ führt. 

In @fig-stats-sig-power-02 wurde der Bereich unter der $H_1$, der links des kritischen Bereichs von $H_0$ liegt, grün eingefärbt. Da es sich hier wieder um eine Fläche handelt, bestimmt diese Fläche eine Wahrscheinlichkeit. Wie könnte diese Wahrscheinlichkeit verbal beschrieben werden?

```{r}
#| fig-cap: "Differenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von $\\alpha$, wenn $H_0$ zutrifft, und $\\beta$ (grün), wenn $H_1$ zutrifft."
#| label: fig-stats-sig-power-02

beta <- tibble(
  x = -300:q_crit,
  y = dnorm(x, 500, sigma)
)
ggplot(dat_power, aes(x,y,fill=hypo,ymin=0,ymax=y)) +
  geom_ribbon(alpha=.5) +
  geom_area(data = up, fill='red', alpha=0.8) +
  geom_area(data = low, fill='red', alpha=.8) +
  geom_area(data = beta, fill='green', alpha=0.5) +
  geom_line() +
  scale_fill_discrete('Hypothese') +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeit') 
```

Die Werte zwischen den beiden kritischen Bereichen beschreiben diejenigen Werte, bei denen die $H_0$ beibehalten wird. Diese Werte werden dementsprechend nicht als *überraschend* unter der $H_0$ eingeschätzt. Folglich würden ein Irrtum vorliegen, wenn in der Realität die $H_1$-Hypothese zutrifft. Daher beschreibt die grüne Fläche in @fig-stats-sig-power-02 ebenfalls die Wahrscheinlichkeit, sich zu irren – aber dieses Mal, wenn die Alternativhypothese $H_1$ zutrifft. Diese Irrtumswahrscheinlichkeit wird als die $\beta$-Wahrscheinlichkeit bezeichnet.

::: {#def-beta-error}
## $\beta$-Wahrscheinlichkeit

Die $\beta$-Wahrscheinlichkeit \index{$\beta$-Wahrscheinlichkeit} beschreibt die Wahrscheinlichkeit, sich gegen die Alternativhypothese $H_1$ zu entscheiden, wenn diese zutrifft. Die $\beta$-Wahrscheinlichkeit wird auch als Fehler II. Art bezeichnet.
:::

Insgesamt resultiert aus den Möglichkeiten die korrekte bzw. falsche Entscheidung in Abhängigkeit vom Zutreffen von $H_0$ oder $H_1$ die folgende Entscheidungsmatrize:

| Entscheidung\\Realität | $H_0$ | $H_1$ |
| --- | --- | --- |
| $H_0$ | korrekt | $\beta$ |
| $H_1$ | $\alpha$ | korrekt |

: Entscheidungsmöglichkeiten und Fehlerarten  {#tbl-stats-sig-alpha-beta}

In Abhängigkeit von welche der beiden Hypothesen $H_0$ oder $H_1$ zutrifft kann entweder eine korrekte Entscheidung getroffen werden oder ein $\alpha$ oder ein $\beta$-Fehler vorliegen. Hier liegt einer der grundlegenden Unterscheidungen zwischen Fisher und Neyman-Pearson vor. Unter Fisher ist keine Alternativhypothese vorgesehen, dementsprechend wird ein p-Wert oberhalb des Signifikanzniveaus auch nicht dahingehend interpretiert, dass die $H_0$ abgelehnt wird, sondern es wurde nicht **ausreichend** Evidenz beobachtet um die $H_0$ abzulehnen. Daher wird die $H_0$ **beibehalten**. Beibehalten ist dabei nicht das gleiche wie bestätigen! Die $H_0$ wird nicht bestätigt sondern lediglich nicht verworfen. Unter Neyman-Pearson hingegen stehen sich zwei spezifische Hypothesen gegenüber. Wenn die eine Hypothese abgelehnt wird, dann bedeutet dies, dass die andere Hypothese angenommen wird.

## Power

In @fig-stats-sig-power-03 sind nochmal die $H_0$ und die $H_1$ eingezeichnet. Allerdings ist nun die komplementäre Fläche unter der $H_1$ blau eingezeichnet.

```{r}
#| fig-cap: "$1-\\beta$ = Power des Tests (blaue Fläche)."
#| label: fig-stats-sig-power-03

beta <- tibble(
  x = -300:q_crit,
  y = dnorm(x, 500, sigma)
)
power <- tibble(
  x = q_crit:max(dat_power$x),
  y = dnorm(x, 500, sigma)
)
ggplot(dat_power, aes(x,y,fill=hypo,ymin=0,ymax=y)) +
  geom_ribbon(alpha=.5) +
  geom_area(data = up, fill='red', alpha=0.8) +
  geom_area(data = low, fill='red', alpha=.8) +
  geom_area(data = beta, fill='green', alpha=0.5) +
  geom_area(data = power, fill='blue', alpha=0.5) +
  geom_line() +
  scale_fill_discrete('Hypothese') +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeit') 
```

Da diese Fläche im kritischen Bereich der $H_0$ liegt, bedeutet dies, dass wenn ein Wert in diesem Bereich beobachtet wird, dann führt er zu einer Ablehnung von $H_0$. Unter der $H_1$ beschreibt diese Fläche dagegen, die Wahrscheinlichkeit, dass ein Wert unter der $H_1$ eintritt. Zusammengenommen beschreibt diese Fläche somit die Wahrscheinlichkeit sich für die Alternativhypothese $H_1$ zu entscheiden, wenn diese auch tatsächlich zutrifft. Diese Wahrscheinlichkeit hat eine eigene Bezeichnung und wird als die **Power** bezeichnet.

::: {#def-power}
## Power \index{Power}

Die Power bezeichnet die Wahrscheinlichkeit, sich für die Alternativhypothese $H_1$ zu entscheiden, wenn diese in der Realität zutrifft.
:::

Zusammengefasst sind die folgenden Terme zentral unter dem Neyman-Pearson-Ansatz: $\alpha$, $\beta$ und Power:

- $\alpha$: Die Wahrscheinlichkeit, sich gegen die $H_0$ zu entscheiden, wenn die $H_0$ zutrifft. Das $\alpha$-Level wird vor dem Experiment festgelegt, um zu kontrollieren, welche Fehlerrate toleriert wird.
- $\beta$: Die Wahrscheinlichkeit, sich gegen die $H_1$ zu entscheiden, wenn die $H_1$ zutrifft.
- Power := $1 - \beta$: Die Wahrscheinlichkeit, sich für die $H_1$ zu entscheiden, wenn die $H_1$ zutrifft. Diese sollte ebenfalls **vor** dem Experiment festgelegt werden.

Die Power ist für die Durchführung eines Experiments von zentraler Bedeutung, da in den allermeisten Fällen die Alternativhypothese $H_1$ diejenige ist, an der die Forscherin interessiert ist. In den seltensten Fällen soll gezeigt werden, dass nichts passiert, sondern meistens das etwas passiert. Daher ist es wichtig die Power möglichst hoch zu halten, da die Power die Wahrscheinlichkeit beschreibt einen Effekt auch wirklich zu beobachten. Ist eine Untersuchung sehr aufwendig für die Durchführenden wie auch die Teilnehmenden, beispielsweise eine Trainingsintervention mit Pre- und Post-Messungen und einer wochenlangen Interventionsphase, dann ist es wenig sinnvoll diese Untersuchung mit nur wenigen Proband:innen durchzuführen und daraus resultierend eine geringe Power zu haben überhaupt einen Effekt zu beobachten. Dies ist aus Sich des Ressourceneinsatzes problematisch wie auch aus ethischer Sicht gegenüber den Teilnehmenden kritisch zu bewerten. Daraus schließt sich dann auch direkt die Frage wie die Power beeinflusst werden bzw. erhöht werden kann kann?

### Wie kann die Power erhöht werden? 

In @fig-stats-sig-power-04 haben sind nochmals die beiden Verteilungen bei $\Delta = 500$ und $\Delta = 0$ abgetragen.

```{r}
#| fig-cap: "Verteilungen wenn $\\Delta$=500 und $\\Delta$=0 in unserem kleine Welt Beispiel mit n = 3."
#| label: fig-stats-sig-power-04

dat <- tibble(
  di = c(differences$d + 500, differences$d),
  hypo = rep(c('H500','H0'), c(n_sim,n_sim))
)
p_h500 <- ggplot(dat, aes(di, fill=hypo)) +
  geom_density(alpha=0.5) +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeit') +
  scale_fill_discrete("Hypothese", labels=c(
    expression(H[0]  ), expression(H[500])
  ))
print(p_h500)
```

Eine Entscheidung für oder gegen eine der Hypothesen ist speziell im Bereich zwischen $0$ und $500$ problematisch. Hier überlappen die beiden Verteilungen, und es ist dementsprechend schwierig, einen beobachteten Wert der in diesem Bereich liegt eindeutig einer der beiden Hypothesen zuzuordnen. Die Power kann daher erhöht werden, indem die Überlappung der beiden Verteilungen verkleinert wird. Dazu sind prinzipiell zwei Möglichkeiten vorhanden: Entweder der Unterschied zwischen den beiden $\Delta$s wird vergrößert (d.h., das Krafttraining müsste effizienter werden) oder die beiden Verteilungen müssen schmaler werden. Da die *Breite* der Verteilung eine Funktion der Standardfehlers $\sigma_e$ ist, kann Letzteres erreicht werden, indem die Streuung der Statistik verkleinert wird. In @fig-stats-sig-power-05 sind alle Parameter gleich gehalten worden wie in @fig-stats-sig-power-04, lediglich die Stichprobengröße wurde von $N = 3$ auf $N = 9$ erhöht.

```{r}
#| fig-cap: "Stichprobenverteilungen der Differenz unter $H_0$ und $H_1:\\delta=500$N bei einer Stichprobengröße von n = 9"
#| label: fig-stats-sig-power-05

sample_k9 <- readr::read_csv('data/sample_k9.csv')
sigma <- sample_k9$sd[1]
d <- sample_k9$m[2]
xx = seq(-4*sigma,d+4*sigma)
n_pts = length(xx)
dat_k9 <- tibble(
  x = rep(xx,2),
  y = c(dnorm(xx, 0, sigma), dnorm(xx, d, sigma)),
  hypo = rep(c('H0','H500'), c(n_pts, n_pts))
)
ggplot(dat_k9, aes(x,y,fill=hypo, ymin=0, ymax=y)) +
  geom_ribbon(alpha=0.5) +
  geom_line() +
  scale_fill_discrete("Hypothese", labels=c(
    expression(H[0]  ), expression(H[500])
  )) +  
  lims(x = c(-750, 1250)) +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeiten') 
```

Es ist zu beobachten, dass der Abstand zwischen den Verteilungen gleich geblieben ist, was nicht weiter verwunderlich ist, da immer noch die beiden Hypothesen $H_0: \Delta = 0$N und $H_1: \Delta = 500$N miteinander verglichen werden. Aber durch die Erhöhung der Stichprobengröße $N$ ist die Streuung der $D$s unter beiden Hypothesen geringer geworden, der Standardfehler ist kleiner geworden. Dies führt dazu, dass die Verteilungen nun steiler sind und dementsprechend weniger stark überlappen. Nochmals, der Standardfehler $\sigma_e$ ist **nicht** gleich der Standardabweichung $\sigma$ in der Population bzw. $s$ in der Stichprobe (siehe @fig-stats-sig-standard-error-deriv).

```{r}
#| label: fig-stats-sig-standard-error-deriv
#| fig-cap: "Zusammenhang zwischen den Parametern der Population, der Stichprobe und dem Standardfehler"

df <- tibble(x = c(0, 1), y = c(0,1))
df_n1 <- tibble(
  x = seq(0,.6,length.out=100),
  y = 0.1*dnorm(x, 0.3, 0.12)
)
df_n2 <- tibble(
  x = seq(1.6, 2., length.out=100),
  y = 0.04*dnorm(x, 1.8, 0.05)
)
df_sample <- tibble(
    y = 0,
    x = runif(13, 0.8, 1.2) 
)

ggplot(df, aes(x,y)) +
  ggforce::geom_circle(data = tibble(x = 1, y = 0.9, r = 0.05),
             aes(x0 = x, y0 = y, r = r), fill='light blue') + 
  geom_curve(data = tibble(
    x = .3, y = .7, xend = .95, yend=.95
  ),
  aes(x, y, xend=xend, yend=yend), curvature=-.5,
  arrow = arrow(length=unit(.1, 'inches'), type='closed')) +
  ggforce::geom_circle(data = tibble(x = 0.3, y = 0.7, r = 0.3),
             aes(x0 = x, y0 = y, r = r), fill='light blue') +
  geom_point(data = df_sample) +
  geom_point(x = mean(df_sample$x), y = 0, color='red', size=2) +
  geom_segment(data = tibble(
    x = 1.6, xend = 2,
    y = seq(.6, 1, length.out=10),
    yend = y
  ), aes(x=x, y=y, xend=xend, yend=yend), linetype='dotted') +
  geom_point(data = tibble(
    x = runif(10, 1.7, 1.9),
    y = seq(.6, 1, length.out=10)
  ), color='red') +
  geom_segment(data = tibble(x=.3, y=0, yend=.35), aes(x,y,xend=x,yend=yend), color='red',
               linetype = 'dashed') +
  geom_segment(data = tibble(x=.15,xend=.45,y=.15), aes(x,y,xend=xend,yend=y), color='green',
               linetype='dashed',
               arrow = arrow(length=unit(.1, 'inches'), type='closed', ends='both')) +
  geom_segment(data = tibble(x=1.8, y=0, yend=.35), aes(x,y,xend=x,yend=yend), color='red',
               linetype = 'dashed') +
  geom_segment(data = tibble(x=1.75,xend=1.85,y=.15), aes(x,y,xend=xend,yend=y), color='green',
               linetype='dashed',
               arrow = arrow(length=unit(.1, 'inches'), type='closed', ends='both')) +
  geom_line(data = df_n1) + 
  geom_line(data = df_n2) + 
  geom_curve(data = tibble(x=1, y=0.64, xend=mean(df_sample$x), yend=0.05),
             aes(x,y,xend=xend,yend=yend), curvature = -0.1,
             arrow = arrow(length=unit(.1, 'inches'), type='closed')) +
  geom_curve(data = tibble(x=mean(df_sample$x), y=0.05, xend=1.55, yend=1),
             aes(x,y,xend=xend,yend=yend), curvature = -0.2,
             arrow = arrow(length=unit(.1, 'inches'), type='closed'),
             linetype = 'dotted') +
  geom_curve(data = tibble(x=.95, y=0.85, xend=0.95, yend=.75),
             aes(x,y,xend=xend,yend=yend), curvature = 0.3,
             arrow = arrow(length=unit(.08, 'inches'), type='closed')) +  
  annotate("text", x = 0.3, y = 0.7,
           label=expression(paste('\U03BC, ',sigma)), size=5) +
  annotate("text", x = 1, y = 0.7,
           label=expression(paste(bar(x),', s')), size=5) +
  annotate("text",
           x = c(0.3, 1,  1.8, 1.8, 1,.3),
           y = c(1.1, 1.1, 1.1,-.1,-.1,-.1),
           label= c("Population","Stichprobe","Mehrere Stichproben",
                    "Stichprobenverteilung", "Daten", "Verteilung")) +
  annotate("text", x = 1.8, y = .4,
          label=expression(paste("Standardfehler ",s[e])), size=5) +
  scale_x_continuous(limits = c(0,2.3)) +
  scale_y_continuous(limits = c(-0.1,1.2)) +
  coord_equal() + theme_void()
```

Warum führt die Erhöhung der Stichprobengröße dazu, dass die Verteilungen steiler werden? Die Statistik die in diesen Fällen betrachtet wurde war der Unterschied $d$ zwischen den Gruppen. Durch die Erhöhung der Stichprobe kommt es nun dazu, dass dieser Unterschied $d$ weniger stark schwankt, denn dadurch, dass mehr Proband:innen betrachtet werden, ist die Chance das zwei extreme Mittelwerte in den beiden Gruppen auftauchen kleiner. Die Vergrößerung der Stichprobe führt dazu, dass extreme Werte eher *rausgemittelt* werden. Dies führt dann dazu, dass wenn die Mittelwerte weniger schwanken auch die Unterschiede zwischen den beiden Mittelwerten weniger schwanken. In der Folge ist die Standardabweichung der Differenzen $d$, der Standardfehler $\sigma_e$ kleiner und die beiden Verteilungen unter der $H_0$ und der $H_1$ respektive werden steiler.

::: {.callout-note}

Für den Standardfehler des Mittelwerts, also wenn die Statistik der Mittelwert ist, dann gilt der folgende Zusammenhang:

| Population | Stichprobe |
| --- | --- | 
| $\sigma_{\bar{X}}=\sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}$ | $s_e=\sqrt{\frac{s^2}{n}}=\frac{s}{\sqrt{n}}$ | 

: Standardfehler des Mittelwerts, n = Stichprobengröße {#tbl-stats-sig-standard-error}

In @tbl-stats-sig-standard-error ist zu sehen, dass ein wurzelförmiger Zusammenhang zwischen dem Standardfehler $s_e$ und der Stichprobengröße besteht. Dieser Zusammenhang wird in verschiedenen Berechnungen zum Standardfehler verschiedener Statistiken immer wieder auftreten.

:::

::: {#exm-se-average}
## Standardfehler des Mittelwerts

Die Gleichheit des Standardfehler des Mittelwerts mit der Standardabweichung der Stichprobenverteilung der Mittelwerte ist im folgenden Beispiel mittels einer Simulation direkt zu beobachten. Dazu werden $1000$ Stichproben der Größe $N = 10$ aus einer Normalverteilung mit $\mu = 0$ und $sigma=2$ gezogen und für jede Stichprobe wird der Mittelwert berechnet. Der theoretische Standardfehler ist:

\begin{equation*}
s_e = \frac{\sigma}{\sqrt{n}} = \frac{2}{\sqrt{10}} \approx 0.63
\end{equation*}

Die Simulation in `R` führt zu:

```{r}
#| echo: true

n_sim <- 1000
N <- 10
mu <- 0
sigma <- 2
x_bars <- replicate(n_sim, mean(rnorm(N, mean=mu, sd=sigma)))
sd(x_bars)
```

Es ist zu beobachten, dass der empirische Standardfehler, im Rahmen der Stichprobenvariabilität, den theoretischen Wert tatsächlich sehr gut approximiert. Wenn die Anzahl der Simulationsdurchgängen erhöht werden würde, würde entsprechend der Unterschied immer kleiner werden.
:::

Wie aus dem Hinweise und dem Beispiel ersichtlich gilt eine wurzelförmiger Zusammenhang zwischen dem Standardfehler $\sigma_e$ und der Stichprobengröße $N$. Dieser Zusammenhang gilt nicht nur den Standardfehler des Mittelwerts sondern wird im weiteren Verlauf des Skriptums immer wieder auftreten. Daher ist es sinnvoll sich nochmal den Zusammenhang zwischen dem Bruch $\frac{1}{\sqrt{N}}$ und $N$ zu vergegenwärtigen.

```{r}
#| label: fig-stats-sig-sqrtfcn
#| fig-cap: "Funktionaler Zusammenhang zwischen $x$ und $\\sqrt{x}$."

tibble(x = seq(0, 100), y = sqrt(x)) |> 
  ggplot(aes(x,y)) +
  geom_line() +
  labs(x = 'x', y = expression(y==sqrt(x)))
```

Dazu ist in @fig-stats-sig-sqrtfcn die Funktion $y = \sqrt{x}$ abgetragen. Es ist zu erkennen, dass die Funktionswerte für kleine Werte von $x$ steil ansteigen und später dann anfangen, immer langsamer größer zu werden. Die Wurzelfunktion wird dennoch für größer werdende $N$ immer weiter wachsen, sogar streng monoton. Angewendet auf die Power-Frage: Wenn der Standardfehler $\sigma_e$ mit der Wurzel der Stichprobengröße $N$ kleiner wird, dann ist dies besonders bei kleinen Stichprobengrößen von Bedeutung. Zum Beispiel ist der Unterschied zwischen $N = 10$ und $N = 20$ deutlich größer, als der gleiche Stichprobenunterschied zwischen $N = 110$ und $N = 120$. Im letzteren Fall fällt die Zunahme von $N$ nicht mehr ganz so stark ins Gewicht. Das heißt, bei kleinen Stichproben sollte um jede zusätzliche Teilnehmerin bzw. jeden zusätzlichen Teilnehmer gekämpft werden. Allgemein gilt: Je größer die Stichprobengröße, desto kleiner der Standardfehler und umso größer die Power.

Nach dieser Betrachtung der Power ist der erste Teilabschnitt zur Neyman-Pearson-Testung abgeschlossen. Was bei dieser Betrachtung gar nicht behandelt wurde, ist woher die Alternativhypothese $H_1$ eigentlich stammt. Bei der Herleitung der $H_0$-Hypothese unter Fisher war ein Argument, dass andere Hypothesen etwas willkürlich erscheinen und die $H_0$ bei der nichts passiert zumindest etwas rationale Motivation mit sich bringt. Leider muss die Antwort auf dieses Problem noch etwas warten und wird erst später im Zusammenhang mit experimentellen Designs behandelt.

## Konfidenzintervalle oder Welche Hypothesen sind kompatibel mit dem beobachteten Effekt?

Bisher sind die Daten nur mittels Hypothesentests betrachtet worden. D. h., es wurde entweder nur eine $H_0$ (Fisher) oder eine $H_0$- und eine $H_1$-Hypothese (Neyman-Pearson) formuliert und dann anhand der Stichprobenverteilung abgeschätzt, wie kompatibel der beobachtete Wert mit der $H_0$-Hypothese ist. Dadurch wurde eine rein dichotome Betrachtung der Daten durchgeführt: Entweder war der beobachtete Wert statistisch signifikant unter der $H_0$ oder eben nicht. Diese Unterteilung der Entscheidung in nur zwei verschiedene Ausgänge – neben dem Problem, dass streng genommen eine Frage beantwortet wird, die oftmals gar nicht gestellt wurde – bringt diese Dichotome Einteilung einige grundlegende Nachteile mit sich. Diese sollen nun genauer betrachtet werden.

Das folgende Beispiel ist entnommen aus @cumming2013[p.1]. Es seien zwei Forschergruppen gegeben. Die beiden Gruppen "Glücklich" und "Pech". Beide Gruppen haben das gleiche Experiment durchgeführt, eine Krafttrainingsintervention. Die Gruppe "Glücklich" hat insgesamt $N = 44$ Teilnehmer in zwei unabhängigen Gruppen untersucht, während die Gruppe "Pech" $N = 36$ Teilnehmer in zwei unabhängigen Gruppen untersucht hat. Die beiden Untersuchungen kamen zu den folgenden Ergebnissen (siehe @tbl-stats-est-prob).

| Gruppe | $D_{\text{MW}}\pm s_e$ | Statistik | p-Wert |
| --- | --- | --- | -- |
| Glücklich | $3.61 \pm 9.62$ | $t(42) = 2.43$ | $0.02$ |
| Pech | $2.23 \pm 8.66$ | $t(34) = 1.25$ | $0.14$ |

: Ergebnisse der Untersuchung der Forschergruppen Glücklich und Pech (D = Differenz) {#tbl-stats-est-prob}

Wenn ein Signifikanzniveau von $P = 0.05$ angesetzt wird, dann hat nur die Gruppe "Glücklich" ein statistisch signifikantes Ergebnis beobachtet. Bei "Glücklich" ist der p-Wert $p = 0.02$. Die Gruppe "Pech" hat dagegen kein statistisch signifikantes Ergebnis beobachtet und kann mit $p = 0.14$ die $H_0$ nicht ablehnen. Wenn nun die zu den beiden Untersuchungen gehörenden Veröffentlichungen betrachtet werden und die Ergebnisse streng dichotom interpretiert werden, dann wären zwei widersprüchliche Ergebnisse dokumentiert. Man könnte versuchen zu erklären, dass die Stichprobengröße in "Pech" zu klein und vielleicht zu variabel war und deswegen die $H_0$ nicht ablehnen konnte. Allerdings, wenn die Effektstärke aus dem Experiment von "Glücklich" angesetzt wird, dann kann gezeigt werden, dass die Power auch für die Stichprobengröße von "Pech" ausgereicht hätte, um relativ sicher ein statistisch signifikantes Ergebnis zu beobachten (Power $> 0.9$). Seien die beiden beobachteten Effekte einmal graphisch dargestellt.

```{r}
#| label: fig-stats-est-prob-01
#| fig-cap: "Gruppendifferenzen für die beiden Forschergruppen"
#| fig-height: 1.5

ggplot(tibble(x = c(3.61, 2.23), y = 1:2), aes(x, y)) +
  geom_point(size = 3) +
  geom_vline(xintercept = 0, linetype = 'dashed', color = 'red') +
  scale_x_continuous('Gruppendifferenz D', limits = c(-4, 4)) +
  scale_y_continuous('Gruppe', limits = c(0.9, 2.1),
                     breaks = 1:2, labels = c('Glücklich', 'Pech'))
```

Wenn die Differenzen in @fig-stats-est-prob-01 genauer betrachtet werden, dann sehen die Ergebnisse eigentlich gar nicht so widersprüchlich aus. Beide Effekte sind in der gleichen Richtung, nur die Effektstärke unterscheidet sich zwischen den beiden Gruppen. Wenn man gezwungen wäre, eine Abschätzung über die Größe der Effekts des Intervention abzugeben, dann würde wahrscheinlich ein Wert zwischen den beiden beobachteten Werten angegeben werden. D.h. die beiden Werte sind gar nicht wirklich widersprüchlich zueinander. Sei nun ein andere Fall betrachtet.

```{r}
#| label: fig-stats-est-prob-02
#| fig-cap: "Beispiel für ein anderes Ergebnis der Gruppendifferenzen für die beiden Forschgruppen"
#| fig-height: 1.5

ggplot(tibble(x = c(3.61, -2.23), y = 1:2, ), aes(x,y)) +
  geom_point(size=3) +
  geom_vline(xintercept = 0, linetype = 'dashed', color = 'red') +
  scale_x_continuous('Gruppendifferenz D', limits = c(-4, 4)) +
  scale_y_continuous('Gruppe', limits = c(0.9, 2.1),
                     breaks = 1:2, labels = c('Glücklich','Pech'))
```

Wäre das Ergebnis aus @fig-stats-est-prob-02 beobachtet worden, dann würde wahrscheinlich schon eher von einem widersprüchlichen Ergebnis in der Literatur gesprochen werden. Basierend auf diesen beiden Datenpunkten würde wahrscheinlich ein Gesamteffekt in der Nähe von $D = 0$ angegeben werden. In beiden Fällen wäre aber unter der rein dichotomen Betrachtung das gleiche Ergebnis ($1$ x statistisch signifikant + $1$ x statistisch nicht signifikant) beobachtet worden. Daraus folgt, dass bei der Interpretation von Forschungsergebnissen nicht nur die statistische Signifikanz betrachtet werden sollte. Auch die Richtung der Effekte und deren Größe spielt eine Rolle bei der Interpretation der Evidenz. Dazu gehört natürlich ebenfalls eine Betrachtung des Forschungsdesigns und insbesondere der Stichprobengröße $N$. Im letzten Beispiel könnte es auch durchaus sein, dass beide Effekte *statistisch signifikant* sind, d.h. dann wäre ein Befund positiv und ein Befund negativ, dies würde dann schon deutlich auf einen Widerspruch deuten.

::: {#exm-ci-01}

In @coleman2023 wurden die Auswirkungen von betreutem gegenüber unbetreutem Krafttraining auf Kraft und Hypertrophie bei trainierten Personen untersucht. $N = 36$ junge Männer und Frauen wurden zufällig einer von zwei experimentellen Gruppen zugewiesen, um ein 8-wöchiges Krafttrainingsprogramm durchzuführen: Eine Gruppe erhielt direkte Aufsicht während der Trainingseinheiten (supervision: SUP), während die Kontrollgruppe dasselbe Programm ohne Betreuung (unsupervised: UNSUP) durchführte. Während die meisten Ergebnisse keine Unterschiede zwischen den Gruppen andeuteten, zeigten die Ergebnisse eine erhöhte Zunahme der Gesamtkörpermuskelmasse um $0.54,\ 90\%CI[0.05, 0.98]$ kg. D. h., wenn wir als Zielgröße die Muskelmasse ansteuern wollen, müssen wir uns anhand des Intervalls überlegen, ob der Aufwand eines vollständig betreuten Trainings hinsichtlich eines möglicherweise minimalen Zuwachses von $\Delta = 0.05$ kg gerechtfertigt ist.

:::

Aus den Ausführungen von eben ist hoffentlich klar geworden, dass es sinnvoll ist den beobachteten Effekt, also im Beispiel die Größe des Unterschieds, ebenfalls zu dokumentieren, da der Effekt natürlich immer für die Interpretation des Ergebnisses wichtig ist. Letztendlich ist die Größe des Effekts auch ausschlaggebend, ob zum Beispiel eine Trainingsintervention mit Athletinnen durchgeführt werden soll. Es reicht nicht zu wissen, dass ein statistisch signifikanter Effekt in Studien gefunden wurde, sondern die Größe und Richtung des Effekts spielt ebenfalls eine Rolle bei der Bewertung.

```{r}
alpha <- 0.05
delta <- 500
mu <- delta
d_hat <- 350
s_hat <- 54 
#s_e <- round(s_hat/sqrt(9))
s_e <- round(s_hat*sqrt(2)/3)
c_i <- d_hat + c(-1,1)*qnorm(1-alpha/2)*s_e
```

Im kleine Welt Beispiel aus dem ersten Kapitel, könnte das Ergebnis der Intervention beispielsweise die folgenden Stichprobenkennwerte bei $N = 9$ haben. Es einen Unterschied von $d = \bar{x}_{treat} - \bar{x}_{con} = `r d_hat`$ zwischen den Gruppen beobachtet. Beide Stichproben zeigen eine Standardabweichung von $s = `r s_hat`$, was zu einem Standardfehler von $s_e = `r s_e`$ führt (Rechnung unterschlagen). Der Standardfehler liefert eine Schätzung über die Präzision des Schätzwertes $D$. Wenn der Standardfehler $s_e$ sehr groß ist, dann bedeutet dies, dass der Wert nur sehr unpräzise geschätzt wurde. Anders herum wurde der Wert sehr präzise geschätzt, $s_e$ sehr klein ist. Präzision wird in diesem Zusammenhang dahingehend interpretiert, welche anderen Populationswerte neben dem beobachteten Wert $D$ ebenfalls *plausibel* sind, den beobachteten Wert $D$ generiert zu haben. Im Beispiel: Welche anderen Unterschiedswerte $D$ sind anhand der beobachteten Daten ebenfalls plausibel?

Der Wert $D = `r d_hat`$ spricht dafür, dass der wahrscheinlichste Wert für $\Delta$ eben $\Delta = `r d_hat`$ ist. Allerdings ist es auch nachvollziehbar, dass der Wert $\Delta = 350 - 0.0000001$ ebenfalls nicht direkt unplausibel ist. Letztendlich zeigen sich mit Hilfe des Standardfehlers $s_e = `r s_e`$, dass der beobachtete Schätzwert mit einer bestimmten Unsicherheit behaftet ist. Mit der gleichen Überlegung kann daher der Wert $\Delta = 350 - \frac{`r s_hat`}{2}$ wahrscheinlich auch noch als nicht komplett abwegig begründet werden. Nach diesen Ausführungen muss daher zunächst einmal der Begriff der "*Plausibilität*" geschärft werden.

Ausgangspunkt ist wieder die $H_0$. Die $H_0$ wird beibehalten, wenn der beobachtete Wert der Teststatistik nicht in den kritischen Bereichen von $H_0$ liegt. Wenn der beobachtete Wert im kritischen Bereich liegt, dann ist der Wert überraschend unter der $H_0$, der Wert erscheint nicht plausibel. Dieser Ansatz kann auch umgedreht werden, wenn der Wert nicht im kritischen Bereich liegt, dann ist er unter der $H_0$ plausibel und nicht überraschend. Dementsprechend kann ein Kriterium für Plausibilität so definiert werden: Wenn der beobachtete Wert unter der angenommenen Hypothese nicht zur Ablehnung der Hypothese führt, dann ist dieser Wert plausibel. Sei nun der beobachteten Wert der Ausgangspunkt. Dann könnte die Frage auch so gestellt werden: Welche Hypothesen, wenn sie vorher als $H_0$ definiert worden wäre, würde unter dem beobachteten Wert nicht abgelehnt werden? Anders gesagt, es wreden all diejenigen $H_0$-Hypothesen gesucht, die, wenn sie so vor dem Experiment angesetzt worden wären, unter dem beobachteten Wert beibehalten worden wären.

In @fig-stats-est-ci-01 ist aus einer Verteilung nur der unkritische Bereich abgetragen. D. h., alle Werte, die im grünen Bereich liegen, sind kompatibel wenn diese Verteile die angesetzten $H_0$ darstellt.

```{r}
#| label: fig-stats-est-ci-01
#| fig-cap: "Bereich, bei dem beobachtete Werte unter der Hypothese beibehalten werden"
#| fig-height: 2

mu <- 0; sigma <- 1; q <- 0.05
p_1 <- tibble(
x = seq(qnorm(q, mu, sigma), qnorm(1-q, mu, sigma), length.out = 100),
p = dnorm(x, mean = mu, sd = sigma)
) |> 
ggplot(aes(x, p)) +
  geom_ribbon(aes(ymin = 0, ymax = p), fill = 'green', alpha = .3) +
  geom_line(size = 2, color = 'green') + 
  scale_x_continuous('Werte der Teststatistik',
                     limits = c(-3, 3),
                     labels = c("","","", 0, "","",""),
                     breaks = -3:3) +
  scale_y_continuous('Dichte')
p_1
```

Sei nun ein beobachteter Wert hinzugefügt (@fig-stats-est-ci-02).

```{r}
#| label: fig-stats-est-ci-02
#| fig-cap: "Bereich, bei dem beobachtete Werte unter der Hypothese beibehalten werden, und ein beobachteter Wert"
#| fig-height: 2

p_1 + geom_point(data = tibble(x = -1, p = 0), color = 'red', size = 4)
```

Der beobachtete Wert liegt im grünen Bereich der $H_0$ Verteilung und ist daher kompatibel mit der angesetzten Hypothese. Kompatibel bedeutet hier, die Hypothese würde anhand des beobachteten Wertes nicht ablehnt werden. Sei nun der beobachteten Wert fixiert und es soll nun betrachtet werden, welche andere Hypothese $H_0^*$ ebenfalls kompatibel ist. Diese neue Hypothese $H_0^*$ ist nach links verschoben. 

```{r}
#| label: fig-stats-est-ci-03
#| fig-cap: "Bereich bei dem beobachtete Werte unter der Hypothese $H_0^*$ beibehalten werden und der beobachtete Wert"
#| fig-height: 2

mu <- -2
tibble(
x = seq(qnorm(q, mu, sigma), qnorm(1-q, mu, sigma), length.out = 100),
p = dnorm(x, mean = mu, sd = sigma)
) |> 
ggplot(aes(x,p)) +
  geom_ribbon(aes(ymin = 0, ymax = p), fill = 'green', alpha=.3) +
  geom_line(size=2, color='green') + 
  geom_point(data = tibble(x = -1, p = 0), color = 'red', size=4) +
  scale_x_continuous('Werte der Teststatistik',
                     limits = c(-4,3),
                     labels = c("","","",0,"","",""),
                     breaks = -3:3) +
  scale_y_continuous('Dichte')
```

In @fig-stats-est-ci-03 ist nun der Bereich für die neue Hypothese $H_0^*$ abgetragen. Es zu erkennen, , dass der beobachtete Wert auch für diese Hypothese im grünen, also *unkritischen*, Bereich liegt. Der beobachtete Wert würde nicht dazu führen, dass die Hypothese $H_0^*$ ablehnt werden würden, wenn diese Hypothese die $H_0$ gewesen wäre. Die Hypothese $H_0^*$ wird als **kompatibel** mit beobachteten Wert interpretiert. Dieser Ablauf kann nun in beide Richtungen weitergeführt. D. h., es werden systematisch $H_0$-Hypothesen von $-\infty$ bis $\infty$ verschoben und geschaut, ob der beobachtete Wert in den unkritischen Bereichen für die jeweilige Hypothese liegt. Die kleinste Hypothese, die am weitesten links liegende Hypothese, bezeichnen nun als $H_{lower}$ bezeichnet und entsprechend die größte Hypothese, die am weitesten rechts liegende Hypothese, als $H_{upper}$. Dadurch wird ein Intervall von Hypothesen ausgezeichnet, mit der Eigenschaft, dass alle Hypothesen zwischen $H_{lower}$ und $H_{upper}$ kompatibel mit dem beobachteten Wert sind. Bei all diesen Überlegungen spielt der tatsächlich in der Population liegende Wert $\Delta$ keine Rolle!

Sei nun eine kleine Vereinfachung der graphischen Darstellung des kompatiblen Bereichs für eine gegebene Hypothese durchgeführt.

```{r}
#| label: fig-stats-est-ci-04
#| fig-cap: "Graphische Darstellung des kompatiblen Bereichs für eine Hypothese."
#| fig-height: 2

mu <- 0
tibble(
x = seq(qnorm(q, mu, sigma), qnorm(1-q, mu, sigma), length.out = 100),
p = dnorm(x, mean = mu, sd = sigma) + 1
) |> 
ggplot(aes(x, p)) +
  geom_ribbon(aes(ymin = 1, ymax = p), fill = 'green', alpha = .3) +
  geom_line(size = 2, color = 'green') + 
  geom_pointrange(data = tibble(xmin = qnorm(q, mu, sigma),
                                xmax = qnorm(1-q, mu, sigma),
                                x = 0,
                                p = 0.9), aes(xmin = xmin, xmax = xmax),
                  size = 1, color = 'green', linewidth = 2) +
  scale_x_continuous('Werte der Teststatistik',
                     limits = c(-3, 3),
                     labels = c("","","", 0, "","",""),
                     breaks = -3:3) +
  scale_y_continuous('Dichte')
```

In @fig-stats-est-ci-04 ist der mit den Daten kompatible Hypothesenbereich mittels einer Linie und eines Punktes dargestellt. Der Punkt zeigt den Wert der jeweiligen Hypothese an, während die Striche rechts und links die untere und obere Grenze des kompatiblen Bereichs darstellen. Die Grenzen des kompatiblen Bereichs sind eine Funktion des gewählten $\alpha$-Levels. Wenn dass $\alpha$ kleiner wird, dann dehnen sich die beiden Striche aus. Wird dagegen $\alpha$ größer, dann verkürzen sich die Flügel entsprechend. Die Irrtumswahrscheinlichkeit $\alpha$ wird verwendet, um kritische Bereiche einer Verteilung zu definieren. Werden nun alle Intervalle systematisch von links nach rechts abgetragen, und die Hypothesen deren unkritischen Bereiche mit dem beobachteten Wert (den Daten) kompatibel sind markiert, dann führt die zu der folgenden Darstellung in @fig-stats-est-ci-05.

```{r}
#| fig-cap: "Kompatibles Intervall (grün), Populationsparameter $\\delta$ (schwarz) und $\\alpha$-Level für die beobachtete Differenz (gelb)."
#| label: fig-stats-est-ci-05

mu_s <- seq(150, 550, length.out = 30) 
q_s <- qnorm(alpha, mu_s, s_e)
df <- tibble(mu_s = mu_s,
             lu = qnorm(alpha/2, mu_s, s_e),
             up = qnorm(1-alpha/2, mu_s, s_e)) %>%
  dplyr::mutate(inside = dplyr::if_else(mu_s >= c_i[1] & mu_s <= c_i[2], 'ja', 'nein'))
ggplot(df, aes(x = mu_s, y = mu_s, ymin = lu, ymax = up, color = inside)) +
  geom_hline(yintercept = d_hat, color = 'yellow', linewidth = 1) +
  geom_hline(yintercept = c_i, color = 'green') +
  geom_hline(yintercept = delta, color = 'black') +
  geom_pointrange(size = 0.3) +
  labs(x = bquote('Mögliche'~delta), y = "") +
  scale_color_manual("plausibel", values = c('green', 'red')) +
  scale_y_continuous(breaks = seq(50, 700, 50)) +
  scale_x_continuous(breaks = NULL) +
  annotate("text", y = 550, x = 100, label = expression(delta == 500), size = 4) +
  annotate("text", y = 400, x = 100, label = expression(d == 350), size = 4) +
  coord_flip() 
```

Der beobachtete Wert ist in Gelb abgetragen, und die Intervalle der unkritischen Bereiche werden von links kommend nach rechts verschoben. In @fig-stats-est-ci-05 sind die Intervalle vertikal etwas versetzt, um sie besser sichtbar zu machen. Das erste Intervall, dessen rechtes Ende die gelbe Linie berührt, ist dabei die Hypothese mit dem geringsten $\delta_{\text{low}}$, die noch kompatibel mit den Daten ist. Das Gleiche gilt auf der rechten Seite mit der größten Hypothese $\delta_{\text{upper}}$, deren linker Rand die gelbe Linie gerade so noch streift. Wenn nun die Werte $\delta_{\text{lower}}$ und $\delta_{\text{upper}}$ als die Randwerte eines Intervalls festgelegt werden, dann sind alle Hypothesen zwischen diesen beiden Grenzen kompatibel mit den beobachteten Daten. D. h., wenn einer dieser Werte initial die $H_0$ gewesen wäre, dann hätten wäre die $H_0$ nicht abgelehnt worden.

Achtung, und das ist ein ganz großes Achtung: Das sind alles Aussagen über die beobachteten Daten. Das ist keine Aussage über die tatsächliche $H_0$, mit der in das Experiment gegangen wurde. In @fig-stats-est-ci-05 sind tatsächlich nur Intervalle zu Hypothesen mit $\Delta \neq 0$ abgetragen. Das erste, rote Intervall ganz links, entspricht der Hypothese $H: \Delta = 150$. Das Verhältnis zwischen verwendeten $H_0$ und dem Intervall der kompatiblen Hypothesen wird nun betrachtet. In  @fig-stats-est-ci-05 ist in Schwarz der tatsächliche, wahre Populationswert $\Delta$ abgetragen. Es ist zu erkennen, dass in diesem vorliegenden Fall der wahre Wert nicht innerhalb des Intervalls der kompatiblen Hypothesen liegt. Sei nun betrachtet, was passiert, wenn das Experiment mehrmals wiederholt wird.

```{r}
#| fig-cap: "Simulation von $n = 100$ Intervallen."
#| cache: false 
#| label: fig-stats-est-ci-06
#| fig-height: 5

foo <- function(mu = 500, se = 132, n = 20, alpha = 0.05) {
  sam <- rnorm(n, mu, se)
  x_hat <- mean(sam)
  s_e <- sd(sam)/sqrt(n)
  q <- qnorm(alpha/2)
  c(x_hat, x_hat + c(1,-1) * q * s_e)
}
N <- 100
set.seed(2)
c_is <- t(replicate(N, foo(mu, sigma, 20)))
colnames(c_is) <- c("x_hat","lo","up")
df_2 <- as_tibble(c_is) %>% dplyr::mutate(id = dplyr::row_number(),
                                   inside = dplyr::if_else((mu >= lo) & (mu <= up), 'ja','nein'))
ggplot(df_2, aes(id, x_hat, color = inside)) + 
  geom_pointrange(aes(ymin = lo, ymax = up), size=0.3) +
  geom_hline(yintercept = mu, color = 'black') +
  scale_color_manual("enthalten", values = c('green','red')) +
  labs(x = 'Experiment[#]', y = "d") +
  coord_flip()
```

In @fig-stats-est-ci-06 ist das Ergebnis von $100$ Experimenten und den resultierenden, kompatiblen Intervallen gezeigt. Der schwarze Strich zeigt wieder den wahren Populationswert $\Delta$ an. Es ist zu erkennen, dass die Mehrheit der berechneten, kompatiblen Intervalle den wahren Wert auch tatsächlich enthält. Aber wieder Achtung: In der Realität wird das Experiment meistens nur einmal durchgeführt. Wenn nun die Frage gestellt wird: *Was ist die Wahrscheinlichkeit, das das berechnete Kompatibilitätsintervall den wahren Populationsparameter enthält?* Nun, entweder der Wert ist in dem Intervall enthalten oder er ist nicht enthalten. Also entweder $P(\text{enthalten}) = 0$ oder $P(\text{enthalten}) = 1$. Daher ist die Frage, mit welcher Wahrscheinlichkeit das Intervall den wahren Parameter enthält, nicht wirklich sinnvoll. Man würde hoffen, dass bei der Durchführung des Experiments eines der grünen Intervalle *getroffen* wurde. Darüber kann man sich aber leider nicht sicher sein.

Wir hatten vorhin hergeleitet, dass die Breite der Intervalle von unserem angesetzten $\alpha$ abhängt. Tatsächlich sagt dieses $\alpha$ etwas darüber aus, wie oft ich erwarten würde, bei großer (unendlicher) Wiederholung des Experiments, dass das Intervall den wahren Populationsparameter enthält. D. h., wenn ich ein $\alpha = 0.05$ ansetze, würde ich bei $1000$ Durchführungen erwarten, dass $950$ der Intervalle den wahren Populationsparameter enthalten und entsprechend $50$ Intervalle den Populationsparameter nicht enthalten. Noch einmal: Das ist eine Aussage über eine Wahrscheinlichkeit bei Wiederholung des Experiments. Eigentlich ist $1000$ auch nicht genug, sondern eher etwas in Richtung $\displaystyle{\lim_{N \to \infty}}$.

Nachdem wir jetzt lange genug um den heißen Brei herumgeredet haben: Das Intervall, das mit den Daten kompatibel ist, wird als Konfidenzintervall bezeichnet. Leider ist die Bezeichnung "Konfidenz" stark irreführend, da für Statistiker "Konfidenz" eben diese Eigenschaft bei Wiederholung des Experiments bezeichnet. In der Alltagssprache ist die Interpretation leider etwas abweichend. Die Aussage „Ich habe eine $95\%$ Konfidenz, dass das Intervall den wahren Populationsparameter enthält“ ist daher eine falsche Interpretation des Konfidenzintervalls, die oft anzutreffen ist. Korrekt ist: Wenn ich das Experiment sehr, sehr oft wiederhole, dann bin ich konfident, dass $95\%$ der Intervalle den wahren Populationsparameter enthalten. Für eine einzige Wiederholung mit eben nur einem resultierenden Konfidenzintervall ist die Wahrscheinlichkeit entweder $1$ oder $0$. Entweder der wahre Populationsparameter ist im Konfidenzintervall enthalten oder eben nicht.

Das Konfidenzintervall hat aber noch ein weiteres Merkmal. Wenn das Intervall sehr eng ist, dann ist nur eine eng umschriebene Menge von Hypothesen mit den Daten kompatibel. Andersherum, wenn das Intervall sehr breit ist, dann sind sehr unterschiedliche Hypothesen mit den Daten kompatibel. Die Breite ist dabei nicht nur vom $\alpha$-Niveau, sondern auch von der Streuung des Standardfehlers der Stichprobenstatistik abhängig. Wenn die Streuung sehr klein ist, dann wird für gegebenes $\alpha$ das Intervall schmaler, als wenn die Streuung sehr groß ist. Daher ist das Konfidenzintervall auch ein Merkmal für die Präzision der Schätzung mittels der Daten.

::: {#def-confidence}

### Konfidenzintervall \index{Konfidenzintervall}

Das Konfidenzintervall wird immer für ein gegebenes $\alpha$-Niveau berechnet. Das Konfidenzintervall gibt **nicht** die Wahrscheinlichkeit an, mit der ein *wahrer* Populationsparameter in dem Intervall liegt. Sondern, das Konfidenzintervall gibt alle mit den Daten **kompatiblen** Populationsparameter, alle kompatiblen $H_0$-Hypothesen, an.
Das $1-\alpha$-Niveau des Konfidenzintervalls gibt an, bei welchem Anteil von Wiederholungen davon auszugehen ist, dass das Konfidenzintervall den wahren Populationsparameter enthält. Die Breite des Konfidenzintervalls kann als eine Metrik für die **Präzision** der Schätzung angesehen werden.
							   
:::

Eine schöne Zusammenfassung zum Konfidenzintervall ist aus @spiegelhalter2019 [p.241]

1. We use probability theory to tell us, for any particular population
parameter, an interval in which we expect the observed statistic to lie
with 95% probability.
2. Then we observe a particular statistic.
3. Finally (and this is the difficult bit) we work out the range of possible
population parameters for which our statistic lies in their 95\%
intervals. This we call a "95\% confidence interval".
4. This resulting confidence interval is given the label "95\%" since, with
repeated application, 95% of such intervals should contain the true
value. Strictly speaking, a 95\% confidence interval does ***not*** mean there is a 95\% probability that this particular interval contains the true value [...]

All clear? If it isn’t, then please be reassured that you have joined
generations of baffled students.

Als kleine Vorschau eine relativ allgemein gültige Formel um Konfidenzintervalle zu berechnen. Glücklicherweise ist es nämlich nicht notwendig alle möglichen Hypothesen von links nach rechts wandern zu lassen um die untere und die obere Grenze des Intervalls zu bestimmen. Das Konfidenzintervall lässt sich oft mit Hilfe der folgenden Formel bestimmen:

\begin{equation}
\textrm{CI}_{1-\alpha} = \text{statistik} \pm q_{\alpha/2} \times s_e 
\label{eqn-stats-estim-ci-calc}
\end{equation}

Das Intervall ist immer eine Kombination aus einem berechneten Wert, der Statistik, beispielsweise dem Mittelwert $\bar{x}$, den zum $\alpha$-Niveau gehörenden Quantilen $q_{\alpha/2}$, oftmals den Quantilen der Standardnormalverteilung $\Phi(z)$, also $z_{\alpha/2}$, multipliziert mit dem Standardfehler $s_e$ der betrachteten Statistik. Unglücklicherweise, wenn dieses Intervall als Zufallszahl vor dem Experiment bestimmt wird, dann ist es tatsächlich eine Wahrscheinlichkeitsaussage. Aber wenn wir eine Realisierung der beteiligten Zufallszahlen, in diesem Fall der Intervallgrenzen, beobachtet haben, dann ist keine Aussage über die Wahrscheinlichkeit mehr sinnvoll, wie wir in @fig-stats-est-ci-06 gesehen haben.

:::{#exm-ci-calc}

Sei zum Beispiel die Statistik des Stichprobenmittelwerts der Körpergröße aus einer Stichprobe aus der Normalbevölkerung $\bar{x}$. Aus der Literatur können wir für die Bevölkerung eine Standardabweichung von $\sigma = 6$ herleiten. Dadurch ergibt sich ein Standardfehler von $s_e = \frac{\sigma}{\sqrt{N}} = \frac{6}{\sqrt{9}} = \frac{6}{3} = 2$. Wir beobachten nun in unserer Stichprobe von $N = 9$ einen Mittelwert von $\bar{x} = 170$ cm. Für die Quantile bei $\alpha = 0.05$ können wir die Standardnormalverteilung $\Phi(Z)$ heranziehen und erhalten für die Quantile $q_{\alpha/2} = q_{0.025} = -1.96$. Daraus folgt nach Formel \eqref{eqn-stats-estim-ci-calc} für das Konfidenzintervall:

\begin{equation*}
95\%CI = 170 \pm 1.96 \cdot 2 = [166.08, 173.92]\text{ cm}
\end{equation*}
:::

:::{#exm-ci-calc-2}
Hätten wir im vorhergehenden Beispiel keine Information über die Standardabweichung $\sigma$ in der Population gehabt, dann müssten wir die Standardabweichung anhand der Stichprobe abschätzen. In diesem Fall müsste diese zusätzliche Unsicherheit berücksichtigt werden, indem anstatt der Standardnormalverteilung $\Phi(z)$ die $t$-Verteilung verwendet wird. Hier würde sich dann ein Quantil von $q_{8,0.025} = -2.3$ ergeben. D. h., wir würden ein breiteres Konfidenzintervall erhalten. Sei beispielsweise die Standardabweichung $s$ in der Stichprobe $s = 6.3$. Dann würden wir das folgende Konfidenzintervall erhalten:

\begin{equation*}
95\%CI = 170 \pm 2.3 \cdot 2.2 = [164.9, 175.1]\text{ cm}
\end{equation*}
:::

::: {#exm-ci-calc-3}
In @grant1996 ist die Handkraft von $N=10$ Elite Kletterer der dominanten Hand in Newton $N$ ermittelt. Dabei wurden die folgenden Werte ermittelt worden (siehe @tbl-stats-est-ci-climb). 

```{r}
#| tbl-cap: "Deskriptive Werte der Kletterdaten."
#| label: tbl-stats-est-ci-climb

climb <- readr::read_csv('data/grip_strength_and_mass.csv') |> dplyr::filter(level == 'Elite')
q_t <- r_2(qt(0.975, 9))
climb_sum <- climb |> dplyr::summarize(m = r_2(mean(strength_N)),
                                       sd = r_2(sd(strength_N)),
                                       se = r_2(sd/dplyr::n()))
ci <- r_2(climb_sum$m + c(-1,1)*q_t*climb_sum$se)
climb_sum |> kable(col.names=c('Mittelwert','STD','se'), digits=2)
```

Das $95\%$ Konfidenzintervall für die Handkraft berechnet sich unter Verwendung der $t$-Verteilung mit $df = 9$ entsprechend nach:

\begin{equation*}
\text{CI}_{95\%} = `r climb_sum$m` \pm `r q_t` \cdot `r climb_sum$se` = [`r paste_com(ci)`]
\end{equation*}

Oder in `R` entsprechend mittels:

```{r}
#| echo: true

555.06 + c(-1,1) * qt(0.975, 9) * 9.65 
```

:::

:::{#exm-ci-calc-4}
Bezogen auf das Eingangsbeispiel mit den beiden Arbeitsgruppen "Glück" und "Pech" lässt sich für die beiden Konfidenzintervalle ergeben sich die beiden folgenden Konfidenzintervalle:
```{r}
lucky_d <- 3.61
lucky_std <- 9.62 
lucky_n <- 44 - 2
unlucky_d <- 2.23
unlucky_std <- 8.66 
unlucky_n <- 36 - 2
lucky_se <- round(lucky_std/sqrt(lucky_n), 2)
unlucky_se <- round(unlucky_std/sqrt(unlucky_n), 2)
lucky_q_t <- round(qt(0.975, lucky_n), 2)
unlucky_q_t <- round(qt(0.975, unlucky_n), 2)
lucky_ci <- round(lucky_d + c(-1,1) * lucky_q_t * lucky_se, 2)
unlucky_ci <- round(unlucky_d + c(-1,1) * unlucky_q_t * unlucky_se, 2)
ci_lucky <- paste(round(lucky_d + c(-1,1) * lucky_q_t * lucky_se, 2), collapse=',')
ci_unlucky <- paste(round(unlucky_d + c(-1,1) * unlucky_q_t * unlucky_se, 2), collapse=',')
```

\begin{align*}
CI_\text{Glücklich} &= `r lucky_d` \pm `r lucky_q_t` \cdot `r lucky_se` = [`r ci_lucky`] \\
CI_\text{Pech} &= `r unlucky_d` \pm `r unlucky_q_t` \cdot `r unlucky_se` = [`r ci_unlucky`] \\
\end{align*}

Hier ist zusehen, das das Konfidenzintervall für die Gruppe "Glücklich" die $H_0: \Delta=0$ nicht enthält, während dies für die Gruppe "Pech" der Fall ist. Aber selbst für die Gruppe "Pech" liegt ein großer Teil des Intervalls im positiven Bereich und somit nicht im direkten Widerspruch zu dem Befund von Gruppe "Glücklich" (siehe auch @fig-stats-estci-04).

```{r}
#| label: fig-stats-estci-04
#| fig-cap: "Konfidenzintervalle für die Gruppendifferenzen der beiden Forschergruppen"
#| fig-height: 1.2

ggplot(tibble(x = c(3.61, 2.23), y = 1:2,
              xmin=c(lucky_ci[1], unlucky_ci[1]),
              xmax=c(lucky_ci[2], unlucky_ci[2])), aes(x, y)) +
  geom_pointrange(aes(xmin=xmin, xmax=xmax)) +
  geom_vline(xintercept = 0, linetype = 'dashed', color = 'red') +
  scale_x_continuous('Gruppendifferenz D', limits = c(-4, 7)) +
  scale_y_continuous('Gruppe', limits = c(0.9, 2.1),
                     breaks = 1:2, labels = c('Glücklich', 'Pech'))
```

:::

Die Abhängigkeit des Standardfehlers $s_e$ spielt auch wieder hinsichtlich der Präzision eines Konfidenzintervalls eine entscheidende Rolle. Wir haben vorher schon erwähnt, dass der Standardfehler abhängig von der Stichprobengröße mit $\sqrt{N}$ entweder größer oder kleiner wird. D. h., mit steigender Stichprobengröße wird der Standardfehler $s_e$ mit $\sqrt{N}$ kleiner. Da der Standardfehler auch bei der Berechnung des Konfidenzintervalls verwendet wird, verkleinert sich mit zunehmender Stichprobengröße daher auch das Konfidenzintervall. D. h., die Präzision unserer beobachteten Werte nimmt mit der Größe $N$ der Stichprobe zu. Dies sollte auch heuristisch einleuchtend sein: Wenn die Stichprobengröße $N$ zunimmt, nimmt die Stichprobenvariabilität ab, und wir haben weniger Unsicherheit in unserer Statistik. Die kritischen Werte rücken enger zusammen, und entsprechend werden weniger Hypothesen mit dem beobachteten Wert kompatibel sein. Das Konfidenzintervall kann somit, wie der Standardfehler $s_e$, als ein Maß für die Unsicherheit in unserer Beobachtung interpretiert werden. Daher sollten wir bei der Planung unseres Experiments auch immer berücksichtigen, welche Präzision wir erreichen wollen, d. h., welche Breite des Konfidenzintervalls für uns noch tolerierbar ist, um eine Entscheidung auf der Basis der Daten zu treffen.

::: {.callout-tip}
Eine Faustregel für die Bestimmung des Konfidenzintervall lautet:
\begin{equation}
\textrm{CI}_{95\%} = \text{statistik} \pm 2 \cdot s_e 
\label{eqn-stats-estimate-ci-thumb}
\end{equation}
D.h. wenn wir unseren Schätzwert haben und dessen Standardfehler $s_e$ kennen, dann $2$mal den Standardfehler $\pm$ unseren Schätzwert ergibt ungefähr das resultierende $95\%$ Konfidenzintervall.
:::

## Dualität von Signifikanztests und Konfidenzintervall

Als letzten, vorläufigen Punkt zu Konfidenzintervallen noch ein Abschnitt zum Verhältnis von Konfidenzintervallen und Signifikanztests. Wenn das Konfidenzintervall mit Niveau $1-\alpha\%$ die $H_0$ nicht beinhaltet, dann bedeutet dies nach unserer Herleitung, dass diese Hypothese nicht mit den beobachteten Daten kompatibel ist. D. h., bei einem Signifikanztest wird die $H_0$ bei einer Irrtumswahrscheinlichkeit von $\alpha$ abgelehnt. Wenn ich also das Konfidenzintervall und die getestete $H_0$ kenne, dann kann ich eine Aussage darüber machen, ob das Ergebnis statistisch signifikant ist oder nicht. Andersherum, wenn ich weiß, dass das Ergebnis statistisch signifikant war oder eben nicht, dann weiß ich auch, ob die $H_0$ im Konfidenzintervall ist oder nicht. Es besteht eine Dualität zwischen beiden Konzepten, wobei mir das Konfidenzintervall deutlich mehr Informationen als der reine Signifikanztest gibt.

::: {#exm-ci-02}

In @fig-stats-estimate-ci-dual ist ein Beispiel abgetragen für ein hypothetisches Konfidenzintervall und der Wert der $H_0$-Hypothese.

```{r}
#| fig-cap: "Der erwartete Wert unter der $H_0$ (rot) und das beobachtete Konfidenzintervall zur beobachteten Statistik $D$."
#| fig-height: 1.3
#| label: fig-stats-estimate-ci-dual

d <- 2.2
df_1 <- tibble::tibble(
  x = d, 
  y = 1,
  xmin = x-2,
  xmax = x+2
)

ggplot2::ggplot(df_1,ggplot2::aes(x, y, xmin=xmin, xmax=xmax)) +
  ggplot2::geom_point(size=6) +
  ggplot2::geom_linerange(linewidth=2) +
  ggplot2::geom_vline(xintercept = 0, linetype='dashed', color='red', linewidth=1.3) +
  ggplot2::scale_x_continuous('Unterschied', breaks = c(0,d),
                              labels = c(expression(H[0]), 'D')) +
  ggplot2::scale_y_continuous('', breaks = NULL, limits = c(-1,3)) +
  ggplot2::theme(text = ggplot2::element_text(size=15))

```

Da die $H_0$-Hypothese nicht im Konfidenzintervall enthalten ist, können wir herleiten, dass ein statistisch signifikantes Ergebnis erzielt wurde.
:::

## (Exkurs) Power und false discovery rate

Der folgende Abschnitt betrachtet, wie sich die durchschnittliche Power auf die Literatur insgesamt auswirkt. Sei eine bestimmte Anzahl von Studien bzw. Veröffentlichungen gegeben. Ein Teil dieser Studien wird signifikante Ergebnisse dokumentieren, während der andere Teil keine signifikanten Ergebnisse gefunden hat. Innerhalb der signifikanten Ergebnisse wird ein Teil der Ergebnisse auf einem tatsächlichen Effekt beruhen, während der andere Teil zwar statistisch signifikant ist, aber eigentlich kein Effekt vorliegt – es wurde also ein Typ-I-Fehler begangen. Den Anteil der Studien, bei denen kein Effekt vorliegt, bezeichnen wir mit $P(H_0)$. Allgemeiner formuliert, ist das die Wahrscheinlichkeit für eine Studie mit einem Nulleffekt. Wenn $N$ die Gesamtzahl der betrachteten Studien bezeichnet, dann berechnet sich die Anzahl der Studien, bei denen ein Nulleffekt vorliegt, folgendermaßen:

\begin{equation*}
N_{H_0} = P(H_0) \cdot N
\end{equation*}

Werden diese Studien mit der Irrtumswahrscheinlichkeit $\alpha$ multipliziert, dann erhält man die Anzahl der Studien, die zwar einen Nulleffekt, also eigentlich keinen Effekt haben, aber aufgrund der Irrtumswahrscheinlichkeit *geirrt* haben und einen statistisch signifikanten Effekt beobachten.

\begin{equation*}
N_{H_0, \text{signifikant}} = P(H_0) \cdot \alpha \cdot N
\end{equation*}

Ein einfaches Beispiel wäre, wenn $N = 200$ Studien gegeben wären und $P(H_0) = 0.5$, dann liegt bei $100$ Studien kein Effekt vor. Aber aus diesen $100$ Studien werden sich $\alpha$ der Fälle irren. Dies führt dazu, dass daher $5$ statistisch signifikante Ergebnisse beobachtet werden. In @fig-stats-sig-fdr-01 ist dies graphisch veranschaulicht.

```{r}
#| fig-cap: "Anteil der signifikanten Nulleffekte (große Punkte) bei $N = 200$ Studien und $P(H_0) = 0.5$."
#| label: fig-stats-sig-fdr-01

fdr_1 <- tibble(
  x = rep(1:20,10),
  y = rep(1:10,each=20),
  h0 = rep(c("H_0","H_1"), each=100),
  sig = rep(1:0, c(5,195))
)
ggplot(fdr_1, aes(x,y, color=h0)) +
  geom_point() +
  geom_point(data = fdr_1 |> dplyr::filter(sig == 1),
             size=4) +
  scale_color_discrete(
    "Effekt",
    labels = c(expression(H[0]),expression(H[1]))) +
  theme_void()
```

Die gleichen Überlegungen können für diejenigen Studien gemacht werden, bei denen tatsächlich ein Effekt vorliegt. Die Anzahl der Studien mit einem Effekt ist genau das Komplement zu denjenigen, bei denen der Nulleffekt vorliegt. Formal:

\begin{equation*}
N_{H_1} = (1-P(H_0))\cdot N
\end{equation*}

Die Anzahl derjenigen Studien, bei denen ein Effekt vorliegt und die auch als statistisch signifikant beobachtet werden, ist abhängig von der Power $\rho$. Dementsprechend berechnet sich die Anzahl der statistisch signifikanten Studien, bei denen auch ein Effekt vorliegt, folgendermaßen:

\begin{equation*}
N_{H_1,\text{signifikant}} = (1-P(H_0))\cdot \rho \cdot N
\end{equation*}

Im Beispiel mit $N = 200$ Studien und $P(H_0) = 0.5$ würde bei $100$ Studien ein Effekt vorliegen und bei einer Power von $\rho = 0.8$ würde bei $80$ dieser Studien ein statistisch signifikantes Ergebnis beobachtet werden (siehe @fig-stats-sig-fdr-02).

```{r}
#| fig-cap: "Anteil der signifikanten Null- und $H_1$-Effekten (große Punkte) bei $N = 200$ Studien, $P(H_0) = 0.5$ und Power $\\rho = 0.8$."
#| label: fig-stats-sig-fdr-02

fdr_2 <- tibble(
  x = rep(1:20,10),
  y = rep(1:10,each=20),
  h0 = rep(c("H_0","H_1"), each=100),
  sig = c(rep(1:0, c(5,95)), rep(1:0, c(80,20)))
)
ggplot(fdr_2, aes(x,y, color=h0)) +
  geom_point() +
  geom_point(data = fdr_2 |> dplyr::filter(sig == 1),
             size=4) +
  scale_color_discrete(
    "Effekt",
    labels = c(expression(H[0]),expression(H[1]))) +
  theme_void()
```

Nun kann der Anteil aller Studien mit statistisch signifikanten Ergebnisse untersucht werden mit denjenigen bei denen zwar ein statisch signifikantes Ergebnis beobachtete wurde aber tatsächlich ein Nulleffekt zugrundeliegt ins Verhältnis zueinander gesetzt werden. Dies führt zu der sogenannten false discovery rate (fdr). Formal ist fdr wie folgt definiert (siehe @bartovs2022).

\begin{equation}
\begin{aligned}
\text{fdr} &= \frac{P(H_0)\cdot \alpha \cdot N}{P(H_0)\cdot \alpha \cdot N + (1-P(H_0))\cdot \rho \cdot N} \\
&=\frac{P(H_0)\cdot \alpha}{P(H_0)\cdot \alpha + (1-P(H_0))\cdot \rho}  
\end{aligned}
\label{eq-stats-sig-fdr}
\end{equation}

Die fdr bestimmt somit den Anteil der statistisch signifikanten Ergebnisse, bei denen es sich um einen falschen Befund handelt. Das heißt, der Anteil an Studien bei denen ein Effekt in Form eines statistisch signifikanten Ergebnisses beobachtet, obwohl eigentlich kein Effekt vorliegt. Es werden in @fig-stats-sig-fdr-02a die großen $H_0$-Punkte durch die Gesamtanzahl der großen Punkte, der signifikanten Punkte geteilt.

```{r}
#| fig-cap: "Signifikante Null- und $H_1$-Effekte bei $N = 200$ Studien, $P(H_0) = 0.5$ und Power $\\rho = 0.8$."
#| label: fig-stats-sig-fdr-02a

ggplot(fdr_2, aes(x,y, color=h0)) +
  geom_point(data = fdr_2 |> dplyr::filter(sig == 1),
             size=4) +
  scale_color_discrete(
    "Effekt",
    labels = c(expression(H[0]),expression(H[1]))) +
  lims(y = c(1,10)) +
  theme_void()
```

Die Formel \eqref{eq-stats-sig-fdr} für die fdr kann nun verwendet werden, um den Zusammenhang der false discovery rate mit der Power für eine gegebene Nulleffektswahrscheinlichkeit $P(H_0)$ und und eine gegebene Irrtumswahrscheinlichkeit $\alpha$ zu betrachten (siehe @fig-stats-sig-fdr-03).

```{r}
#| fig-cap: "Zusammenhang zwischen der Power $\\rho$ und der false discovery rate fdr bei $\\alpha = 0.05$."
#| label: fig-stats-sig-fdr-03

alpha <- 0.05
p_H_0 <- c(0.25, 0.5, 0.75, 0.8)
fdr <- function(rho, p_H_0 = 0.5, alpha = 0.05) p_H_0 * alpha/(p_H_0 * alpha + (1-p_H_0)*rho)
n <- 100
tibble(
  power = rep(seq(.1, .99, length.out = n),length(p_H_0)),
  p_H_0 = rep(p_H_0, each=n),
  fdr = fdr(power, p_H_0, alpha),
  p_H_0f = forcats::as_factor(p_H_0)
) |> 
ggplot(aes(power, fdr, color=p_H_0f, group=p_H_0f)) +
  geom_line() +
  scale_colour_discrete(expression(p(H[0]))) +
  labs(x = expression(paste("Power ",rho))) +
  lims(y = c(0,1))
```

In @fig-stats-sig-fdr-03 ist zu erkennen, dass die fdr besonders hoch ist, wenn die Power in der Literatur niedrig ist. Gleichzeitig steigt die fdr an, wenn die Wahrscheinlichkeit für keinen Effekt $P(H_0)$ ansteigt. In @fig-stats-sig-fdr-03 ist dieser Effekt besonders groß, wenn die Grundrate der tatsächlichen Effekte $1-P(H_0)$ niedrig ist bzw. eben $P(H_0)$ sehr hoch ist. Wenn nur $20\%$ der Studien tatsächlich Effekte zugrunde liegen, sind $2/3$ der Ergebnisse falsch positiv. Daher, wenn die mittlere Power in der Literatur in einer wissenschaftlichen Disziplin durch vorwiegend kleine Studien niedrig ist, sinkt die Wahrscheinlichkeit, dass ein gegebenes statistisch signifikantes Ergebnis tatsächlich durch einen Effekt hervorgerufen wurde. Für eine Forschungsdisziplin ist es daher wichtig, dass die Studien über eine ausreichende Power verfügen, um einerseits die gewünschten Effekte tatsächlich beobachten zu können und andererseits die Literatur durch falsch-positive Ergebnisse nicht zu verfälschen (siehe @button2013). Bei dieser Betrachtung fließen Probleme wie p-hacking oder HARKing noch gar nicht mit ein.

## Die Fisher vs. Neyman-Pearson Kontroverse

Ein Problem bei der didaktischen Aufbereitung der Inhalte besteht darin, dass der konzeptionelle Aufbau leider nicht mit der wissenschaftlichen Realität zusammenpasst. In der wissenschaftlichen Praxis werden die Begrifflichkeiten statistische Signifikanz, $\alpha$-Fehler usw. so verwendet, als ob sie alle zu dem gleichen theoretischen Konstrukt gehören. Leider ist dies nicht der Fall und es wird in der Praxis eine Chimäre eingesetzt die konzeptionelle Widersprüchlich ist und auch innerhalb der statistischen Primärliteratur zu keiner befriedigenden Lösung gebracht wurde.

Der Großteil der in der angewandten Statistik verwendet Methoden geht auf die Arbeiten von Ronald A. Fisher (1890-1962) zurück. Unter Fishers statistischem System war die sogenannte Nullhypothese $H_0$ zentral und die Ablehnung derer basierend auf dem dazugehörenden p-Wert. Aus Fishers Ansatz erfolgte auch die Konzepte der $H_0$ Hypothese als diejenige unter der *nichts* passiert. Der p-Wert ist dabei eine Funktion eines *einzelnen* Datensatzes. Unter Fisher waren die Konzepte Typ-I und Typ-II Fehler nicht vorhanden. Dieser Ansatz wurde unter dem Begriff *inductive inference* zusammengefasst (@fisher1935). Zu diesem Zweck wird eine $H_0$-Hypothese aufgestellt und diese wird anhand der Daten mittels des p-Werts beurteilt. Der p-Wert wird unter Fisher als mögliche Evidenz gegen die $H_0$ Hypothese angesehen. D.h. wenn ein sehr *kleiner* p-Wert beobachtet wird, wird dies als Evidenz gegen die Korrektheit der Annahme der $H_0$ betrachtet. Allerdings hat sich Fisher gegen die *Annahme* der $H_0$ ausgesprochen, sondern fehlende Evidenz als nicht Ausreichend um die $H_0$ abzulehnen, also provisorisch beizubehalten [loucca2008, S.20]. Aus Fishers inductive inference kommt auch die Begrifflichkeit der *Signifikanz*. Der Ansatz wurde als Induktiv angesehen, weil von den Daten auf die Allgemeinheit geschlossen wird und dieser Schritt ist immer mit Unsicherheit behaftet.

Dem Gegenüber steht der Ansatz mit einer Nullhypothese $H_0$ und einer Altervativhypothese $H_1$ der den Arbeiten der Statistiker Jerzy Neyman (1894-1981) und Egon S. Pearson (1895-1980) beruht. Unter diesem System ist die Minimierung eines Entscheidungsfehler bei Wiederholung des Experiments die zentrale Kernidee. Wie können Entscheidungen getroffen werden die im Mittel die Anzahl der fehlerhaften Entscheidungen bei wiederholter Durchführung des Experiments minimieren? Aus diesem Ansatz haben Pearson-Neyman die Konzepte Typ-I und Typ-II Fehler, Power und Konfidenzintervalle entwickelt [@neyman1935; @lehmann1993]. Unter diesem System hat der p-Wert keine ausgezeichnete Bedeutung. Zentrall ist die Entscheidung die $H_0$ anzunehmen bzw. die $H_0$ abzulehnen um über **viele Wiederholungen des Experiments** hinweg optimale Entscheidungen zu treffen. Optimale Entscheidungen bedeutet möglichst wenige falsche Entscheidungen zu treffen. Der Hintergrund vor dem Neyman und Pearson die Theorie entwickelt haben, ist stärker durch einen industriellen Kontext geprägt um zum Beispiel bei einem Fließband sicher zu stellen, dass die Qualität der Produkte einem bestimmten Standard entspricht. In diesem Zusammenhang ist die *Wiederholung* von Experimenten, die zufällige Auswahl von Objekten, dementsprechend einfacher nachvollziehbar. Unter dem System von Pearson-Neyman ist die Durchführung eines Experiments dabei ohne die explizite Berechnung der Power und folglich der expliziten Aufstellung einer $H_1$ Hypothese bedeutungslos. Auch erklärt sich hier die Interpretation von Konfidenzintervallen als eine Überdeckungswahrscheinlichkeit bei Wiederholung des Experiments. Pearson-Neyman haben diesen Ansatz unter dem Begriff *inductive behavior* zusammengefasst. Das konkrete Niveau der Signifikanz, der p-Wert, ist weniger von Bedeutung, sondern nur welche Entscheidung getroffen wird und wie die Anzahl der Fehlentscheidungen minimiert werden können.

D.h. unter Fisher ist die Begriffe der Evidenz und der damit verbundene **p-Wert** und der **Signifikanz**test zentral. Dagegen sind unter Neyman-Pearson die Konzepte Fehler, $\alpha$ bzw. $\beta$, und **Hypothesen**test zentral. Damit verbunden ist auch beispielsweise, das das Konzept der Power unter Fischers Signifikanztest keine Rolle spielt, da keine $H_1$ vorhanden ist. Es lässt sich zeigen, dass diese beiden Ansätze auf Basis der gleichen Daten zu unterschiedlichen Aussagen kommen können, d.h. im Widerspruch zueinander stehen (@berger2003, @lehmann1993). Die Unterschiede in der Auffassung wie Daten statistisch bewertet werden sollen, hat zu einer regelrechten Fehde vor allem zwischen Fisher und Pearson geführt (@neyman1956, @loucca2008). Unglücklicherweise sind im Laufe der Jahre die beiden Ansätze bei Anwendern der Statistik immer mehr verschmolzen und haben dazu geführt, dass ein Großteil der wissenschaftlichen Analysen mit einem in sich widersprüchlichen statistischen System durchgeführt wird (@hubbard2003, @hager2013, @gigerenzer2004, @nuzzo2014). Ironischerweise sind beide Seiten sich einig darüber gewesen, dass das Signifikanzlevel bzw. der Fehlerlevel mit $5$% nicht sakrosankt sein ist, sondern entsprechend der Fragestellung angepasst werden sollte (@lehmann1993). Beide Ansätze haben zudem zwei Abstrahierungen in die statistische Analyse die sich nicht mit der Realität überein bringen lassen. Aus Fishers Ansatz ist dies das Konzept einer unendlich großen Population aus der Stichproben gezogen werden, während bei Neyman-Pearson die (unendlich oft) wiederholte Testung benötigt wird.

Die Verschmelzung der beiden Konzepte hat auch dazu geführt, dass der p-Wert und das $\alpha$-Level miteinander vermischt werden. Der p-Wert ist eine Aussage über die Daten unter der Annahme der $H_0$. Das $\alpha$-Level ist dagegen eine Aussage über die Fehlerrate über viele Experimente hinweg. Bei einem einzelnen Experiment wird entweder ein Fehler oder eben nicht gemacht. Ein p-Wert von $p = 0.03$ ist daher keine Aussage über Fehlerrate, sondern eine Aussage darüber mit welcher Wahrscheinlichkeit ein beobachteter oder ein extremerer Wert unter einer angenommenen $H_0$ auftritt. In dieser Aussage taucht das Konzept eines Fehlers nicht auf. Tatsächlich ist die Missinterpretation nicht nur unter AnwenderInnen sondern auch unter StatistikerInnen weit verbreitet (@hubbard2003). Allerdings sollte auch erwähnt werden, dass die Ansicht, dass die beiden Systeme nicht miteinander vereinbar sind nicht von allen geteilt wird [vgl. @mayo2018, S.173ff]


## Things to know

- $H_0$ und $H_1$
- $\alpha$-Fehler
- $\beta$-Fehler
- Power
- Konfidenzintervall
- Dualität von Konfidenzintervall und Hypothesentest 
- Konfidenzintervall - Faustregel
- Unterschied zwischen Signifikanztest und Hypothesentest

## Zum Nach- und Weiterlesen

Hier sind Artikel @hoekstra2014 und @greenland2016 die sich mit den wiederkehrenden Problem bei der Interpretation von Konfidenzintervallen beschäftigen.

