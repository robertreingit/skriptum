# Inductive Behavior

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

```{r stats_significance_defs}
n <- 20
set.seed(123)
world <- tibble(ID = paste0('P',stringr::str_pad(1:n, width=2, pad="0")),
                Kraft = sample(2000:2500, n))
world$Kraft[13] <- 1800
world$Kraft[17] <- 3200
d_gr <- 100
d_x <- 2 
mu_lummer <- 0
sd_lummer <- 230
sample_k9 <- readr::read_csv('data/sample_k9.csv')
```

## Was passiert wenn eine "andere" Hypothese zutrifft? 

In @fig-stats-sig-power-01 ist neben der uns schon bekannten Stichprobenverteilung unter der $H_0$ für unsere kleine Welt eine weitere Verteilung unter einer Alternativhypothese, die wir mit $H_1$ bezeichnen. Unter der $H_1$ ist der wahre Unterschied zwischen den beiden Verteilungen $\Delta = 500N$.

```{r}
#| fig-cap: "Differenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von $\\alpha$, wenn $H_0$ zutrifft, und unter einer Alternativhypothese $H_1: \\Delta = 500N$."
#| label: fig-stats-sig-power-01

differences <- readr::read_csv('data/combinations_differences.csv')
n_sim <- dim(differences)[1]
sigma <- sd(differences$d)
xx <- -750:1250
n_pts <- length(xx)
q_crit <- qnorm(0.975, 0, sd = sigma)
dat_power <- tibble(
  x = rep(xx,2),
  y = c(dnorm(xx,0,sigma), dnorm(xx,500,sigma)),
  hypo = rep(c("H0","H500"), c(n_pts, n_pts))
)
low <- tibble(x = seq(-750,-q_crit), y = dnorm(x, 0, sigma), hypo='H50')
up <- tibble(x = seq(q_crit, 750), y = dnorm(x, 0, sigma), hypo='H50')
ggplot(dat_power, aes(x,y,fill=hypo,ymin=0,ymax=y)) +
  geom_ribbon(alpha=.5) +
  geom_area(data = up, fill='red', alpha=0.8) +
  geom_area(data = low, fill='red', alpha=.8) +
  geom_line() +
  scale_fill_discrete('Hypothese') +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeit') 
```

In @fig-stats-sig-power-01 ist zu sehen, dass sich die beiden Verteilungen überschneiden. Der kritische Bereich unter der $H_0$ fängt etwa bei $500N$ an, sodass der Bereich zwischen etwa $100-400N$ unter beiden Verteilungen wohl relativ *wahrscheinlich* ist. Das heißt, wenn wir einen Wert in diesem Bereich beobachten würden, könnten wir nicht wirklich trennscharf argumentieren, aus welcher Stichprobenverteilung der Wert tatsächlich stammt. In @fig-stats-sig-power-02 haben wir den Bereich unter der $H_1$, der links des kritischen Bereichs von $H_0$ liegt, grün eingefärbt. Da es sich hier wieder um eine Fläche handelt, bestimmt diese Fläche eine Wahrscheinlichkeit. Wie könnte diese Wahrscheinlichkeit verbal beschrieben werden?

```{r}
#| fig-cap: "Differenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von $\\alpha$, wenn $H_0$ zutrifft, und $\\beta$ (grün), wenn $H_1$ zutrifft."
#| label: fig-stats-sig-power-02

beta <- tibble(
  x = -300:q_crit,
  y = dnorm(x, 500, sigma)
)
ggplot(dat_power, aes(x,y,fill=hypo,ymin=0,ymax=y)) +
  geom_ribbon(alpha=.5) +
  geom_area(data = up, fill='red', alpha=0.8) +
  geom_area(data = low, fill='red', alpha=.8) +
  geom_area(data = beta, fill='green', alpha=0.5) +
  geom_line() +
  scale_fill_discrete('Hypothese') +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeit') 
```

Die Werte zwischen den beiden kritischen Bereichen beschreiben diejenigen Werte, bei denen wir die $H_0$ beibehalten würden, da wir diese Werte nicht als *überraschend* unter der $H_0$ einschätzen. Dementsprechend würden wir uns irren, wenn in der Realität allerdings die $H_1$-Hypothese zutreffen würde. Daher beschreibt die grüne Fläche in @fig-stats-sig-power-02 ebenfalls die Wahrscheinlichkeit, sich zu irren – aber dieses Mal, wenn die Alternativhypothese $H_1$ zutrifft. Diese Irrtumswahrscheinlichkeit bezeichnet man als $\beta$-Wahrscheinlichkeit.

::: {#def-beta-error}
## $\beta$-Wahrscheinlichkeit

Die $\beta$-Wahrscheinlichkeit \index{$\beta$-Wahrscheinlichkeit} beschreibt die Wahrscheinlichkeit, sich gegen die Alternativhypothese $H_1$ zu entscheiden, wenn diese zutrifft. Die $\beta$-Wahrscheinlichkeit wird auch als Fehler II. Art bezeichnet.
:::

Wenn wir jetzt das Komplement der grünen Fläche nehmen, dann erhalten wir wiederum eine Wahrscheinlichkeit. In @fig-stats-sig-power-03 ist diese Fläche blau eingezeichnet.

```{r}
#| fig.cap: "$1-\\beta$ = Power des Tests (blaue Fläche)."
#| label: fig-stats-sig-power-03

beta <- tibble(
  x = -300:q_crit,
  y = dnorm(x, 500, sigma)
)
power <- tibble(
  x = q_crit:max(dat_power$x),
  y = dnorm(x, 500, sigma)
)
ggplot(dat_power, aes(x,y,fill=hypo,ymin=0,ymax=y)) +
  geom_ribbon(alpha=.5) +
  geom_area(data = up, fill='red', alpha=0.8) +
  geom_area(data = low, fill='red', alpha=.8) +
  geom_area(data = beta, fill='green', alpha=0.5) +
  geom_area(data = power, fill='blue', alpha=0.5) +
  geom_line() +
  scale_fill_discrete('Hypothese') +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeit') 
```

Diese Fläche beschreibt die Wahrscheinlichkeit, sich für die Alternativhypothese $H_1$ zu entscheiden, wenn diese auch tatsächlich zutrifft, und wird als die Power bezeichnet.

::: {#def-power}
## Power \index{Power}

Die Power bezeichnet die Wahrscheinlichkeit, sich für die Alternativhypothese $H_1$ zu entscheiden, wenn diese in der Realität zutrifft.
:::

Nochmal zu @fig-stats-sig-power-03: Die Werte unter der blauen Fläche werden mit einer Wahrscheinlichkeit beobachtet, die derjenigen der blauen Fläche entspricht. Jedes Mal, wenn so ein Wert eintritt, liegt dieser im kritischen Bereich unter der $H_0$ (rote Fläche) und wir entscheiden uns gegen die $H_0$.

Zusammengefasst haben wir die folgende Liste bezüglich der Terme $\alpha$, $\beta$ und Power:
- $\alpha$: Die Wahrscheinlichkeit, sich gegen die $H_0$ zu entscheiden, wenn die $H_0$ zutrifft. Das $\alpha$-Level wird vor dem Experiment festgelegt, um zu kontrollieren, welche Fehlerrate toleriert wird.
- $\beta$: Die Wahrscheinlichkeit, sich gegen die $H_1$ zu entscheiden, wenn die $H_1$ zutrifft.
- Power := $1 - \beta$: Die Wahrscheinlichkeit, sich für die $H_1$ zu entscheiden, wenn die $H_1$ zutrifft. Sollte ebenfalls **vor** dem Experiment festgelegt werden.

Die verschiedenen Entscheidungsmöglichkeiten haben wir schon einmal zusammengefasst (siehe @tbl-stats-sig-power).

| Entscheidung\\Realität | $H_0$ | $H_1$ |
| --- | --- | --- |
| $H_0$ | korrekt | $\beta$ |
| $H_1$ | $\alpha$ | korrekt |

: Entscheidungsmöglichkeiten und Fehlerarten  {#tbl-stats-sig-power}

## Wie kann die Power erhöht werden? 

Die Frage, die sich nun stellt, ist: Wie kann wir die Power erhöht werden? In @fig-stats-sig-power-04 haben sind nochmals die beiden Verteilungen bei $\Delta = 500$ und $\Delta = 0$ abgetragen.

```{r}
#| fig-cap: "Verteilungen wenn $\\Delta$=500 und $\\Delta$=0 in unserem kleine Welt Beispiel mit n = 3."
#| label: fig-stats-sig-power-04

dat <- tibble(
  di = c(differences$d + 500, differences$d),
  hypo = rep(c('H500','H0'), c(n_sim,n_sim))
)
p_h500 <- ggplot(dat, aes(di, fill=hypo)) +
  geom_density(alpha=0.5) +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeit') +
  scale_fill_discrete("Hypothese", labels=c(
    expression(H[0]  ), expression(H[500])
  ))
print(p_h500)
```

Eine Entscheidung ist speziell problematisch im Bereich zwischen $0$ und $500$. Hier überlappen die beiden Verteilungen, und es entstehen dementsprechend Schwierigkeiten, einen beobachteten Wert in diesem Bereich eindeutig einer der beiden Hypothesen zuzuordnen. Die Power kann daher erhöht werden, indem die Überlappung der beiden Verteilungen verkleinert wird. Dazu sind zwei Möglichkeiten vorhanden: Entweder der Unterschied zwischen den beiden $\Delta$s wird vergrößert (d.h., das Krafttraining müsste effizienter werden) oder die beiden Verteilungen werden schmaler gemacht, indem die Streuung der Werte verkleinert wird.

In @fig-stats-sig-power-05 sind alle Parameter gleich geblieben wie in @fig-stats-sig-power-04, aber die Stichprobengröße wurde von $n = 3$ auf $n = 9$ erhöht.

```{r}
#| fig-cap: "Stichprobenverteilungen der Differenz unter $H_0$ und $H_1:\\delta=500$N bei einer Stichprobengröße von n = 9"
#| label: fig-stats-sig-power-05

sample_k9 <- readr::read_csv('data/sample_k9.csv')
sigma <- sample_k9$sd[1]
d <- sample_k9$m[2]
xx = seq(-4*sigma,d+4*sigma)
n_pts = length(xx)
dat_k9 <- tibble(
  x = rep(xx,2),
  y = c(dnorm(xx, 0, sigma), dnorm(xx, d, sigma)),
  hypo = rep(c('H0','H500'), c(n_pts, n_pts))
)
ggplot(dat_k9, aes(x,y,fill=hypo, ymin=0, ymax=y)) +
  geom_ribbon(alpha=0.5) +
  geom_line() +
  scale_fill_discrete("Hypothese", labels=c(
    expression(H[0]  ), expression(H[500])
  )) +  
  lims(x = c(-750, 1250)) +
  labs(x = 'Differenzen[N]', y = 'relative Häufigkeiten') 
```

Es ist zu beobachten, dass die Position der Verteilungen gleich geblieben ist. Dies sollte nicht weiter verwundern, da immer noch die beiden Hypothesen $H_0: \Delta = 0$N und $H_1: \Delta = 500$N miteinander verglichen werden. Aber durch die Erhöhung der Stichprobengröße ist die Streuung der $D$s unter beiden Hypothesen kleiner geworden. Dies führt dazu, dass die Verteilungen nun steiler sind und dementsprechend weniger stark überlappen. Die Streuung der Statistik wird, wie schon oben erwähnt, als Standardfehler $s_e$ bezeichnet und beschreibt die Standardabweichung der Statistik. Der Standardfehler ist **nicht** gleich der Standardabweichung in der Population bzw. der Stichprobe (siehe @fig-stats-sig-standard-error-deriv).

```{r}
#| label: fig-stats-sig-standard-error-deriv
#| fig-cap: "Zusammenhang zwischen den Parametern der Population, der Stichprobe und dem Standardfehler"

df <- tibble(x = c(0, 1), y = c(0,1))
df_n1 <- tibble(
  x = seq(0,.6,length.out=100),
  y = 0.1*dnorm(x, 0.3, 0.12)
)
df_n2 <- tibble(
  x = seq(1.6, 2., length.out=100),
  y = 0.04*dnorm(x, 1.8, 0.05)
)
df_sample <- tibble(
    y = 0,
    x = runif(13, 0.8, 1.2) 
)

ggplot(df, aes(x,y)) +
  ggforce::geom_circle(data = tibble(x = 1, y = 0.9, r = 0.05),
             aes(x0 = x, y0 = y, r = r), fill='light blue') + 
  geom_curve(data = tibble(
    x = .3, y = .7, xend = .95, yend=.95
  ),
  aes(x, y, xend=xend, yend=yend), curvature=-.5,
  arrow = arrow(length=unit(.1, 'inches'), type='closed')) +
  ggforce::geom_circle(data = tibble(x = 0.3, y = 0.7, r = 0.3),
             aes(x0 = x, y0 = y, r = r), fill='light blue') +
  geom_point(data = df_sample) +
  geom_point(x = mean(df_sample$x), y = 0, color='red', size=2) +
  geom_segment(data = tibble(
    x = 1.6, xend = 2,
    y = seq(.6, 1, length.out=10),
    yend = y
  ), aes(x=x, y=y, xend=xend, yend=yend), linetype='dotted') +
  geom_point(data = tibble(
    x = runif(10, 1.7, 1.9),
    y = seq(.6, 1, length.out=10)
  ), color='red') +
  geom_segment(data = tibble(x=.3, y=0, yend=.35), aes(x,y,xend=x,yend=yend), color='red',
               linetype = 'dashed') +
  geom_segment(data = tibble(x=.15,xend=.45,y=.15), aes(x,y,xend=xend,yend=y), color='green',
               linetype='dashed',
               arrow = arrow(length=unit(.1, 'inches'), type='closed', ends='both')) +
  geom_segment(data = tibble(x=1.8, y=0, yend=.35), aes(x,y,xend=x,yend=yend), color='red',
               linetype = 'dashed') +
  geom_segment(data = tibble(x=1.75,xend=1.85,y=.15), aes(x,y,xend=xend,yend=y), color='green',
               linetype='dashed',
               arrow = arrow(length=unit(.1, 'inches'), type='closed', ends='both')) +
  geom_line(data = df_n1) + 
  geom_line(data = df_n2) + 
  geom_curve(data = tibble(x=1, y=0.64, xend=mean(df_sample$x), yend=0.05),
             aes(x,y,xend=xend,yend=yend), curvature = -0.1,
             arrow = arrow(length=unit(.1, 'inches'), type='closed')) +
  geom_curve(data = tibble(x=mean(df_sample$x), y=0.05, xend=1.55, yend=1),
             aes(x,y,xend=xend,yend=yend), curvature = -0.2,
             arrow = arrow(length=unit(.1, 'inches'), type='closed'),
             linetype = 'dotted') +
  geom_curve(data = tibble(x=.95, y=0.85, xend=0.95, yend=.75),
             aes(x,y,xend=xend,yend=yend), curvature = 0.3,
             arrow = arrow(length=unit(.08, 'inches'), type='closed')) +  
  annotate("text", x = 0.3, y = 0.7,
           label=expression(paste('\U03BC, ',sigma)), size=5) +
  annotate("text", x = 1, y = 0.7,
           label=expression(paste(bar(x),', s')), size=5) +
  annotate("text",
           x = c(0.3, 1,  1.8, 1.8, 1,.3),
           y = c(1.1, 1.1, 1.1,-.1,-.1,-.1),
           label= c("Population","Stichprobe","Mehrere Stichproben",
                    "Stichprobenverteilung", "Daten", "Verteilung")) +
  annotate("text", x = 1.8, y = .4,
          label=expression(paste("Standardfehler ",s[e])), size=5) +
  scale_x_continuous(limits = c(0,2.3)) +
  scale_y_continuous(limits = c(-0.1,1.2)) +
  coord_equal() + theme_void()
```

Warum führt die Erhöhung der Stichprobengröße dazu, dass die Verteilungen steiler werden? Die Statistik die in diesen Fällen betrachtet wurde war der Unterschied $d$ zwischen den Gruppen. Durch die Erhöhung der Stichprobe kommt es nun dazu, dass dieser Unterschied $d$ weniger stark schwankt, denn dadurch, dass mehr Probandinnen betrachtet werden, ist die Chance das zwei extreme Mittelwerte in den beiden Gruppen auftachen kleiner. Die Vergrößerung der Stichprobe führt dazu, dass extreme Werte eher *rausgemittelt* werden. Dies führt dann dazu, dass wenn die Mittelwerte weniger schwanken auch die Unterschiede zwischen den beiden Mittelwerten weniger schwanken. In der Folge ist die Standardabweichung der Differenzen $d$, der Standardfehler kleiner und die beiden Verteilungen unter der $H_0$ und der $H_1$ respektive werden steiler.

::: {.callout-note}

Für den Standardfehler des Mittelwerts, also wenn die Statistik der Mittlwert ist, dann gilt der folgende Zusammenhang:

| Population | Stichprobe |
| --- | --- | 
| $\sigma_{\bar{X}}=\sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}$ | $s_e=\sqrt{\frac{s^2}{n}}=\frac{s}{\sqrt{n}}$ | 

: Standardfehler des Mittelwerts, n = Stichprobengröße {#tbl-stats-sig-standard-error}

In @tbl-stats-sig-standard-error ist zu sehen, dass ein wurzelförmiger Zusammenhang zwischen dem Standardfehler $s_e$ und der Stichprobengröße besteht. Dieser Zusammenhang wird uns in verschiedenen Berechnungen zum Standardfehler verschiedener Statistiken immer wieder begegnen.

:::

::: {#exm-se-average}
## Standardfehler des Mittelwerts

Die Gleichheit des Standardfehler des Mittelwerts mit der Standardabweichung der Stichprobenverteilung der Mittelwerte ist im folgenden Beispiel einfach zu nachzuvollziehen. Dazu werden $1000$ Stichproben der Größe $N = 10$ aus einer Normalverteilung mit $\mu = 0$ und $sigma=2$ gezogen und für jede Stichprobe wird der Mittelwert berechnet. Der theoretische Standardfehler ist:

\begin{equation*}
s_e = \frac{\sigma}{\sqrt{n}} = \frac{2}{\sqrt{10}} \approx 0.63
\end{equation*}
```{r}
#| echo: true

n_sim <- 1000
N <- 10
mu <- 0
sigma <- 2
x_bars <- replicate(n_sim, mean(rnorm(N, mean=mu, sd=sigma)))
sd(x_bars)
```

Es ist zu beobachten, dass der empirische Standardfehler, im Rahmen der Stichprobenvariabilität, den theoretischen Wert tatsächlich sehr gut approximiert. Bei mehr Simulationsdurchgängen wird entsprechend der Unterschied immer kleiner.
:::

```{r}
#| label: fig-stats-sig-sqrtfcn
#| fig-cap: "Funktionaler Zusammenhang zwischen $x$ und $\\sqrt{x}$."

tibble(x = seq(0, 100), y = sqrt(x)) |> 
  ggplot(aes(x,y)) +
  geom_line() +
  labs(x = 'x', y = expression(y==sqrt(x)))
```

In @fig-stats-sig-sqrtfcn ist zur Erinnerung aus der Schule die Funktion $y = \sqrt{x}$ abgetragen. Daran sehen wir, dass die Funktionswerte für kleine Werte von $x$ steil ansteigen und später dann anfangen, immer langsamer größer zu werden. Angewendet auf unsere Power-Frage: Wenn der Standardfehler mit der Wurzel der Stichprobengröße $n$ kleiner wird, dann ist dies besonders bei kleinen Stichprobengrößen von Bedeutung, also zum Beispiel der Unterschied zwischen $n = 10$ und $n = 20$. Der gleiche Stichprobenunterschied dagegen zwischen $n = 110$ und $n = 120$ fällt nicht mehr ganz so stark ins Gewicht. Das heißt, bei kleinen Stichproben sollte um jede zusätzliche Teilnehmerin bzw. jeden zusätzlichen Teilnehmer gekämpft werden. Allgemein gilt: Je größer die Stichprobengröße, desto größer die Power.



Bisher haben wir die Daten mittels Hypothesentests betrachtet. D. h., wir haben eine $H_0$- und $H_1$-Hypothese formuliert und dann anhand der Stichprobenverteilung abgeschätzt, wie kompatibel unser beobachteter Wert mit der $H_0$-Hypothese ist. Dadurch haben wir eine dichotome Betrachtung der Daten durchgeführt: Entweder war der beobachtete Wert statistisch signifikant unter der $H_0$ oder eben nicht. Diese Unterteilung der Entscheidung in nur zwei verschiedene Ausgänge – neben dem Problem, dass wir eine Frage beantworten, die wir oftmals gar nicht gestellt haben – bringt jedoch einige grundlegende Nachteile mit sich, die wir nun genauer betrachten.

Das folgende Beispiel ist entnommen aus @cumming2013[p.1]. Wir haben zwei Forschergruppen, die Gruppen "Glücklich" und "Pech". Beide Gruppen haben das gleiche Experiment durchgeführt, eine Krafttrainingsintervention. Die Gruppe "Glücklich" hat insgesamt $N = 44$ Teilnehmer in zwei unabhängigen Gruppen untersucht, während die Gruppe "Pech" $N = 36$ Teilnehmer in zwei unabhängigen Gruppen untersucht hat. Die beiden Untersuchungen kamen zu den folgenden Ergebnissen (siehe @tbl-stats-est-prob).

| Gruppe | $D_{\text{MW}}\pm s_e$ | Statistik | p-Wert |
| --- | --- | --- | -- |
| Glücklich | $3.61 \pm 9.62$ | $t(42) = 2.43$ | $0.02$ |
| Pech | $2.23 \pm 8.66$ | $t(34) = 1.25$ | $0.14$ |

: Ergebnisse der Untersuchung der Forschergruppen Glücklich und Pech (D = Differenz) {#tbl-stats-est-prob}

Wenn wir eine Irrtumswahrscheinlichkeit von $\alpha = 0.05$ ansetzen, dann hat unter den beobachteten Daten nur die Gruppe "Glücklich" ein statistisch signifikantes Ergebnis, da der p-Wert $p = 0.02 < 0.05 = \alpha$ ist. Die Gruppe "Pech" dagegen hat kein statistisch signifikantes Ergebnis und kann mit $p = 0.14 > 0.05 = \alpha$ die $H_0$nicht ablehnen. Wenn wir nun die zu den beiden Untersuchungen gehörenden Veröffentlichungen lesen würden und die Ergebnisse streng dichotom betrachten würden, dann hätten wir zwei widersprüchliche Ergebnisse. Wir könnten versuchen zu erklären, dass die Stichprobengröße in "Pech" zu klein und vielleicht zu variabel war. Allerdings, wenn wir die Effektstärke aus dem Experiment von "Glücklich" ansetzen, dann hätte die Power auch für die Stichprobengröße von "Pech" ausgereicht (Power $> 0.9$), um relativ sicher ein statistisch signifikantes Ergebnis zu beobachten. Stellen wir die beiden beobachteten Effekte einmal graphisch dar.

```{r}
#| label: fig-stats-est-prob-01
#| fig-cap: "Gruppendifferenzen für die beiden Forschergruppen"
#| fig-height: 1.5

ggplot(tibble(x = c(3.61, 2.23), y = 1:2), aes(x, y)) +
  geom_point(size = 3) +
  geom_vline(xintercept = 0, linetype = 'dashed', color = 'red') +
  scale_x_continuous('Gruppendifferenz D', limits = c(-4, 4)) +
  scale_y_continuous('Gruppe', limits = c(0.9, 2.1),
                     breaks = 1:2, labels = c('Glücklich', 'Pech'))
```

Wenn wir uns die Differenzen in @fig-stats-est-prob-01 anschauen, dann sehen die Ergebnisse eigentlich gar nicht so widersprüchlich aus. Beide Effekte sind in die gleiche Richtung, nur die Effektstärke unterscheidet sich zwischen den beiden Gruppen. Wenn uns jemand zwingen würde, eine Abschätzung zu geben, wie groß der Effekt der Trainingsintervention ist, dann würden wir wahrscheinlich einen Wert zwischen den beiden beobachteten Werten angeben.

Schauen wir uns mal einen anderen Fall an.


```{r}
#| label: fig-stats-est-prob-02
#| fig-cap: "Beispiel für ein anderes Ergebnis der Gruppendifferenzen für die beiden Forschgruppen"
#| fig-height: 1.5

ggplot(tibble(x = c(3.61, -2.23), y = 1:2, ), aes(x,y)) +
  geom_point(size=3) +
  geom_vline(xintercept = 0, linetype = 'dashed', color = 'red') +
  scale_x_continuous('Gruppendifferenz D', limits = c(-4, 4)) +
  scale_y_continuous('Gruppe', limits = c(0.9, 2.1),
                     breaks = 1:2, labels = c('Glücklich','Pech'))
```

Hätten wir das Ergebnis aus @fig-stats-est-prob-02 beobachtet, dann würden wir wahrscheinlich schon eher von einem widersprüchlichen Ergebnis in der Literatur sprechen. Wenn wir wieder unter Zwang einen Wert angeben müssten, dann wahrscheinlich einen in der Nähe von $D = 0$. In beiden Fällen haben wir aber unter der rein dichotomen Betrachtung das gleiche Ergebnis (1 x statistisch signifikant + 1 x statistisch nicht signifikant). Daraus können wir schließen, dass wir bei der Interpretation von Forschungsergebnissen nicht nur die statistische Signifikanz betrachten sollten, sondern auch die Richtung der Effekte und deren Größe. Diese sollten ebenfalls einbezogen werden, wenn wir uns ein Bild über die Evidenz machen wollen. Dazu gehört natürlich ebenfalls eine Betrachtung des Forschungsdesigns und insbesondere der Stichprobengröße $N$.

::: {#exm-ci-01}

In @coleman2023 wurden die Auswirkungen von betreutem gegenüber unbetreutem Krafttraining auf Kraft und Hypertrophie bei trainierten Personen untersucht. $N = 36$ junge Männer und Frauen wurden zufällig einer von zwei experimentellen Gruppen zugewiesen, um ein 8-wöchiges Krafttrainingsprogramm durchzuführen: Eine Gruppe erhielt direkte Aufsicht während der Trainingseinheiten (supervision: SUP), während die Kontrollgruppe dasselbe Programm ohne Betreuung (unsupervised: UNSUP) durchführte. Während die meisten Ergebnisse keine Unterschiede zwischen den Gruppen andeuteten, zeigten die Ergebnisse eine erhöhte Zunahme der Gesamtkörpermuskelmasse um $0.54,\ 90\%CI[0.05, 0.98]$ kg. D. h., wenn wir als Zielgröße die Muskelmasse ansteuern wollen, müssen wir uns anhand des Intervalls überlegen, ob der Aufwand eines vollständig betreuten Trainings hinsichtlich eines möglicherweise minimalen Zuwachses von $\Delta = 0.05$ kg gerechtfertigt ist.

:::

## Welche Hypothesen sind kompatibel mit dem beobachteten Effekt?

Aus den Ausführungen von eben ist hoffentlich klar geworden, dass wir den beobachteten Effekt, also im Beispiel die Größe des Unterschieds, ebenfalls dokumentieren müssen, da er für die Interpretation des Ergebnisses wichtig ist. Letztendlich ist die Größe des Effekts auch ausschlaggebend, ob ich zum Beispiel eine Trainingsintervention mit meinen Athletinnen durchführe. Mir reicht es nicht zu wissen, dass ein statistisch signifikanter Effekt in Studien gefunden wurde; ich möchte auch die Größe des Effekts kennen.

```{r}
alpha <- 0.05
delta <- 500
mu <- delta
d_hat <- 350
s_hat <- 54 
#s_e <- round(s_hat/sqrt(9))
s_e <- round(s_hat*sqrt(2)/3)
c_i <- d_hat + c(-1,1)*qnorm(1-alpha/2)*s_e
```

Gehen wir zurück zu unserem Lummerland-Beispiel. Das Ergebnis der Intervention könnte beispielsweise die folgenden Stichprobenkennwerte bei $N = 9$ haben. Wir beobachten einen Unterschied von $D = \bar{x}_{treat} - \bar{x}_{con} = `r d_hat`$ zwischen den Gruppen. Beide Stichproben zeigen eine Standardabweichung von $s = `r s_hat`$, was zu einem Standardfehler von $s_e = `r s_e`$ führt (Rechnung unterschlagen). Dies ist unsere **Schätzung der Populationsparameter** $\mu$ und $\sigma$ anhand der Stichprobe. Der Standardfehler gibt uns eine Schätzung über die Präzision unseres Schätzwertes $D$. Wenn der Standardfehler $s_e$ sehr groß ist, dann bedeutet dies, dass wir den Wert nur sehr unpräzise geschätzt haben. Andersherum, wenn $s_e$ sehr klein ist. Daran schließt sich die Frage an, was die Präzision für meine Schätzung bedeutet. Präzision in diesem Zusammenhang bedeutet, welche anderen Werte neben dem beobachteten Wert ebenfalls plausibel genug sind, um den beobachteten Wert generiert zu haben. In unserem Beispiel: Welche anderen Unterschiedswerte $D$ sind anhand der beobachteten Daten ebenfalls plausibel?

Der Wert $D = `r d_hat`$ spricht dafür, dass der wahrscheinlichste Wert für $\Delta$ eben $\Delta = `r d_hat`$ ist. Allerdings ist es auch nachvollziehbar, dass der Wert $\Delta = 350 - 0.0000001$ ebenfalls nicht direkt unplausibel ist. Letztendlich zeigen wir mit Hilfe des Standardfehlers $s_e = `r s_e`$, dass unsere Schätzung eben mit einer Unsicherheit behaftet ist. Mit der gleichen Überlegung könnten wir daher den Wert $\Delta = 350 - \frac{`r s_hat`}{2}$ wahrscheinlich auch noch als nicht komplett abwegig begründen. Nach diesen Ausführungen müssen wir uns daher zunächst einmal überlegen, was genau "plausibel" in diesem Zusammenhang heißen könnte.

Steigen wir wieder mit der $H_0$ ein. Wir behalten die $H_0$ bei, wenn der beobachtete Wert der Teststatistik nicht in den kritischen Bereichen unter der Annahme der Korrektheit von $H_0$ liegt. Wenn der beobachtete Wert im kritischen Bereich liegt, dann sehen wir ihn unter der $H_0$ als überraschend oder eben nicht plausibel an. Diesen Ansatz können wir daher umdrehen, und umgekehrt, wenn der Wert nicht im kritischen Bereich liegt, dann ist er unter der $H_0$ plausibel und nicht überraschend. Dementsprechend können wir ein Kriterium für Plausibilität so definieren: Wenn der beobachtete Wert unter der angenommenen Hypothese nicht zur Ablehnung der Hypothese führt. Gehen wir jetzt von einem beobachteten Wert aus, dann können wir die Frage so stellen: Welche Hypothesen, wenn wir sie vorher als $H_0$ definiert hätten, würden unter dem beobachteten Wert nicht abgelehnt werden? Anders gesagt, wir suchen nun all diejenigen $H_0$-Hypothesen, die, wenn wir sie so vor dem Experiment angesetzt hätten, beibehalten worden wären.

In @fig-stats-est-ci-01 ist eine Verteilung und derjenige Bereich abgetragen, den wir bisher als nicht kritisch bezeichnet haben. D. h., alle Werte, die im grünen Bereich liegen, sind kompatibel mit der angesetzten $H_0$.

```{r}
#| label: fig-stats-est-ci-01
#| fig-cap: "Bereich, bei dem beobachtete Werte unter der Hypothese beibehalten werden"
#| fig-height: 2

mu <- 0; sigma <- 1; q <- 0.05
p_1 <- tibble(
x = seq(qnorm(q, mu, sigma), qnorm(1-q, mu, sigma), length.out = 100),
p = dnorm(x, mean = mu, sd = sigma)
) |> 
ggplot(aes(x, p)) +
  geom_ribbon(aes(ymin = 0, ymax = p), fill = 'green', alpha = .3) +
  geom_line(size = 2, color = 'green') + 
  scale_x_continuous('Werte der Teststatistik',
                     limits = c(-3, 3),
                     labels = c("","","", 0, "","",""),
                     breaks = -3:3) +
  scale_y_continuous('Dichte')
p_1
```

Fügen wir nun einen beobachteten Wert hinzu (@fig-stats-est-ci-02).

```{r}
#| label: fig-stats-est-ci-02
#| fig-cap: "Bereich, bei dem beobachtete Werte unter der Hypothese beibehalten werden, und ein beobachteter Wert"
#| fig-height: 2

p_1 + geom_point(data = tibble(x = -1, p = 0), color = 'red', size = 4)
```

Der Wert liegt im grünen Bereich und ist daher kompatibel mit der angesetzten Hypothese. Kompatibel definiert, als wir würden die Hypothese unter dem beobachteten Wert nicht ablehnen.

Halten wir den beobachteten Wert nun fixiert und schauen uns eine andere Hypothese $H_0^*$ an. Diese neue Hypothese $H_0^*$ ist nach links verschoben. Dies führt dementsprechend dazu, dass der mit der Hypothese kompatible Bereich ebenfalls nach links verschoben ist.


```{r}
#| label: fig-stats-est-ci-03
#| fig-cap: "Bereich bei dem beobachtete Werte unter der Hypothese $H_0^*$ beibehalten werden und der beobachtete Wert"
#| fig-height: 2

mu <- -2
tibble(
x = seq(qnorm(q, mu, sigma), qnorm(1-q, mu, sigma), length.out = 100),
p = dnorm(x, mean = mu, sd = sigma)
) |> 
ggplot(aes(x,p)) +
  geom_ribbon(aes(ymin = 0, ymax = p), fill = 'green', alpha=.3) +
  geom_line(size=2, color='green') + 
  geom_point(data = tibble(x = -1, p = 0), color = 'red', size=4) +
  scale_x_continuous('Werte der Teststatistik',
                     limits = c(-4,3),
                     labels = c("","","",0,"","",""),
                     breaks = -3:3) +
  scale_y_continuous('Dichte')
```

In @fig-stats-est-ci-03 ist nun der Bereich für die neue Hypothese $H_0^*$ abgetragen. Wie zu sehen ist, liegt der beobachtete Wert auch für diese Hypothese in dem Bereich, den wir unter der neuen Hypothese $H_0^*$ als unkritisch bzw. kompatibel betrachten. Diesen Ablauf können wir nun in beide Richtungen weiterführen. D. h., wir schieben systematisch $H_0$-Hypothesen von $-\infty$ bis $\infty$ und schauen, ob der beobachtete Wert in den unkritischen Bereichen für die jeweilige Hypothese liegt. Die kleinste Hypothese, die am weitesten links liegende Hypothese, bezeichnen wir nun als $H_{lower}$ und entsprechend die größte Hypothese, die am weitesten rechts liegende Hypothese, als $H_{upper}$. Dadurch haben wir jetzt einen Bereich von Hypothesen ausgezeichnet, mit der Eigenschaft, dass alle Hypothesen zwischen $H_{lower}$ und $H_{upper}$ kompatibel mit dem beobachteten Wert sind. Bei all diesen Überlegungen spielt der tatsächlich in der Population liegende Wert $\Delta$ keine Rolle!

Führen wir eine kleine Vereinfachung der graphischen Darstellung des kompatiblen Bereichs für eine gegebene Hypothese durch.

```{r}
#| label: fig-stats-est-ci-04
#| fig-cap: "Graphische Darstellung des kompatiblen Bereichs für eine Hypothese."
#| fig-height: 2

mu <- 0
tibble(
x = seq(qnorm(q, mu, sigma), qnorm(1-q, mu, sigma), length.out = 100),
p = dnorm(x, mean = mu, sd = sigma) + 1
) |> 
ggplot(aes(x, p)) +
  geom_ribbon(aes(ymin = 1, ymax = p), fill = 'green', alpha = .3) +
  geom_line(size = 2, color = 'green') + 
  geom_pointrange(data = tibble(xmin = qnorm(q, mu, sigma),
                                xmax = qnorm(1-q, mu, sigma),
                                x = 0,
                                p = 0.9), aes(xmin = xmin, xmax = xmax),
                  size = 1, color = 'green', linewidth = 2) +
  scale_x_continuous('Werte der Teststatistik',
                     limits = c(-3, 3),
                     labels = c("","","", 0, "","",""),
                     breaks = -3:3) +
  scale_y_continuous('Dichte')
```

In @fig-stats-est-ci-04 ist der mit den Daten kompatible Hypothesenbereich mittels einer Linie und eines Punktes dargestellt. Der Punkt zeigt den Wert der jeweiligen Hypothese an, während die Striche rechts und links die untere und obere Grenze des kompatiblen Bereichs darstellen. Die Grenzen des kompatiblen Bereichs sind eine Funktion des gewählten $\alpha$-Levels. Wenn mein $\alpha$ kleiner wird, dann dehnen sich die beiden Striche aus. Wenn mein $\alpha$ größer wird, dann verkürzen sie sich dementsprechend. Noch einmal: Wir verwenden $\alpha$, um kritische Bereiche zu definieren. Tragen wir nun einmal alle Intervalle systematisch von links nach rechts ab, deren Hypothesen mit dem beobachteten Wert (den Daten) kompatibel sind. Dies führt dann zu der Darstellung in @fig-stats-est-ci-05.

```{r}
#| fig-cap: "Kompatibles Intervall (grün), Populationsparameter $\\delta$ und $\\alpha$-Level für die beobachtete Differenz (gelb)."
#| label: fig-stats-est-ci-05

mu_s <- seq(150, 550, length.out = 30) 
q_s <- qnorm(alpha, mu_s, s_e)
df <- tibble(mu_s = mu_s,
             lu = qnorm(alpha/2, mu_s, s_e),
             up = qnorm(1-alpha/2, mu_s, s_e)) %>%
  dplyr::mutate(inside = dplyr::if_else(mu_s >= c_i[1] & mu_s <= c_i[2], 'ja', 'nein'))
ggplot(df, aes(x = mu_s, y = mu_s, ymin = lu, ymax = up, color = inside)) +
  geom_hline(yintercept = d_hat, color = 'yellow', linewidth = 1) +
  geom_hline(yintercept = c_i, color = 'green') +
  geom_hline(yintercept = delta, color = 'black') +
  geom_pointrange(size = 0.3) +
  labs(x = bquote('Mögliche'~delta), y = "") +
  scale_color_manual("plausibel", values = c('green', 'red')) +
  scale_y_continuous(breaks = seq(50, 700, 50)) +
  scale_x_continuous(breaks = NULL) +
  annotate("text", y = 550, x = 100, label = expression(delta == 500), size = 4) +
  annotate("text", y = 400, x = 100, label = expression(d == 350), size = 4) +
  coord_flip() 
```

Der beobachtete Wert ist in Gelb abgetragen, und wir schieben die Intervalle von links kommend nach rechts. In @fig-stats-est-ci-05 haben wir die Intervalle vertikal etwas versetzt, um sie besser sichtbar zu machen. Das erste Intervall, dessen rechtes Ende die gelbe Linie berührt, ist dabei die Hypothese mit dem geringsten $\delta_{\text{low}}$, die noch kompatibel mit den Daten ist. Das Gleiche gilt auf der rechten Seite mit der größten Hypothese $\delta_{\text{upper}}$, deren linker Rand die gelbe Linie gerade so noch streift.

Wenn wir nun die Werte $\delta_{\text{lower}}$ und $\delta_{\text{upper}}$ als die Randwerte eines Intervalls festlegen, dann sind alle Hypothesen zwischen diesen beiden Grenzen kompatibel mit den Daten. D. h., wenn einer dieser Werte initial unsere $H_0$ gewesen wäre, dann hätten wir die $H_0$ nicht abgelehnt.

Achtung, und das ist ein ganz großes Achtung: Das sind alles Aussagen über die beobachteten Daten. Das ist keine Aussage über die tatsächliche $H_0$, mit der wir in das Experiment gegangen sind. Das Verhältnis zwischen dieser $H_0$ und diesem Intervall schauen wir uns als Nächstes an. Vorher werfen wir noch einmal einen kurzen Blick auf @fig-stats-est-ci-05. In Schwarz ist der tatsächliche wahre Populationswert abgetragen. Wir sehen, dass in diesem Fall der wahre Wert auch innerhalb des Intervalls liegt.

Schauen wir uns nun an, wie das aussieht, wenn wir das Experiment mehrmals wiederholen.


```{r}
#| fig-cap: "Simulation von $n = 100$ Intervallen."
#| cache: false 
#| label: fig-stats-est-ci-06
#| fig-height: 5

foo <- function(mu = 500, se = 132, n = 20, alpha = 0.05) {
  sam <- rnorm(n, mu, se)
  x_hat <- mean(sam)
  s_e <- sd(sam)/sqrt(n)
  q <- qnorm(alpha/2)
  c(x_hat, x_hat + c(1,-1) * q * s_e)
}
N <- 100
set.seed(2)
c_is <- t(replicate(N, foo(mu, sigma, 20)))
colnames(c_is) <- c("x_hat","lo","up")
df_2 <- as_tibble(c_is) %>% dplyr::mutate(id = dplyr::row_number(),
                                   inside = dplyr::if_else((mu >= lo) & (mu <= up), 'ja','nein'))
ggplot(df_2, aes(id, x_hat, color = inside)) + 
  geom_pointrange(aes(ymin = lo, ymax = up), size=0.3) +
  geom_hline(yintercept = mu, color = 'black') +
  scale_color_manual("enthalten", values = c('green','red')) +
  labs(x = 'Experiment[#]', y = "d") +
  coord_flip()
```

In @fig-stats-est-ci-06 ist das Ergebnis von 100 Experimenten und den resultierenden Intervallen gezeigt. Der schwarze Strich zeigt wieder den wahren Populationswert an. Wir können erkennen, dass die Mehrheit der berechneten, kompatiblen Intervalle den wahren Wert auch tatsächlich enthält. Aber wieder Achtung: In der Realität führen wir das Experiment meistens nur einmal durch. Wenn ich jetzt die Frage stelle: *Was ist die Wahrscheinlichkeit, dass mein berechnetes Kompatibilitätsintervall den wahren Populationsparameter enthält?*

Nun, entweder der Wert ist in dem Intervall enthalten oder er ist nicht enthalten. Also entweder $P(\text{enthalten}) = 0$ oder $P(\text{enthalten}) = 1$. Daher ist die Frage, mit welcher Wahrscheinlichkeit das Intervall den wahren Parameter enthält, nicht wirklich sinnvoll. Ich würde hoffen, dass ich bei meiner Durchführung des Experiments eines der grünen Intervalle habe, aber ich kann mir nicht sicher sein. Vielleicht habe ich Pech gehabt und ausgerechnet eines der roten Intervalle bekommen.

Wir hatten vorhin hergeleitet, dass die Breite der Intervalle von unserem angesetzten $\alpha$ abhängt. Tatsächlich sagt dieses $\alpha$ etwas darüber aus, wie oft ich erwarten würde, bei großer (unendlicher) Wiederholung des Experiments, dass das Intervall den wahren Populationsparameter enthält. D. h., wenn ich ein $\alpha = 0.05$ ansetze, würde ich bei $1000$ Durchführungen erwarten, dass $950$ der Intervalle den wahren Populationsparameter enthalten und entsprechend $50$ Intervalle den Populationsparameter nicht enthalten. Noch einmal: Das ist eine Aussage über eine Wahrscheinlichkeit bei Wiederholung des Experiments. Eigentlich ist $1000$ auch nicht genug, sondern eher etwas in Richtung $\displaystyle{\lim_{N \to \infty}}$.

Nachdem wir jetzt lange genug um den heißen Brei herumgeredet haben: Das Intervall, das mit den Daten kompatibel ist, wird als Konfidenzintervall bezeichnet. Leider ist die Bezeichnung "Konfidenz" stark irreführend, da für Statistiker "Konfidenz" eben diese Eigenschaft bei Wiederholung des Experiments bezeichnet. In der Alltagssprache ist die Interpretation leider etwas abweichend. Die Aussage „Ich habe eine $95\%$ Konfidenz, dass das Intervall den wahren Populationsparameter enthält“ ist daher eine falsche Interpretation des Konfidenzintervalls, die oft anzutreffen ist. Korrekt ist: Wenn ich das Experiment sehr, sehr oft wiederhole, dann bin ich konfident, dass $95\%$ der Intervalle den wahren Populationsparameter enthalten. Für eine einzige Wiederholung mit eben nur einem resultierenden Konfidenzintervall ist die Wahrscheinlichkeit entweder $1$ oder $0$. Entweder der wahre Populationsparameter ist im Konfidenzintervall enthalten oder eben nicht.

Das Konfidenzintervall hat aber noch ein weiteres Merkmal. Wenn das Intervall sehr eng ist, dann ist nur eine eng umschriebene Menge von Hypothesen mit den Daten kompatibel. Andersherum, wenn das Intervall sehr breit ist, dann sind sehr unterschiedliche Hypothesen mit den Daten kompatibel. Die Breite ist dabei nicht nur vom $\alpha$-Niveau, sondern auch von der Streuung des Standardfehlers der Stichprobenstatistik abhängig. Wenn die Streuung sehr klein ist, dann wird für gegebenes $\alpha$ das Intervall schmaler, als wenn die Streuung sehr groß ist. Daher ist das Konfidenzintervall auch ein Merkmal für die Präzision der Schätzung mittels der Daten.

::: {#def-confidence}

### Konfidenzintervall \index{Konfidenzintervall}

Das Konfidenzintervall wird immer für ein gegebenes $\alpha$-Niveau berechnet. Das Konfidenzintervall gibt **nicht** die Wahrscheinlichkeit an, mit der ein *wahrer* Populationsparameter in dem Intervall liegt. Sondern, das Konfidenzintervall gibt alle mit den Daten **kompatiblen** Populationsparameter, alle kompatiblen $H_0$-Hypothesen, an.
Das $1-\alpha$-Niveau des Konfidenzintervalls gibt an, bei welchem Anteil von Wiederholungen davon auszugehen ist, dass das Konfidenzintervall den wahren Populationsparameter enthält. Die Breite des Konfidenzintervalls kann als eine Metrik für die **Präzision** der Schätzung angesehen werden.
							   
:::

Eine schöne Zusammenfassung zum Konfidenzintervall ist aus @spiegelhalter2019 [p.241]

1. We use probability theory to tell us, for any particular population
parameter, an interval in which we expect the observed statistic to lie
with 95% probability.
2. Then we observe a particular statistic.
3. Finally (and this is the difficult bit) we work out the range of possible
population parameters for which our statistic lies in their 95\%
intervals. This we call a "95\% confidence interval".
4. This resulting confidence interval is given the label "95\%" since, with
repeated application, 95% of such intervals should contain the true
value. Strictly speaking, a 95\% confidence interval does ***not*** mean there is a 95\% probability that this particular interval contains the true value [...]

All clear? If it isn’t, then please be reassured that you have joined
generations of baffled students.

Als kleine Vorschau eine relativ allgemein gültige Formel um Konfidenzintervalle zu berechnen. Glücklicherweise ist es nämlich nicht notwendig alle möglichen Hypothesen von links nach rechts wandern zu lassen um die untere und die obere Grenze des Intervalls zu bestimmen. Das Konfidenzintervall lässt sich oft mit Hilfe der folgenden Formel bestimmen:

\begin{equation}
\textrm{CI}_{1-\alpha} = \text{statistik} \pm q_{\alpha/2} \times s_e 
\label{eqn-stats-estim-ci-calc}
\end{equation}

Das Intervall ist immer eine Kombination aus einem berechneten Wert, der Statistik, beispielsweise dem Mittelwert $\bar{x}$, den zum $\alpha$-Niveau gehörenden Quantilen $q_{\alpha/2}$, oftmals den Quantilen der Standardnormalverteilung $\Phi(z)$, also $z_{\alpha/2}$, multipliziert mit dem Standardfehler $s_e$ der betrachteten Statistik. Unglücklicherweise, wenn dieses Intervall als Zufallszahl vor dem Experiment bestimmt wird, dann ist es tatsächlich eine Wahrscheinlichkeitsaussage. Aber wenn wir eine Realisierung der beteiligten Zufallszahlen, in diesem Fall der Intervallgrenzen, beobachtet haben, dann ist keine Aussage über die Wahrscheinlichkeit mehr sinnvoll, wie wir in @fig-stats-est-ci-06 gesehen haben.

:::{#exm-ci-calc}

Sei zum Beispiel die Statistik des Stichprobenmittelwerts der Körpergröße aus einer Stichprobe aus der Normalbevölkerung $\bar{x}$. Aus der Literatur können wir für die Bevölkerung eine Standardabweichung von $\sigma = 6$ herleiten. Dadurch ergibt sich ein Standardfehler von $s_e = \frac{\sigma}{\sqrt{N}} = \frac{6}{\sqrt{9}} = \frac{6}{3} = 2$. Wir beobachten nun in unserer Stichprobe von $N = 9$ einen Mittelwert von $\bar{x} = 170$ cm. Für die Quantile bei $\alpha = 0.05$ können wir die Standardnormalverteilung $\Phi(Z)$ heranziehen und erhalten für die Quantile $q_{\alpha/2} = q_{0.025} = -1.96$. Daraus folgt nach Formel \eqref{eqn-stats-estim-ci-calc} für das Konfidenzintervall:

\begin{equation*}
95\%CI = 170 \pm 1.96 \cdot 2 = [166.08, 173.92]\text{ cm}
\end{equation*}
:::

:::{#exm-ci-calc-2}
Hätten wir im vorhergehenden Beispiel keine Information über die Standardabweichung $\sigma$ in der Population gehabt, dann müssten wir die Standardabweichung anhand der Stichprobe abschätzen. In diesem Fall müsste diese zusätzliche Unsicherheit berücksichtigt werden, indem anstatt der Standardnormalverteilung $\Phi(z)$ die $t$-Verteilung verwendet wird. Hier würde sich dann ein Quantil von $q_{8,0.025} = -2.3$ ergeben. D. h., wir würden ein breiteres Konfidenzintervall erhalten. Sei beispielsweise die Standardabweichung $s$ in der Stichprobe $s = 6.3$. Dann würden wir das folgende Konfidenzintervall erhalten:

\begin{equation*}
95\%CI = 170 \pm 2.3 \cdot 2.2 = [164.9, 175.1]\text{ cm}
\end{equation*}
:::

::: {#exm-ci-calc-3}
In @grant1996 ist die Handkraft von $N=10$ Elite Kletterer der dominanten Hand in Newton $N$ ermittelt. Dabei wurden die folgenden Werte ermittelt worden (siehe @tbl-stats-est-ci-climb). 

```{r}
#| tbl-cap: "Deskriptive Werte der Kletterdaten."
#| label: tbl-stats-est-ci-climb

climb <- readr::read_csv('data/grip_strength_and_mass.csv') |> dplyr::filter(level == 'Elite')
q_t <- r_2(qt(0.975, 9))
climb_sum <- climb |> dplyr::summarize(m = r_2(mean(strength_N)),
                                       sd = r_2(sd(strength_N)),
                                       se = r_2(sd/dplyr::n()))
ci <- r_2(climb_sum$m + c(-1,1)*q_t*climb_sum$se)
climb_sum |> kable(col.names=c('Mittelwert','STD','se'), digits=2)
```

Das $95\%$ Konfidenzintervall für die Handkraft berechnet sich unter Verwendung der $t$-Verteilung mit $df = 9$ entsprechend nach:

\begin{equation*}
\text{CI}_{95\%} = `r climb_sum$m` \pm `r q_t` \cdot `r climb_sum$se` = [`r paste_com(ci)`]
\end{equation*}

Oder in `R` entsprechend mittels:

```{r}
#| echo: true

555.06 + c(-1,1) * qt(0.975, 9) * 9.65 
```

:::

:::{#exm-ci-calc-4}
Bezogen auf das Eingangsbeispiel mit den beiden Arbeitsgruppen "Glück" und "Pech" lässt sich für die beiden Konfidenzintervalle ergeben sich die beiden folgenden Konfidenzintervalle:
```{r}
lucky_d <- 3.61
lucky_std <- 9.62 
lucky_n <- 44 - 2
unlucky_d <- 2.23
unlucky_std <- 8.66 
unlucky_n <- 36 - 2
lucky_se <- round(lucky_std/sqrt(lucky_n), 2)
unlucky_se <- round(unlucky_std/sqrt(unlucky_n), 2)
lucky_q_t <- round(qt(0.975, lucky_n), 2)
unlucky_q_t <- round(qt(0.975, unlucky_n), 2)
lucky_ci <- round(lucky_d + c(-1,1) * lucky_q_t * lucky_se, 2)
unlucky_ci <- round(unlucky_d + c(-1,1) * unlucky_q_t * unlucky_se, 2)
ci_lucky <- paste(round(lucky_d + c(-1,1) * lucky_q_t * lucky_se, 2), collapse=',')
ci_unlucky <- paste(round(unlucky_d + c(-1,1) * unlucky_q_t * unlucky_se, 2), collapse=',')
```

\begin{align*}
CI_\text{Glücklich} &= `r lucky_d` \pm `r lucky_q_t` \cdot `r lucky_se` = [`r ci_lucky`] \\
CI_\text{Pech} &= `r unlucky_d` \pm `r unlucky_q_t` \cdot `r unlucky_se` = [`r ci_unlucky`] \\
\end{align*}

Hier ist zusehen, das das Konfidenzintervall für die Gruppe "Glücklich" die $H_0: \Delta=0$ nicht enthält, während dies für die Gruppe "Pech" der Fall ist. Aber selbst für die Gruppe "Pech" liegt ein großer Teil des Intervalls im positiven Bereich und somit nicht im direkten Widerspruch zu dem Befund von Gruppe "Glücklich" (siehe auch @fig-stats-estci-04).

```{r}
#| label: fig-stats-estci-04
#| fig-cap: "Konfidenzintervalle für die Gruppendifferenzen der beiden Forschergruppen"
#| fig-height: 1.2

ggplot(tibble(x = c(3.61, 2.23), y = 1:2,
              xmin=c(lucky_ci[1], unlucky_ci[1]),
              xmax=c(lucky_ci[2], unlucky_ci[2])), aes(x, y)) +
  geom_pointrange(aes(xmin=xmin, xmax=xmax)) +
  geom_vline(xintercept = 0, linetype = 'dashed', color = 'red') +
  scale_x_continuous('Gruppendifferenz D', limits = c(-4, 7)) +
  scale_y_continuous('Gruppe', limits = c(0.9, 2.1),
                     breaks = 1:2, labels = c('Glücklich', 'Pech'))
```

:::

Die Abhängigkeit des Standardfehlers $s_e$ spielt auch wieder hinsichtlich der Präzision eines Konfidenzintervalls eine entscheidende Rolle. Wir haben vorher schon erwähnt, dass der Standardfehler abhängig von der Stichprobengröße mit $\sqrt{N}$ entweder größer oder kleiner wird. D. h., mit steigender Stichprobengröße wird der Standardfehler $s_e$ mit $\sqrt{N}$ kleiner. Da der Standardfehler auch bei der Berechnung des Konfidenzintervalls verwendet wird, verkleinert sich mit zunehmender Stichprobengröße daher auch das Konfidenzintervall. D. h., die Präzision unserer beobachteten Werte nimmt mit der Größe $N$ der Stichprobe zu. Dies sollte auch heuristisch einleuchtend sein: Wenn die Stichprobengröße $N$ zunimmt, nimmt die Stichprobenvariabilität ab, und wir haben weniger Unsicherheit in unserer Statistik. Die kritischen Werte rücken enger zusammen, und entsprechend werden weniger Hypothesen mit dem beobachteten Wert kompatibel sein. Das Konfidenzintervall kann somit, wie der Standardfehler $s_e$, als ein Maß für die Unsicherheit in unserer Beobachtung interpretiert werden. Daher sollten wir bei der Planung unseres Experiments auch immer berücksichtigen, welche Präzision wir erreichen wollen, d. h., welche Breite des Konfidenzintervalls für uns noch tolerierbar ist, um eine Entscheidung auf der Basis der Daten zu treffen.

::: {.callout-tip}
Eine Faustregel für die Bestimmung des Konfidenzintervall lautet:
\begin{equation}
\textrm{CI}_{95\%} = \text{statistik} \pm 2 \cdot s_e 
\label{eqn-stats-estimate-ci-thumb}
\end{equation}
D.h. wenn wir unseren Schätzwert haben und dessen Standardfehler $s_e$ kennen, dann $2$mal den Standardfehler $\pm$ unseren Schätzwert ergibt ungefähr das resultierende $95\%$ Konfidenzintervall.
:::

## Dualität von Signifikanztests und Konfidenzintervall

Als letzten, vorläufigen Punkt zu Konfidenzintervallen noch ein Abschnitt zum Verhältnis von Konfidenzintervallen und Signifikanztests. Wenn das Konfidenzintervall mit Niveau $1-\alpha\%$ die $H_0$ nicht beinhaltet, dann bedeutet dies nach unserer Herleitung, dass diese Hypothese nicht mit den beobachteten Daten kompatibel ist. D. h., bei einem Signifikanztest wird die $H_0$ bei einer Irrtumswahrscheinlichkeit von $\alpha$ abgelehnt. Wenn ich also das Konfidenzintervall und die getestete $H_0$ kenne, dann kann ich eine Aussage darüber machen, ob das Ergebnis statistisch signifikant ist oder nicht. Andersherum, wenn ich weiß, dass das Ergebnis statistisch signifikant war oder eben nicht, dann weiß ich auch, ob die $H_0$ im Konfidenzintervall ist oder nicht. Es besteht eine Dualität zwischen beiden Konzepten, wobei mir das Konfidenzintervall deutlich mehr Informationen als der reine Signifikanztest gibt.

::: {#exm-ci-02}

In @fig-stats-estimate-ci-dual ist ein Beispiel abgetragen für ein hypothetisches Konfidenzintervall und der Wert der $H_0$-Hypothese.

```{r}
#| fig-cap: "Der erwartete Wert unter der $H_0$ (rot) und das beobachtete Konfidenzintervall zur beobachteten Statistik $D$."
#| fig-height: 1.3
#| label: fig-stats-estimate-ci-dual

d <- 2.2
df_1 <- tibble::tibble(
  x = d, 
  y = 1,
  xmin = x-2,
  xmax = x+2
)

ggplot2::ggplot(df_1,ggplot2::aes(x, y, xmin=xmin, xmax=xmax)) +
  ggplot2::geom_point(size=6) +
  ggplot2::geom_linerange(linewidth=2) +
  ggplot2::geom_vline(xintercept = 0, linetype='dashed', color='red', linewidth=1.3) +
  ggplot2::scale_x_continuous('Unterschied', breaks = c(0,d),
                              labels = c(expression(H[0]), 'D')) +
  ggplot2::scale_y_continuous('', breaks = NULL, limits = c(-1,3)) +
  ggplot2::theme(text = ggplot2::element_text(size=15))

```

Da die $H_0$-Hypothese nicht im Konfidenzintervall enthalten ist, können wir herleiten, dass ein statistisch signifikantes Ergebnis erzielt wurde.
:::

## (Exkurs) Power und false discovery rate

Der folgende Abschnitt betrachtet, wie sich die durchschnittliche Power auf die Literatur insgesamt auswirkt. Sei eine bestimmte Anzahl von Studien bzw. Veröffentlichungen gegeben. Ein Teil dieser Studien wird signifikante Ergebnisse dokumentieren, während der andere Teil keine signifikanten Ergebnisse gefunden hat. Innerhalb der signifikanten Ergebnisse wird ein Teil der Ergebnisse auf einem tatsächlichen Effekt beruhen, während der andere Teil zwar statistisch signifikant ist, aber eigentlich kein Effekt vorliegt – es wurde also ein Typ-I-Fehler begangen. Den Anteil der Studien, bei denen kein Effekt vorliegt, bezeichnen wir mit $P(H_0)$. Allgemeiner formuliert, ist das die Wahrscheinlichkeit für eine Studie mit einem Nulleffekt. Wenn $N$ die Gesamtzahl der betrachteten Studien bezeichnet, dann berechnet sich die Anzahl der Studien, bei denen ein Nulleffekt vorliegt, folgendermaßen:

\begin{equation*}
N_{H_0} = P(H_0) \cdot N
\end{equation*}

Werden diese Studien mit der Irrtumswahrscheinlichkeit $\alpha$ multipliziert, dann erhält man die Anzahl der Studien, die zwar einen Nulleffekt, also eigentlich keinen Effekt haben, aber aufgrund der Irrtumswahrscheinlichkeit *geirrt* haben und einen statistisch signifikanten Effekt beobachten.

\begin{equation*}
N_{H_0, \text{signifikant}} = P(H_0) \cdot \alpha \cdot N
\end{equation*}

Ein einfaches Beispiel wäre, wenn $N = 200$ Studien gegeben wären und $P(H_0) = 0.5$, dann liegt bei $100$ Studien kein Effekt vor. Aber aus diesen $100$ Studien werden sich $\alpha$ der Fälle irren. Dies führt dazu, dass daher $5$ statistisch signifikante Ergebnisse beobachtet werden. In @fig-stats-sig-fdr-01 ist dies graphisch veranschaulicht.

```{r}
#| fig-cap: "Anteil der signifikanten Nulleffekte (große Punkte) bei $N = 200$ Studien und $P(H_0) = 0.5$."
#| label: fig-stats-sig-fdr-01

fdr_1 <- tibble(
  x = rep(1:20,10),
  y = rep(1:10,each=20),
  h0 = rep(c("H_0","H_1"), each=100),
  sig = rep(1:0, c(5,195))
)
ggplot(fdr_1, aes(x,y, color=h0)) +
  geom_point() +
  geom_point(data = fdr_1 |> dplyr::filter(sig == 1),
             size=4) +
  scale_color_discrete(
    "Effekt",
    labels = c(expression(H[0]),expression(H[1]))) +
  theme_void()
```

Die gleichen Überlegungen können für diejenigen Studien gemacht werden, bei denen tatsächlich ein Effekt vorliegt. Die Anzahl der Studien mit einem Effekt ist genau das Komplement zu denjenigen, bei denen der Nulleffekt vorliegt. Formal:

\begin{equation*}
N_{H_1} = (1-P(H_0))\cdot N
\end{equation*}

Die Anzahl derjenigen Studien, bei denen ein Effekt vorliegt und die auch als statistisch signifikant beobachtet werden, ist abhängig von der Power $\rho$. Dementsprechend berechnet sich die Anzahl der statistisch signifikanten Studien, bei denen auch ein Effekt vorliegt, folgendermaßen:

\begin{equation*}
N_{H_1,\text{signifikant}} = (1-P(H_0))\cdot \rho \cdot N
\end{equation*}

Im Beispiel mit $N = 200$ Studien und $P(H_0) = 0.5$ würde bei $100$ Studien ein Effekt vorliegen und bei einer Power von $\rho = 0.8$ würde bei $80$ dieser Studien ein statistisch signifikantes Ergebnis beobachtet werden (siehe @fig-stats-sig-fdr-02).

```{r}
#| fig-cap: "Anteil der signifikanten Null- und $H_1$-Effekten (große Punkte) bei $N = 200$ Studien, $P(H_0) = 0.5$ und Power $\\rho = 0.8$."
#| label: fig-stats-sig-fdr-02

fdr_2 <- tibble(
  x = rep(1:20,10),
  y = rep(1:10,each=20),
  h0 = rep(c("H_0","H_1"), each=100),
  sig = c(rep(1:0, c(5,95)), rep(1:0, c(80,20)))
)
ggplot(fdr_2, aes(x,y, color=h0)) +
  geom_point() +
  geom_point(data = fdr_2 |> dplyr::filter(sig == 1),
             size=4) +
  scale_color_discrete(
    "Effekt",
    labels = c(expression(H[0]),expression(H[1]))) +
  theme_void()
```

Nun kann der Anteil aller Studien mit statistisch signifikanten Ergebnisse untersucht werden mit denjenigen bei denen zwar ein statisch signifikantes Ergebnis beobachtete wurde aber tatsächlich ein Nulleffekt zugrundeliegt ins Verhältnis zueinander gesetzt werden. Dies führt zu der sogenannten false discovery rate (fdr). Formal ist fdr wie folgt definiert (siehe @bartovs2022).

\begin{equation}
\begin{aligned}
\text{fdr} &= \frac{P(H_0)\cdot \alpha \cdot N}{P(H_0)\cdot \alpha \cdot N + (1-P(H_0))\cdot \rho \cdot N} \\
&=\frac{P(H_0)\cdot \alpha}{P(H_0)\cdot \alpha + (1-P(H_0))\cdot \rho}  
\end{aligned}
\label{eq-stats-sig-fdr}
\end{equation}

Die fdr bestimmt somit den Anteil der statistisch signifikanten Ergebnisse, bei denen es sich um einen falschen Befund handelt. Das heißt, der Anteil an Studien bei denen ein Effekt in Form eines statistisch signifikanten Ergebnisses beobachtet, obwohl eigentlich kein Effekt vorliegt. Es werden in @fig-stats-sig-fdr-02a die großen $H_0$-Punkte durch die Gesamtanzahl der großen Punkte, der signifikanten Punkte geteilt.

```{r}
#| fig-cap: "Signifikante Null- und $H_1$-Effekte bei $N = 200$ Studien, $P(H_0) = 0.5$ und Power $\\rho = 0.8$."
#| label: fig-stats-sig-fdr-02a

ggplot(fdr_2, aes(x,y, color=h0)) +
  geom_point(data = fdr_2 |> dplyr::filter(sig == 1),
             size=4) +
  scale_color_discrete(
    "Effekt",
    labels = c(expression(H[0]),expression(H[1]))) +
  lims(y = c(1,10)) +
  theme_void()
```

Die Formel \eqref{eq-stats-sig-fdr} für die fdr kann nun verwendet werden, um den Zusammenhang der false discovery rate mit der Power für eine gegebene Nulleffektswahrscheinlichkeit $P(H_0)$ und und eine gegebene Irrtumswahrscheinlichkeit $\alpha$ zu betrachten (siehe @fig-stats-sig-fdr-03).

```{r}
#| fig-cap: "Zusammenhang zwischen der Power $\\rho$ und der false discovery rate fdr bei $\\alpha = 0.05$."
#| label: fig-stats-sig-fdr-03

alpha <- 0.05
p_H_0 <- c(0.25, 0.5, 0.75, 0.8)
fdr <- function(rho, p_H_0 = 0.5, alpha = 0.05) p_H_0 * alpha/(p_H_0 * alpha + (1-p_H_0)*rho)
n <- 100
tibble(
  power = rep(seq(.1, .99, length.out = n),length(p_H_0)),
  p_H_0 = rep(p_H_0, each=n),
  fdr = fdr(power, p_H_0, alpha),
  p_H_0f = forcats::as_factor(p_H_0)
) |> 
ggplot(aes(power, fdr, color=p_H_0f, group=p_H_0f)) +
  geom_line() +
  scale_colour_discrete(expression(p(H[0]))) +
  labs(x = expression(paste("Power ",rho))) +
  lims(y = c(0,1))
```

In @fig-stats-sig-fdr-03 ist zu erkennen, dass die fdr besonders hoch ist, wenn die Power in der Literatur niedrig ist. Gleichzeitig steigt die fdr an, wenn die Wahrscheinlichkeit für keinen Effekt $P(H_0)$ ansteigt. In @fig-stats-sig-fdr-03 ist dieser Effekt besonders groß, wenn die Grundrate der tatsächlichen Effekte $1-P(H_0)$ niedrig ist bzw. eben $P(H_0)$ sehr hoch ist. Wenn nur $20\%$ der Studien tatsächlich Effekte zugrunde liegen, sind $2/3$ der Ergebnisse falsch positiv. Daher, wenn die mittlere Power in der Literatur in einer wissenschaftlichen Disziplin durch vorwiegend kleine Studien niedrig ist, sinkt die Wahrscheinlichkeit, dass ein gegebenes statistisch signifikantes Ergebnis tatsächlich durch einen Effekt hervorgerufen wurde. Für eine Forschungsdisziplin ist es daher wichtig, dass die Studien über eine ausreichende Power verfügen, um einerseits die gewünschten Effekte tatsächlich beobachten zu können und andererseits die Literatur durch falsch-positive Ergebnisse nicht zu verfälschen (siehe @button2013). Bei dieser Betrachtung fließen Probleme wie p-hacking oder HARKing noch gar nicht mit ein.

## Things to know

- $H_0$ und $H_1$
- $\alpha$-Fehler
- $\beta$-Fehler
- Power
- Konfidenzintervall
- Dualität von Konfidenzintervall und Hypothesentest 
- Konfidenzintervall - Faustregel
- Unterschied zwischen Signifikanztest und Hypothesentest

## Zum Nach- und Weiterlesen

Hier sind Artikel @hoekstra2014 und @greenland2016 die sich mit den wiederkehrenden Problem bei der Interpretation von Konfidenzintervallen beschäftigen.

