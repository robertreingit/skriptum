[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scriptum - Fortgeschrittene Statistik",
    "section": "",
    "text": "Vorwort\nDies ist das Skriptum für den Master-Statistikkurse Fortgeschrittene Statistik und ist die Vorlage für die Kurse LTC4 und SBG4. Es werden in den Kursen nicht alle Themen des Skriptums behandelt. Das Skriptum befindet sich derzeit noch in einem frühen Stadium, so dass die Inhalte noch nicht vollständig ausgearbeitet sind."
  },
  {
    "objectID": "stats_title.html",
    "href": "stats_title.html",
    "title": "Statistik",
    "section": "",
    "text": "Die erste Frage die sich im Umgang mit der Anwendung von Verfahren der Statistik stellt ist: Wofür benötigen wir Statistik überhaupt?\nBeispielsweise wurden ein Datensatz gesammelt, bei dem zwei Gruppen miteinander verglichen werden, eine Treatmentgruppe (TRT) und eine Kontrollgruppe (CON). In beiden Gruppen wurden jeweils \\(N_i = 20\\) Personen untersucht. Es wurde das folgende Ergebnis erhalten (siehe Abbildung 1).\n\n\n\n\n\nAbbildung 1: Boxplot der Kontroll- und der Treatmentgruppe bezüglich einer abhängigen Variable\n\n\n\n\nOffensichtlich sind die Werte in der Treatmentgruppe deutlich höher als diejenigen in der Kontrollgruppe. Warum ist es nicht ausreichend das offensichtliche zu dokumentieren? Warum ist eine statistische Analyse der Daten notwendig?\nDiese Fragestellung wird in dem folgenden Abschnitt untersucht. Gleichzeitig werden die notwendigen Werkzeuge entwickelt um die verschiendenen Schritte die einer statistische Analyse von Daten zugrundeliegenen zu verstehen und anwenden zu können."
  },
  {
    "objectID": "stats_basics.html#ein-experiment",
    "href": "stats_basics.html#ein-experiment",
    "title": "1  Eine kleine Welt der Unsicherheit",
    "section": "1.1 Ein Experiment",
    "text": "1.1 Ein Experiment\nWir wollen nun eine Krafttrainingsstudie durchführen um die Beinkraft zu erhöhen. Wir haben allerdings nur sehr wenige Ressourcen (bzw. wir sind faul) und können insgesamt nur sechs Messungen durchführen. Aus einem kürzlich durchgeführten Census haben wir aber die Kraftwerte der ganzen Population. Wir stellen die Kraftwerte zunächst mittels einer Tabelle dar (siehe Tabelle 1.1)\n\n\n\n\nTabelle 1.1: Kraftwerte (in Newton) der Lummerländer an der einbeinigen Beinpresse\n\n\nID\nKraft[N]\nID\nKraft[N]\n\n\n\n\nP01\n2414\nP11\n2243\n\n\nP02\n2462\nP12\n2497\n\n\nP03\n2178\nP13\n1800\n\n\nP04\n2013\nP14\n2152\n\n\nP05\n2194\nP15\n2089\n\n\nP06\n2425\nP16\n2090\n\n\nP07\n2305\nP17\n3200\n\n\nP08\n2117\nP18\n2196\n\n\nP09\n2298\nP19\n2485\n\n\nP10\n2228\nP20\n2440\n\n\n\n\n\n\nSelbst bei 20 Werten ist diese Darstellung wenig übersichtlich. Wir könnten zwar Zeile für Zeile durchgehen und nach etwas notieren und suchen würden wir sehen das der Maximalwert bei \\(3200\\)N für P17 und der Minimalwert von Person P13 bei \\(1800\\)N liegt. Aber wirklich einfach ist diese Darstellung nicht. Für solche univariaten Daten (uni = eins) kann eine übersichtlichere Darstellung mittels eines sogenannten Dotplots erreicht werden (siehe Abbildung 1.2).\n\n\n\n\n\nAbbildung 1.2: Dotplot der Lummerlandkraftdaten\n\n\n\n\nHier kann deutlich schneller abgelesen werden was das Minimum und das Maximum der Daten ist, sowie es kann auch direkt abgeschätzt werden in welchem Bereich sich der Großteil der Daten befindet. Allerdings wird durch diese Art der Darstellung die Information über welche Person die jeweiligen Werte besitzt nicht mehr dargestellt. Dies stellt in den meisten Fällen allerdings kein Problem dar, da wir in den meisten Fällen aussagen über die Gruppe und weniger über einzelne Personen machen wollen.\nGehen wir jetzt von der folgenden Fragestellung aus. Wir wollen den Gesundheitsstatus unserer Lummerländer verbessern und wollen dazu ein Krafttraining durchführen. Da evidenzbasiert arbeiten wollen, möchten wir überprüfen ob wirklich ein Verbesserung der Kraft durch das Training stattgefunden hat. Da es sich aber gleichzeitig um unsere selbst geschaffene Welt handelt führen wir natürlich ein perfektes Krafttraining, eine perfekte Intervention, durch. D.h wir stellen uns immer wieder als unwissend da und geben vor das wir gar nicht wissen, das das Training perfekt effektiv ist.\nD.h. wir führen gleichzeitig ein Gedankenexperiment durch. Wir führen ein Krafttraining für die Beine durch. Das Training ist perfekt und verbessert die Kraftleistung um genau \\(+100\\)N. Dieser Kraftzuwachs unabhängig davon welche Person aus unserer Population das Training durchführt (Warum ist das keine realistische Annahme?). Wir wollen zwei Gruppen miteinander vergleichen eine Interventionsgruppe und eine Kontrollgruppe. In beiden Gruppen sollen jeweils \\(n_{\\text{TRT}} = n_{\\text{CON}} = 3\\) TeilnehmerInnen bzw. Teilnehmer einbezogen werden da wir nicht mehr Ressourcen für mehr ProbandInnen haben.\nDie erste Frage die sich nun stellt ist wie wählen wir die sechs Personen aus unserer Population aus und wie teilen wir die sechs Personen in die beiden Gruppen? Nach etwas überlegen kommen wir darauf, dass wir am besten eine zufällige Stichprobe ziehen sollten (Warum?).\n\nDefinition 1.2 (Stichprobe) Eine Stichprobe ist eine Teilmenge der Objekte aus der Population.\n\n\nDefinition 1.3 (Zufallsstichprobe) Eine Zufallsstichprobe ist eine Teilmenge der Objekte aus der Population die zufällig ausgewählt wurde.\n\nDiese sechs Personen, unsere Stichprobe, wird dann wiederum zufällig auf die beiden Gruppen aufgeteilt.\nEin Zufallszahlengenerator hat die Zahlen \\(i = \\{3,7,8,9,10,20\\}\\) gezogen. Die entsprechenden Personen werden aus der Population ausgewählt und wiederum zufällig in die beiden Gruppen aufgeteilt (siehe Tabelle 1.2).\n\n\n\n\nTabelle 1.2: Zufällig ausgewählte Stichprobe der Kontrollgruppe (CON) und der Interventionsgruppe (TRT).\n\n\nID\nKraft[N]\nGruppe\n\n\n\n\nP08\n2117\nCON\n\n\nP09\n2298\nCON\n\n\nP03\n2178\nCON\n\n\nP07\n2305\nTRT\n\n\nP10\n2228\nTRT\n\n\nP20\n2440\nTRT\n\n\n\n\n\n\nMit diesen sechs Personen führen wir jetzt unser Experiment durch. Die drei Personen aus der Kontrollgruppe, unterlaufen im Interventionszeitraum nur ein Stretchtraining während die Interventionsgruppe zweimal die Woche für 12 Wochen unser perfektes Krafttraining durchführt. Nach diesem Zeitraum messen wir alle Personen aus beiden Gruppen und erhalten das folgende Ergebnis (siehe Tabelle 1.3).\n\n\nTabelle 1.3: Ergebnis der Intervention in Experiment 1 für die Kontroll- und die Interventionsgruppe.\n\n\n\n\n(a) Kontrollgruppe\n\n\nID\nKraft[N]\n\n\n\n\nP08\n2117\n\n\nP09\n2298\n\n\nP03\n2178\n\n\n\\(\\bar{K}\\)\n2198\n\n\n\n\n\n\n(b) Interventionsgruppe\n\n\nID\nKraft[N]\n\n\n\n\nP07\n2405\n\n\nP10\n2328\n\n\nP20\n2540\n\n\n\\(\\bar{K}\\)\n2424\n\n\n\n\n\n\nFür beide Gruppen ist jeweils der Mittelwert berechnet worden, um die Wert miteinander vergleichen zu können. Später werden wir noch weitere Maße kennenlernen die es ermöglichen zwei Mengen von Werten miteinander zu vergleichen.\n\nDefinition 1.4 (Mittelwert) Der Mittelwert über \\(n\\) Werte berechnet sich nach der Formel:\n\\[\n\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n}\n\\tag{1.1}\\]\nDer Mittelwert wird mit einem Strich über der Variable dargestellt.\n\nDamit lernen wir direkt auch ein neues Konzept kennen. Nämlich das der Statistik. Ein Wert der auf der erhobenen Stichprobe berechnet wird, wird als Statistik bezeichnet.\n\nDefinition 1.5 (Statistik) Ein auf einer Stichprobe berechnet Wert, wird als Statistik bezeichnet.\n\nUm jetzt Unterschied zwischen den beiden Gruppen zu untersuchen berechnen wir die Differenz D zwischen den beiden Mittelwerten \\(D = \\bar{K}_{\\text{TRT}} - \\bar{K}_{\\text{CON}}\\). Die Differenz kann natürlich auch in die andere Richtung berechnet werden und es würde sich das Vorzeichen ändern. Hier gibt es keine Vorgaben, sondern die Richtung kann frei bestimmt werden. Wenn bekannt ist in welcher Richtung der Unterschied berechnet wird, dann stellt dies keine Problem dar. Im vorliegenden Fall ziehen wir die Interventionsgruppe von der Kontrollgruppe ab, da wir davon ausgehen, dass die Intervention zu einer Krafterhöhung führt und wir dadurch einen positiven Unterschied erhalten (vgl. Gleichung 1.2)\n\\[\nD = 2424N - 2198N = 226 N\n\\tag{1.2}\\]\nDa der Wert D, wiederum auf den Daten der Stichprobe berechnet wird, handelt es sich ebenfalls um eine Statistik.\n\n\n\n\n\nAbbildung 1.3: Dotplot der beiden Stichproben. Senkrechte Striche zeigen die jeweiligen Mittelwerte an.\n\n\n\n\nIn Abbildung 1.3 sind die Werte der beiden Gruppen, deren Mittelwerte \\(\\bar{K}_{\\text{CON}}\\) und \\(\\bar{K}_{\\text{TRT}}\\) und der Unterschied \\(D\\) zwischen diesen abgebildet. Wie erwartet zeigt die Interventionsgruppen den höheren Kraftwert im Vergleich zu der Kontrollgruppe. Allerdings ist der Wert mit \\(D = 226\\) größer als der tatsächliche Zuwachs von \\(\\Delta_{\\text{Training}} = 100\\) (Warum ist das so?).\nDer Unterschied zwischen den beiden Gruppen ist natürlich auch zum Teil auf die Unterschiede die zwischen den beiden Gruppen vor der Intervention bestanden haben zurück zu führen. Was wäre denn passiert, wenn wir eine andere Stichprobe gezogen hätten?\nSei \\(i = \\{12,2,19,4,8,16\\}\\) eine zweite Stichprobe. Dies würde zu den folgenden Werten führen nach der Intervention führen.\n\n\n\n\nTabelle 1.4: Ergebnis der Intervention in Experiment 2 für die Kontroll- und die Interventionsgruppe.\n\n\nID\nKraft[N]\nGruppe\n\n\n\n\nP08\n2117\nCON\n\n\nP09\n2298\nCON\n\n\nP03\n2178\nCON\n\n\nP07\n2405\nTRT\n\n\nP10\n2328\nTRT\n\n\nP20\n2540\nTRT\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 1.4: Dotplot der beiden Stichproben in Experiment 2. Senkrechte Striche zeigen die jeweiligen Mittelwerte an.\n\n\n\n\nIn Abbildung 1.4 sind wiederum die Datenpunkte, Mittelwerte und der Unterschied abgetragen. In diesem Fall ist allerdings die Differenz zwischen den beiden Gruppen genau in der anderen Richtung \\(D = -308\\), so dass die Interpretation des Ergebnisses genau in der anderen Richtung wäre. Nämlich, nicht nur hat das Krafttraining zu keiner Verbesserung in der Kraftfähigkeit geführt, sondern zu einer Verschlechterung!\nEs hätte aber auch sein können, das wir noch eine andere Stichprobe gezogen hätten, z.B. \\(i = \\{6,5,7,20,14,16\\}\\). Dies würde zu dem folgenden Ergebnis führen (siehe Tabelle 1.5).\n\n\n\n\nTabelle 1.5: Mittelwertsdaten aus Experiment 3 und der Unterschied \\(D\\) zwischen den beiden Gruppenmittelwerten\n\n\nGruppe\nKraft[N]\n\n\n\n\nCON\n2308\n\n\nTRT\n2327\n\n\n\\(D\\)\n19\n\n\n\n\n\n\nIn diesem Fall haben wird zwar wieder einen positiven Unterschied zwischen den beiden Gruppen in der zu erwartenden Richtung gefunden. Der Unterschied von \\(D = 19\\) ist allerdings deutlich kleiner als das tatsächlichen \\(\\Delta = 100\\). Daher würden wir möglicherweise das Ergebnis so interpretieren, führen, dass wir das Krafttraining als ineffektiv bewerten würden und keine Empfehlung ausprechen.\nZusammengenommen, ist keines der Ergebnisse 100% korrekt. Entweder der Unterschied zwischen den beiden Gruppen ist deutlich zu groß, oder in der anderen Richtung oder deutlich zu klein. Das Ergebnis des Experiments hängt ursächlich damit zusammen, welche Stichprobe gezogen wird. Diese Einsicht gilt in jedem Fall generell für jedes Ergebnis eines Experiments.\nDas Phänomen, das der Wert der berechneten Statistik zwischen Wiederholungen des Experiments schwankt wird als Stichprobenvariabilität bezeichnet.\n\nDefinition 1.6 (Stichprobenvariabilität) Durch die Anwendung von Zufallsstichproben, variiert eine auf den Daten berechnete Statistik. Die Variabilität wird als Stichprobenvariabilität bezeichnet.\n\nStreng genommen, führt die Stichprobenvariabilität für sich genommen noch nicht dazu, das sich die Statistik zwischen Wiederholungen des Experiments verändert, sondern die zu untersuchenden Werte in der Population müssen selbst auch noch eine Streuung aufweisen. Wenn wir eine Population untersuchen würden, bei der alle Personen die gleiche Beinkraft hätten, würden unterschiedliche Stichproben immer den gleichen Mittelwert haben und wiederholte Durchführung des Experiment würden immer wieder zu dem selben Ergebnis führen. Dieser Fall ist in der Realität aber praktisch nie gegeben und sämtlich Parameter für die wir uns hier interessieren zeigen immer eine natürlich Streuung in der Population. Diese Streuung in der Population führt daher zu dem besagten Ergebnis, das das gleiche Experiment mehrmals wiederholt zu unterschiedlichen Zufallsstichproben führt und dementsprechend immer zu unterschiedlichen Ergebnissen führt.\nDaher ist eine der zentrale Aufgabe der Statistik mit dieser Variabilität umzugehen und die Forscherin trotzdem in die Lage zu versetzen rationale Entscheidungen zu treffen. Eine implizite Kernannahme dabei ist, das wir mit Hilfe von Daten überhaupt etwas über die Welt lernen können. D.h. das uns die Erhebung von Daten überhaupt auch in die Lage versetzt rationale Entscheidungen zu treffen. Entscheidungen wie ein spezialisiertes Krafttraining mit einer klinischen Population durchzführen oder eine bestimmte taktische Variante mit meiner Mannschaft zu trainieren um die Gegner besser auszuspielen. Alle diese Entscheidungen sollten rational vor dem Hintergrund von Variabilität getroffen werden und auch möglichst oft korrekte Entscheidungen zu treffen. Wie wir sehen werden, kann uns die Statistik leider nicht garantieren immer die korrekte Entscheidungen zu treffen. Nochmal auf den Punkt gebracht nach Wild und Seber (2000, p.28)\n\nThe subject matter of statistics is the process of finding out more about the real world by collecting and then making sense of data.\n\nUntersuchen wir jedoch zunächst unsere Einsicht, das Wiederholungen des gleichen Experiments zu unterschiedlichen Ergebnissen führt, weiter. In unserem Beispiel aus Lummerland haben wir nämlich den Vorteil, das uns die Wahrheit bekannt ist. In Abbildung 1.5 ist die Verteilung unsere bisheringen drei \\(D\\)s abgetragen.\n\n\n\n\n\nAbbildung 1.5: Bisherige Verteilung der Unterschiede \\(D\\)\n\n\n\n\nDie drei Werte liegen ja relativ weiter auseiander. Eien Anschlussfrage könnte jetzt sein: “Welche weiteren Werte sind denn überhaupt möglich mit der vorliegenden Population?”."
  },
  {
    "objectID": "stats_basics.html#die-stichprobenverteilung",
    "href": "stats_basics.html#die-stichprobenverteilung",
    "title": "1  Eine kleine Welt der Unsicherheit",
    "section": "1.2 Die Stichprobenverteilung",
    "text": "1.2 Die Stichprobenverteilung\nWir können jetzt ja einfach mal das Experiment anfangen zu wiederholen. In Abbildung 1.6 sind mal 15 verschiedene Stichproben abgetragen. Wir haben in jeder Zeile jeweils sechs TeilnehmerInnen gezogen. Drei für die Kontrollgruppe und drei für die Inervationsgruppe. Für jede dieser Zeilen können wir jeweils den Gruppenmittelwert berechnen und den Unterschied \\(D\\) bestimmen.\n\n\n\n\n\nAbbildung 1.6: Beispiele für verschiedene Möglichkeiten zwei Stichproben mit jeweils \\(n_i = 3\\) aus der Population zu ziehen\n\n\n\n\nWarum eigentlich bei 15 aufhören. Wir haben ja den Vorteil, das unsere Population relativ übersichtlich ist. Vielleicht können wir uns ja noch aus unserer Schulezeit an Kombinatorik erinnern. Da haben wir den Binomialkoeffizienten kennengelernt. Die Anzahl der möglichken Kombination von \\(k\\) Elementen aus einer Menge von \\(n\\) Elementen berechnet sich nach:\n\\[\n\\text{Anzahl} = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\tag{1.3}\\]\nIn unserem Fall wollen wir zunächst sechs Elemente aus \\(N = 20\\) auswählen und dann drei Elemente aus den sechs gezogenen Elementen auswählen um diese entweder der Interventionsgruppe oder der Kontrollgruppe zu zuweisen (Warum brauchen wir uns nur eine Gruppe anzuschauen?). Die Anzahl der möglichen Stichprobenkombinationen ist folglich:\n\\[\n\\text{Anzahl} = \\binom{20}{6}\\binom{6}{3} = 7.752\\times 10^{5}\n\\tag{1.4}\\]\nDas sind jetzt natürlich selbst bei dieser kleinen Population ein große Menge von einzelnen Experimenten, aber dafür sind Computer da, die können alle diese Experiment in kurzer Zeit durchführen. In Abbildung 1.7 ist die Verteilung aller möglichen Experimentausgänge, d.h. alle Differenzen \\(D\\) zwischen der Interventions- und der Kontrollgruppe, abgebildet.\n\n\n\n\n\nAbbildung 1.7: Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe bei einer Intervention mit \\(\\Delta = 100\\) (im Graphen mittels der roten Linie angezeigt).\n\n\n\n\nAuf der x-Achse sind die möglichen Differenzen \\(D\\) abgetragen, während auf der y-Achse die relative Häufigkeit, d.h. die Häufigkeit für einen bestimmten \\(D\\)-Wert geteilt durch die Anzahl \\(7.752\\times 10^{5}\\) aller möglichen Werte. Die Verteilung der D’s wird als Stichprobenverteilung bezeichnet.\n\nDefinition 1.7 Die Stichprobenverteilung kennzeichnet die Verteilung der beobachteten Statistik.\n\nDie Abbildung 1.7 zeigt, dass die überwiegende Anzahl der Ausgänge tatsächlich auch im Bereich von \\(\\Delta = 100\\) liegen. Noch präziser das Maximum der Verteilung, also die höchste relative Häufigkeit liegt genau auf der roten Linie. Dies sollte uns etwas beruhigen, denn es zeigt, das unsere Art der Herangehensweise mittels zweier Stichproben auch tatsächlich in den meisten Fällen einen nahezu korrekten Wert ermittelt. Allerdings zeigt die Stichprobenverteilung auch das Werte am rechten Ende die deutlich zu hoch sind wie auch Werte am linken Ende der Verteilung die deutlich in der falschen Richtung möglich sind. Das bedeutet, wenn wir das Experiment nur einmal durchführen wir uns eigentlich nie sich sein können, welches dieser vielen Experimente wir durchgeführt haben. Es ist zwar warscheinlicher, dass wir eins aus der Mitte der Verteilung durchgeführt haben, einfach da die Anzahl größer ist, aber wir haben keine 100% Versicherung, das wir nicht Pech gehabt haben und das Experiment ganz links mit \\(D = -500\\) oder aber das Experiment ganz rechts mit \\(D = 700\\) durchgeführt haben. Diese Unsicherheit wird leider keine Art von Experiment vollständig auflösen können. Eine weitere Eigenschaft der Verteilung ist ihre Symmetrie bezüglich des Maximums mit abnehmenden relativen Häufigkeiten umso weiter von Maximum \\(D\\) entfernt ist (Warum macht das heuristisch Sinn?).\nDie Darstellungsform von Abbildung 1.7 wird als Histogramm bezeichnet und eignet sich vor allem dazu die Verteilung einer Variablen z.B. \\(x\\) darzustellen. Dazu wird der Wertebereich von \\(x\\) zwischen dem Minimalwert \\(x_{\\text{min}}\\) und dem Maximalwert \\(x_{\\text{max}}\\) in \\(k\\) gleich große Intervalle unterteilt und die Anzahl der Werte innerhalb jedes Intervalls wird abgezählt und durch die Anzahl der Gesamtwerte geteilt um die relative Häufigkeit zu erhalten.\nZum Beispiel für die Werte:\n\\[\nx_i \\in \\{1,1.5,1.8,2.1,2.2,2.7,2.8,3.5,4 \\}\n\\] könnte das Histogram ermittelt werden, indem der Bereich von \\(x_{\\text{min}} = 1\\) bis \\(x_{\\text{max}} = 4\\) in vier Intervalle unterteilt wird und dann die Anzahl der Werte in den jewiligen Intervallen ermittelt wird (siehe Abbildung 1.8). Die ermittelte Anzahl würde dann noch durch die Gesamtanzahl \\(9\\) der Elemente geteilt um die relative Häufigkeit zu berechnen.\n\n\n\n\n\nAbbildung 1.8: Beispiel für die Darstellung eines Histogramms für die Daten \\(x_i\\).\n\n\n\n\nDie Form des Histogramms hängt davon ab wie viele Intervalle verwendet werden, so wird die Auflösung mit mehr Intervallen besser, aber es die Anzahl wird geringer und andersherum wird die Auflösung mit weniger Intervallen geringer aber die Anzahl der Elemente pro Intervall wird größer und somit stabiler. Daher sollte in den meisten praktischen Fällen die Anzahl variiert werden um sicher zu gehen, das nicht nur zufällig eine spezielle Darstellung verwendet wird.\nZurück zu unserer Verteilung von \\(D\\) unter \\(\\Delta = 100\\)N in Abbildung 1.7. Wie schon besprochen sind alle Werte zwischen etwa \\(D = -500N\\) und \\(D = 700\\)N plausibel bzw. möglich. Schauen wir uns doch einmal an, was passiert wenn das Training überhaupt nichts bringen würde und es keine Verbesserung gibt, also \\(\\Delta = 0\\).\n\n\n\n\n\nAbbildung 1.9: Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe wenn \\(\\Delta = 0\\) (rote Linie).\n\n\n\n\nDie Verteilung in Abbildung 1.9 sieht praktisch genau gleich aus, wie diejenige für \\(\\Delta = 100\\). Der einzige Unterschied ist lediglich das sie nach links verschoben ist und zwar scheinbar genau um die \\(100\\)N Unterschied zwischen den beiden \\(\\Delta\\)s. Dies ist letztendlich auch nicht weiter verwunderlich, bei der Berechnung des Unterschied \\(D\\) zwischen den beiden Gruppen kommen in beiden Fällen genau die gleichen Kombination vor. Bei \\(\\Delta = 100\\) wird aber zu der Interventionsgruppe das \\(\\Delta\\) dazuaddiert bevor die Differenz der Mittelwerte berechnet wird. Da aber gilt:\n\\[\nD = \\frac{1}{3}\\sum_{i=1}^3 x_{\\text{KON}i} - \\frac{1}{3}\\sum_{j=1}^3 (x_{\\text{TRT}j} + \\Delta) = \\bar{x}_{\\text{KON}} - \\bar{x}_{\\text{TRT}} + \\Delta\n\\]\nDaher bleibt die Form der Verteilung immer genau gleich und wird lediglich um den Wert \\(\\Delta\\) im Vergleich zur Nullintervention verschoben. Wobei mit Nullintervention Umgangssprachlich die Intervention bezeichnet, bei der nichts passiert also \\(\\Delta = 0\\) gilt."
  },
  {
    "objectID": "stats_basics.html#unsicherheit-in-lummerland",
    "href": "stats_basics.html#unsicherheit-in-lummerland",
    "title": "1  Eine kleine Welt der Unsicherheit",
    "section": "1.3 Unsicherheit in Lummerland",
    "text": "1.3 Unsicherheit in Lummerland\nDas führt jetzt aber zu einem Problem für uns. Gehen wir jetzt nämlich von diesen beiden Annahmen aus, das entweder die Intervention effektiv ist \\(\\Delta = 100\\) gilt oder das die Intervention nichts bringt also \\(\\Delta = 0\\) gilt. Wenn wir diese beiden Verteilungen übereinander legen erhalten wir Abbildung 1.10. Wir haben die Darstellung jetzt etwas verändert und eine Kurve durch die relativen Häufigkeiten gelegt. Dieser Graphen wird jetzt nicht mehr als Histogramm sondern als Dichtegraph bezeichnet.\n\n\n\n\n\nAbbildung 1.10: Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe wenn \\(\\Delta = 0\\) und \\(\\Delta = 100\\).\n\n\n\n\nIn Abbildung 1.10 ist klar zu sehen, dass die beiden Graphen zu großen Teilen überlappen und dazu noch in einem Bereich wo beide Ergebnisse ihrer höchsten relativen Häufigkeiten, also auch die größte Wahrscheinlichkeit haben unter den jeweiligen Annahmen aufzutreten. Unser Problem besteht jetzt darin, dass wir in der Realität gar nicht diese Information haben welchen Effekt unser Training auf die Stichprobe ausführt. Wenn wir dies wüssten, dann müssten wir das Experiment ja gar nicht durchführen. Wir haben im Normalfall nur ein einziges Ergebnis, nämlich den Ausgang unseres einen Experiments.\n\n\n\n\n\nAbbildung 1.11: Zuweisung eines beobachteten Unterschieds \\(D\\) nach einem Experiment\n\n\n\n\nWenn wir jetzt unser Experiment einmal durchgeführt haben und ein einziges Ergebnis für \\(D\\) erhalten haben, sei zum Beispiel \\(D = 50\\) dann haben wir ein Zuweisungsproblem (siehe Abbildung 1.11). Wie weisen wir unser Ergebnis jetzt den beiden möglichen Realität zu? Einmal kann es sein, das das Krafttraining aber auch gar nichts gebracht hat und wir haben lediglich eine der vielen möglichen Stichprobenkombination beobachtet haben die zu einem positiven Wert für \\(D\\) führt. Oder aber das Krafttraining ist effektiv gewesen und hat zu einer Verbesserung von \\(\\Delta = 100\\)N geführt und wir haben lediglich ein Stichprobenkombination aus den vielen möglichen Stichprobenkombination gezogen die zu einem Ergebnis von \\(D = 50\\) führt. Noch mal, in der Realität wissen wir nicht welche der beiden Annahmen korrekt ist und können es auch nie vollständig wissen. Denn egal wie viele Experimente wir machen, wir können immer den zwar unwahrscheinlichen aber nicht unmöglichen Fall haben, das wir nur Werte beispielsweise aus dem linken Teil der Verteilung beobachten. Das heißt wir haben immer mit einer Ungewissheit zu kämpfen. Wir können nicht im Sinne eines Beweises zeigen, das das Training effektiv ist.\nDie Methoden der Statistik liefern uns nun Werkzeuge an die Hand um trotzdem rational zu Entscheiden welche der beiden Annahmen möglicherweise wahrscheinlicher ist. Gleichzeitig ermöglicht uns die Statistik abzuschätzen respektive zu berechnen wie groß die Unsicherheit in dieser Entscheidung ist. Die Statistik sagt dabei immer nur etwas über die beobachteten Daten aus. Die Statistik sagt jedoch nichts über die zugrundeliegenden wissenschaftlichen Theorien aus.\nSchauen wir uns jetzt als vorläufig letzten Punkt an welche Entscheidungsmöglichkeiten wir haben."
  },
  {
    "objectID": "stats_basics.html#eine-entscheidung-treffen",
    "href": "stats_basics.html#eine-entscheidung-treffen",
    "title": "1  Eine kleine Welt der Unsicherheit",
    "section": "1.4 Eine Entscheidung treffen",
    "text": "1.4 Eine Entscheidung treffen\nWir hatten im Beispiel zwei verschiedene Annahmen, einmal das das Training nichts bringt und keine Verbesserung der Kraftfähigkeit folgt \\(\\Delta = 0N\\). Andererseits hatten wir das Beispiel gestartet damit, dass die Kraftfähigkeit um \\(100N\\) zunimmt, also \\(\\Delta = 100N\\). Wie bezeichnen jetzt diese beiden Annahmen als Hypothesen und bezeichnen \\(\\Delta = 0N\\) als die Nullhypothese \\(H_0\\) und \\(\\Delta = 100N\\) als die Alternativhypothese \\(H_1\\).\nWenn wir jetzt das Experiment durchgeführt haben, können wir uns also entweder für die \\(H_0\\) oder die \\(H_1\\) entscheiden. Aus Gründen der Symmetrie ist dies gleichbedeutend wenn wir uns nur auf die \\(H_0\\) fokussieren und entweder die \\(H_0\\) annehmen bzw. beibehalten oder verwerfen also uns gegen \\(H_0\\) entscheiden.\n\n\n\n\nTabelle 1.6: Entscheidungsmöglichkeiten wenn entweder \\(H_0\\) oder \\(H_{1}\\) zutrifft.\n\n\n\n\n\n\n\n\n\nRealität\n\n\n\n\n$H_0$\n$H_1$\n\n\n\n\n$H_0$\nkorrekt\n$\\beta$\n\n\n$H_1$\n$\\alpha$\nkorrekt\n\n\n\n\n\n\n\n\nIn Tabelle 1.6 sind die verschiedenen Entscheidungsmöglichkeiten abgetragen. In der Realität gehen wir, wie gesagt, von zwei Fällen aus. Entweder trifft die \\(H_0\\) oder die \\(H_1\\) zu. Wenn die \\(H_=\\) zutrifft und wir uns für die \\(H_0\\) entscheiden, dann haben wir eine korrekte Entscheidung getroffen. Wenn \\(H_0\\) zutrifft und wir allerdings die \\(H_0\\) ablehnen, also uns für die \\(H_1\\) entscheiden ist unsere Entscheidung falsch und wir begehen einen Fehler. Dieser Fehler wird als Fehler 1. Art bzw. \\(\\alpha\\)-Fehler bezeichnet. Trifft in der Realität dagegen die \\(H_1\\) zu und wir entscheiden uns gegen die \\(H_0\\) und für die \\(H_1\\), dann haben wir wiederum eine korrekte Entscheidung getroffen. Zuletzt, wenn die \\(H_1\\) zutrifft und wir uns aber für die \\(H_0\\) entscheiden, also die \\(H_0\\) beibehalten bzw. uns gegen die \\(H_1\\) entscheiden, treffen wir wieder eine falsche Entscheidung. Dieser Fehler wird als Fehler 2. Art, bzw. \\(\\beta\\)-Fehler bezeichnet.\n\nDefinition 1.8 Wenn eine Entscheidung gegen die \\(H_0\\) getroffen wird, obwohl die \\(H_0\\) korrekt ist, wird dies als \\(\\alpha\\)-Fehler bezeichnet.\n\n\nDefinition 1.9 Wenn eine Entscheidung gegen die \\(H_1\\) getroffen wird, obwohl die \\(H_1\\) korrekt ist, wird dies als \\(\\beta\\)-Fehler bezeichnet.\n\n\n\n\n\nWild, Christopher J, und Georg AF Seber. 2000. Chance encounters: A first course in data analysis and inference. Wiley Press."
  },
  {
    "objectID": "stats_significance.html",
    "href": "stats_significance.html",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "",
    "text": "3 Parameterschätzung"
  },
  {
    "objectID": "stats_significance.html#wie-treffe-ich-eine-entscheidung",
    "href": "stats_significance.html#wie-treffe-ich-eine-entscheidung",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.1 Wie treffe ich eine Entscheidung?",
    "text": "2.1 Wie treffe ich eine Entscheidung?\nIn unserem kleine Welt Bespiel waren wir in der komfortablen Position, das wir genau wussten was passiert bzw. welcher Prozess unseren beobachteten Datenpunkt erzeugt hat. D.h wir kannten den datengenerieren Prozesses.\n\nDefinition 2.1 (Datengenerierender Prozess (DGP)) Der Prozess in der realen Welt der die beobachteten Daten und damit die daraus folgende Statistik erzeugt wird als datengenerierender Prozess bezeichnet.\n\nLetztendlich zielt unsere Untersuchung, unser Experiment, darauf ab, Informationen über den DGP zu erhalten, weil diese Information uns erlaubt Aussagen über die reale Welt zu treffen. Dabei muss allerdings beachtet werden, dass dieser Prozess in den allermeisten Fällen ein starke Vereinfachung des tatsächlichen Prozesses in der Realität darstellt. Meistens sind die Abläufe in der Realität zu komplex um sie ins Gänze abzubilden. Somit wird fast immer nur ein Modell verwendet.\nZurück zu unseren Problem, wenn wir ein Experiment durchführen, dann haben wir normalerweise nur eine einzige beobachtete Statistik. In unseren bisherigen Beispiel also den berechneten Unterschied \\(D\\) in der Kraftfähigkeit nach der Intervention zwischen der Kontroll- und der Interventionsgruppe.\n\n\n\n\n\nAbbildung 2.1: Beobachteter Unterschied nach der Durchführung unseres Experiments\n\n\n\n\nIn Abbildung 2.1 ist der beobachtete Wert, \\(D = 50\\) abgetragen. Wir wissen von vorne herein, dass dieser Wert beeinflusst ist durch die zufällige Wahl der Stichprobe und die daran geknüpfte Streuung der Werte in der Population. Wie können wir den nun überhaupt eine Aussage treffen darüber, ob das Krafttraining was bringt oder vielleicht nur einen sehr kleinen Effekt zeigt oder möglicherweise sogar schädlich ist also zu einer Abnahme der Kraft führt?\nÜberlegen wir uns zunächst, welche Prozesse unseren beobachteten Wert zustande gebracht haben könnten. Wir haben schon zwei Prozesse kennengelernt, einmal den Prozess mit \\(\\Delta = 100\\) wie auch den Prozess mit \\(\\Delta = 0\\)\n\n\n\n\n\nAbbildung 2.2: Mögliche datengenerierende Prozesse für den beobachteten Unterschied \\(D\\) (rot)\n\n\n\n\nIn Abbildung 2.2 ist wieder unser beobachteter Wert \\(D = 50\\) und die beiden Verteilungen abgetragen. Leider können wir nicht eineindeutig sagen, welche der beiden Verteilungen, bzw. deren zugrundeliegende Prozesse, unseren beobachteten Wert erzeugt haben könnte. Da unser beobachteter Wert \\(D\\) genau zwischen den beiden Maxima der Verteilungen liegt. Etwas motiviertes Starren auf die Abbildung wird uns allerdings auf die Idee bringen, dass der beobachtete Wert nicht nur von diesen beiden Verteilungen erzeugt worden sein muss, sondern durchaus noch mehr Verteilungen in Frage kommen.\n\n\n\n\n\nAbbildung 2.3: Beispiele für weitere mögliche Verteilungen als DGP.\n\n\n\n\nAbbildung 2.3 zeigt, dass selbst die Verteilung mit \\(\\Delta = -250N\\) und \\(\\Delta = 350N\\) nicht unplausibel sind den beobachteten Wert erzeugt zu haben. Warum aber bei diesen fünf Verteilungen aufhören, warum sollte \\(\\Delta\\) nicht \\(-50\\) oder \\(127\\) sein. Und überhaupt, keiner kann behaupten die Natur kennt nur ganzzahlige Werte (siehe \\(\\pi\\)). Warum sollte \\(D\\) also nicht auch \\(123.4567N\\) sein?\nWenn diese Überlegung weitergeführt wird, dann wird schnell klar, dass letztendlich eine unendliche Anzahl von Verteilung in der Lage ist unseren beobachteten Wert plausibel zu generieren. D.h. wir haben ein Experiment durchgeführt und den ganzen Aufwand betrieben und haben wochenlang mit unseren ProbandInnen Krafttraining durchgeführt und sind hinterher eigentlich keinen Schritt weiter da wir immer noch nicht wissen was der datengenerierende Prozess ist. Also können wir selbst nach dem Experiment nicht sagen ob unser Krafttraining tatsächlich wirksam ist.\nZum Glück werden wir später sehen, das unser Unterfangen nicht ganz so aussichtslos ist. Schauen wir uns zum Beispiel die Verteilung für \\(\\Delta = -350N\\) an (Abbildung 2.4).\n\n\n\n\n\nAbbildung 2.4: Verteilung für \\(\\Delta = -350N\\) und der beobachtete Wert \\(D\\)\n\n\n\n\nUnser beobachteter Wert unter der Annahme das \\(\\Delta = -350N\\) ist nicht vollkommen unmöglich, aber so richtig wahrscheinlich erscheint er auch nicht. Der Wert liegt relativ weit am Rand der Verteilung. Die Kurve ist dort schon ziemlich nahe bei Null. D.h. der beobachtete Wert ist zwar durchaus möglich, aber es wäre schon überraschend wenn wir bei einer Durchführung des Experiments ausgerechnet so einen Wert beobachten würden wenn unsere angenommenes \\(\\Delta\\) korrekt ist.\nWenn wir jetzt dagegen von der Annahme ausgehen, dass dem DGP der Wert \\(\\Delta = 50N\\) zugrundeliegen würde, hätten wir die Verteilung in Abbildung 2.5. Zunächst ist dieser Wert möglich unter der Annahme. Zusätzlich liegt der beobachtete Wert mitten drin in dem Teil der Verteilung der auch zu erwarten wäre. D.h. der beobachtete Wert ist durchaus plausibel unter der Annahme und bei der einmaligen Durchführung des Experiments würde uns der beobachtete Wert nicht unbedingt überraschen.\n\n\n\n\n\nAbbildung 2.5: Verteilung für \\(\\Delta = 50N\\) und der beobachtete Wert \\(D\\)\n\n\n\n\nDiesen Ansatz können wir verwenden um mit Hilfe unseres Experiments doch etwas über den DGP auszusagen. Allerdings müssen wir uns noch einmal etwas eingehender mit Verteilungen auseinandersetzen um z.B. genauer zu bestimmen welche Ergebnisse uns überraschen würden. D.h. wir müssen uns erst ein mal ein paar neue Konzepte erarbeiten."
  },
  {
    "objectID": "stats_significance.html#lage--und-skalenparameter",
    "href": "stats_significance.html#lage--und-skalenparameter",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.2 Lage- und Skalenparameter",
    "text": "2.2 Lage- und Skalenparameter\nIn Abbildung 2.3 hatten wir mehrere Verteilungen abgebildet. Die Verteilung haben die gleiche Form sind aber gegeneinander verschoben. D.h. sie unterscheiden sich bezüglich ihrer Position bzw. Lage. Der Parameter der bei einer Verteilungen die Lage steuert ist der sogenannte Erwartungwerts \\(\\mu\\) der auch als Mittelwert bezeichnet wird. Dieser Mittelwert \\(\\mu\\) unterscheidet sich allerdings von dem uns bereits bekannten Mittelwert \\(\\bar{x}\\) in der Stichprobe. In einem späteren Abschnitt werden wir uns genauer anschauen wie der Mittelwert \\(\\mu\\) berechnet wird.\n\n2.2.1 Mittelwert \\(\\mu\\) der Population\nDa der Mittelwert \\(\\mu\\) die Position der Verteilung bestimmt, ist \\(\\mu\\) ein Parameter der Verteilung. Die Beschreibung als Parameter der Verteilung bedeutet somit, dass die Verteilung von \\(\\mu\\) abhängt, oder formaler das die Verteilung eine Funktion von \\(\\mu\\) ist. Wenn wir uns an Funktionen aus der Schule zurück erinnen wo wir Funktionen \\(f\\) von \\(x\\) kennengelernt haben und als \\(f(x)\\) dargestellt haben. Übertragen auf die Verteilung könnte dies mittels \\(f(\\mu)\\) dargestellt werden.\nBetrachten wir zwei Verteilungen die sich bezüglich ihrer Mittelwerte \\(\\mu\\) unterscheiden. Zum Beispiel sei \\(\\mu_1 = 0\\) und \\(\\mu_2 = 3\\). Wie in Abbildung 2.6 zu sehen ist, führt dies dazu, das die beiden Verteilungen gegeneinander verschoben sind.\n\n\n\n\n\nAbbildung 2.6: Verteilungen mit zwei unterschiedlichen Mittelwerten\n\n\n\n\nWie bereits erwähnt, wird der Mittelwert \\(\\mu\\) der Verteilung auch als Erwartungswert bezeichnet. Dies kann dahingehend interpretiert werden, das wenn Stichproben aus dieser Verteilungen gezogen werden, im Mittel der Wert \\(\\mu\\) erwartet werden kann. Soweit ist dies eigentlich noch nichts wirklich Neues, sondern hatten dies schon vorher gesehen, als wir alle möglichen Unterschiede zwischen der Kontrollgruppe und der Interventionsgruppe ermittelt haben. Hier war der Mittelwert der Verteilung genau derjenige Wert von \\(\\Delta\\).\nAn dieser Stelle nochmal der Unterschied zwischen \\(\\mu\\) und \\(\\bar{x}\\). Der Mittelwert \\(\\mu\\) ist eine Eigenschaft der Population, also letztendlich ein Wert den wir niemals kennen werden ohne die gesamte Population zu untersuchen. Der Mittelwert \\(\\bar{x}\\) ist eine Eigenschaft der Stichprobe aus der Population. Also der konkrete Wert den wir anhand der Stichprobe berechnen. In vielen Fällen versuchen wir über \\(\\bar{x}\\) einen Rückschluss auf \\(\\mu\\) zu ziehen.\n\n\n2.2.2 Standardabweichung \\(\\sigma\\) der Population\nAls zweite Eigenschaft von Verteilungen schauen wir uns jetzt die Streuung in der Population an. Die Streuung in der Population wird als Varianz bezeichnet und wird mit dem Symbol \\(\\sigma^2\\) bezeichnet. Schauen wir uns zunächst an, welchen Einfluss \\(\\sigma^2\\) auf die Form der Verteilung hat. In Abbildung 2.7 sind wieder zwei Verteilungen abgetragen. Dieses Mal ist \\(\\mu\\) in beiden Fällen gleich, aber die Varianzen \\(\\sigma^2\\) sind mit \\(\\sigma_1^2 = 2\\) und \\(\\sigma_2^2=1\\) unterschiedlich.\n\n\n\n\n\nAbbildung 2.7: Verteilungen mit unterschiedlichen Varianzen\n\n\n\n\nIn Abbildung 2.7 ist zu sehen, dass beide Verteilungen ihren Mittelpunkt an der gleichen Stelle haben, aber die rote Verteilung mit \\(\\sigma_1^2=2\\) breiter ist als die andere Verteilung. Dies bedeutet das die Werte in der Verteilung stärker um den Mittelwert herum streuen. Wenn wir Werte aus der türkisen Verteilung ziehen, dann sollten diese näher um den Mittelwert \\(\\mu = 0\\) liegen, als dies bei der roten Verteilung der Fall ist.\nDie Varianz \\(\\sigma^2\\) ist ebenfalls wie der Mittelwert ein Parameter der Verteilung. Sie bestimmt die die Form der Verteilung. D.h. wenn wir wieder unsere Schreibweise von eben verwenden und die Funktion \\(f\\) die Verteilung beschreibt, dann gilt \\(f(\\sigma^2)\\) oder eben zusammen mit dem Mittelwert \\(\\mu\\), \\(f(\\mu, \\sigma^2)\\).\nWenn aus der Varianz \\(\\sigma^2\\) die Wurzel gezogen wird, dann wird der resultierende Wert \\(\\sigma\\) als Standardabweichung bezeichnet. Da die Varianz \\(\\sigma^2\\) nur positive Werte annehmen kann, ist die Wurzelfunktion bzw. deren Umkehrung die Quadierung eineindeutig. Wenn wir die Standardabweichung kennen, dann kennen wir auch die Varianz und umgekehrt.\nIn der Stichprobe wird die Standardabweichung meistens mit dem Zeichen \\(s\\) bezeichnet und mittels der folgenden Formel berechnet:\n\\[\\begin{equation}\ns = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}}\n\\label{eq-std}\n\\end{equation}\\]\nD.h. die Standardabweichung ist die mittlere quadrierte Abweichung vom Mittelwert (siehe Formel \\(\\eqref{eq-std}\\)). Die Standardabweichung wird verwendet um die Streuung der Daten zu beschreiben. Die Standardabweichung hat den Vorteil, dass sie die gleiche Einheit hat wie der Mittelwert. Da die Abweichungen quadriert werden, also die quadrierten Einheiten haben, hat die Standardabweichung \\(s\\) die gleiche Einheit wie der Mittelwert \\(\\bar{x}\\). Da die Varianz die quadrierte Standardabweichung ist, hat die Varianz der Stichprobe \\(s^2\\) daher die quadrierten Einheiten.\nWenn wir uns an unsere erstes Beispiel aus der kleinen Welt erinnern, dort hatten wir in der Kontrollgruppe die Personen \\(i = \\{3,8,9\\}\\) gezogen, berechnen wir für diese Stichprobe die Standardabweichung erhalten mit dem Mittelwert \\(\\bar{x} = 2198\\):\n\\[\ns = \\sqrt{\\frac{(2178-2198)^2+(2117-2198)^2+(2298-2198)^2}{2}} = 92\n\\]\nWir erhalten einen Wert von \\(s = 92N\\). Wenn dieser Wert größer wird, dann streuen die Wert entsprechend weiter um den Mittelwert herum und entsprechend verringert sich die Streuung wenn die Standardabweichung \\(s\\) abnimmt."
  },
  {
    "objectID": "stats_significance.html#entscheidungen-und-mu-und-sigma",
    "href": "stats_significance.html#entscheidungen-und-mu-und-sigma",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.3 Entscheidungen und \\(\\mu\\) und \\(\\sigma\\)",
    "text": "2.3 Entscheidungen und \\(\\mu\\) und \\(\\sigma\\)\nZeichnen wir in eine Verteilung die Standardabweichung ein, ergibt sich vollgendes Bild (siehe Abbildung 2.8).\n\n\n\n\n\nAbbildung 2.8: Verteilung mit verschiedenen mehrfachen der Standardabweichung \\(\\sigma\\)$\n\n\n\n\nEin großteil der Werte liegt in dem Bereich \\(\\mu \\pm 1\\times\\sigma\\). Der Bereich \\(\\mu \\pm 2\\times\\sigma\\) beinhaltet schon fast alle Werte, während der Bereich \\(\\mu \\pm 3\\times\\sigma\\) fast alle Werte. Wenn wir die Verteilung noch etwas weiter nach links und rechts abtragen würden, würden wir sehen, dass auch noch Werte jenseits von \\(\\mu \\pm 3\\times\\sigma\\) liegen, aber nur noch sehr wenige. Diese Einsicht können wir dazu benutzen umgekehrt zu denken, wenn wir annehmen, das unsere Statistik dieser Verteilung folgt, welche Werte würde uns den überraschen. Welche Werte würden wir als Evidenz sehen um zu folgern: Ich glaube nicht, dass die beobachtete Statistik aus der angenommen Verteilung stammt!?\nNun, zum Beispiel wenn der Wert mehr als \\(3\\times\\sigma\\) vom Mittelwert \\(\\mu\\) entfernt ist, dann wäre das zwar nicht unmöglich, aber es wäre schon ziemlich unwahrscheinlich so einen Wert zu beobachten. Vielleicht ist uns das aber ein zu schwer zu erreichender Wert, ein Kompromiss könnte ein Wert jenseits von \\(2\\times\\sigma\\) von \\(\\mu\\) entfernt, könnte auch schon als überraschen bezeichnet werden. Tatsächlich ist, die Wahrscheinlichkeit einen Wert jenseits von \\(2\\times\\sigma\\) zu beobachten etwa 5%. D.h. wir könnten einen Entscheidungsprozess erstellen bei dem wir sagen, wenn wir eine bestimmte Stichprobenverteilung für unsere Statistik annehmen. Wenn wir bei unserer Ausführung einen Wert beobachten der weiter als \\(2\\times\\sigma\\) von \\(\\mu\\) entfernt sind. Dann sind wir überrascht und sehen das als Evidenz gegen die Verteilungsannahme an.\nOder als Liste:\n\nSetze eine Verteilung der Statistik mit definierten \\(\\mu\\) und \\(\\sigma\\) als Annahme an.\nZiehe eine Zufallsstichprobe.\nBerechne die Statistik auf der Stichprobe.\nÜberprüfe wie viele Standardabweichungen \\(\\sigma\\) die Statistik von \\(\\mu\\) entfernt liegt.\n\n\n2.3.1 Detour - Schätzer\nSchauen wir uns noch einmal den Mittelwert \\(\\mu\\) der Population und den Mittelwert \\(\\bar{x}\\) der Stichprobe und deren Zusammenhang an. Der Mittelwert \\(\\bar{x}\\) der Stichprobe wird als sogenannter Schätzer verwendet. Diesen Begriff werden wir später noch genauer untersuchen. Im Moment reicht es sich zu merken, dass ein Schätzer eine Statistik ist, mit der wir einen Parameter der Population, z.B. \\(\\mu\\), abschätzen wollen. Wie schon mehrmals erwähnt, den wahren Wert \\(\\mu\\) aus der Population werden wir mittels unserer Stichprobe niemals 100% korrekt bestimmen wir können aber mittels geschickt gewählter Statistiken Schätzer konstruieren die bestimmte Eigenschaften haben.\nNehmen wir zum Beispiel den Mittelwert \\(\\bar{x}\\). In unserer kleinen Welt kennen wir den Mittelwert \\(\\mu\\) unserer Population. Der Wert beträgt \\(\\mu = 2291.3\\). Schauen wir uns einmal an, was passiert, wenn wir alle möglichen Stichproben der Größe \\(N = 10\\) unserer kleinen Welt bestimmen und die Verteilung der Mittelwert abtragen (siehe Abbildung 2.9).\n\n\n\n\n\nAbbildung 2.9: Verteilung der Mittelwerte von Stichproben der Größe \\(n=10\\), Kleine Welt Population \\(\\mu\\) (rot)\n\n\n\n\nIn Abbildung 2.9 sehen wir, dass im Mittel der Stichprobenmittelwert \\(\\bar{x}\\) tatsächlich um den wahren Populationsmittelwert \\(\\mu\\) herum zentriert ist. Einzelne Ausgänge des Experiments können zwar daneben liegen, der Großteil der Experiment gruppiert sich jedoch um \\(\\mu\\) herum. Der Stichprobenmittelwert \\(\\bar{x}\\) ist daher eine gute Statistik um den tatsächlichen Populationsmittelwert \\(\\mu\\) abzuschätzen."
  },
  {
    "objectID": "stats_significance.html#welche-verteilung-setzen-wir-an",
    "href": "stats_significance.html#welche-verteilung-setzen-wir-an",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.4 Welche Verteilung setzen wir an?",
    "text": "2.4 Welche Verteilung setzen wir an?\nKommen wir aber wieder zurück zu unserem Ausgangsproblem, dass wir anhand unserer beobachteten Stichprobe etwas über die Effektivität der Kraftintervention aussagen wollen. Wie hilft uns jetzt die Kenntnis von Mittelwert \\(\\mu\\) oder \\(\\bar{x}\\) und der Standardabweichung \\(\\sigma\\) bzw. \\(s\\) weiter? Wenn die Verteilung unserer Statistik der Form folgt wie sie bisher jetzt mehrmals beobachtet haben, dann können wir davon ausgehen, dass wenn wir eher Wert in der Nähe des Mittelpunkts erwarten würden. Wie werden selten genau den Mittelpunkt beobachten aber wir würde schon sehr überrascht sein, wenn wir Werte weit ab des Mittelwerts beobachten würden. Ab welcher Weite diese Werte als überraschen eingestuft werden hängt dabei von der Streuung der Verteilung an. Wenn \\(\\sigma\\) groß ist, überraschen uns weit entfernte Werte weniger als wenn \\(\\sigma\\) klein ist.\n\n\n\n\n\nAbbildung 2.10: Welche Verteilung nehmen wir?\n\n\n\n\nSpielen wir verschiedene Möglichkeiten einmal durch. Wir vernachlässigen zunächst einmal \\(\\sigma\\) und konzentrieren uns auf \\(\\mu\\). Wir benötigen eine einzelne Referenzverteilung um unseren beobachteten Wert \\(\\Delta\\), den Unterschied zwischen den beiden Gruppen, mit der Verteilung in Beziehung zu setzen. Wir könnten zum Beispiel sagen, dass wir davon ausgehen, dass der Unterschied zwischen den beiden Gruppen \\(\\Delta_{\\text{wahr}} = 75N\\) ist. D.h. dies wäre der wahre Unterschied zwischen den beiden Gruppen. Wir treffen ihn nicht genau, da wir eine Zufallsstichprobe gezogen haben und die Stichprobenvariabilität dazu führt, dass wir nicht genau den Unterschied treffen. Allerdings, wird wieder einmal etwas starren auf den Wert \\(75N\\) zu der Einsicht führen, dass \\(75\\) vollkommen willkürlich ist. Warum nicht \\(85N\\) oder \\(25\\) oder warum überhaupt ganzzahlig, \\(\\pi\\) ist schließlich auch keine ganzzahlige Zahl, also könnten wir genauso gut \\(74.1234N\\) nehmen. Schnell wird daher klar, dass keine Zahl so richtig gut begründet werden kann. Wir brauchen aber eine Zahl um unseren Apparatus mit Verteilungen ansetzen zu können. Tatsächlich gibt es eine Zahl die zwar auch willkürlich ist, aber doch etwas besser begründet werden kann, nämlich die Zahl \\(\\Delta_{\\text{wahr}} = 0\\). Warum ist der Wert \\(0\\) in diesem Fall speziell. Nun, er bedeutet, dass wir davon ausgehen, dass zwischen den beiden Gruppen kein Unterschied besteht, also die Intervention überhaupt nichts gebracht hat. Dies ist zwar keine wirklich interessante Annahme, aber sie hat trotz ihr Willkürlichkeit doch etwas mehr Gewicht als eine beliebige andere Zahl. Wir bezeichnen diese Annahme jetzt auch noch als die \\(H_0\\)-Hypothese. Die \\(0\\) bei \\(H\\) bedeutet dabei nicht unbedingt, dass die \\(H_0\\) davon ausgeht, dass nicht passiert, sondern nur, das das unsere Ausgangsannahme ist. In vielen Fällen hat die \\(H_0\\) tatsächlich auch die Annahem das nichts passiert, dies muss aber nicht immer der Fall sein. Daher ist unsere Referenzverteilung für die Stichproben in unseren Fall die Hypothese (siehe Formel \\(\\eqref{eq-stats-sig-H0}\\)):\n\\[\\begin{equation}\nH_0: \\Delta = 0\n\\label{eq-stats-sig-H0}\n\\end{equation}\\]\noder graphisch (siehe Abbildung 2.11)\n\n\n\n\n\nAbbildung 2.11: Verteilung wenn nichts passiert mit den beiden Bereichen jenseits von zwei Standardfehlern ausgezeichnet.\n\n\n\n\nDiese Referenzverteilung können wir nun verwenden um eine Entscheidung bezüglich unseres beobachteten Werts zu treffen. Die Streuung in der Referenz- bzw. Stichprobenverteilung wird als Standardfehler bezeichnet im Gegensatz zur Streuung in der Population \\(\\sigma\\) und in der Stichprobe \\(s\\).\n\nDefinition 2.2  \n## Standardfehler\n{Standardfehler}\nDie Streuung der Stichprobenverteilung wird als Standardfehler \\(\\sigma_e\\) bezeichnet. Wir dieser Wert anhand der Stichprobe abgeschätzt hat der Standardfehler das Symbol \\(s_e\\)."
  },
  {
    "objectID": "stats_significance.html#statistisch-signifikanter-wert",
    "href": "stats_significance.html#statistisch-signifikanter-wert",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.5 Statistisch signifikanter Wert",
    "text": "2.5 Statistisch signifikanter Wert\nKommen wir nun zu dem wichtigen Konzept des statistisch signifikanten Werts. Im vorhergehend Abschnitt haben wir eine Stichprobenverteilung für unsere Statistik, den Unterschied zwischen den Mittelwerten der beiden Gruppen, hergeleitet. Wir gehen von der Verteilung aus, bei der es keinen Unterschied \\(H_0: \\Delta = 0\\) zwischen den beiden Gruppen gibt. \\(\\Delta=0\\) hat somit die Bedeutung, das das Krafttraining nicht effektiv war. Dazu haben wir als Kriterium hergeleitet, dass wir Werte die mehr als \\(2\\) Standardabweichungen von Mittelwert entfernt sind, als unwahrscheinlich ansehen, da diese Werte etwa eine Wahrscheinlichkeit von \\(5\\%\\) haben. Präziser, Werte die mehr als zwei Standardfehler vom Mittelwert entfernt sind. Da, unserer angenommenere Mittelwert mit \\(\\Delta = 0\\) zu \\(\\mu = 0\\) wird, bedeutet dies, das wir Werte die entweder kleiner als \\(-2\\times\\) Standardfehler oder größer als \\(2\\times\\) Standardfehler sind, als unwahrscheinlich unter der Annahme von \\(H_0: \\mu = 0\\) betrachten. Als Entscheidungsregel:\n\\[\n|\\text{beobachteter Wert }| &gt; 2\\times \\sigma_e \\Rightarrow \\text{ Evidenz gegen } H_0\n\\]\n\n\n\n\n\nflowchart TD\n    A[Statistik T] --&gt; B{Entscheidung: T &gt; 2xs_e}\n    B --&gt; D(Nein)\n    D --&gt; E[H0 beibehalten]\n    B --&gt; F(Ja)\n    F --&gt; G[H0 ablehnen]\n\n\nAbbildung 2.12: Entscheidungsregel zur \\(H_0\\)\n\n\n\n\nIn Abbildung 2.13 ist die Entscheidungsregel noch einmal graphisch anhand der Verteilung dargestellt. Wir haben unsere Stichprobenverteilung unter der \\(H_0: \\mu = \\Delta = 0\\) und schneiden rechts und links jeweils einen Bereich der Verteilung ab. Diesen Bereich bezeichnen wir als kritischen Bereich. Wenn unser beobachteter Wert im kritischen Bereich liegt, sehen wir dies als Evidenz gegen die Korrektheit der Annahme \\(H_0\\) an.\n\n\n\n\n\nAbbildung 2.13: Die \\(H_0\\) Verteilung wenn nichts passiert unterteilt in Regionen die zur Entscheidung für die \\(H_0\\) (grün) und gegen die \\(H_0\\) (rot, kritische Regionen) führen.\n\n\n\n\nWenn der Stichprobenwert der Statistik in der kritischen Region auftritt, dann wird von einem statistisch signifikanten Effekt gesprochen. Unter der \\(H_0\\) bin ich überrascht diesen Wert zu sehen! Allerdings, dieser Wert ist nicht unmöglich, sondern lediglich unwahrscheinlich unter der Annahme \\(H_0\\). Unwahrscheinlich ist dabei kein absolutes Maß, sondern nur eine willkürliche Festsetzung die wir getroffen haben.\nWir hatten vorhin vorhin gesagt, dass Werte jenseits von \\(2\\times \\sigma_e\\) etwa eine Wahrscheinlichkeit von \\(5\\%\\) haben. Dies Bedeutet nun, dass die Wahrscheinlichkeit Werte aus dem kritischen Bereich zu beobachten bei etwas \\(5%\\) liegt, wenn die \\(H_0\\) zutrifft. Oder anders, wenn die \\(H_0\\) in der Realität zutrifft, also den DGP korrekt beschreibt, und ich das Experiment \\(100\\times\\) wiederhole, dann würde ich etwa \\(5\\) Experimente erwarten bei denen der beobachtete Wert im kritischen Bereich liegt. werde ich mich in \\(5\\) Fällen, irrtümlich gegen die \\(H_0\\) entscheiden, obwohl sie korrekt ist. D.h. in \\(5\\) Fällen würde mich irren, da ich einen Wert im kritischen Bereich beobachtet habe, trotzdem die \\(H_0\\) zutrifft. Daher wird die Wahrscheinlichkeit die ich benutze um einen kritschen Bereich ausweisen als Irrtumswahrscheinlichkeit bezeichnet. Da die Irrtumswahrscheinlichkeit ein zentrales Konzept in der Statistik ist, erhält sie auch ein eigenes Symbol \\(\\alpha\\).\n\nDefinition 2.3 (Irrtumswahrscheinlichkeit \\(\\alpha\\) ) Die Wahrscheinlichkeit mit der fälschlicherweise eine korrekte \\(H_0\\)-Hypothese abgelehnt wird, wird als Irrtumswahrscheinlichkeit bezeichnet. Die Irrtumswahrscheinlichkeit wird mit Symbol \\(\\alpha\\) bezeichnet und auch als Fehler I. Art bezeichnet.\n\nEines der grundlegenden Probleme, das oftmals nicht beachtet wird bei der Interpretation von statistisch signifikanten Ergebnis bezieht sich darauf, dass ich nicht weiß, welches der \\(100\\) Experimente ich durchgeführt habe. Zusätzlich, dies ist keine Aussage über die Wahrscheinlichkeit mit der die \\(H_0\\) in der Realität zutrifft. Ob die \\(H_0\\) zutrifft hat die Wahrscheinlichkeit entweder \\(P(H_0) = 1\\) oder \\(P(H_0) = 0\\). Entweder sie trifft zu oder eben nicht. Darüber wird hier keine Aussage gemacht, sondern nur ob unter der Annahme das \\(H_0\\) zutrifft, der beobachtete Wert in einem wahrscheinlichen oder einem unwahrscheinlichen Bereich liegt. Und nochmal, wahrscheinlich war eine willkürliche Festlegung unsererseits."
  },
  {
    "objectID": "stats_significance.html#der-p-wert",
    "href": "stats_significance.html#der-p-wert",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.6 Der p-Wert",
    "text": "2.6 Der p-Wert\n\n\n\n\n\nDer gelben Flächen zeigen den p-Wert für den Wert der Statistik von d = 2,5 an.\n\n\n\n\nDer p-Wert gibt die Wahrscheinlichkeit für den gefundenen oder einen noch extremeren Wert unter der \\(H_0\\) an.\n\n\n\n\n\nVerschiedene P-Werte\n\n\n\n\n“[A] p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” (Wasserstein und Lazar 2016, p.131)\n“[T]he P value is the probability of seeing data that are as weird or more weird than those that were actually observed.” (Christensen 2018, p.38)\n\n2.6.1 Signifikanter Wert - Das Kleingedruckte\n\nVor dem Experiment wird für ein \\(H_0\\) ein \\(\\alpha\\)-Level angesetzt (per Konvention \\(\\alpha=0,05 = 5\\%\\))\nAnhand des \\(\\alpha\\)-Levels können kritische Werte (\\(k_{lower}, k_{upper}\\)) bestimmt werden. Diese bestimmen die Grenzen der kritischen Regionen.\nWenn der gemessene Wert w der Statistik in die kritische Region fällt, also \\(w \\leq k_{lower}\\) oder \\(w \\geq k_{upper}\\) gilt, dann wird von einem statistisch signifikanten Wert gesprochen und die dazugehörige Hypothese wird abgelehnt. Äquivalent: Der p-Wert ist kleiner als \\(\\alpha\\).\nDa in \\(\\alpha\\)-Fällen ein Wert in der kritischen Region auftritt, auch wenn die \\(H_0\\) zutrifft, wird in \\(\\alpha\\)-Fällen ein \\(\\alpha\\)-Fehler gemacht.\nWenn der Wert w der Statistik nicht in den kritischen Regionen liegt, oder gleichwertig der p-Wert größer als \\(\\alpha\\) ist, wird die \\(H_0\\) beibehalten. D.h. nicht, dass kein Effekt vorliegt, sondern lediglich, dass anhand der Daten keine Evidenz diesbezüglich gefunden werden konnte!\nDie statistische Signifikanz sagt nichts über die Wahrscheinlichkeit der Theorie aus!\nEin p-Wert von \\(p = 0.0001\\) heißt nicht, dass mit 99,99% Wahrscheinlichkeit ein Effekt vorliegt!\nStatistisch signifikant heißt nicht automatisch praktisch relevant!\n\n\n\n\nAusschnitt aus Altman und Bland (1995)\n\n\nEine weitere Erklärung für den p-Wert nach Wasserstein und Lazar (2016)\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis."
  },
  {
    "objectID": "stats_significance.html#was-passiert-nun-aber-wenn-die-andere-hypothese-zutrifft",
    "href": "stats_significance.html#was-passiert-nun-aber-wenn-die-andere-hypothese-zutrifft",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.7 Was passiert nun aber wenn die “andere” Hypothese zutrifft?",
    "text": "2.7 Was passiert nun aber wenn die “andere” Hypothese zutrifft?\n\n\n\n\n\nDifferenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von \\(\\alpha\\) wenn \\(H_0\\) zutrifft."
  },
  {
    "objectID": "stats_significance.html#wir-machen-einen-beta-fehler",
    "href": "stats_significance.html#wir-machen-einen-beta-fehler",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.8 Wir machen einen \\(\\beta\\)-Fehler!",
    "text": "2.8 Wir machen einen \\(\\beta\\)-Fehler!\n\n\n\n\n\nDifferenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von \\(\\alpha\\) wenn \\(H_0\\) zutrifft und \\(\\beta\\) (grün) wenn \\(H_1\\) zutrifft."
  },
  {
    "objectID": "stats_significance.html#snap1989---the-power",
    "href": "stats_significance.html#snap1989---the-power",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.9 Snap!(1989) - The Power",
    "text": "2.9 Snap!(1989) - The Power\n\n\n\n\n\n\\(1-\\beta\\) = Power des Tests (blaue Fläche)."
  },
  {
    "objectID": "stats_significance.html#terminologie-noch-mal",
    "href": "stats_significance.html#terminologie-noch-mal",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.10 Terminologie noch mal",
    "text": "2.10 Terminologie noch mal\n\n\\(\\alpha\\): Die Wahrscheinlichkeit sich gegen die \\(H_0\\) zu entscheiden, wenn die \\(H_0\\) zutrifft. \\(\\alpha\\)-Level wird vor dem Experiment festgelegt um zu kontrollieren welche Fehlerrate toleriert wird.\n\\(\\beta\\): Die Wahrscheinlichkeit sich gegen die \\(H_1\\) zu entscheiden, wenn die \\(H_1\\) zutrifft.\nPower := \\(1 - \\beta\\): Die Wahrscheinlichkeit sich für die \\(H_1\\) zu entscheiden, wenn die \\(H_1\\) zutrifft. Sollte ebenfalls vor dem Experiment festgelegt werden."
  },
  {
    "objectID": "stats_significance.html#wie-können-wir-die-power-erhöhen",
    "href": "stats_significance.html#wie-können-wir-die-power-erhöhen",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.11 Wie können wir die Power erhöhen?",
    "text": "2.11 Wie können wir die Power erhöhen?\n\n\n\n\n\nVerteilungen wenn \\(\\delta\\)=500 und \\(\\delta\\)=0 in unserem kleine Welt Beispiel mit n = 3."
  },
  {
    "objectID": "stats_significance.html#stichprobengröße-von-n-3-auf-n-9-erhöhen",
    "href": "stats_significance.html#stichprobengröße-von-n-3-auf-n-9-erhöhen",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.12 Stichprobengröße von n = 3 auf n = 9 erhöhen?",
    "text": "2.12 Stichprobengröße von n = 3 auf n = 9 erhöhen?\n\n\n\n\n\nStichprobenverteilungen der Differenz unter \\(H_0\\) und \\(H_1:\\delta=500\\)N bei einer Stichprobengröße von n = 9"
  },
  {
    "objectID": "stats_significance.html#standardfehler",
    "href": "stats_significance.html#standardfehler",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "2.13 Standardfehler",
    "text": "2.13 Standardfehler\nDie Standardabweichung der Stichprobenverteilung wird als Standardfehler \\(s_e\\) bezeichnet1. Der Standardfehler ist nicht gleich der Standardabweichung in der Population bzw. der Stichprobe. Es gilt für den Mittelwert:"
  },
  {
    "objectID": "stats_significance.html#problem-bei-einer-dichotomen-betrachtung-der-daten",
    "href": "stats_significance.html#problem-bei-einer-dichotomen-betrachtung-der-daten",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.1 Problem bei einer dichotomen Betrachtung der Daten",
    "text": "3.1 Problem bei einer dichotomen Betrachtung der Daten\n\n\n\n\n\nAbbildung 3.1: Auszug aus Cumming (2013, p.1)"
  },
  {
    "objectID": "stats_significance.html#wie-groß-ist-der-effekt",
    "href": "stats_significance.html#wie-groß-ist-der-effekt",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.2 Wie groß ist der Effekt?",
    "text": "3.2 Wie groß ist der Effekt?\n\n\n\n\n\nStichprobenverteilungen der Differenz unter \\(H_0\\) und \\(H_1:\\delta=500\\)N bei einer Stichprobengröße von n = 9"
  },
  {
    "objectID": "stats_significance.html#schätzung-der-populationsparameter",
    "href": "stats_significance.html#schätzung-der-populationsparameter",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.3 Schätzung der Populationsparameter",
    "text": "3.3 Schätzung der Populationsparameter\nKleine Welt: Experiment wird einmal mit n = 9 durchgeführt\n\n3.3.1 Beobachtete Stichprobenkennwerte\n\\[\\begin{align*}\nd = \\bar{x}_{treat} - \\bar{x}_{con} &= 350 \\\\\ns &= 132 \\\\\ns_e &= 44\n\\end{align*}\\]\nWie präzise ist meine Schätzung und welche anderen Unterschiedswerte sind anhand der beobachteten Daten noch plausibel?"
  },
  {
    "objectID": "stats_significance.html#welche-deltas-sind-plausibel-für-d-350",
    "href": "stats_significance.html#welche-deltas-sind-plausibel-für-d-350",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.4 Welche \\(\\delta\\)s sind plausibel für \\(d = 350\\)?",
    "text": "3.4 Welche \\(\\delta\\)s sind plausibel für \\(d = 350\\)?\n\n\n\n\n\nVerschiedene Verteilungen von Gruppendifferenzen, beobachteter Unterschied (rot)\n\n\n\n\nPlausibel unter einem gegebenem \\(\\alpha\\)-Level!"
  },
  {
    "objectID": "stats_significance.html#alle-möglichen-deltas-die-plausibel-sind",
    "href": "stats_significance.html#alle-möglichen-deltas-die-plausibel-sind",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.5 Alle möglichen \\(\\delta\\)s die plausibel sind",
    "text": "3.5 Alle möglichen \\(\\delta\\)s die plausibel sind\n\n\n\n\n\nKonfidenzintervall (grün), Populationsparameter \\(\\delta\\) und \\(\\alpha\\)-Level für die beobachtete Differenz (gelb)."
  },
  {
    "objectID": "stats_significance.html#was-passiert-wenn-ich-das-experiment-ganz-oft-wiederhole",
    "href": "stats_significance.html#was-passiert-wenn-ich-das-experiment-ganz-oft-wiederhole",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.6 Was passiert wenn ich das Experiment ganz oft wiederhole?",
    "text": "3.6 Was passiert wenn ich das Experiment ganz oft wiederhole?\n\n\n\n\n\nSimulation von \\(n = 100\\) Konfidenzintervallen."
  },
  {
    "objectID": "stats_significance.html#konfidenzintervall---das-kleingedruckte",
    "href": "stats_significance.html#konfidenzintervall---das-kleingedruckte",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.7 Konfidenzintervall - Das Kleingedruckte",
    "text": "3.7 Konfidenzintervall - Das Kleingedruckte\n\nDas Konfidenzintervall für ein gegebenes \\(\\alpha\\)-Niveau gibt nicht die Wahrscheinlichkeit an mit der der wahre Parameter in dem Intervall liegt.\nDas Konfidenzintervall gibt alle mit den Daten kompatiblen Populationsparameter an.\nDas \\(\\alpha\\)-Niveau des Konfidenzintervalls gibt an bei welchem Anteil von Wiederholungen davon auszugehen ist, das das Konfidenzintervall den wahren Populationsparameter enthält."
  },
  {
    "objectID": "stats_significance.html#konfidenzintervall-herleiten-nach-spiegelhalter2019-p.241",
    "href": "stats_significance.html#konfidenzintervall-herleiten-nach-spiegelhalter2019-p.241",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.8 Konfidenzintervall herleiten nach Spiegelhalter (2019, p.241)",
    "text": "3.8 Konfidenzintervall herleiten nach Spiegelhalter (2019, p.241)\n\nWe use probability theory to tell us, for any particular population parameter, an interval in which we expect the observed statistic to lie with 95% probability.\nThen we observe a particular statistic.\nFinally (and this is the difficult bit) we work out the range of possible population parameters for which our statistic lies in their 95% intervals. This we call a “95% confidence interval”.\nThis resulting confidence interval is given the label “95%” since, with repeated application, 95% of such intervals should contain the true value.2\n\nAll clear? If it isn’t, then please be reassured that you have joined generations of baffled students."
  },
  {
    "objectID": "stats_significance.html#konfidenzintervall-berechnen-vorschau",
    "href": "stats_significance.html#konfidenzintervall-berechnen-vorschau",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.9 Konfidenzintervall berechnen (Vorschau)",
    "text": "3.9 Konfidenzintervall berechnen (Vorschau)\n\\[\n\\textrm{CI}_{1-\\alpha} = \\bar{x} \\pm z_{\\alpha/2} \\times s_e\n\\]"
  },
  {
    "objectID": "stats_significance.html#dualität-von-signifikanztests-und-konfidenzintervall",
    "href": "stats_significance.html#dualität-von-signifikanztests-und-konfidenzintervall",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "3.10 Dualität von Signifikanztests und Konfidenzintervall",
    "text": "3.10 Dualität von Signifikanztests und Konfidenzintervall\nWenn das Konfidenzintervall mit Niveau \\(1-\\alpha\\%\\) die \\(H_0\\) nicht beinhaltet, dann wird auch bei einem Signifikanztest die \\(H_0\\) bei einer Irrtumswahrscheinlichkeit von \\(\\alpha\\) abgelehnt.\n\n\n\n\nAltman, Douglas G, und J Martin Bland. 1995. „Statistics notes: Absence of evidence is not evidence of absence“. Bmj 311 (7003): 485.\n\n\nChristensen, Ronald. 2018. Analysis of variance, design, and regression: Linear modeling for unbalanced data. CRC Press.\n\n\nCohen, Jacob. 1988. Statistical power analysis for the behavioral sciences. 2. Aufl. Routledge.\n\n\nCumming, Geoff. 2013. Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis. Routledge.\n\n\nSpiegelhalter, David. 2019. The art of statistics: learning from data. Penguin UK.\n\n\nWasserstein, Ronald L, und Nicole A Lazar. 2016. „The ASA statement on p-values: context, process, and purpose“. Taylor & Francis."
  },
  {
    "objectID": "stats_significance.html#footnotes",
    "href": "stats_significance.html#footnotes",
    "title": "2  Statistische Signifikanz, p-Wert und Power",
    "section": "",
    "text": "Der Standardfehler schätzt die Reliabilität der Statistik ab (Cohen (1988))↩︎\nStrictly speaking, a 95% confidence interval does not mean there is a 95% probability that this particular interval contains the true value […]↩︎"
  },
  {
    "objectID": "stats_distributions.html",
    "href": "stats_distributions.html",
    "title": "3  Verteilungen",
    "section": "",
    "text": "4 Verteilungszoo"
  },
  {
    "objectID": "stats_distributions.html#die-verteilung---1.-deep-dive",
    "href": "stats_distributions.html#die-verteilung---1.-deep-dive",
    "title": "3  Verteilungen",
    "section": "3.1 Die Verteilung - 1. deep dive",
    "text": "3.1 Die Verteilung - 1. deep dive\nWir versuchen jetzt als erstes zu Verstehen was nochmal genau der Graph der Verteilung bedeutet. Auf der x-Achse werden die verschiedenen möglichen Werte der jeweiligen Statistik abgebildet. In unserem bisherigen Beispiel was das die Unterschiede \\(D\\) zwischen der Kontroll- und der Treatmentgruppe. Der Wert auf der y-Achse was zunächst die relative Häufigkeit was auch Sinn gemacht hatte, da wir nur eine bestimmte endliche Anzahl von möglichen Unterschieden \\(D\\) (ihr erinnert auch an die Zahl) vorliegen hatten. Was passiert aber wenn wir tatsächlich eine kontiuierliche Statistik haben, also eine Statistik die alle Werte innerhalb eines Intervalls einnehmen kann. Um den Fall zu verstehen fangen wir aber erst mal wieder mit einem einfachen Modell an.\n\n3.1.1 Der Münzwurf\nWir fangen mit dem einfachsten Experiment an: dem Münzwurf. Beim Münzwurf haben wir zwei mögliche Ausgänge unseres Experiments, entweder Kopf oder Zahl. Wir gehen von einer perfekten Münze aus, d.h. die Münze ist vollkommen symmetrisch auf beiden System und keine der Seiten ist in irgendeiner Form schwere oder beeinflusst in einer Art den Ausgang.\nWenn wir uns an die Schule zurück erinnern, dann haben wir in Wahrscheinlichkeitstheorie schon mal was gehört, das im Fall gleichwahrscheinlicher Ereignisse die Wahrscheinlichkeit für ein bestimmtes Ereignis, mittels der Anzahl der vorteilhaften Ausgänge geteilt durch die Anzahl der möglichen Ausgänge berechnet wird. Also beim einmaligen Münzwurf haben wir zwei Ausgänge \\(\\{\\text{Kopf}, \\text{Zahl}\\}\\) und jeweils nur vorteilhaften Ausang als entweder Kopf oder Zahl, daher folgt daraus.\n\\[\\begin{align}\nP(\\text{Kopf}) &= \\frac{1}{2} \\\\\nP(\\text{Zahl}) &= \\frac{1}{2}\n\\end{align}\\]\nWenn wir das jetzt als Graphen in Form einer Wahrscheinlichkeitsverteilung abtragen, dann sieht das noch wenig interessant aus (siehe Abbildung 3.1). Das Muster ist aber trotzdem wichtig, damit wir später wissen worauf wir hier eigentlich schauen. Auf der x-Achse haben wir die möglichen Ausgänge, Kopf oder Zahl, und auf der y-Achse haben wir die Wahrscheinlichkeit abgetragen.\n\n\n\n\n\nAbbildung 3.1: Wahrscheinlichkeitsverteilung des einmaligen Münzwurfes\n\n\n\n\nDa sich mit einem Münzwurf aber so wenig anfangen lässt, machen wir das Ganze jetzt etwas komplizierter und schauen uns an, wie unser Experiment aussieht wenn wir zwei Münzwwürfe uns anschauen. Rein operational, wir schmeißen unsere Münze in die Luft, schreiben uns das Ergebnis auf, und machen das Ganze noch ein zweites Mal und schreiben uns das Ergebnis auf. D.h. was auch immer im ersten Durchgang passiert, hat keine Auswirkungen auf das Ergebnis des zweiten Wurfs. Wir könnten auch zwei Münzen nehmen und beide gleichzeitig in die Luft werfen. Das wäre das gleiche Experiment. Welche Ausgänge haben wir jetzt beim zweimaligen Münzwurf? Zunächst einmal haben wir jetzt nicht mehr nur einen einzelnen Ausgang sondern wir haben ein Ausgangstupel, eine Liste mit zwei Elementen. Etwas motiviertes krizteln auf einem Schmierblatt wird wahrscheinlich relativ schnell zu folgender Tabelle führen (siehe Tabelle 3.1)\n\n\nTabelle 3.1: Mögliche Ausgänge bei einem zweimaligen Münzwurf\n\n\nAusgang 1. Wurf\nAusgang 2. Wurf\nTupel\n\n\n\n\nKopf\nKopf\n(Kopf, Kopf)\n\n\nKopf\nZahl\n(Kopf, Zahl)\n\n\nZahl\nKopf\n(Zahl, Kopf)\n\n\nZahl\nZahl\n(Zahl, Zahl)\n\n\n\n\nJetzt können wir uns wieder fragen, was die Wahrscheinlichkeit für die jeweiligen Ereignistupel ist. Eine direkte Methode wäre, wieder mittels der Symmetrie zu argumentieren. Es gibt vier verschiedene Ausgänge von denen jetzt keiner in irgendeiner Weise bevorzugt ist, daraus würde folgen das alle vier Ausgänge eine Wahrscheinlichkeit von \\(P = \\frac{1}{4}\\) haben.\nEine weitere Möglichkeit wäre mit den Wahrscheinlichkeiten aus dem einfachen Wurf an das Problem heran zu gehen. Wir betrachten die beiden Münzwürfe jetzt wieder sequentiell (siehe Abbildung 3.2). Im ersten Schritt können wir entweder Kopf oder Zahl beobachten. Beide Wahrscheinlichkeiten sind \\(P = \\frac{1}{2}\\). Darauf folgend können wir wieder zwei verschiedene Ausgänge beobachten, eben Kopf oder Zahl, wieder mit der Wahrscheinlichkeit \\(P = \\frac{1}{2}\\).\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B(Kopf)\n    A --&gt; C(Zahl)\n    B --&gt; D(Kopf)\n    B --&gt; E(Zahl)\n    C --&gt; F(Kopf)\n    C --&gt; G(Zahl)\n\n\nAbbildung 3.2: Auswahlmöglichkeiten beim sequentiellen zweimaligen Münzwurf\n\n\n\n\nDa die Münzwürfe voneinander unabhängig sind und keinen Einfluss aufeinander ausüben, folgt daraus, dass die Wahrscheinlichkeiten für jede spezielle Folge von Kopf oder Zahl sich berechnet nach:\n\\[\nP(\\text{Ausgang}) = P(\\text{1. Wurf}) \\times P(\\text{2. Wurf})\n\\tag{3.1}\\]\nAlso in unseren Fall:\n\\[\nP(\\text{Ausgang}) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\n\\tag{3.2}\\]\nWomit wir wieder beim gleichen Ergebnis wie vorher angekommen sind. Der Vorteil dieser Herangehensweise ist jedoch, dass wir damit eine einfache Möglichkeit gefunden haben das Ergebnis auf mehr als nur zwei Würfe zu verallgemeinern. Nehmen wir zum Beispiel den dreifachen Münzwurf, dann können wir die Wahrscheinlichkeit für die Folge \\(P(\\text{KKZ}) = \\frac{1}{2}\\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8}\\) direkt angeben.\nBleiben wir aber erst noch mal kurz beim zweimaligen Münzwurf und schauen uns die Wahrscheinlichkeitsverteilung an. Hier stoßen wir nämlich auf ein Problem in der Darstellung. Wenn wir bei dem Muster aus Abbildung 3.1 bleiben wollen und auf der x-Achse die möglichen Ergnisse und auf der y-Achse die dazugehörende Wahrscheinlichkeit abtragen wollen, dann ist nicht ganz klar wie wir die Ergebnisse ordnen sollen. Eine mögliche Lösung ist in Abbildung 3.3 zu sehen.\n\n\n\n\n\nAbbildung 3.3: Wahrscheinlichkeitsverteilung des zweimaligen Münzwurfes (K: Kopf, Z: Zahl)\n\n\n\n\nDies ist natürlich nicht die einzige Möglichkeit wie wir die Ereignisse ordenen können sondern wahrscheinlich ist jede der 24 möglichen Anordnungen gleich sinnig. Wir könnten auch beispielsweise nicht mehr die beiden einzelnen Ausgänge als Ereignisse wählen, sondern könnten zum Beispiel nur noch die Anzahl der Köpfe in unseren zwei Würfen zählen. Dies würde zu der folgenden Zuordnung führen (siehe Tabelle 3.2).\n\n\nTabelle 3.2: Zuordnung der Anzahl der Köpfe zu den Ereignissen beim zweimaligen Münzwurf\n\n\nEreignisse\nAnzahl der Köpfe\n\n\n\n\n(Kopf, Kopf)\n2\n\n\n(Kopf, Zahl)\n1\n\n\n(Zahl, Kopf)\n1\n\n\n(Zahl, Zahl)\n0\n\n\n\n\nWir verliegen bei dieser Zuordnung nachtürlich die Information bei welchem Wurf die Zahl beobachtet wurde, aber eigentlich interessiert uns das sowieso nicht so brennend. In der Terminologie der Wahrscheinlichkeitstheorie wird die Anzahl der Köpfe als Zufallsvariable bezeichnet.\n\nDefinition 3.1 (Zufallsvariable) Eine Zufallsvariable ist die Abbildung eines Zufallsereignisses auf eine Zahl.\n\nAnders dargestellt, ist eine Zufallsvariable eine Funktion, die einem Ereignis eine Zahl zuordnet (siehe Abbildung 3.4.\n\n\n\n\n\nflowchart LR \n    A[Ereignis] --&gt; B(Zahl)\n\n\nAbbildung 3.4: Abbildung des Ereignisses auf eine Zahl\n\n\n\n\nWenn wir uns jetzt die Wahrscheinlichkeiten für unsere Zufallsvariable anschauen, dann sehen wir aber, dass wir nicht mehr vier verschiedne Ausgänge haben, sondern nur noch drei und das die gleiche Wahrscheinlichkeit für nicht gleich sind.\n\n\nTabelle 3.3: Wahrscheinlichkeitstabelle für Zufallsvariable “Anzahl der Köpfe beim zweimaligen Münzwurf”.\n\n\n\n\n\n\n\nEreignisse\nZufallsvariale\nWahrscheinlichkeit\n\n\n\n\n(Zahl, Zahl)\nKeine Köpfe\n\\(\\frac{1}{4}\\)\n\n\n(Kopf, Zahl)(Zahl,Kopf)\n1 Kopf\n\\(\\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\\)\n\n\n(Kopf,Kopf)\n2 Köpfe\n\\(\\frac{1}{4}\\)\n\n\n\n\nJetzt können wir wieder eine Wahrscheinlichkeitsverteilung für unsere Zufallsvariable abtragen (siehe Abbildung 3.5).\n\n\n\n\n\nAbbildung 3.5: Wahrscheinlichkeitsverteilung für die Anzahl der Köpfe beim zweimaligen Münzwurf\n\n\n\n\nNur um nebenbei noch einmal das offensichtliche Anzusprechen. Die Summe aller Wahrscheinlichkeiten aller Ereignisse muss \\(1\\) sein. Das sollte auch direkt einsichtig sein. Wenn ich alle möglichen Ereignisse abfrage also: “Was ist die Wahrscheinlichkeit das ich keine Köpfe, 1 Kopf oder 2 Köpfe beim zweimaligen Münzwurf erhalte”, dann sind das alle möglichen Ausgänge und dementsprechend sollte die Wahrscheinlichkeit dafür “1” sein oder mathematisch ausgedrückt:\n\\[\nP(\\text{0 Köpfe} \\cup \\text{1 Kopf} \\cup \\text{2 Köpfe}) = \\frac{1}{4} + \\frac{1}{2} + \\frac{1}{4} = 1\n\\]\nJetzt gehen wir zum nächst komplizierteren Fall. Die Anzahl der Köpfe bei drei Münzwürfen. Welche Möglichkeiten gibt es hier? Nun bei drei Würfen kann entweder \\(0, 1, 2\\) oder \\(3\\) Kopf auftreten. Wenn wir die Wahrscheinlichkeiten für diese vier Ereignisse berechnen wollen, können wir aber nicht einfache \\(\\frac{1}{4}\\) für jedes Ereignis als Wahrscheinlichkeit ansetzen (Warum?). Schauen wir uns erst einmal wieder die möglichen Tupel, oder auch die Elemenarereignisse, den wir erinnern uns, dass die Anzahl der Köpfe eine Zufallsvariable ist. Also eine Abbildung der 3-fach Tupel auf eine der Zahlen \\(\\{0, 1, 2, 3\\}\\).\n\n\nTabelle 3.4: Abbildung der 3-fach Tupel auf die Anzahl Kopf beim dreifachen Münzwurf\n\n\nElementarereignis\nAnzahl Kopf\n\n\n\n\n(Z,Z,Z)\n\\(0\\)\n\n\n(K,Z,Z)\n\\(1\\)\n\n\n(Z,K,Z)\n\\(1\\)\n\n\n(Z,Z,K)\n\\(1\\)\n\n\n(K,K,Z)\n\\(2\\)\n\n\n(Z,K,K)\n\\(2\\)\n\n\n(K,Z,K)\n\\(2\\)\n\n\n(K,K,K)\n\\(3\\)\n\n\n\n\nDie Elementarereignisse in Tabelle 3.4 sind wieder alle gleichwahrscheinlich, daher können wir jetzt wieder einfache abzählen. Es gibt insgesamt \\(8\\) mögliche Ausgänge, davon haben jeiweils einer \\(0\\)-mal oder \\(3\\)-mal Kopf und jeweils \\(3\\) Ausgänge haben \\(1\\)-mal oder \\(2\\)-mal Kopf. Daraus folgt für die Wahrscheinlichkeitsfunktion (siehe Tabelle 3.5).\n\n\nTabelle 3.5: Wahrscheinlichkeitsfuntion für den dreifachen Münzwurf\n\n\nAnzahl Kopf\nP\n\n\n\n\n\\(0\\)\n\\(\\frac{1}{8}\\)\n\n\n\\(1\\)\n\\(\\frac{3}{8}\\)\n\n\n\\(2\\)\n\\(\\frac{3}{8}\\)\n\n\n\\(3\\)\n\\(\\frac{1}{8}\\)\n\n\n\n\nDas Ganze auch wieder als Graph (siehe ?fig-sts-coin-toss-3)\n\n\n\n\n\nAbbildung 3.6: Wahrscheinlichkeitsverteilung für die Anzahl der Köpfe beim dreimaligen Münzwurf\n\n\n\n\nBleiben wir noch einmal kurz bei dem Beispiel und versuchen uns die Wahrscheinlichkeiten anders herzuleiten. Sollten wir zum Beispiel einmal in die Verlegenheit kommen und 20 Münzwürfe untersuchen wollen, dann wir die Tabelle relative schnell relativ unhandlich.\nSei \\(N\\) die Anzahl der Würfe die wir durchführen. Wenn wir \\(N\\) kennen, wissen wir auch direkt welche möglichen Ausgänge bei dem Experiment möglich sind, nämlich alle Zahlen zwischen \\(0\\) und \\(N\\). \\(0\\) wenn wir kein Kopf geworfen haben, und \\(N\\) wenn wir nur Kopf geworfen haben. Dementsprechend sind alle Zahlen dazwischen auch noch möglich.\nSchauen wir uns jetzt noch mal den dreimaligen Münzwurf an. Wenn wir kein Kopf werfen in \\(3\\) Würfen und betrachten die Würfe wieder sequentiell, dann haben wir \\(\\frac{1}{2}\\) für die erste Zahl, \\(\\frac{1}{2}\\) für die zweite Zahl und \\(\\frac{1}{2}\\) für die dritte Zahl. Also insgesamt \\(P(1 \\text{ Kopf}) = \\frac{1}{2} \\times \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{8}\\). Aber diese Wahrscheinlichkeit hat ja jedes Elementarereignis egal ob es (K,K,K) oder (K,Z,K) oder (Z,Z,K) usw. ist. Jetzt haben wir aber das Problem, das wir für \\(1\\times\\) oder \\(2\\times\\) Kopf nicht nur eine Möglichkeit vorhanden diese Anzahl an Kopf zu beobachten. In Tabelle 3.4 haben wir bereits gezeigt, dass jeweils drei verschiedene Möglichkeiten, Kombination von Kopf und Zahl, möglich sind. D.h. wir haben jetzt ein Abzählproblem. Können wir irgendwie direkt bestimmen wie viele unterschiedliche Möglichkeiten es gibt?\nSchauen wir uns den Fall \\(1\\times\\) Kopf im 3-fach Tupel an. Auf wie viele Arten können wir 3-fach Tupel erzeugen mit nur einem Kopf. Nun, der Kopf ist entweder an der ersten, der zweiten oder der dritten Stelle und die jeweils anderen Position im Tupel sind mit Zahl besetzt. Das hört sich aber ähnlich wie ein Problem an wie wie etwas was wir schon vorher einmal gehört haben. Als wir uns die Anzahl der möglichen Stichproben aus unserer kleinen Welt angeschaut haben. Dort hatten wir das Problem, das wir bestimmen wollten auf wie viele Möglichkeiten wir zwei Stichproben mit jeweils drei Personen aus 20 Personen ziehen können. Dabei sind wir auf den Binomialkoeffizienten gestoßen Gleichung 1.3.\n\\[\n\\text{Anzahl} = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\nFormal berechnet der Binomialkoeffizient die Möglichkeiten \\(k\\) Objekte aus \\(n\\) Objekten zu ziehen. Wenden wir das mal auf unseren Dreifachwurf an mit \\(n = N = 3\\) und \\(k = 1\\). Ausgeschrieben, auf wie viele Arten können wir \\(1\\times\\) Kopf aus drei Positionen auswählen.\n\\[\n\\text{Kombinationen mit }1\\times\\text{ Kopf} = \\binom{3}{1} = \\frac{3!}{1!(3-1)!} = \\frac{3\\times 2 \\times 1}{1\\times 2 \\times 1} = 3\n\\]\nPasst. Probieren wir das auch direkt mit dem Ereignis \\(2\\times\\) Kopf, also mit \\(N = 3\\) und \\(k = 2\\), aus.\n\\[\n\\text{Kombinationen mit } 2\\times \\text{ Kopf} = \\binom{3}{2} = \\frac{3!}{2!(3-2)!} = \\frac{3\\times 2 \\times 1}{2\\times 1 \\times 1} = 3\n\\]\nPasst auch. Jetzt müssen wir noch nur die beiden Fälle \\(0\\times\\) und \\(3\\times\\) Kopf behandeln. Wenn wir in einem Mathebuch den Binomialkoeffizienten nachschlagen, dann sind dort die beiden folgenden Definition zu finden für die Fälle \\(k=0\\) und \\(k=n\\).\n\\[\\begin{align*}\n\\binom{N}{N} &= 1 \\\\\n\\binom{N}{0} &= 1\n\\end{align*}\\]\nWenn wir diese Definition für die anderen beiden verbleibenden Fälle anwenden, erhalten wir:\n\\[\\begin{align*}\n\\text{Kombinationen mit } 0\\times \\text{ Kopf} &= \\binom{3}{0} = 1 \\\\\n\\text{Kombinationen mit } 3\\times \\text{ Kopf} &= \\binom{3}{3} = 1\n\\end{align*}\\]\nDamit können wir nun für alle möglichen Ausgängen die Anzahl der möglichen Elementarereignisse mittels bestimmen. Allgemein erhalten wir dadurch eine Formel für die Wahrscheinlichkeiten der Ereignisse für den dreifachen Münzwurf.\n\\[\nP(k \\times \\text{Kopf}) = \\binom{3}{k} \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\binom{3}{k} \\left(\\frac{1}{2}\\right)^3\n\\tag{3.3}\\]\nWeil wir natürlich sofort nach einer allgemeinen Lösung streben führen wir jetzt noch ein paar Symbole ein. Die Zufallsvariable, also die Anzahl von Kopf, bezeichnen wir mit dem Großbuchstaben \\(Y\\). Einen speziellen Ausgang bezeichnen wir mit dem Kleinbuchstaben \\(y\\). Damit würden allgemein die Wahrscheinlichkeit für irgend eines der Ereignisse mit \\(Y = y\\) bezeichnen. Und wenn wir sagen wir das Ereignis \\(2\\times\\) Kopf bezeichnen, mit \\(y = 2\\). Also, die Wahrscheinlichkeit für \\(3\\times\\) Kopf mit:\n\\[\nP(Y = 3) = \\binom{3}{3}\\left(\\frac{1}{2}\\right)^3\n\\]\nDie nächste Verallgemeinerung die wir Vornehmen ist dass wir für die Wahrscheinlichkeit das Kopf auftritt das Symbol \\(p\\) benutzen. So könnten wir auch modellieren, wenn wir eine unfaire Münze haben. Wenn jetzt aber \\(p \\neq \\frac{1}{2}\\) gilt, also zum Beispiel die Wahrscheinlichkeit für Kopf \\(p = \\frac{2}{3}\\) wäre, dann ist die Wahrscheinlichkeit für Zahl nicht mehr die Gleiche wie für Kopf. Die Wahrscheinlichkeit für Zahl wäre dann \\(1 - p\\). Wenn wir für die Wahrscheinlichkeit für das Auftreten von Zahl das Symbol \\(q\\) einführen, muss die Wahrscheinlichkeit für Kopf oder Zahl gleich \\(1\\) sein, formal:\n\\[\np + q = 1\n\\] Daraus folgt, dass \\(q = p - 1\\). Wenn wir das auf unseren Münzwurf übertragen, müssen wir das dementsprechend berücksichtigen. Wir können uns aber zunutze machen, dass wir wissen wie viele Würfe durchgeführt wurden, nämlich \\(N\\), und wie viele davon Kopf waren, nämlich \\(y\\). Damit wissen wir automatisch auch die Anzahl von Zahl, \\(N - y\\). Jedes Kopf, hat die Wahrscheinlichkeit \\(p\\) und jede Zahl hat die Wahrscheinlichkeit \\(q = 1 - p\\). Das gilt unabhängig von der Reihenfolge, da z.B. die Wahrscheinlichkeiten \\(KKZK\\) und \\(ZKKK\\) gleich \\(ppqp = qppp\\) sind. Insgesamt haben wir \\(y \\times K\\) und \\((n-y) \\times Z\\) also \\(p^y\\) und \\(q^{n-y}\\). Diesen Zusammenhang können wir in eine Formel stecken.\n\\[\\begin{equation}\nP(Y = y) = \\binom{N}{y}p^y (1-p)^{N-y} = \\binom{N}{y}p^y q^{N-y} \\label{eq-binom-distribution}\n\\end{equation}\\]\nDamit haben wir jetzt auch direkt unsere erste theoretische Verteilung kennengelernt, die in der Statistik eine zentrale Rolle spielt. Die Verteilung in Formel \\(\\eqref{eq-binom-distribution}\\) wird als die Binomialverteilung bezeichnet. Da die Formel \\(\\eqref{eq-binom-distribution}\\) von den Parametern \\(p\\) und \\(n\\) abhängt, wird die Binomialverteilung als eine Familie von Verteilungen bezeichnet.\n\n\n\n\n\n\n\n(a) \\(p = 0.5, n = 10\\)\n\n\n\n\n\n\n\n(b) \\(p = 0.3, n = 7\\)\n\n\n\n\nAbbildung 3.7: Beispiel für verschiedene Binomialverteilungen\n\n\nSchauen wir uns aber noch mal ob wir mit den ganzen Symbolen wirklich unseren dreifachen Münzwurf zurückbekommen. Es gilt \\(N = 3, p = \\frac{1}{2}\\). Daraus folgt das \\(q = 1 - p = 1 - \\frac{1}{2}=\\frac{1}{2}\\). Wenn wir uns noch an \\(x^a x^b = x^{a+b}\\) aus der Schule erinnern folgt:\n\\[\\begin{align*}\nP(Y = 0) &= \\binom{3}{0} \\left(\\frac{1}{2}\\right)^{0}\\left(\\frac{1}{2}\\right)^3 = \\binom{3}{0}\\left(\\frac{1}{2}\\right)^3 = 1 \\left(\\frac{1}{2}\\right)^3 \\\\\nP(Y = 1) &= \\binom{3}{1} \\left(\\frac{1}{2}\\right)^{1}\\left(\\frac{1}{2}\\right)^2 = \\binom{3}{1}\\left(\\frac{1}{2}\\right)^3 = 3 \\left(\\frac{1}{2}\\right)^3 \\\\\nP(Y = 2) &= \\binom{3}{0} \\left(\\frac{1}{2}\\right)^{2}\\left(\\frac{1}{2}\\right)^1 = \\binom{3}{2}\\left(\\frac{1}{2}\\right)^3 = 3 \\left(\\frac{1}{2}\\right)^3 \\\\\nP(Y = 3) &= \\binom{3}{0} \\left(\\frac{1}{2}\\right)^{3}\\left(\\frac{1}{2}\\right)^0 = \\binom{3}{3}\\left(\\frac{1}{2}\\right)^3 = 1 \\left(\\frac{1}{2}\\right)^3 \\\\\n\\end{align*}\\]\nTatsächlich können wir unser Ergebnis von oben wiedergewinnen. Die Funktion der Binomialverteilung (Formel \\(\\eqref{eq-binom-distribution}\\)) wird als Wahrscheinlichkeitsfuntion bezeichnet.\n\nDefinition 3.2 (Wahrscheinlichkeitsfunktion) Eine Wahrscheinlichkeitsfunktion ist eine mathematische Funktion, die die Wahrscheinlichkeiten für alle möglichen Ausgänge eines diskreten Zufallsexperiments angibt. Sie wird auch als diskrete Wahrscheinlichkeitsverteilung bezeichnet. Eine Wahrscheinlichkeitsfunktion ordnet jedem möglichen Ausgang \\(x\\) eines Experiments eine Wahrscheinlichkeit \\(P(X = x)\\) zu. Die Wahrscheinlichkeit liegt zwischen 0 und 1. Die Summe aller Wahrscheinlichkeiten für alle möglichen Ergebnisse muss gleich 1 sein. Eine Wahrscheinlichkeitsfunktion kann als Tabelle oder als Formel dargestellt werden\n\nFür die Eigenschaften einer Verteilung gibt es einer weitere Darstellungsform, die Verteilungsfunktion.\n\nDefinition 3.3 (Verteilungsfunktion) Die Verteilungsfunktion gibt die Wahrscheinlichkeit \\(P\\) an, dass eine Zufallsvariable \\(X\\) einen Wert kleiner oder gleich einem bestimmten Wert \\(x\\) annimmt, formal \\(P(X \\leq x)\\). Sie wird daher auch als kumulative Verteilungsfunktion bezeichnet.\n\nUm die Definition der Verteilungsfunktion leichter nachzuvollziehen schauen wir uns das Ganze graphisch an (siehe Abbildung 3.8).\n\n\n\n\n\n\n\n(a) Wahrscheinlichkeitsfunktion \\(P(X = x)\\)\n\n\n\n\n\n\n\n(b) Verteilungsfunktion \\(P(X \\leq x)\\)\n\n\n\n\nAbbildung 3.8: Zusammenhang zwischen der Wahrscheinlichkeits- und der Verteilungsfunktion bei \\(p = 0.5, n = 10\\)\n\n\nDie Wahrscheinlichkeitsfunktion gibt, wie schon bekannt, die Wahrscheinlichkeit für eine bestimmtes Ereignis an. Zum Beispiel, die Wahrscheinlichkeit bei \\(p = 0.5, n = 10, 5\\times\\) Kopf zu sehen ist etwas unter \\(0.25\\). Wir könnten uns aber auch fragen, was die Wahrscheinlichkeit ist \\(5\\) oder weniger Köpfe zu beobachten. Diese Wahrscheinlichkeit setzt sic zusammen aus \\(P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) + P(X = 5)\\). Genau diesen Wert gibt die Verteilungsfunktion (siehe Abbildung 3.8 (b)) an.\nDie beiden Funktionen sind dabei eineindeutig aufeinander abbildbar. Wenn die Verteilungsfunktion bekannt ist, dann kann daraus die Wahrscheinlichkeitsfunktion berechnet werden und anders herum wenn die Wahrscheinlichkeitsfunktion bekannt ist, dann kann, wie wir eben gesehen haben, die Verteilungsfunktion berechnet werden. Später bei den kontinuierlichen Verteilungen lernen wir noch die Dichtefunktion kennen, welche die Funktion der Wahrscheinlichkeitsfunktion einnimmt.\nFür unser Ausgangsproblem ist jetzt aber mit der Verteilungsfunktion die Möglichkeit gegeben, das wir bestimmte Wahrscheinlichkeitsbereiche unserer Verteilung auszeichnen können. Denn die Wahrscheinlichkeitsfunktion liefert uns die Antwort auf die Frage, welchen Wertebereich wir für eine gegebene Verteilung eher nicht erwarten würden. Schauen wir uns zum Beispiel die Verteilung bei \\(p = 0.5\\) und \\(n = 30\\).\n\n\n\n\n\nAbbildung 3.9: Wahrscheinlichkeitsfunktion bei \\(p = 0.5\\) und \\(n = 30\\)\n\n\n\n\nIn Abbildung 3.9 sehen wir, dass wir zum Beispiel recht überrascht wären, wenn wir bei einem Durchgang von \\(30\\) Münzwürfen einen Wert von z.B. \\(x = 29 \\times\\) Kopf beobachten würden. Es ist nicht unmöglich, aber es wäre schon überraschend. Diesen Grad der Überraschung können wir als Kriterium nehmen, um zu entscheiden ob wir eine bestimmt Beobachtung dazu verwenden würden diese als Evidenz für oder gegen eine bestimmte Verteilungsannahme zu sehen.\nSetzen wir unser Kriterium z.B. bei 2% an. Die Entscheidung wird jetzt folgendermaßen getroffen. Wenn wir einen Wert beobachten der unter der Annahme einer fairen Münze die wir \\(30\\times\\) aus dem Bereich der Werte von \\(\\leq2\\%\\) kommt. Dann sehen wir dies als gegen die Annahme an.\nIm Folgenden werden vier verschiedene Verteilungen noch einmal etwas genauer vorgestellt, da diese Verteilung immer wieder im weiteren Verlauf auftauchen werden. Dies sind die Normalverteilung, die \\(t\\)-Verteilung, die \\(\\chi^2\\)-Verteilung und die \\(F\\)-Verteilung. Dabei ist es, außer bei der Normalverteilung, weniger wichtig sich die Formeln einzuprägen sondern es soll eher darum gehen die Form der Verteilung, den Wertebereich und die Parameter der Verteilung zu kennen. Also zum Beispiel wird die Normalverteilung durch zwei Parameter \\(\\mu\\) und \\(\\sigma^2\\) spezifiziert während die \\(\\chi^2\\)-Verteilung nur über einen einzelnen Parameter den Freiheitsgrad \\(df\\) bestimmt wird. Streng genommen wird auch nicht über vier Verteilungen gesprochen, sondern es handelt sich um jeweils Verteilungsfamilien, da es beispielsweise nicht die eine Normalverteilung gibt, sondern die Form wie eben beschrieben von den beiden Parametern abhängt. Dies gilt in gleich3em Maßen ebenfalls für die anderen behandelten Verteilungen."
  },
  {
    "objectID": "stats_distributions.html#normalverteilung",
    "href": "stats_distributions.html#normalverteilung",
    "title": "3  Verteilungen",
    "section": "3.2 Normalverteilung",
    "text": "3.2 Normalverteilung\nBeginnen wir mit der Normalverteilung.\n\\[\nf(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)}\n\\]\nDie Normalverteilung ist eine symmetrische Verteilung und hat die uns schon oft begegnete Glockenform (siehe Abbildung 3.10).\n\n\n\n\n\nAbbildung 3.10: Dichtefunktion der Normalverteilung mit den Parametern \\(\\mu = 0\\) und \\(\\sigma = 1\\).\n\n\n\n\nDer Wertebereich der Normalverteilung ist \\(X \\in [-\\infty, \\infty]\\). Das Maximum liegt genau beim Erwartungswert \\(\\mu\\) der dementsprechend die Verteilung in die linken 50% und die rechten 50% unterteilt. Das Abfallen der Flanken wird über die Varianz \\(\\sigma^2\\) geregelt. Wird \\(\\sigma^2\\) größer, fallen die Flanken flacher ab, wird \\(\\sigma^2\\) kleiner, fallen die Flanken schneller ab (siehe Abbildung 3.11).\n\n\n\n\n\n\n\n(a) \\(\\sigma^2 = 1\\)\n\n\n\n\n\n\n\n(b) \\(\\sigma^2 = 2\\)\n\n\n\n\nAbbildung 3.11: Veränderung der Dichtefunktion bei unterschiedlichen Varianzen \\(\\sigma^2\\)\n\n\nDie Standardabweichung kann dazu verwendet werden, die Dichtfunktion in verschiedene Abschnitte zu unterteilen. Es gelten die folgenden Zusammenhänge (siehe Tabelle 3.6):\n\n\nTabelle 3.6: Wahrscheinlichkeiten für verschiedene Intervalle um \\(\\mu\\) in Abhängigkeit von \\(\\sigma\\)\n\n\n\\(x \\in\\)\nP\n\n\n\n\n\\([-\\sigma,\\sigma]\\)\n0.682\n\n\n\\([-2\\sigma,2\\sigma]\\)\n0.955\n\n\n\\([-3\\sigma,3\\sigma]\\)\n0.997\n\n\n\n\nÜbertragen auf den Dichtegraphen folgt (siehe Abbildung 3.12):\n\n\n\n\n\nAbbildung 3.12: Dichtefunktion von \\(\\mathcal{N}(\\mu,\\sigma^2)\\)\n\n\n\n\nWie in Tabelle 3.6 zu sehen ist, hat der Bereich \\([-2\\sigma, 2\\sigma]\\) eine Wahrscheinlichkeit von etwas über \\(0.95\\). Daher, wenn ich einen Bereich um den Erwartungswert \\(\\mu\\) auszeichnen möchte, der genau eine Wahrscheinlichkeit von \\(0.95\\) hat, dann muss \\(\\sigma\\) mit einem kleineren Wert als \\(2\\) multipliziert werden, nämlich \\(1.96\\). Das wird hier noch mal speziell erwähnt, da die Zahl \\(1.96\\) später immer wieder auftaucht. Formal:\n\\[P(x\\in[\\mu-1.96\\sigma, \\mu+1.96\\sigma]) = 0.95\\]\nAnders herum, wenn es darum geht in Konfidenzintervall abzuschätzen, dann funktioniert auch die Faustregel, Teststatistik \\(\\pm 2\\times\\) Standardfehler.\n\n3.2.1 Die Standardnormalverteilung\nEine Sonderrolle in der Familie der Normalverteilungen spielt die Standardnormalverteilung mit \\(\\mu = 0\\) und \\(\\sigma^2 = 1\\). Tatsächlich taucht diese so oft aus, dass die Mathematiker ihr ein eigenes Symbol spendiert haben \\(\\phi(x)\\)\n\\[\n\\phi(x) = \\mathcal{N}(\\mu = 0, \\sigma^2 = 1)\n\\]\nIm Fall der Standardnormalverteilung nehmen Tabelle 3.6 und Abbildung 3.12 besonders einfache Formen an da die Intervalle jeweils \\([-1,1]\\), \\([-2,2]\\) und \\([-3,3]\\) sind (siehe Abbildung 3.13).\n\n\n\n\n\nAbbildung 3.13: Dichtefunktion der Standardnormalverteilung \\(\\phi(x)\\) mit \\(\\mu=0\\) und \\(\\sigma^2=1\\)\n\n\n\n\n\n\n3.2.2 z-Transformation\nEs besteht mittels einer einfachen Möglichkeit jede beliebiege Normalverteilung \\(\\mathcal{N}(\\mu,\\sigma^2)\\) auf die Standardnormalverteilung \\(\\mathcal{N}(0,1)\\) abzubilden. Die Transformation wird als z-Transformation bezeichnet und hat die folgende Form:\n\\[\nz = \\frac{X - \\mu_X}{\\sigma_X}\n\\tag{3.4}\\]\nD.h. der Mittelwert der Verteilung von \\(X\\) wird von X abgezogen und die Differenz wird durch die Standardabweichung der Population \\(\\sigma_X\\) geteilt. Die Umkehrfunktion ist dementsprechend:\n\\[\nX = \\mu_X + z \\sigma_X\n\\tag{3.5}\\]\n\n\n3.2.3 Zentraler Grenzwertsatz\nDie Normalverteilung spielt in der Wahrscheinlichkeitstheorie und der Statistik aus verschiedenen Gründen eine Spezialrolle. Ein Grund dafür ist der sogenannte Zentrale Grenzwertsatz, den wir hier nicht beweisen sondern nur kurz diskutieren.\n\nAussage 3.1 (Zentraler Grenzwertsatz) Seien \\(X_1, X_2, \\ldots, X_n\\) n unabhängige, gleichverteilte Zufallsvariablen mit \\(E[X_i]=\\mu\\) und \\(Var[X_i]=\\sigma^2\\) endlich. \\[\n\\lim_{n\\to\\infty}\\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\ \\rightarrow\\ \\mathcal{N}(\\mu=0,\\sigma^2=1)\n\\]\n\nIn Worten besagt der Zentrale Grenzwertsatz, dass egal welche Ursprungsform die Verteilung einer Zufallsvariablen \\(X\\) hat, wenn die Stichprobengröße gegen unendlich geht, die konvergiert die Differenz des Stichprobenmittelwerts und des Mittelwert der Verteilung geteilt durch den Stichprobenstandardfehler gegen die Standardnormalverteilung. Grenzwertsätz sind manchmal etwas schwierig zu interpretieren, da hier noch keine Aussage gemacht wird, wie groß die Stichprobe sein muss, damit diese Abschätzung valide ist. In der Praxis wird oft ab einer gefühlt großen Stichproben diese Abschätzung als zulässig angesehen."
  },
  {
    "objectID": "stats_distributions.html#t-verteilung",
    "href": "stats_distributions.html#t-verteilung",
    "title": "3  Verteilungen",
    "section": "4.1 t-Verteilung",
    "text": "4.1 t-Verteilung\n\n\n\n\n\nBeispiel für verschiedene Dichtefunktionen der t-Verteilung"
  },
  {
    "objectID": "stats_distributions.html#chi2-verteilung",
    "href": "stats_distributions.html#chi2-verteilung",
    "title": "3  Verteilungen",
    "section": "4.2 \\(\\chi^2\\)-Verteilung",
    "text": "4.2 \\(\\chi^2\\)-Verteilung\n\n\n\n\n\nBeispiele für verschiedene Dichtefunktion der \\(\\chi^2\\)-Verteilung."
  },
  {
    "objectID": "stats_distributions.html#f-verteilung",
    "href": "stats_distributions.html#f-verteilung",
    "title": "3  Verteilungen",
    "section": "4.3 F-Verteilung",
    "text": "4.3 F-Verteilung\n\n\n\n\n\nBeispiele für verschiedene Dichtefunktion der F-Verteilung."
  },
  {
    "objectID": "stats_hypotheses.html#wahrscheinlichkeitstheorie",
    "href": "stats_hypotheses.html#wahrscheinlichkeitstheorie",
    "title": "4  Hypothesen testen",
    "section": "4.1 Wahrscheinlichkeitstheorie",
    "text": "4.1 Wahrscheinlichkeitstheorie"
  },
  {
    "objectID": "stats_hypotheses.html#rechenregeln-zum-erwartungswert-und-der-varianz",
    "href": "stats_hypotheses.html#rechenregeln-zum-erwartungswert-und-der-varianz",
    "title": "4  Hypothesen testen",
    "section": "4.2 Rechenregeln zum Erwartungswert und der Varianz",
    "text": "4.2 Rechenregeln zum Erwartungswert und der Varianz\n\n4.2.1 Erwartungwert\nFür eine diskrete Zufallsvariable \\(X\\) auf einer endlichen Menge \\(\\{x_i, i = 1, \\ldots, n\\}\\) mit \\(n\\) Elementen ist der Erwartungswert definiert mit:\n\\[\nE[X] = \\sum_{i=1}^n x_i P(x_i)\n\\]\nD.h. jedes mögliche Ereignis wird mit seiner Wahrscheinlichkeit multipliziert und die Summe über alle diese Möglichkeiten wird gebildet. Da der eine zentrale Rolle in der Wahrscheinlichkeitstheorie und der Statistik spielt, hat er ein eigenes Symbol bekommen \\(\\mu\\). Daher wird uns immer wieder die Schreibweise:\n\\[\nE[X] = \\mu_X\n\\]\nOder wenn der Zusammenhang klar ist und nur von einer bestimmten Zufallsvariablen gesprochen wird, dann auch nur \\(\\mu\\). Es hat sich eingebürgert, die Größe \\(\\mu\\) als den Mittelwert der Population zu bezeichnen auch wenn es sich dabei nicht unbedingt um den Mittelwert handelt wie er üblicherweise verstanden wird und z.B. bei der Stichprobe berechnet wird (\\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)). Bei dem Erwartungswert handelt es sich um den gewichteten Mittelwert und wird daher manchnal die Unterscheidung vorgenommen wenn von dem Mittelwert der Population \\(\\mu\\) und dem Mittelwert der Stichprobe \\(\\bar{x}\\) gesprochen wird.\nIm folgenden werden verschiedene Rechenregeln mit dem Erwartungswert aufgelistet. Diesen Regeln werden wir immer wieder begegnen wenn wir später Erwarungswerte für Statistiken berechnen. Die erste Regel bezieht sich darauf, wenn eine Zufallsvariable mit einer Konstanten \\(a\\) multipliziert wird. Konstant heißt, bei \\(a\\) handelt es sich nicht um eine Zufallsvariable und \\(a\\) hat immer den gleichen Wert. Der Erwartungswert berechnet sich dann mittels:\n\\[\nE[aX] = \\sum_{i=1}^n a x_i P(x_i) = a \\sum_{i=1}^n x_i P(x_i) = a E[X]\n\\]\nIn den meisten Fällen sind wir nicht an einer einzelnen Zufallsvariablen interessiert, sondern, beispielsweise wenn wir eine Stichprobe untersuchen, es liegen mehrere Zufallsvariablen vor. Im einfachsten Fall starten wir mit zwei unabhängigen Zufallsvariablen \\(X\\) und \\(Y\\). Die beiden Variablen können auf der gleichen Ereignismenge definiert sein, können aber auch auf unterschiedlichen Ereignismengen, z.B. \\(\\{x_i, i = 1, \\ldots, n\\}\\) und \\(\\{y_j, j = 1, \\ldots, m\\}\\) definiert sein. Wollen wir den Mittelwert von \\(X\\) und \\(Y\\) berechnen und davon den Erwartungswert berechnen, müssen wir verstehen wie sich die Addition unabhängiger Zufallsvariblen auf den Erwartungswert auswirkt. Tatsächlich ist diese Operation relativ einfach zu verstehen, der Erwartungswert von \\(E[X + Y]\\) berechnet sich mittels:\n\\[\nE[X + Y] = \\sum_{i=1}^n x_i P(x_i) + \\sum_{j=1}^m y_j P(x_j) = E[X] + E[Y]\n\\]\nDiese Formel generalisiert für unabhängige \\(X_i, i = 1, \\ldots, n\\) zu:\n\\[\nE[X_1 + X_2 + \\ldots + X_n] = E[X_1] + E[X_2] + \\ldots + E[X_n]\n\\]\nIn Kombination mit der Regel für konstante Terme mit den Konstanten \\(a_1, a_2, \\ldots, a_n\\) folgt:\n\\[\nE[a_1 X_1 + a_2 X_2 + \\ldots + a_n X_n] = a_1 E[X_1] + a_2 E[X_2] + \\ldots + a_n E[X_n]\n\\]\n\nBeispiele\nNehmen wir zur Veranschaulichung ein einfaches Beispiel mit einer Zufallsvariable \\(X\\) welche die folgende Verteilung hat (siehe Tabelle 4.1):\n\n\nTabelle 4.1: Verteilung der Zufallsvariablen \\(X\\)\n\n\nx\n0\n1\n2\n3\n\n\n\n\n\\(P(x)\\)\n\\(\\frac{1}{8}\\)\n\\(\\frac{5}{8}\\)\n\\(\\frac{1}{8}\\)\n\\(\\frac{1}{8}\\)\n\n\n\n\nDann berechnet sich der Erwartungswert \\(E[X]\\) mittels:\n\\[\nE[X] = \\sum_{i=1}^4 x_i P(x_i) = \\frac{1}{8} \\cdot 0 + \\frac{1}{8}\\cdot 1 + \\frac{5}{8}\\cdot2 + \\frac{1}{8}\\cdot3 = 1.25\n\\]\nHier kann auch eine interessante Eigenschaft des Erwartungswerts beobachtet werden, nämlich das der berechnete Wert gar nicht in der Menge der möglichen Werte der Zufallsvariablen vorkommen muss. In der Ereignismenge von \\(X\\) sind nur ganzzahlige Werte.\nHaben wir eine zweite Zufallsvariable \\(Y\\) mit der Verteilung (siehe Tabelle 4.2)\n\n\nTabelle 4.2: Verteilung der Zufallsvariablen \\(X\\)\n\n\ny\n0\n1\n2\n3\n\n\n\n\n\\(P(y)\\)\n\\(\\frac{2}{8}\\)\n\\(\\frac{2}{8}\\)\n\\(\\frac{1}{8}\\)\n\\(\\frac{3}{8}\\)\n\n\n\n\nMit \\(E[Y]\\):\n\\[\nE[Y] = \\sum_{i=1}^4 y_i P(y_i) = \\frac{2}{8} \\cdot 0 + \\frac{2}{8}\\cdot 1 + \\frac{1}{8}\\cdot2 + \\frac{3}{8}\\cdot3 = 1.625\n\\]\nDann folgt für den Erwarungswert von \\(E[X + Y]\\):\n\\[\nE[X + Y] = E[X] + E[Y] = 1.25 + 1.625 = 2.875\n\\]\nDefinieren wir eine neue Zufallsvariable \\(Z\\) mit \\(Z := a \\cdot X\\) mit der Konstanten \\(a := 2\\). Dann folgt für den Erwartungswert von \\(E[Z]\\):\n\\[\nE[Z] = E[aX] = aE[X] = 2 \\cdot 1.25 = 2.5\n\\]\nEin ganz anderes Beispiel, welches noch mal den Begriff Erwartungswert veranschaulicht, bezieht sich auf ein Glückspiel mit dem Namen Chuck-a-Lcuk. Das Beispiel ist Gross, Harris, und Riehl (2019) entnommen. Das Spiel wird mit einem 1 € Einsatz gespielt. Es werden drei Würfel geworfen und die folgende Regeln bestimmen den Gewinn (siehe Tabelle 4.3).\n\n\nTabelle 4.3: Gewinnauschüttung bei Chuck-a-Luck\n\n\nAusgang\nGewinn\n\n\n\n\nkeine 6\n0 EU\n\n\nmin. eine 6\n2 EU\n\n\n3 x 6\n27 EU\n\n\n\n\nDie Frage die sich nun stellt, ist ob dieses Spiel fair ist bzw. lohnt es sich einen 1 € Einsatz zu setzen? Diese Frage kann mit dem Erwartungswert beantwortet werden. Um den Erwartungswert zu berechnen benötigen wir allerdings zunächst die Wahrscheinlichkeiten für die verschiedenen Ausgänge.\nDie Wahrscheinlichkeit keine \\(6\\) zu werfen ist für jeden Würfel einzeln \\(\\frac{5}{6}\\), dementsprechend, da die Würfel unabhängig voneinander sind, kann diese Wahrscheinlichkeit dreimal miteinander multipliziert werden.\n\\[\nP(0 \\times 6) = \\left(\\frac{5}{6}\\right)^3 = \\frac{125}{216} \\approx 0.579\n\\]\nD.h. in knapp 60% der Fälle wird beim dem Spiel kein Gewinn ausgeschüttet. Berechnen wir zunächst den Fall, dass drei Sechsen geworfen werden. Der ist Parallel zu keiner Sechs, nur das jetzt für einzelnen Würfel die Wahrscheinlichkeit \\(\\frac{1}{6}\\) ist. Es folgt.\n\\[\nP(3 \\times 6) = \\left(\\frac{1}{6}\\right)^3 = \\frac{1}{216} \\approx 0.005\n\\]\nD.h. die Wahrscheinlichkeit für \\(3 \\times 6\\) ist gerade einmal ein halbes Prozent. D.h. in 500 Spielen, würde wir dieses Ereignis nur ein einziges Mal erwarten.\nLetzlich bleibt noch das Ereignis mindestens eine \\(6\\). Hier nehmen wir das Komplementärereignis zu mindestens eine Sechs heißt, nämlich keine Sechs und ziehen dessen Wahrscheinlichkeit von 1, dem sicheren Ereignis, ab. Da diese Menge auch die drei Sechsen beinhaltet, müssen wir dessen Wahrscheinlichkeit auch noch abziehen.\n\\[\nP(\\text{min. eine } 6) = 1 - P(0 \\times 6) - P(3 \\times 6) = \\frac{216}{216} - \\frac{125}{216} - \\frac{1}{216} = \\frac{90}{216} = 0.41\\bar{6}\n\\]\nDie Wahrscheinlichkeit für mindestens eine Sechs ist dementsprechend etwas über 40%. Jetzt wenden wir wieder die Formel für den Erwartungswert an um die zu erwartende Gewinnsumme zu bestimmen. Die Gewinnsumme nimmt jetzt den Wert der Zufallsvariablen ein.\n\\[\nE[X] = \\frac{125}{216}\\times 0 + \\frac{90}{216}\\times 2 + \\frac{1}{216}\\times27 = \\frac{207}{216} \\approx 0.958\n\\]\nIm Mittel erwarten wir bei dem Spiel einen Gewinn von \\(0.958\\)€ bei einem Einsatz von \\(1\\) €. Daher wird im Mittel ein Verlust bei dem Spiel gemacht.\nAls letztes Beispiel schauen wir uns den Erwartungswert des Mittelwerts \\(\\bar{x}\\) an.\n\\[\nE[\\bar{x}] = E\\left[\\frac{1}{n}\\sum_{i=1}^n x_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E[x_i] = \\frac{1}{n}\\sum_{i=1}^n \\mu = \\frac{1}{n}n \\mu = \\mu\n\\]\n\n\n\n4.2.2 Varianz"
  },
  {
    "objectID": "stats_hypotheses.html#schätzer",
    "href": "stats_hypotheses.html#schätzer",
    "title": "4  Hypothesen testen",
    "section": "4.3 Schätzer",
    "text": "4.3 Schätzer\nErwartungstreue"
  },
  {
    "objectID": "stats_hypotheses.html#hypothesentestung",
    "href": "stats_hypotheses.html#hypothesentestung",
    "title": "4  Hypothesen testen",
    "section": "4.4 Hypothesentestung",
    "text": "4.4 Hypothesentestung\n\n4.4.1 Der t-Test\nDas Verhältnis einer standardnormalverteilten Variable z und eine \\(\\chi^2\\)-verteilten Variable s folgt einer \\(t\\)-Verteilung.\n\\[\nT = \\frac{\\hat{\\Delta}}{\\hat{s}_e(\\hat{\\delta})} \\sim t\\text{-Verteilung}\n\\]\n\n\n4.4.2 \\(\\chi^2\\)-Test der Varianz\nSei \\(\\hat{\\sigma}^2\\) ein Schätzer für eine Varianz und \\(H_0: \\sigma^2 = \\sigma_0^2\\) die Nullhypothese, dann lässt sich eine Teststatistik über die folgende Formel konstruieren:\n\\[\nT = d \\frac{\\hat{\\sigma}^2}{\\sigma_0^2} \\sim \\chi^2(d\\text{ Freiheitsgrade})\n\\]\n\n\n4.4.3 F-Test von Varianzverhältnissen\nSeien zwei normalverteilte Stichproben gegeben und deren Varianzen über \\(\\hat{\\sigma}_A^2\\) und \\(\\hat{\\sigma}_B^2\\) abgeschätzt werden dann kann eine Teststatisk über die Gleichheit der beiden Varianzen \\(\\sigma_A^2 = \\sigma_B^2\\) über die folgende Formel konstruiert werden.\n\\[\nT = \\frac{\\hat{\\sigma}^2_A}{\\hat{\\sigma}^2_B} \\sim F(df_A, df_B)\n\\]\nDie beiden Varianzen folgen dabei jeweils einer \\(\\chi^2\\) Verteilung mit Freiheistgraden \\(df_A\\) und \\(df_B\\), so dass die Statistik \\(T\\) einer \\(F\\)-Verteilung mit \\((df_A, df_B)\\) Freiheitsgeraden folgt und die \\(H_0\\) lautet \\(H_0: \\frac{\\sigma_A^2}{\\sigma_B^2} = 1\\)\n\n\n\n\nGross, Benedict, Joe Harris, und Emily Riehl. 2019. Fat Chance: Probability from 0 to 1. Cambridge University Press."
  },
  {
    "objectID": "slm_title.html",
    "href": "slm_title.html",
    "title": "Das einfache Regressionmodell",
    "section": "",
    "text": "Wir beginnen nun mit dem einfachen Regressionsmodell. Das Modell knüpft an unsere Vorkenntnisse aus der Schule mit linearen Gleichungen an. Ausgehend von diesem Modell werden schrittweise neue Konzept eingeführt. Diese Herangehensweise hat den Vorteil, dass eine einfaches mentales Template immer wieder auf die neuen Konzepte abgebildet werden kann. Diese stetige Aufbau vollzieht sich über den ersten Teil des einfachen Regressionsmodells und wird dann im folgenden Teil, welcher die multiple Regression behandelt, fortgeführt. Dabei wird auch gezeigt, wie vorher voneinander unabhängig gelernte Methoden, wie die Regression und die ANOVA letztendlich aus dem gleichen Ansatz entstehen und es eigentlich keinen Unterschied zwischen den beiden Ansätzen gibt."
  },
  {
    "objectID": "slm_basics.html#back-to-school",
    "href": "slm_basics.html#back-to-school",
    "title": "5  Einführung",
    "section": "5.1 Back to school",
    "text": "5.1 Back to school\nWir beginnen mit ein Konzept mit dem wir sehr gut umgehen können. Nämlich der Punkt-Steigungsform aus der Schule (siehe Gleichung 5.1).\n\\[\ny = m x + b\n\\tag{5.1}\\]\nWir haben eine abhängige Variable \\(y\\) und eine lineare Formel \\(mx + b\\) die den funktionalen Zusammenhang zwischen den Variablen \\(y\\) und \\(x\\) beschreibt. Um das Ganze einmal konkret zu machen setzen wir \\(m = 2\\) und \\(b = 3\\) fest. Die Formel Gleichung 5.1 wird dann zu:\n\\[\ny = 2 x + 3\n\\tag{5.2}\\]\nUm ein paar Werte für \\(y\\) zu erhalten setzen wir jetzt verschiedene Wert für \\(x\\) ein indem wir \\(x\\) in Einserschritten zwischen \\([0, \\ldots, 5]\\) erhöhen. Um die Werte darzustellen verwenden wir zunächst eine Tabelle (vlg. Tabelle 5.1)\n\n\n\n\nTabelle 5.1: Tabelle der Daten\n\n\nx\ny\n\n\n\n\n0\n3\n\n\n1\n5\n\n\n2\n7\n\n\n3\n9\n\n\n4\n11\n\n\n5\n13\n\n\n\n\n\n\nWenig überraschend nimmt \\(y\\) für den Wert \\(x = 0\\) den Wert \\(3\\) an und z.B. für den Wert \\(x = 3\\) nimmt \\(y\\) den Wert \\(2 \\cdot 3 + 3 = 9\\) an.\nEine andere Darstellungsform ist naturlich eine graphische Darstellung in dem wir die Werte von \\(y\\) gegen \\(x\\) auf einem Graphen abtragen (siehe Abbildung 5.1).\n\n\n\n\n\nAbbildung 5.1: Graphische Darstellung der Daten aus Tabelle 5.1\n\n\n\n\nWiederum wenig überraschen sehen wir einen linearen Zuwachs der \\(y\\)-Wert mit den größerwerdenden \\(x\\)-Werte. Da in der Definition der Formel Gleichung 5.2 nirgends festgelegt wurde, dass diese nur für ganzzahlige \\(x\\)-Werte gilt, haben wir direkt eine Gerade durch die Punkte gelegt. Hier wird auch die Bedeutung von \\(m\\) und \\(b\\) direkt klar. Die Variable \\(m\\) bestimmt die Steigung der Gleichung während \\(b\\) den y-Achsenabschnitt beschreibt.\n\nDefinition 5.1 (\\(y\\)-Achsenabschnitt) Der y-Achsenabschnitt ist der Wert den \\(y\\) einnimmt wenn \\(x\\) den Wert \\(0\\) annimmt. Sei \\(y\\) durch eine lineare Gleichung \\(y = mx + b\\) definiert, dann wird der y-Achsenabschnitt durch den Wert \\(b\\) bestimmt.\n\nDie Variable \\(m\\) dahingehend bestimmt die Steigung der Gerade.\n\nDefinition 5.2 (Steigungskoeffizient) Wenn \\(y\\) durch eine lineare Gleichung \\(y = mx + b\\) definiert ist, dann bestimmt die Variable \\(m\\) die Steiung der dazugehörenden Gerade. D.h. wenn sich die Variable \\(x\\) um einen Einheit vergrößert (verkleinert) wird der Wert von \\(y\\) um \\(m\\) Einheiten größer (kleiner). Gilt \\(m &lt; 0\\) dann umgekehrt.\n\nDiese beiden trivialen Konzepte mit eigenen Definitionen zu versehen erscheint im ersten Moment vielleicht etwas übertrieben. Wie sich allerdings später zeigen wird, sind diese beiden Einsichten immer wieder zentral wenn es um die Interpretation von linearen statistischen Modellen geht.\nSoweit so gut. Führen wir direkt ein paar Symbole ein, die uns später noch behilflich sein werden. Sei jetzt die Menge der \\(x\\)-Werte geben \\(x = [0, 1, 2, 3, 4, 5]\\). Strenggenommen handelt es sich wieder um ein Tupel, da wir jetzt die Reihenfolge nicht mehr ändern. Wir führen nun einen Index \\(i\\) ein, um einzelne Werte in dem Tupel über ihre Position zu bestimmen und wir hängen diesen Index \\(i\\) an \\(x\\) an. Dann wird aus \\(x\\), \\(x_i\\).\n\n\n\n\nTabelle 5.2: \\(x\\)-Werte und ihr Index \\(i\\)\n\n\nIndex \\(i\\)\n\\(x\\)-Wert\n\n\n\n\n1\n0\n\n\n2\n1\n\n\n3\n2\n\n\n4\n3\n\n\n5\n4\n\n\n6\n5\n\n\n\n\n\n\nDamit können wir jetzt einen speziellen Wert zum Beispiel den dritten Wert mit \\(x_3 = 2\\) bestimmen. Wenden wir unseren Index auf unsere Gleichung 5.1 an, folgt daraus, dass \\(y\\) jetzt auch einen Index \\(i\\) erhält.\n\\[\ny_i = m x_i + b \\qquad i \\text{ in } [1,2,3,4,5,6]\n\\]\nWir bezeichnen die beiden Variablen \\(m\\), die Steigung, und \\(b\\), den y-Achsenabschnitt, jetzt auch mit neuen Variablen die auch noch einen Index erhalten. Aus \\(m\\) wird \\(\\beta_1\\) und aus \\(b\\) wird \\(\\beta_0\\). Damit wird der y-Achsenabschnitt mit \\(\\beta_0\\) bezeichnet und die Steigung wird mit \\(\\beta_1\\) bezeichnet. Dann wir aus unserer Gleichung:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i\n\\tag{5.3}\\]\nDas ist immer noch unsere einfache Punkt-Steigungsform, wir haben lediglich den Index \\(i\\) eingeführt um unterschiedliche \\(y-x\\)-Wertepaare zu bezeichnen und wir haben den \\(y\\)-Achsenabschnitt und die Steigung mit neuen Symbolen versehen.\nBei dem bisherigen Zusammenhang handelt es sich um einen funktionalen Zusammenhang zwischen den beiden Variablen \\(x\\) und \\(y\\). Funktional deswegen, weil wir eine definiertes mathematisches Modell angeben können, d.h. wir haben eine mathematische Funktion welche die Beziehung zwischen den beiden Variablen beschreibt. Wenn wir den Wert für \\(x\\) kenne, dann können wir den präzisen Wert für \\(y\\) ausreichen, indem wir ihn in Gleichung 5.1 einsetzen. Aus der Schule kennen wir auch noch die Darstellung \\(y = f(x)\\). Streng genommen ist diese Darstellung für Gleichung 5.1 nicht ausreichend, denn um den Wert für \\(y\\) auszurechnen benötigen wir auch noch Kenntnis über die Werte \\(m\\) und \\(b\\), bzw. in unsere weiteren Darstellung \\(\\beta_0\\) und \\(\\beta_1\\). Daher sollte der Zusammenhang eigentlich mit \\(y = f(x, \\beta_0, \\beta_1)\\) bezeichnet werden. Es gilt aber immernoch, für gegebene \\(x, \\beta_0\\) und \\(\\beta_1\\) ist der Wert für \\(y\\) fest determiniert.\nWenn wir mit realen Daten arbeiten, dann funktioniert dieser Ansatz leider nicht ganz. Selbst wenn wir ein Experiment gleich durchführen werden wir immer etwas unterschiedliche Werte im Sinne der Messungenauigkeit messen. Wenn wir biologische Systeme messen, kommt dazu das diese in den seltensten Fällen zeitstabil sind sondern immer bestimmte Veränderungen von einem Zeitpunkt zum nächsten auftauchen. In Abbildung 5.2 sind Sprungweiten von mehreren Weitspringerinnen gegen die Anlaufgeschwindigkeit abgetragen. Bei der Betrachtung der Daten erscheint ein linearer Zusammenhang zwischen diesen beiden Variablen durchaus als plausibel.\n\n\n\n\n\nAbbildung 5.2: Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite beim Weitsprung\n\n\n\n\nIn Abbildung 5.2 sind zwei Punkte rot markiert. Die beiden Werte haben praktisch die gleichen \\(x\\)-Werte allerdings unterscheiden sich die \\(y\\)-Werte deutlich von einander. Und dies sind nicht die einzigen Beispielpaare bei denen die \\(x\\)-Werte nahe beiandern liegen, während die \\(y\\)-Werte deutlich weiter voneiander entfernt liegen als bei einen funktionalen Zusammenhang nach Gleichung 5.1 zu erwarten wäre. Diese Abweichungen kommen durch zufällige Einflussfaktoren wie eben zum Beispiel die Veränderungen angesprochener biologischer Faktoren, Messunsicherheiten, beim Weitsprung draußen sind auch immer externe Einflüsse mögliche, vielleicht wenn es sich um den gleichen Springer handelt, hat er auch beim zweiten Mal keine Lust mehr gehabt. Wenn die Punkte zwei unterschiedliche Springer sind, dann kommt auch dazu, dass zwei Weitspringer bei identischer Anlaufgeschwindigkeit unterschiedliche Sprungfähigkeiten haben oder auch technisch nicht gleich gesprungen sind und so weiter und so fort. Insgesamt führen alle diese Einflüsse dazu, dass wir nicht mehr einen streng funktionalen Zusammenhang zwischen unseren beiden Variablen \\(x\\) der Anlaufgeschwindigkeit und \\(y\\) der Sprungweite vorfinden. Wie wir mit diesen Einflüssen umgehen ist das zentrale Thema des nächsten Abschnitts und markiert auch unseren Eingang zur einfachen linearen Regression."
  },
  {
    "objectID": "slm_basics.html#die-einfache-lineare-regression",
    "href": "slm_basics.html#die-einfache-lineare-regression",
    "title": "5  Einführung",
    "section": "5.2 Die einfache lineare Regression",
    "text": "5.2 Die einfache lineare Regression\nBleiben wir bei unserem Beispiel aus Abbildung 5.2 und interpretieren das als praktisches Problem. Wir sind eine Weitsprungtrainerin und stehen jetzt vor der Aufgabe in unserem Training etwas zu verändern um die Weitsprungleistung zu verbessern. Wir haben wir haben uns dazu entschlossen am Anlauf etwas zu verbessern wissen jetzt aber nicht ob, das wirklich lohnenswert ist. Von einer befreundeten Trainerin haben wir einen Datensatz bekommen von Anlaufgeschwindigkeiten und den dazugehörigen Sprungweiten. Schauen wir uns zunächst die einmal die Struktur der Daten an.\n\n\n\n\nTabelle 5.3: Ausschnitt der Sprungdaten\n\n\njump_m\nv_ms\n\n\n\n\n4.36\n6.13\n\n\n4.31\n6.39\n\n\n4.56\n6.56\n\n\n4.75\n6.44\n\n\n5.52\n7.30\n\n\n5.63\n7.19\n\n\n5.70\n7.30\n\n\n\n\n\n\nIn Tabelle 5.3 ist ein Ausschnitt Sprungdaten abgebildet. Wir haben eine einfache Struktur der Daten. Wir haben eine Tabelle mit zwei Spalten. jump_m bezeichnet die Sprungweiten und v_ms die Anlaufgeschwindigkeiten. Damit wir die Datenpaare voneinander unterscheiden bzw. identifzieren können führen wir unseren bereits besprochenen Index \\(i\\) und können so einzelne Paare ansprechen.\n\n\n\n\nTabelle 5.4: Ausschnitt der Sprungdaten\n\n\ni\njump_m\nv_ms\n\n\n\n\n1\n4.36\n6.13\n\n\n2\n4.31\n6.39\n\n\n3\n4.56\n6.56\n\n\n4\n4.75\n6.44\n\n\n5\n5.52\n7.30\n\n\n6\n5.63\n7.19\n\n\n7\n5.70\n7.30\n\n\n\n\n\n\nDas waren bisher aber nur Formalitäten. Wir wollen jetzt denn Zusammenhang zwischen den beiden Variablen modellieren. Wir könnten wahrscheinlich auch einfach Pi-mal-Daumen abschätzen wie groß der Zusammenhang ist. Wenn wir jetzt aber einen unserer Läufer haben, der z.B. etwa \\(9m/s\\) anläuft, welchen Vergleichswerte nehmen wir dann aus Abbildung 5.2. Den unteren oder den oberen der beiden roten Werte? Oder vielleicht den Mittelwert? Welchen Wert nehmen wir wenn unserer Athlete \\(9.7m/s\\) anläuft. Da haben wir leider keinen Vergleichswert in unserer Tabelle. Daher wäre es schon ganz praktisch eine Formel nach dem Muster von Gleichung 5.3 zu haben. Wie wir allerdings schon festgestellt haben, geht dies nicht so einfach da wir eben das Problem mit den Einflussfaktoren haben, die dazu führen, dass die Werte eben nicht streng auf eine Gerade liegen. Somit liegt die Herausforderung nun eine Gerade zu finden die möglichst genau die Daten wiederspiegelt.\n\n\n\n\n\nAbbildung 5.3: Mögliche Geraden um den Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite zu modellieren\n\n\n\n\nIn Abbildung 5.3 sind die Daten zusammen mit verschiedenen möglichen Geraden abgebildet. Eine kurze Überlegung macht schnell klar, dass es im Prinzip unendlich viele unterschiedliche Geraden gibt die durch die Datenpunkte gelegt werden können. D.h. es gibt unendlich viele Kombinationen von \\(\\beta_0\\) und \\(\\beta_1\\), die die jeweiligen Geraden bezeichnen. Daher muss jetzt eine Kriterium gefunden werden, welches ermöglicht aus diesen unendlich vielen Geraden eine auszuwählen die im Sinne des Kriterium optimal ist.\nTatsächlich gibt es dort auch verschiedene Möglichkeiten Kriterien anzuwenden, dasjenige dass jedoch am weitesten verbreitet ist aus verschiedenen Gründen sind die quadratierten Abweichungen von der Gerade. Schauen wir uns die Herleitung dazu schrittweise an. In Abbildung 5.4 ist zur Übersicht nur ein Ausschnitt der Daten zusammen mit einer möglichen Gerade eingezeichnet. Die senkrechten Abweichungen der Geraden zu den jeweiligen Datenpunkten sind rot eingezeichnet. Es ist ersichtlich, dass für diese Wahl der Geraden es zwei Punkte gibt die tatsächlich auch ziemlich genau auf der Geraden liegen während die anderen Punkte zum Teil oberhalb bzw. unterhalb der Geraden liegen. Das Kriterium wäre jetzt dementsprechen die jenige Geraden aus den unendlich vielen zu finden, bei der diese Abweichung ein Minimum annehmen.\n\\[\n\\text{min}\\sum_{i=1}^n y_i - (\\beta_0 + \\beta_1 x_i) = \\sum_{i=1}^n y_i - \\beta_0 - \\beta_1 x_i\n\\]\n\n\n\n\n\nAbbildung 5.4: Abweichungen der Gerade von der Datenpunkten für die Daten mit eine Anlaufgeschwindigkeit zwischen \\(8m/s\\) und \\(10m/s\\).\n\n\n\n\nUnglücklicherweise haben die einfachen Abweichungen die unhandliche Eigenschaft, dass dann die Gerade \\(y_i = \\hat{y}\\) optimal ist.\n\\[\n\\sum_{i=1}^n y_i - \\hat{y} = \\sum_i^n y_i - \\sum_{i=1}^n \\hat{y} = \\sum_{i=1}^n y_i - n\\hat{y} = \\sum_{i=1}^n y_i - n\\frac{1}{n}\\sum_{i=1}^n y_i = \\sum_{i=1}^n y_i - \\sum_{i=1}^n y_i= 0\n\\]\nWir können das Kriterium aber auch noch etwas schärfer machen. Wenn wir sagen, dass wir größere Abweichungen stärker gewichten wollen als kleinere Abweichungen. D.h. große Abweichungen zwischen der Gerade und den Datenpunkten sollten stärker berücksichtigt werden, als kleine Abweichungen. Dies können wir erreichen indem wir die Abweichungen noch zusätzlich quadrieren. Dies hat auch noch den Vorteil noch verschiedene andere mathematische Vorteile, unter anderem führt dies dazu, dass wir eine Gerade erhalten, die auch tatsächlich die Steigung der Punkte berücksichtigt und nicht einfache nur eine horizontale Gerade durch die Punkte zeichnet. Dementsprechend erhalten wir die folgende Funktion, die es zu minimieren gilt:\n\\[\n\\text{min} \\sum_{i=1}^n(y_i - (\\beta_0 + \\beta_1 x_i))^2\n\\tag{5.4}\\]\nDie Abweichungen zwischen der zu findenden Gerade und den Datenpunkten werden als Residuen \\(e_i\\) bezeichnet. Dementsprechend ist die Minimierungsgleichung auch als:\n\\[\n\\text{min} \\sum_{i=1}^n e_i^2\n\\] darzustellen, mit \\(e_i := y_i - (\\beta_0 + \\beta_1 x_i)\\). Führen wir noch eine weitere Bezeichnung \\(E\\) ein, mit der wir die Minimierungsfunktion bezeichnen (\\(E\\) nach englisch error).\n\\[\nE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2\n\\]\nDas Minimum läßt sich finden, indem die partiellen Ableitungen von \\(E\\) nach \\(\\beta_0\\) und \\(\\beta_1\\) berechnet werden und, wie wir es aus der Schule kennen, die Ableitungen gleich Null gesetzt werden.\n\\[\\begin{align*}\n\\frac{\\partial E}{\\partial \\beta_0} &= -2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) = 0 \\\\\n\\frac{\\partial E}{\\partial \\beta_1} &= -2 \\sum_{i=1}^n x_i (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\end{align*}\\]\nDiese Gleichungen lassen sich umstellen und nach \\(\\beta_0\\) und \\(\\beta_1\\) auflösen:\n\\[\\begin{align}\n\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1} \\bar{x} \\label{eq-slm-basics-norm1}\n\\end{align}\\]\n\\(\\bar{x}\\) und \\(\\bar{y}\\) sind wieder die Mittelwerte von \\(x_i\\) und \\(y_i\\). Diese beiden Gleichungen werden als die Normalengleichungen bezeichnet.\nWir führen noch einen weiteren Term ein, den vorhergesagten Wert \\(\\hat{y}_i\\) von \\(y_i\\) anhand der Geradengleichung. Das Hütchen über \\(y_i\\) ist dabei immer das Signal dafür, das es sich um einen abgeschätzten Wert handelt. Wenn wir \\(\\beta_0\\) und \\(\\beta_1\\) anhand der Normalengleichung bestimmen, dann sind das mit großer Wahrscheinlichkeit nicht die wahren Werte aus der Population, sondern wir haben sie nur anhand der Daten abgeschätzt. Daher bekommen die berechneten Werte ebenfalls ein Hütchen \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\). Insgesamt nimmt die lineare Geradengleichung dann die folgende Form an:\n\\[\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i\n\\]\nGraphisch sind die \\(\\hat{y}_i\\)s die Werte auf der Geraden für die gegebenen \\(x_i\\)-Werte.\n\n\n\n\n\nAbbildung 5.5: Die vorhergesaten Werte \\(\\hat{y}_i\\) auf der Gerade.\n\n\n\n\nFür den vorliegenden Fall der Weitsprungdaten erhalten wir die Werte für die Koeffizienten nach Einsetzen der beobachteten Werte in Formel \\(\\eqref{eq-slm-basics-norm1}\\) mit \\(\\hat{\\beta}_0 = -0.14\\) und \\(\\hat{\\beta}_1 = 0.76\\). Somit folgt für die Geradengleichung:\n\\[\n\\hat{y}_i = -0.14 + 0.76 \\cdot x_i\n\\]\nWir erhalten die graphische Darstellung der Geradengleichung indem die \\(x_i\\)-Werte eingesetzt werden und eine Gerade durch die Punkte gezogen wird. Oder auch einfacher für den größten und den kleinsten \\(x_i\\)-Wert.\n\n\n\n\n\nAbbildung 5.6: Die Regressionsgerade der Sprungdaten.\n\n\n\n\nUm uns auch zu vergewissern, dass unsere Berechnungen korrekt sind, schauen wir uns noch einmal an, wie sich \\(E\\) verhält, wenn wir unterschiedliche Kombinationen von Werten für \\(\\beta_0\\) und \\(\\beta_1\\) in die lineare Gleichung einsetzen.\n\n\n\n\n\nAbbildung 5.7: Heatmap von \\(log(E)\\) für verschiedene Werte von \\(\\beta_0\\) und \\(\\beta_1\\)\n\n\n\n\nIn Abbildung 5.7 sind verschiedene Werte für \\(E\\) in Form einer heatmap dargestellt. Die Abweichungen wurden \\(log\\)-transformiert (d.h. der Logarithmus der \\(E\\)-Werte wurde berechnet), da sonst die Unterschiede in der diagnaolen Bildrichtung zu schnell wachsen und die Unterschiede nicht mehr so einfach zu erkennen sind. Werte näher an Weiß bedeuten kleine Werte und Werte näher an Rot bedeuten größere Werte von \\(E\\). Das berechnete Paar für \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) mit \\(\\hat{\\beta}_0 = -0.14\\) und \\(\\hat{\\beta}_1 = 0.76\\) ist schwarz eingezeichnet. Die Abbildung zeigt, dass dieses Wertepaar tatsächlich ein Minimum bezüglich der Funktion \\(E\\) ist, da in alle Richtung weg von dem schwarzen Punkt die Werte für \\(E\\) zunehmen. Da wir nur einen Ausschnitt der möglichen Werte sehen, handelt es sich zunächst um eine lokales Minimum aber es lässt sich zeigen, dass es sich dabei auch um ein globales Minimum handelt. Diese Eigenschaft hängt mit der Form der Funktion \\(E\\) zusammen. In Tabelle 5.5 sind beispielhaft ein paar Werte für \\(log(E)\\) für Paare von \\(\\beta_0\\) und \\(\\beta_1\\) angezeigt, die in Abbildung 5.7 gelb eingezeichnet sind.\n\n\n\n\nTabelle 5.5: Werte von \\(log(E)\\) für verschiedenen Kombinationen von \\(\\beta_0\\) und \\(\\beta_1\\).\n\n\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(log(E)\\)\n\n\n\n\n-0.48\n0.67\n70.34\n\n\n-0.44\n0.68\n51.85\n\n\n-0.38\n0.72\n22.04\n\n\n-0.30\n0.75\n6.46\n\n\n-0.22\n0.75\n3.77\n\n\n-0.14\n0.76\n2.41\n\n\n\n\n\n\n\n5.2.1 Schritt-für-Schritt Herleitung der Normalengleichungen\nUm die Herleitung der Normalengleichungen Schritt-für-Schritt nachvollziehen zu können benötigen wir zunächst einmal ein paar algebraische Tricks.\nFür den Mittelwert gilt: \\[\n\\bar{x} = \\frac{1}{n}\\sum x_i \\Leftrightarrow \\sum x_i = n \\bar{x}\n\\]\nBei Summen und konstanten \\(a\\) konstant gilt: \\[\\begin{align}\n    \\sum a &= n a \\\\\n    \\sum a x_i &= a \\sum x_i \\\\\n    \\sum (x_i + y_i) &= \\sum x_i + \\sum y_i\n\\end{align}\\]\nWenn eine Summe abgeleitet wird, kann in die Ableitung in die Summe reingezogen werden. \\[\n\\frac{d}{d x}\\sum f(x) = \\sum\\frac{d}{d x} f(x)\n\\]\nHier ein zwei Umformungen bei Summen und dem Kreuzprodukt bzw. dem Quadrat. \\[\\begin{alignat}{2}\n&& \\sum(x_i-\\bar{x})(y_i-\\bar{y}) \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (x_iy_i-\\bar{x}y_i-x_i\\bar{y}+\\bar{x}\\bar{y}) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_i y_i - \\sum\\bar{x}y_i - \\sum x_i \\bar{y} + \\sum \\bar{x} \\bar{y} \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu&&  \\sum x_iy_i - n\\bar{x}\\bar{y}-n\\bar{x}\\bar{y}+n\\bar{x}\\bar{y} \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_iy_i - n\\bar{x}\\bar{y} \\nonumber\n\\end{alignat}\\] \\[\\begin{alignat}{2}\n&& \\sum(x_i - \\bar{x})^2 \\\\\n\\Leftrightarrow\\mkern40mu && \\sum(x_i^2 - 2 x_i \\bar{x} + \\bar{x}^2) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum\\bar{x}^2 \\nonumber\\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_i^2 - 2\\bar{x}n\\bar{x} + n\\bar{x}^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_i^2 - n \\bar{x}^2 \\nonumber\n\\end{alignat}\\]\nZurück zu unserem Problem. Es gilt \\(E\\) zu minimieren:\n\\[\\begin{alignat}{2}\n&& E = \\sum e_i^2 = \\sum (y_i - \\hat{y}_i)^2 \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2 \\nonumber\n\\end{alignat}\\]\nDie Gleichung hängt von zwei Variablen \\(\\beta_0\\) und \\(\\beta_1\\). Um das Minimum der Gleichung zu erhalten, verfährt man wie in der Schule, indem man die Ableitung gleich Null setzt. Der vorliegenden Fall ist jedoch etwas komplizierter, da die Gleichung von zwei Variablen abhängt. Daher müssen wir die partiellen Ableitungen \\(\\frac{\\partial}{\\partial \\beta_0}\\) und \\(\\frac{\\partial}{\\partial \\beta_1}\\) verwendet. Wir erhalten dadurch ein Gleichungssystem mit zwei Gleichungen (die jeweiligen Ableitungen) in zwei Unbekannten (\\(\\beta_0\\) und \\(\\beta_1\\)). Die Lösung erfolgt, indem zuerst eine Gleichung nach der einen Unbekannten umgestellt wird und das Ergebnis dann in die andere Gleichung eingesetzt wird.\nWir beginnen mit der partiellen Ableitung nach \\(\\beta_0\\) für den y-Achsenabschnitt. (Zurück an die Schule erinnern: Äußere Ableitung mal innere Ableitung)\n\\[\\begin{alignat}{2}\n&& \\frac{\\partial \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2}{\\partial \\beta_0} \\\\\n\\Leftrightarrow\\mkern40mu && \\sum\\frac{\\partial}{\\partial \\beta_0}(y_i - \\beta_0- \\beta_1 x_i)^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum 2(y_i - \\beta_0- \\beta_1 x_i) (-1) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && -2 \\sum (y_i - \\beta_0- \\beta_1 x_i) \\nonumber\n\\end{alignat}\\] Zum minimieren gleich Null setzen. \\[\\begin{alignat}{2}\n&& -2 \\sum (y_i - \\beta_0- \\beta_1 x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0- \\beta_1 x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i - \\sum \\beta_0- \\sum \\beta_1 x_i = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && n \\bar{y} - n \\beta_0- \\beta_1 n \\bar{x} = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\bar{y} - \\beta_0- \\beta_1 \\bar{x} = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\bar{y} - \\beta_1 \\bar{x} = \\beta_0\\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\beta_0= \\bar{y} - \\beta_1 \\bar{x}\n\\end{alignat}\\]\nEs folgt nach dem gleichen Prinzip die Herleitung für die Steigung \\(\\beta_1\\) und indem die Lösung für \\(\\beta_0\\) eingesetzt wird.\n\\[\\begin{alignat}{2}\n&& \\frac{\\partial \\sum (y_i - \\beta_0 - \\beta_1x_i)^2}{\\partial \\beta_1} \\\\\n\\Leftrightarrow\\mkern40mu && \\sum\\frac{\\partial}{\\partial b}(y_i - \\beta_0 - \\beta_1x_i)^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum2(y_i - \\beta_0 - \\beta_1x_i) -x_i \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && -2 \\sum(y_i - \\beta_0 - \\beta_1x_i)x_i\n\\end{alignat}\\] Wiederum gleich Null setzen. \\[\\begin{alignat}{2}\n&& -2 \\sum(y_i - \\beta_0 - \\beta_1x_i)x_i = 0 \\nonumber\\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0 - \\beta_1x_i)x_i = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i x_i - \\beta_0 x_i - \\beta_1x_i x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - \\beta_0 \\sum x_i - b\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n \\beta_0 \\bar{x} - \\beta_1\\sum x_i^2 = 0 \\nonumber\n\\end{alignat}\\] Einsetzen der Lösung für \\(\\beta_0\\) führt zu: \\[\\begin{alignat}{2}\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n (\\bar{y} - \\beta_1 \\bar{x}) \\bar{x} - \\beta_1\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n\\bar{y}\\bar{x} + n \\beta_1\\bar{x}^2 - \\beta_1\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n\\bar{y}\\bar{x} = \\beta_1 \\sum x_i^2 - \\beta_1n \\bar{x}^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (x_i-\\bar{x})(y_i-\\bar{y}) = \\beta_1 (\\sum x_i^2 - n\\bar{x}^2) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum x_i^2 - n\\bar{x}^2} = \\beta_1\\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\beta_1= \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2} \\nonumber\n\\end{alignat}\\]\nSomit erhält man die beiden Normalengleichungen der Regression.\nÜber diese beiden Gleichungen erhalten wir die gewünschten Koeffizienten \\(\\hat{\\beta_0}\\) und \\(\\hat{\\beta_1}\\). Die Methode wird als die als die Methode der kleinsten Quadrate bezeichnet oder im Englischen Root-Mean-Square (RMS)."
  },
  {
    "objectID": "slm_basics.html#was-bedeuten-die-koeffizienten",
    "href": "slm_basics.html#was-bedeuten-die-koeffizienten",
    "title": "5  Einführung",
    "section": "5.3 Was bedeuten die Koeffizienten?",
    "text": "5.3 Was bedeuten die Koeffizienten?\nGehen wir zurück nun zu unseren Ausgangsproblem der Weitspringer, was haben wir jetzt durch die Berechnung der Gerade eigentlich gewonnen? Dazu müssen wir erst einmal verstehen was die beiden Koeffizienten \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) bedeuten. Wenn wir zurück zu Gleichung 5.1 gehen, haben die beiden Koeffzienten den \\(y\\)-Achsenabschnitt und die Steigung der Geraden beschrieben. In unserem Beispiel haben wir anhand der Daten einen \\(y\\)-Achsenabschnitt \\(\\hat{\\beta}_0\\) von \\(-0.14\\) berechnet. D.h ein Weitspringer der mit einer Anlaufgeschwindigkeit von \\(x = 0\\) anläuft, landet \\(14\\)cm hinter der Sprunglinie. Dies macht offensichtlich nicht viel Sinn (warum?). Der Grund warum hier ein offensichtlich unrealistischere Wert berechnet wurde, werden wir später noch genauer betrachten. Wir können trotzdem zwei Eigenschaften von \\(\\hat{\\beta}_0\\) beobachten. 1) der Koeffizient hat eine Einheit, nämlich die gleiche Einheit wie die Variable \\(y\\). 2) Ob der Wert zu interpretieren ist, hängt von der Verteilung der Daten ab. Schauen wir uns nun den Steigungskoeffizienten \\(\\hat{\\beta}_0\\) an. Der Steigungskoeffizient in Gleichung 5.1 zeigt an, wie sich der \\(y\\)-Wert verändert, wenn sich der \\(x\\)-Wert um einen Einheit verändert. In unserem Fall welcher Unterschied zu erwarten ist zwischen zwei Weitspringern die sich in der Anlaufgeschwindigkeit um eine \\(m/s\\) unterscheiden. D.h. der Steigungskoeffizient ist ebenfalls in der Einheit der \\(y\\)-Variable zu interpretieren.\nUnsere Trainerin kann jetzt die berechnete Gerade dazu nehmen um zu überprüfen ob es sich lohnen würde Trainingszeit in den Anlauf zu stecken und welche Verbesserung dort zu erwarten sind. Allerdings fehlt dazu noch etwas, wir wissen nämlich noch nicht ob die berechnete Gerade auch wirklich die Daten gut wiederspiegelt. Im Beispiel erscheint dies anhand der Grafik als relativ plausibel. Das muss aber nicht immer so sein. Wir können nämlich für alle möglichen Daten eine Gerade berechnen ohne das diese Gerade die Daten wirklich auch nur annährend korrekt wiedergibt. In Formel \\(\\eqref{eq-slm-basics-norm1}\\) steht nirgends für welche Daten die Berechnung nur erlaubt ist.\n\n\n\n\n\nAbbildung 5.8: Gefittete Gerade durch die Daten einer Funktion \\(f(x) = x^3\\).\n\n\n\n\nIn Abbildung 5.8 sind synthetische Daten der Funktion \\(f(x) = x^3\\) abgebildet und die mittels ?eq-slm-basics-norm1 berechneten Gerade eingezeichnet. Die Gerade ist zwar in der Lage die ansteigenden Werte zu modellieren aber eben nicht Schwingungen die durch die kubische Abhängigkeit zustande kommen. Aber, nichts verhindert die Anwendung der Formel auf die Daten.\nDer gleiche Effekt ist auch in Abbildung 5.9 wieder zu beobachten. Hier besteht eine sinusförmige Abhängigkeit zwischen \\(y\\) und \\(x\\). Wir können wieder \\(\\eqref{eq-slm-basics-norm1}\\) anwenden und erhalten auch ein Ergebnis für \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\). Allerdings repräsentiert die Gerade in keinster Weise den tatsächlichen Zusammenhang zwischen den Daten.\n\n\n\n\n\nAbbildung 5.9: Gefittete Gerade durch die Daten einer Funktion \\(f(x) = sin(x) + 3\\).\n\n\n\n\nIm nächsten Kapitel werden wir uns daher damit beschäftigen die Repräsentation der Daten näher zu betrachten und zu präzisieren.\nWir nehmen noch eine weitere Eigenschaft der Gerade mit, die zunächst nichts mit der Interpretation der Koeffizienten zu tun hat, aber später noch mal von Interesse sein wird. Die Gerade hat nämlich die Eigenschaft durch den Punkt (\\(\\bar{x}\\), \\(\\bar{y}\\)) zu gehen. Dies kann daran gesehen werden wenn in die Gleichung \\(\\bar{x}\\) für \\(x_i\\) eingesetzt wird. Anhand der Normalgleichungen kann die Geradengleichung in der Form.\n\\[\ny_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i = \\underbrace{\\bar{y} - \\hat{\\beta}_1 \\bar{x}}_{\\text{Def. }\\hat{\\beta}_0} + \\hat{\\beta}_1 \\cdot x_i\n\\]\nWird jetzt für \\(x_i\\) der Wert \\(\\bar{x}\\) eingesetzt folgt daher.\n\\[\ny_i = \\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 \\bar{x} = \\bar{y}\n\\]\nD.h. für den Wert \\(\\bar{x}\\) nimmt die Geradengleichung der Wert \\(\\bar{y}\\) an. Für die Sprungdaten ist die auch noch mal in Abbildung 5.10 graphisch dargestellt.\n\n\n\n\n\nAbbildung 5.10: Regressionsgerade der Sprungdaten und der Punkt \\((\\bar{x}, \\bar{y})\\)\n\n\n\n\nEine Eigenschaft die im weiteren Verständnis immer wieder auftaucht bezieht sich auf die \\(x\\)-Werte. Bei der Regression wird im Allgemeinen davon ausgegangen, dass die beobachteten \\(x\\)-Werte fixiert sind. D.h. trotzdem die \\(x\\)-Werte bei einem Experiment zufällig sein können, werden diese in den nachfolgenden Schritten als fixiert angesehen. Daher ist in der Formel \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) auch nur \\(\\epsilon_i\\) die einzige zufällige Variable."
  },
  {
    "objectID": "slm_basics.html#die-einfache-lineare-regression-in-r",
    "href": "slm_basics.html#die-einfache-lineare-regression-in-r",
    "title": "5  Einführung",
    "section": "5.4 Die einfache lineare Regression in R",
    "text": "5.4 Die einfache lineare Regression in R\nIn R wird eine Regression mit der Funktion lm() berechnet. Die für uns zunächst wichtigsten Parameter von lm() sind der erste Parameter formula und der zweite Parameter data. Mit der Formel wird der Zusammenhang zwischen den Variablen beschrieben, dabei können die Namen bzw. Bezeichner aus dem tibble() benutzt werden, die an den zweiten Parameter data übergeben werden. D.h. die Spaltennamen aus dem tibble() werden in formula verwendet.\nIn unserem Weitsprungbeispiel konnten wir in Tabelle 5.3 sehen, das das tibble() zwei Spalten mit den Namen v_ms, den Anlaufgeschwindigkeiten, und jump_m, den Weitsprungweiten enthielt. Dementsprechend, müssen wir diese beiden Bezeichner in formula verwenden, um unser Regressionsmodell zu beschreiben. Die Form der Modellbeschreibung folgt, dabei einer bestimmten Syntax die wir uns zunächst anschauen müssen. Zentrales Element der Syntax ist das Tilde Zeichen ~ (Win: ALTGR++, MacOS: ___), welches interpretiert wird als modelliert mit. Der Term der auf der linken Seite steht bezeichnet die abhängige Variable während die Terme auf der rechten Seite der Tilde stehen die unabhängige Variablen spezifizieren. Dementsprechend kann der Satz “Y wird mittels X modelliert” in die Formelsyntax mit Y ~ X übersetzt. Die komplette Syntax orientiert sich an eine Arbeit von Wilkinson und Rogers (1973).\nWenn ein konstanter in der Syntax benötigt wird, dann wird dieser mit einer \\(1\\) bezeichnet. Also zum Beispiel wenn wir Gleichung 5.3 modellieren wollen benutzen wir die Syntax y ~ 1 + x. Die beiden Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\) brauchen wir nicht explizit anzugeben, sondern R generiert uns automatisch anhand der Bezeichner Koeffizienten, die allerdings die Namen der Bezeichner bekommen. Dazu kommt noch eine Besonderheit, dass R bei einer Regressionsgleichung automatisch davon ausgeht, dass ein konstanter Term verwendet werden soll, d.h. der Term +1 wird automatisch dazugefüht. Wenn wir ein Modell ohne einen \\(y\\)-Achsenabschnitt fitten wollen, dann müssen wir dies R explizit mitteilen, indem wir -1 der linken Seite hinzufügen, also z.B. y ~ x - 1. Die Syntax generalisiert dann später einfach, wenn zusätzliche Terme in der multiplen Regression benötigt werden, in dem weitere unabhängige Variablen durch + dazugefügt werden. Dementsprechend würde sich die Formel y ~ x_1 + x_1 übersetzen in die abhängige Variable \\(y\\) wird mittels der unabhängigen Variablen x_1 und x_2 und einem konstaten Term modelliert. In Tabelle 5.6 sind weitere Beispiele für die Struktur der Formelsyntax für lm() gezeigt.\n\n\nTabelle 5.6: Formelsyntaxbeispiele für lm() (y-Ab = y-Achsenabshnitt, StKoef = Steigungskoeffizient)\n\n\nModell\nFormel\nErklärung\n\n\n\n\n\\(y=\\beta_0\\)\ny ~ 1\ny-Ab\n\n\n\\(y=\\beta_0+\\beta x\\)\ny ~ x\ny-Ab und StKoef\n\n\n\\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2\\)\ny ~ x1 + x2\ny-Ab und 2 StKoe\n\n\n\n\nWenn wir jetzt also unsere Weitsprungdaten modellieren wollen, verwenden wir die folgenden Befehle.\n\nlm(jump_m ~ v_ms, data = jump)\n\n\nCall:\nlm(formula = jump_m ~ v_ms, data = jump)\n\nCoefficients:\n(Intercept)         v_ms  \n    -0.1385       0.7611  \n\n\nPer default ist das Ergebnis von lm() nicht wirklich besonders hilfreich und es werden nur die beiden berechneten Koeffizienten ausgegeben. Dabei bezeichnet der Term (Intercept) den automatisch dazugefügten konstanten Term in der Formel, sprich den \\(y\\)-Achsenabschnitt \\(\\hat{\\beta}_0\\) und mit v_ms den Steigungskoeffizienten \\(\\hat{\\beta}_1\\). Um aus lm() mehr Informationen heraus zu bekommen, ist es sinnvoll das Ergebnis einen Variable zuzuweisen. In dem vorliegenden Arbeit wird dazu in den meisten Fällen eine Variante des Bezeichners mod benutzt, als Kurzform vom model. Diese Bezeichnung ist aber wie alle Bezeichner in R vollkommen willkürlich und entspringt nur der Tippfaulheit des Autors.\n\nmod &lt;- lm(jump_m ~ v_ms, data = jump)\n\nUm jetzt mehr Informationen aus dem gefitteten lm()-Objekt zu bekommen werden Helferfunktion verwendet. Die wichtigste Funktion ist die summary()-Funktion (?summary.lm).\n\nsummary(mod)\n\n\nCall:\nlm(formula = jump_m ~ v_ms, data = jump)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.44314 -0.22564  0.02678  0.19638  0.42148 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.13854    0.23261  -0.596    0.555    \nv_ms         0.76110    0.02479  30.702   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2369 on 43 degrees of freedom\nMultiple R-squared:  0.9564,    Adjusted R-squared:  0.9554 \nF-statistic: 942.6 on 1 and 43 DF,  p-value: &lt; 2.2e-16\n\n\nHier bekommen wir schon deutlich mehr Informationen mitgeteilt. Als erstes die Formell die wir lm() übergeben haben. Dann folgt ein Abschnitt über die Residuen, gefolgt von den Koeffzienten und im unteren Abschnitt noch weitere Statistiken. Wir konzentrieren uns zunächst einmal nur auf die Tabelle im Abschnitt Coefficients. Hier begegnen uns wieder in der ersten Spalte die Bezeichner für die beiden \\(\\beta\\)s in Form von \\(\\beta_0\\) (Intercept) und \\(\\beta_1\\) v_ms. In der zweiten Spalte daneben stehen die berechneten Koeffizienten die wir jetzt schon mehrmals gesehen haben. Die weiteren Spalten ignorieren wir hier zunächst. Im Laufe der folgenden Kapitel werden wir uns die weiteren Statistiken anschauen und deren Bedeutung verstehen.\nBei der Benutzung von lm() werden uns noch weitere Helferfunktionen begegnen, die den Umgang mit dem gefitteten Modell vereinfachen. Wollen wir zum Beispiel die beiden Koeffiziente aus dem Modell extrahieren können wir dazu die Funktion coefficients() oder auch nur kurz coef() verwenden. Koeffizienten und Standardschätzfehler\n\ncoef(mod)\n\n(Intercept)        v_ms \n -0.1385361   0.7611019 \n\n\nDie Funktion coef() gibt einen Vektor benannten Vektor zurück der entweder über die Bezeichner oder einfach über die Position der Koeffizienten angesprochen werden kann. Möchte ich zum Beispiel den Steigungskoeffizienten verwendent werwende ich:\n\ncoef(mod)[1]\n\n(Intercept) \n -0.1385361 \n\n\noder\n\ncoef(mod)['v_ms']\n\n     v_ms \n0.7611019 \n\n\nEin etwas übersichtlicher Zugang ist wieder zunächst einmal das Ergebnis von coef() einer Variablen zuweisen und diese dann weiter benutzen.\n\njump_betas &lt;- coef(mod)\njump_betas[1]\n\n(Intercept) \n -0.1385361 \n\n\nDie Koeffizienten kann ich zum Beispiel benutzen um die Regressionsgerade in ein Streudiagramm hinzuzufügen (Das tibble() mit den Sprungdaten hat den Bezeichner jump). Entweder mit dem ggplot2() Grafiksystem.\n\nggplot(jump,\n       aes(x = v_ms, y = jump_m)) +\n  geom_abline(intercept = jump_betas[1],\n              slope = jump_betas[2],\n              color = 'red') +\n  geom_point()\n\n\n\n\nOder mit den Standard R-Grafiksystem. Hier kann der Funktion abline() das gefittete lm()-Objekt direkt übergeben werden und die Koeffizienten werden automatisch extrahiert.\n\nplot(jump_m ~ v_ms, data = jump)\nabline(mod, color = 'red')\n\n\n\n\nSchauen wir uns noch mal ein ganz einfaches Beispiel, bei dem wir tatsächlich wissen welcher Zusammenhang zwischen den beiden Variablen. Wir halten das Beispiel ganz einfache und nehmen vier verschiedene \\(x\\)-Werte mit \\(x_i = i\\). Wir setzen \\(\\beta_0 = 1\\) und \\(\\beta_1 = 0.5\\). Wir generieren die vier Werte mit R, speichern diese in einem tibble() mit dem Bezeichner data und berechnen die resultierenden Koeffizienten mittels lm().\n\ndata &lt;- tibble(\n  x = 1:4,\n  y = 1 + 0.5 * x\n) \nmod &lt;- lm(y ~ x, data)\ncoef(mod)\n\n(Intercept)           x \n        1.0         0.5 \n\n\nUnd tatsächlich können wir die korrekten Koeffizienten mittels der einfachen linearen Regression wiedergewinnen. Diesen Ansatz mittels synthetisch generierten Daten die eingeführten Konzepte und Ansätze zu überprüfen werden wir im weiteren Verlauf des Skripts immer wieder anwenden, da er die Möglichkeit bietet relativ einfach und nachvollziehbar das Verhalten verschiedener Ansätze auszutesten.\nZusammenfassend lässt sich sagen, das wir jetzt gelernt haben wie wir ein einfaches Regressionmodell der Form Gleichung 5.3 an einen beliebigen Datensatz fitten können. Die Berechnung der beiden Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\) erfolgt mittels ?eq-slm-basics-norm1. Dabei berechnen wir die Koeffizienten nicht von Hand sondern lassen die von R mittels der lm() durchführen. Die Berechnung ist dabei vollkommen mechanisch und die Koeffizienten per-se sagen nichts darüber aus, ob das lineare Modell die Daten tatsächlich auch widerspiegelt. Dazu müssen wir noch etwas mehr Theorie aufbauen um Aussagen darüber zu treffen ob das Modell adäquat ist. Dies gehen wir in den folgenden Abschnitten und Kapiteln an.\n\n\n\n\nWilkinson, G. N., und C. E. Rogers. 1973. „Symbolic description of factorial models for analysis of variance“. Applied Statistics 22 (3): 392–99."
  },
  {
    "objectID": "slm_inference.html#statistische-überprüfung-von-beta_1-und-beta_0",
    "href": "slm_inference.html#statistische-überprüfung-von-beta_1-und-beta_0",
    "title": "6  Inferenz",
    "section": "6.1 Statistische Überprüfung von \\(\\beta_1\\) und \\(\\beta_0\\)",
    "text": "6.1 Statistische Überprüfung von \\(\\beta_1\\) und \\(\\beta_0\\)\nDer erste Schritt um eine Verteilung zu bekommen ist allerdings, dass wir zunächst einmal eine Zufallsvariable benötigen. Bisher haben wir den Zusammenhang zwischen Variablen über die Formel\n\\[\ny_i = \\beta_0 + \\beta_1 \\cdot x_i\n\\]\nbeschrieben. In dieser Form ist allerdings noch kein zufälliges Element vorhanden. Für ein gegebenes \\(x_i\\) bekommen wir ein genau spezifiziertes \\(y_i\\). Allerdings haben wir bei der Herleitung gesehen, dass die Daten in den seltensten Fällen genau auf der Gerade liegen, sondern wir die Parameter \\(\\hat{\\beta}_0\\) und \\(\\hat{beta}_1\\) so gewählt haben, dass die quadrierten Abweichungen, die Residuen \\(\\epsilon_i\\) minimal werden. Dies Residuen verwenden wir jetzt um eine zufälliges Element in unsere Regression rein zu bekommen. Ein mögliche Annahme ist, das die Residuen beispielsweise Normalverteilt sind.\nWarum könnte dies Sinn machen. In dem vorhergehenden Weitsprungbeispiel haben wir informell hergeleitet, dass die Weitsprungleistung von unzähligen weiteren Faktoren beeinflusst werden kann, welche dazu führen, dass für eine gegebene Anlaufgeschwindigkeit nicht immer die gleiche Weitsprungweite erzielt wird. Generell, ist diese Art der Begründung bei biologischen System meistens plausibel. In vorhergehenden Abschnitt haben wir dazu aber auch noch gesehen, dass die Normalverteilung eben gut geeignet ist, um solche Prozesse, bei denen viele kleine additive Effekt auftreten. Dieser Argumentation folgend ist es plausibel diese Einflüsse auch beim Regressionsfall mittels einer Normalverteilung zu modellieren. Dazu führen wir noch eine weitere Annahme an, nämlich dass diese Einflüsse im Mittel in gleichen Maßen die Werte nach nach oben wie auch nach unten ablenken. D.h. die Werte nach oben und unten von der Regressionsgerade abweichen. Dies erlaubt uns jetzt die Annahme genau zu spezifizieren.\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nD.h also, wir gehen davon aus, dass die Residuen normalverteilt sind, mit einem Mittelwert von \\(\\mu = 0\\) und einer noch näher zu spezifizierenden Varianz \\(\\sigma^2\\). Das führt dann zu der folgenden Formulierung des Regressionsmodells.\n\\[\nY_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\tag{6.1}\\]\n\\(Y\\) wird jetzt groß geschrieben, da es sich um eine Zufallsvariable handelt. Dies führt jetzt dazu, das das Regressionsmodell in zwei Teile unterschieden werden kann. Einmal eine deterministischen Teil \\(\\beta_0 + \\beta_1 \\cdot x\\) und einen stochastischen Teil \\(\\epsilon_i\\). Dies führt dazu, dass \\(Y_i\\) ebenfalls stochastisch ist und zu einer Zufallsvariable wird.\nSchauen wir uns weiter an, wie sich \\(Y_i\\) verhält, wenn wir \\(x_i\\) als Konstante \\(x\\) mit ein bestimmten Wert annehmen. Dann wird aus Gleichung 6.1 \\(Y_i = \\beta_0 + \\beta_i \\cdot x + \\epsilon_i\\). Folglich bleibt der deterministische Teil immer gleich, wird zu einer Konstante. Da \\(\\epsilon_i\\) normalverteilt ist ist \\(Y_i\\) ebenfalls normalverteilt. Der Mittelwert der Normalverteilung von \\(Y_i\\) \\(\\mu_{Y_i}\\) ist allerdings nicht gleich Null, sondern die Normalverteilung von \\(\\epsilon_i\\) wird um die Konstante \\(\\beta_0 + \\beta_1 \\cdot x\\) verschoben (siehe Abbildung 6.1). Das führt dazu, dass \\(Y_i\\) der Verteilung \\(\\mathcal{N}(\\beta_0 + \\beta_1 x)\\) folgt.\n\n\n\n\n\n\n\n(a) Verteilung von \\(\\epsilon_i\\)\n\n\n\n\n\n\n\n(b) Verteilung von \\(Y_i\\)\n\n\n\n\nAbbildung 6.1: Relation der Lageparameter von \\(e_i\\) und \\(Y_i\\)\n\n\nDaraus folgt jetzt aber zusätzlich, dass für jedes gegebenes \\(X\\) die \\(Y\\)-Werte einer Normalverteilung folgen. Lediglich die Verschiebung des Mittelwert der jeweiligen \\(Y\\)-Normalverteilung hängt von \\(X\\) über die Formel \\(\\beta_0 + \\beta_1 \\cdot X\\) zusammen. Formal:\n\\[\nY|X \\sim N(\\beta_0+ \\beta_1 X,\\sigma^2)\n\\]\nDie Schreibweise \\(|X\\) wird übersetzt für gegenbenes \\(X\\) und sagt aus, dass die Verteilung von \\(Y\\) von \\(X\\) abhängt. Es handelt sich dabei um eine bedingte Wahrscheinlichkeit. Die Varianz der jeweiligen \\(Y\\)-Werte ist dabei die zuvor angenommen Varianz der \\(\\epsilon_i\\) also \\(\\sigma^2\\). Eine wichtige Annahme die noch mal betont werden sollte, wir gehen davon aus, dass die einzelnen Punkte unabhängig voneinander sind. Im Weitsprungbeispiel würde dies bedeuten, dass jeder Sprung von einem anderen Athleten kommen muss.\nWenn wir die Verteilungen von \\(Y\\) graphisch führ beispielweise drei verschiedene \\(X\\)-Wert darstellen, dann folgt daraus die folgende Abbildung (siehe Abbildung 6.2). D.h. für jeden \\(X\\)-Wert werden mehrere \\(Y\\)-Werte beobachtet, die jeweils einer Normalverteilung folgen.\n\n\n\n\n\nAbbildung 6.2: Verteilung der Daten für verschiedene \\(x\\)-Werte\n\n\n\n\nIn Abbildung 6.2 ist klar zu sehen, wie für jeden der drei Punkte von \\(X\\) die beobachteten \\(Y\\)-Werte einer Normalverteilung. Die Breite der Verteilung ist an jedem Punkte gleich, nämlich \\(=\\sigma^2\\) während der Mittelwert der Gleichung \\(\\beta_0 + \\beta_1 X\\) folgend entlang der Regressionsgerade verschoben ist.\nWenn wir uns zurück an die Ausführungen zur statistischen Signifikanz erinnern, dann haben wir in dem Zusammenhang vom einem datengenerierenden Prozess gesprochen (Definition 2.1) (DGP). In unserem jetzigen Modell können wir dementsprechend zwei Komponenten als Teile des DGP identizifieren. Entsprechend Gleichung 6.1 besteht der DGP aus dem deterministischen Teil \\(\\beta_0 + \\beta_1 X\\) und dem stochastischen Teil \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\). Diese Einsicht können wir verwenden um die Eigenschaften dieses Modells bezüglich Aussagen über statistische Signifikanz zu untersuchen.\nWir fokussieren uns jetzt auf ein vereinfachtes Modell bei dem wir zusätzlich noch \\(\\beta_0 = 0\\) setzen, und wir uns erst mal nur für die Eigenschaften von \\(\\beta_1\\) interessieren. Gehen wir nun davon aus, dass zwischen \\(X\\) und \\(Y\\) der Zusammenhang \\(\\beta_1 = 1\\) besteht. D.h. wenn \\(X\\) um eine Einheit vergrößert wird, dann wird \\(Y\\) ebenfalls um eine Einheit größer.\n\\[\nY = 0 + 1 \\cdot X + \\epsilon, \\quad \\epsilon\\sim\\mathcal{N}(0,\\sigma^2)\n\\tag{6.2}\\]\nJetzt müssen wir noch einen Wert für \\(\\sigma^2\\) festlegen. Sei dieser einfach einmal \\(\\sigma = \\frac{1}{2}\\). Jetzt können wir R benutzen um Experimente, also Beobachtungen, anhand dieses DGP zu simulieren. Der Einfachheit halber legen wir ein übersichtliches \\(N = 12\\) fest und nehmen uns jeweils drei \\(X\\)-Werte z.B. mit \\(X \\in \\{-1, 0, 1\\}\\), d.h. wir ziehen für jeden \\(X\\)-Wert vier \\(Y\\)-Werte.\n\nN &lt;- 12\nbeta_0 &lt;- 0\nbeta_1 &lt;- 1\nsigma &lt;- 1/2\ndat_sim_1 &lt;- tibble(\n  x_i = rep(-1:1, each=4),\n  y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma)\n)\n\nWenn wir uns die generierten Daten anschauen, dann sehen wir wenig überraschend 12 verschiedene Werte für \\(y_i\\) und jeweils \\(3 \\times 4\\) verschiedene Werte für \\(x_i\\) (siehe Tabelle 6.1).\n\n\n\n\nTabelle 6.1: Eine Simulation des Modells Gleichung 6.2\n\n\nx_i\ny_i\n\n\n\n\n-1\n-1.00\n\n\n-1\n-0.63\n\n\n-1\n-1.46\n\n\n-1\n-1.11\n\n\n0\n0.50\n\n\n0\n0.08\n\n\n0\n0.81\n\n\n0\n0.36\n\n\n1\n1.02\n\n\n1\n1.56\n\n\n1\n0.72\n\n\n1\n1.08\n\n\n\n\n\n\nWenn wir die Daten graphisch darstellen erhalten wir (Abbildung 6.3):\n\nggplot(dat_sim_1, aes(x_i, y_i)) + \n  geom_point()\n\n\n\n\nAbbildung 6.3: Streudiagramm der Daten aus Tabelle 6.1\n\n\n\n\nEbenfalls wenig überraschend, die Punkte sind auf den \\(x\\)-Werten \\(-1, 0\\) und \\(1\\) zentriert und liegen nicht alle aufeinander, da sie einer Zufallsstichprobe aus \\(\\mathcal{N}(0, \\frac{1}{4})\\) entspringen.\nJetzt kann ich natürlich für diese Daten unsere Normalengleichungen anwenden und Werte für \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) berechnen. Oder eben direkt in R.\n\nmod_sim_1 &lt;- lm(y_i ~ x_i, dat_sim_1)\ncoef(mod_sim_1)\n\n(Intercept)         x_i \n  0.1594703   1.0735086 \n\n\nWir sehen, dass die berechneten Werte für \\(\\beta_0\\) und \\(\\beta_1\\) schon in der Nähe der tatsächlichen Werte liegen (siehe ?eq-slm-inf-mod-1), aber auf Grund der Stichprobenvariabilität eben nicht genau auf diesen Werten. Was passiert denn jetzt, wenn ich das Ganze noch einmal durchlaufen lassen?\n\ndat_sim_2 &lt;- tibble(\n  x_i = rep(-1:1, each=4),\n  y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma)\n)\nmod_sim_2 &lt;- lm(y_i ~ x_i, dat_sim_2)\ncoef(mod_sim_2)\n\n(Intercept)         x_i \n  0.1256822   1.1168710 \n\n\nWieder wenig überraschend, da jedes Mal wenn ich rnom() eine neue Ziehung aus der Normalverteilung generiert wird, erhalte ich neue Werte für \\(y_i\\) und dementsprechend andere Werte für \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\). Nochmal, warum? Stichprobenvariabilität! Jetzt sind wir wieder bei dem gleichen Prinzip, das wir im Rahmen der kleinen Welt ausgiebig behandelt haben. Schauen wir uns jetzt doch einfach mal was passiert wenn wir die Simulation nicht \\(2\\times\\) sondern z.B. \\(1000\\times\\) durchführen.\n\nN_sim &lt;- 1000\nbeta_1_s &lt;- numeric(N_sim)\nx_i &lt;- rep(-1:1, each=4)\nfor (i in 1:N_sim) {\n  daten_temporaer &lt;- tibble(x_i,\n                            y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma))\n  model_temporaer &lt;- lm(y_i ~ x_i, daten_temporaer)\n  beta_1_s[i] &lt;- coef(model_temporaer)[2]\n}\n\nWir erhalten jetzt einen Vektor beta_1_s mit \\(1000\\) beobachteten \\(\\hat{\\beta}_1\\). Da das etwas viele Werte sind um die uns einzeln anzuschauen, erstellen ein Histogramm der \\(\\hat{\\beta}_1\\)s. (Abbildung 6.4).\n\nhist(beta_1_s, xlab = expression(hat(beta)[1]), main='')\nabline(v = beta_1, col='red', lty=2)\n\n\n\n\nAbbildung 6.4: Histogram der auf den simulierten Daten berechneten \\(\\hat{\\beta}_1\\). Wahrer Wert von \\(\\beta_1\\) rot eingezeichnet.\n\n\n\n\nIn Abbildung 6.4 begegnet uns zunächst einmal wieder unsere altbekannte Glockenkurve. Schön ist, dass deren Mittelwert im Bereich des wahren Werts von \\(\\beta_1\\) liegt und Werte mit größer werdender Abweichung vom wahren Wert in ihrer Häufigkeit abnehmen. Aber die Häufigkeit ist nicht Null, sondern eben nur geringer. Werte in der Nähe von \\(\\beta_1\\) weisen dagegen eine größere Häufigkeit aufweisen. Das sollte uns jetzt auch irgendwie zufrieden stimmen, denn dies bedeutet, dass wir in der Lage sind mit unserem Regressionsmodell im Mittel tatsächlich den korrekten Wert abzuschätzen. Allerdings, wie immer, bei einer einzelnen Durchführung des Experiments können wir alles von perfekt spot-on bis komplett danebenliegen und würden es nicht wissen.\nWir können jetzt aber auch wieder ganz parallel zu unseren Herleitungen in der kleinen Welt einen Entscheidungsprozess spezifizieren. Wenn Abbildung 6.4 den DGP beschreibt und das die Verteilung der zu erwartenden \\(\\hat{\\beta}_1\\) unter dem Modell sind. Bei der Dürchführung eines neuen Experiments, dann würden wir sagen, dass wenn unserer beobachteter Wert in den Rändern der Verteilung von Abbildung 6.4 liegt, das wir eher nicht davon ausgehen, dass unserer neues Experiment den gleichen DGP zugrundeliegen hat. D.h wir definieren uns jetzt Grenzen am oberen und am unteren Rand der Verteilung. Wenn jetzt ein neuer beobachteter Wert entweder unterhalb der unteren Grenze oder oberhalb der oberen Grenze liegt, dann sagen wir: Wir sind jetzt aber sehr überrascht diesen Wert zu sehen, wenn der dem gleichen datengenerierenden Prozess entstammen soll. Daher glauben wir nicht, dass dieses Experiment den gleichen DGP besitzt.\nUm diese Entscheidung treffen zu können, müssen wir also Grenzen definieren. Dazu können wir zunächst einmal einfach die Quantilen der Verteilung nehmen und schneiden z.B. unten \\(2.5\\%\\) und oben \\(2.5\\%\\) ab. So kommen wir dann insgesamt auf \\(5\\%\\), um auf die übliche Irrtumswahrscheinlichkeit von \\(\\alpha = 0.05\\) zu kommen. Dazu benutzen wir R und zwar quantile()-Funktion^[Im folgenden Snippet werden die Werte auf zwei Kommastellen mit round() der besseren Darstellung wegen gerundet).\n\n\n 2.5% 97.5% \n 0.65  1.35 \n\n\nMittels dieser Werte können wir zwei disjunkte Wertmenge definieren, einmal die Werte innerhalb von \\(\\hat{\\beta}_1 \\in [0.65,1.35]\\) bei denen wir nicht überrascht sind, und die unter der Annahme \\(\\beta_1 = 1\\) erwartbar sind und die Werte \\(\\hat{\\beta}_1 \\notin [0.65,1.35]\\) diejenigen Werte die uns überraschen würden unter der Annahme. Ins Histogramm übertragen (siehe Abbildung 6.5).\n\n\n\n\n\nAbbildung 6.5: Histogram der auf den simulierten Daten berechneten \\(\\hat{\\beta}_1\\). Wahrer Wert von \\(\\beta_1\\) rot eingezeichnet und kritische Werte grün.\n\n\n\n\nFühren wir nun ein Experiment noch einmal durch. Wir beobachten einen Wert für \\(\\hat{\\beta}_1\\) von \\(1.46\\). Dieser Wert liegt außerhalb unseres definierten Intervalls \\([0.65, 1.35]\\), daher sehen wir diesen Wert als derart unwahrscheinlich unter dem angenommenen DGP, das wir sagen: Wir glauben nicht, dass diesem Experiment nicht der angenommene DGP zugrunde liegt. Graphisch wieder dargestellt (siehe Abbildung 6.6).\n\n\n\n\n\nAbbildung 6.6: Histogram der auf den simulierten Daten berechneten \\(\\hat{\\beta}_1\\). Wahrer Wert von \\(\\beta_1\\) rot eingezeichnet und kritische Werte grün und der beobachtete Wert als roter Punkt.\n\n\n\n\nDaher würden wir diesen Wert als statisisch signifikant bezeichnen und würden unsere Annahme ablehnen.\nJetzt sind wir aber etwas hin und her zwischen Experiment, Annahmen und Schlussfolgerungen gesprungen. Normalerweise kennen wir die Stichprobenverteilung nicht vor dem Experiment, sondern, wir sind am dem Wert \\(\\beta_1\\) interessiert. Wenn wir den Wert schon wissen würden, dann müssten wir ja gar kein Experiment mehr durchführen. D.h. wir haben eigentlich noch keinen klaren Vorkenntnisse. Mit welcher Annahme gehen wir dann in das Experiment rein? Nun, wir schon bei kleinen Welt Beispiel, starten wir mit der Annahme das zwischen den beiden Variablen kein Zusammenhang besteht. Übertragen auf die Modellparameter also, dass kein linearer Zusammenhang zwischen den beiden Variablen besteht.\n\\[\\begin{align*}\nH_0: \\beta_1 &= 0 \\\\\nH_1: \\beta_1 &\\neq 0\n\\end{align*}\\]\nUm die Stichprobenverteilung unter der \\(H_0\\) formal Herleitung zu können, ist der Erwartungswert von \\(\\hat{\\beta}_1\\) und dessen Standardfehler notwendig. Es lässt sich zeigen, dass die folgenden Zusammenhänge unter den gesetzten Annahmen bestehen:\n\\[\nE[\\hat{\\beta}_0] = \\beta_0\n\\]\nAlso der Schätzer von \\(\\beta_1\\) ist erwartungstreu (biased) und der Standardfehler des Schätzer lässt sich wie folgt bestimmen.\n\\[\n\\sigma_{\\beta_1} = \\sqrt{\\frac{\\sigma^2}{\\sum{(X_i - \\bar{X})^2}}}\n\\tag{6.3}\\]\nHier taucht jetzt zum ersten Mal der Parameter \\(\\sigma^2\\) formal auf. Wo kommt diese Variance her? Sie gehört zu unserer Annahme der Verteilung der \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\). Bisher haben wir aber noch gar keine Möglichkeit kennen gelerntm, diese abzuschätzen. Wieder nach etwas motivierten Starren auf die verschiedenen Formeln, könnte heuristisch plausibel sein, dass die Varianz, also die Streuung der \\(\\epsilon_i\\) mit der Streuung unserer Werte um die Regressionsgerade zusammenhängen könnten. Formal hatten wir diese als Residuen bezeichnet und mit \\(e_i = \\hat{y}_i - y_i\\) bezeichnet. Vormals hatten wir diese Abweichungen als Fehler bezeichnet, aber unter den jetzt eingeführten Annahmen, handelt es sich nicht wirklich um Fehler, sondern die Abweichungen sind eine Folge davon, dass \\(Y_i\\) für jeden Wert von \\(X_i\\) nicht nur einen einzigen Wert hat, sondern eben einer Verteilung folgt \\(Y_i|X_i \\sim \\mathcal{N}(\\beta_0 + beta_1, \\sigma^2)\\) deren Form über die \\(\\epsilon_i\\) bestimmt wird.\nDie \\(e_i\\) sind tatsächlich die Schätzer für die wahren \\(\\epsilon_i\\) also \\(e_i = \\hat{\\epsilon_i} = \\hat{y}_i - y_i\\). Es lässt sich nun wieder zeigen, dass mittels dieser \\(e_i\\) ein erwartungstreuer Schätzer für \\(\\sigma^2\\) erzeugen lässt. Nämlich die mittleren quadrierten Abweichungen (MSE).\n\\[\n\\hat{\\sigma} = \\frac{\\sqrt{\\sum_{i=1}^N e_i^2}}{N-2} = \\frac{\\text{SSE}}{N-2} = \\text{MSE}\n\\tag{6.4}\\]\nDa das später immer wieder auftauchen wird, hier auch noch mal in die zwei Komponenten zerlegt. Der Zähler wird als Summe der quadrierten Abweichungen (SSE) bezeichnet und durch den Term \\(N-2\\), der als Freiheitsgerade bezeichnet wird, geteilt. Dann mit die Formel und deren Bezeichnung mittlere Abweichung zusammenpasst, wäre es schöner wenn die Summe durch die Anzahl \\(N\\) der Terme geteilt wird, allerdings verhält sich das in diesem Fall ähnlich wie bei der Varianz einer Stichprobe wo die Summe auch durch \\(N-1\\) geteilt wird (zur Erinnerung \\(s = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})^2}{N-1}\\)). Jetzt wird dementsprechend nicht durch \\(N-1\\) sondern durch \\(N-2\\) geteilt.\nFür unser Problem der Stichprobenverteilung ist jetzt aber wichtiger, dass wir mittels Gleichung 6.4 den Standardfehler von \\(\\hat{\\beta}_1\\) bestimmen können, indem wir für \\(\\sigma^2\\) das mittels der Daten ermittelte \\(\\hat{\\sigma}^2\\) einsetzen.\n\\[\n\\hat{\\sigma}_{\\beta_1} = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum{(X_i - \\bar{X})^2}}}\n\\tag{6.5}\\]\nDies erlaubt uns jetzt nach unserem bereits bekannten Muster eine Teststatistik für die \\(H_0\\) herzuleiten:\n\\[\nt = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\hat{\\sigma}_{\\beta_1}}\n\\]\nUnter der \\(H_0\\) mit \\(\\beta_1 = 0\\) wird daraus\n\\[\nt = \\frac{\\hat{\\beta}_1}{\\hat{\\sigma}_{\\beta_1}}\n\\tag{6.6}\\]\nDiese Teststatistik folgt einer t-Verteilung mit \\(N-2\\) Freiheitsgeraden. Da diese Formel wieder etwas aus der Luft gegriffen erscheint, hier noch mal eine Simulation zusammen mit der theoretischen Testverteilung.\n\nN &lt;- 45\nn_sim &lt;- 1000\nx &lt;- runif(N, -1, 1)\nsigma &lt;- 1\nexperiment &lt;- function() {\n  y &lt;- rnorm(N, mean = 0, sd = sigma)\n  mod &lt;- lm(y~x)\n  b &lt;- coef(mod)[2]\n  c(beta_0 = coef(mod)[1],\n    beta_1 = coef(mod)[2],\n    sigma = sigma(mod))\n}\nbetas &lt;- t(replicate(n_sim, experiment()))\nbetas &lt;- tibble(beta_0 = betas[,1],\n                beta_1 = betas[,2],\n                sigma = betas[,3]) |&gt; \n  mutate(\n   s_e_beta_1 = sqrt(sigma**2/sum( (x - mean(x))**2)),\n   t = beta_1 / s_e_beta_1)\nt_theoretical &lt;- tibble(\n  t = seq(-3, 3, length.out = 150),\n  p = dt(t, N - 2)\n)\n\nggplot(betas, aes(t)) +\n  geom_histogram(aes(y = ..density..), bins = 20) +\n  geom_line(data = t_theoretical, aes(t, p), color = 'red') +\n  labs(x = \"t\", y = 'Relative Häufigkeit') \n\n\n\n\nAbbildung 6.7: Verteilung von t bei 1000 Simulationen unter der Annahme der \\(H_0\\) und die theoretische Verteilung von t (rot).\n\n\n\n\nIn Abbildung 6.7 können wir sehen, dass die theoretische Verteilung in rot die beobachtete Verteilung sehr gut abschätzt.\nIn R kann der Wert \\(\\hat{\\sigma}^2\\) über die Funktion sigma() aus dem gefitteten lm()-Modell extrahiert werden.\n\nsigma(mod)\n\n[1] 0.2369055\n\n\nSchauen wir uns die Stichprobenverteilung von \\(\\hat{\\sigma}^2\\) anhand unserer Simulation an. Es ist wieder zu beobachten, das im Mittel der korrekte, im Modell definierte, Wert von \\(\\sigma = 1\\) beobachtet wird (siehe Abbildung 6.8).\n\n\n\n\n\nAbbildung 6.8: Verteilung von \\(\\hat{\\sigma}\\) in der Simulation und der wahre Wert in rot eingezeichnet\n\n\n\n\nAber wie immer, leider steht uns bei einem realen Experiment diese Information nicht zur Verfügung und wir haben nur einen einzelnen Wert, der alles von komplett daneben bis ziemlich perfekt sein kann.\nSchauen wir uns noch einmal die Ausgabe zu unserem Weitsprungmodell mittels summary() an. Unter Residual Standard Error sehen wir, dass hier \\(\\hat{\\sigma}\\) angegeben wird. Dieser Wert wird auch als mittlerer Schätzfehler bezeichnet und kann als Maß verwendet werden, welche Abweichung das Modell im Mittel hat. Die Einheit sind wieder in den Einheiten der abhängigen Variable, so kann auch schon abgeschätzt werden mit welcher Präzision das Modell die Daten fittet.\n\nsummary(mod)\n\n\nCall:\nlm(formula = jump_m ~ v_ms, data = jump)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.44314 -0.22564  0.02678  0.19638  0.42148 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.13854    0.23261  -0.596    0.555    \nv_ms         0.76110    0.02479  30.702   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2369 on 43 degrees of freedom\nMultiple R-squared:  0.9564,    Adjusted R-squared:  0.9554 \nF-statistic: 942.6 on 1 and 43 DF,  p-value: &lt; 2.2e-16\n\n\nIn unserem Fall beobachten wir \\(0.24m\\). Diesen Wert muss jetzt unsere Trainerin im Sinne der Weitsprungleistung der deren Varianz interpretieren und ein Abschätzung treffen zu können.\nNach der Herleitung der Teststatistik für \\(\\beta_1\\), können wir jetzt auch weitere Teil der Ausgabe von summary() interpretieren. In der Tabelle stehen entsprechend die Standardfehler für \\(\\hat{\\beta}_1\\) und \\(\\hat{\\beta}_0\\). Für \\(\\beta_0\\) wird genau die gleiche Vorgehensweise wie auch bei \\(\\beta_1\\) angewendet. Die Nullhypothese \\(H_0\\) ist hier ebenfalls das der Parameter standardmäßig als Null angesetzt wird. Der Standardfehler von \\(\\beta_0\\) errechnet sich nach:\n\\[\\begin{equation}\n\\sigma^2[\\beta_0] = \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right)\n\\label{eq-slm-inf-beta0-se}\n\\end{equation}\\]\nAn Formel \\(\\eqref{eq-slm-inf-beta-0-se}\\) ist zu erkennen, dass wenn die \\(X\\)-Werte den Mittelwert \\(0\\) haben, dass \\(\\sigma^2[\\beta_0]\\) gleich dem Standardfehler für den Mittelwert SEM wird. Was auch wiederum Sinn macht, da in diesem Fall \\(\\beta_0 = \\bar{y}\\) gilt.\nDies führt dies zu den beiden zu überprüfenden Hypothesen für \\(\\beta_0\\):\n\\[\\begin{align*}\nH_0: \\beta_0 &= 0 \\\\\nH_1: \\beta_0 &\\neq 0\n\\end{align*}\\]\nDementsprechend überprüft die Hypothesentestung ob der \\(y\\)-Achsenabschnitt gleich Null ist. Hier sollte berücksichtigt werden, dass diese Hypothese in den seltensten Fällen tatsächlich auch von Interesse ist und lediglich besagt, dass entweder der \\(y\\)-Achsenabschnitt durch den Nullpunkt geht, oder dass wenn tatsächlich \\(\\beta_1 = 0\\) gilt, der Mittelwert von \\(y\\) gleich Null ist, was ebenfalls in den seltensten Fällen von Interesse ist.\nDie Spalten 3 und 4 in summary() unter Coefficients: können jetzt interpretiert werden, da es sich hierbei um die \\(t\\)-Teststatistik handelt und den entsprechenden p-Wert unter der jeweiligen \\(H_0\\). Die Hypothesen sind ungerichtet."
  },
  {
    "objectID": "slm_inference.html#herleitung-der-eigenschaften-von-hatbeta_1",
    "href": "slm_inference.html#herleitung-der-eigenschaften-von-hatbeta_1",
    "title": "6  Inferenz",
    "section": "6.2 Herleitung der Eigenschaften von \\(\\hat{\\beta}_1\\)",
    "text": "6.2 Herleitung der Eigenschaften von \\(\\hat{\\beta}_1\\)\nUm den Schätzer \\(\\hat{\\beta}_1\\) für \\(\\beta_1\\) formal herzuleiten. Beginnen wir zunächst mit der folgenden Formel, wobei wir im folgenden den Schätzer mit \\(b_1\\) bezeichnen.\n\\[\nb_1 = \\sum k_i Y_i\n\\tag{6.7}\\]\nD.h. wir zeigen zunächst, dass \\(b_1\\) durch eine lineare Kombination der \\(Y_i\\)-Werte berechnet werden kann. Die Koeffizienten \\(k_i\\) der Summe sind dabei wie folgt definiert:\n\\[\nk_i = \\frac{X_i - \\bar{X}}{\\sum(X_i - \\bar{X})^2}\n\\tag{6.8}\\]\nDer Grund für diese zunächst etwas uneinsichtige Definition wird im weiteren klarer werden. Zunächst haben die \\(k_i\\) verschieldene Eigenschaften die wir uns im Späteren zunutze machen wollen. Zunächst erst einmal noch ein paar Identitäten die wir später auch noch verwenden.\nDie erste Identität bezieht sich auf das Kreuzprodukt der Abweichungen von \\(X_i\\) und \\(Y_i\\) von ihren jeweiligen Mittelwerten.\n\\[\\begin{align*}\n\\sum(X_i-\\bar{X})(Y_i-\\bar{Y}) &= \\sum(X_i - \\bar{X})Y_i  -\\underbrace{\\sum(X_i - \\bar{X})}_{=0}\\bar{Y}  \\\\\n&= \\sum(X_i - \\bar{X})Y_i\n\\end{align*}\\]\nWenn wir in der Formel \\((Y_i-\\bar{Y})\\) durch \\((X_i-\\bar{X})\\) austauschen, folgt noch eine weitere nützliche Identität:\n\\[\n\\sum(X_i-\\bar{X})^2 = \\sum(X_i - \\bar{X})X_i\n\\]\nWerden die jeweiligen \\(k_i\\) mit den dazugehörigen \\(X_i\\) multipliziert und die Definition der \\(k_i\\) (siehe Gleichung 6.8) beachten, erhalten wir:\n\\[\n\\sum k_i X_i = \\frac{\\sum(X_i - \\bar{X})X_i}{\\sum(X_i-\\bar{X})^2} = \\frac{\\sum(X_i-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2} = 1\n\\]\nD.h. Die Summe der \\(k_i X_i\\) ist gleich \\(1\\). Aus der Definition Gleichung 6.8 folgt weiterhin.\n\\[\n\\sum k_i = \\sum \\left(\\frac{X_i-\\bar{X}}{\\sum(X_i-\\bar{X})^2}\\right)= \\frac{\\sum(X_i-\\bar{X})}{\\sum(X_i-\\bar{X})^2} = \\frac{0}{\\sum(X_i-\\bar{X})^2} = 0\n\\]\nD.h. die Summe der \\(k_i\\) ist gleich Null.\nWenn wir jetzt wieder die Definition unseres Schätzer für \\(\\beta_1\\) verwenden (siehe ?eq-slm-basics-norm1). Dann erhalten unter der Verwendung der Identität der Kreuzprodukte den gewünschten Zusammenhang zwischen \\(b_1\\) und \\(Y_i\\).\n\\[\\begin{align*}\nb_1 &= \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} \\\\\n&= \\frac{\\sum(X_i - \\bar{X})Y_i}{\\sum(X_i - \\bar{X})^2} = \\sum k_i Y_i\\\\\n\\end{align*}\\]\nWenden wir jetzt den Erwartungswert auf \\(Y_i\\) an, dann werden die \\(k_i\\) als konstant angesehen und nur die \\(Y_i\\) sind Zufallsvariablen. Da aber \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) gilt und in dieser Formel wiederum nur \\(\\epsilon_i\\) eine Zufallsvariable mit \\(\\beta_0\\) und \\(\\beta_1 X_i\\) konstant ist und zudem die \\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) also \\(E[\\epsilon_i] = 0\\) laut der Annahme gilt, folgt:\n\\[\\begin{align*}\n    E[b_1] &= E\\left[\\sum k_i Y_i\\right] = \\sum k_i E[Y_i] = \\sum k_i (\\beta_0 + \\beta_1 X_i) \\\\\n    &= \\beta_0 \\sum k_i + \\beta_1 \\sum k_i X_i = \\beta_1\n\\end{align*}\\]\nD.h. ?eq-slm-basics-norm1 ist ein erwartungstreuer Schätzer für \\(\\beta_1\\). Das gleiche gilt auch für den Schätzer \\(b_0\\) für \\(\\beta_0\\).\nLeiten wir noch eine weitere Identität über die Summe der \\(k_i^2\\) her:\n\\[\n\\sum k_i^2 = \\sum \\left[\\frac{X_i-\\bar{X}}{\\sum(X_i-\\bar{X})^2}\\right]^2 = \\frac{\\sum(X_i-\\bar{X})^2}{\\left[\\sum(X_i-\\bar{X})^2\\right]^2} = \\frac{1}{\\sum(X_i-\\bar{X})^2}\n\\] Können wir auch noch die Varianz bzw. den Standardfehler unseres Schätzers für \\(\\beta_1\\) herleiten. Es gilt nämlich:\n\\[\\begin{align*}\n    \\sigma^2[b_1] &= \\sigma^2\\left[\\sum k_i Y_i\\right] = \\sum k_i^2 \\sigma^2[Y_i] \\\\\n    &= \\sum k_i^2 \\sigma^2 = \\sigma^2 \\sum k_i^2 \\\\\n    &= \\sigma^2 \\frac{1}{\\sum(X_i-\\bar{X})^2}\n    \\label{eq-slm-inf-beta1-deriv}\n\\end{align*}\\]\nWir erhalten die bereits eingeführte Formel. Wiederum eine Einsicht aus der Herleitung der Formel folgt, dass die Varianz \\(\\sigma^2\\) als konstant angesehen wird, d.h. \\(\\sigma_i^2 = \\sigma^2\\). Dies hat uns erlaubt im zweiten Schritt \\(\\sigma^2\\) aus der Summe heraus zu ziehen. Wenn die Varianz \\(\\sigma^2\\) nicht konstant ist, dann ist der berechnete Standardfehler für \\(\\hat{\\beta}_1 = b_1\\) nicht korrekt.\nEine interessante Eigenschaft des Standardfehler von \\(\\hat{\\beta}_1\\) ist in Formel \\(\\eqref{eq-slm-inf-beta1-deriv}\\) zu sehen. Im Nenner stehen die Abweichungen der \\(X\\)-Werte vom Mittelwert \\(\\hat{X}\\). D.h. wenn die \\(X\\)-Werte weiter auseinander sind, dann führt dies dazu, dass der Standardfehler \\(\\sigma^2[b_1]\\) kleiner wird. Intuitive macht dies auch Sinn, wenn ich eine Gerade bestimmen will, dann ist es einfacher die Gerade anhand weit auseinander liegenden Stütztwerten zu bestimmen im Vergleich zu wenn ich eng beinander liegende \\(X\\)-Werte verwende."
  },
  {
    "objectID": "slm_inference.html#maximum-likelihood-methode-bei-der-einfachen-linearen-regression",
    "href": "slm_inference.html#maximum-likelihood-methode-bei-der-einfachen-linearen-regression",
    "title": "6  Inferenz",
    "section": "6.3 Maximum-likelihood Methode bei der einfachen linearen Regression",
    "text": "6.3 Maximum-likelihood Methode bei der einfachen linearen Regression\nEin anderer Herleitung für \\(\\beta_0\\) und \\(\\beta_1\\) kann über die sogenannten Maximum Likelihood durchgeführt werden. Dabei gehen direkt die Verteilungsannahmen direkt ein.\nFür eine gegebene Zufallsvariable die jeweilige Dichte eines gegebenen Wertes über die Dichtefunktion berechnet werden. Wenn ein Zufallsvariable \\(X\\) einer Normalverteilung folgt, dann wird die Verteilung von \\(X\\) nach der bereits kennengelernte Dichtefunktion der Normalverteilung beschrieben.\n\\[\nf(X|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2}\\frac{(X - \\mu)^2}{\\sigma^2}\\right)\n\\]\nHier wird die Dichte von \\(X\\) als eine Funktion von \\(\\mu\\) und \\(\\sigma^2\\) aufgefasst. Es ist aber auch möglich, die Zufallsvariable \\(X\\) als gegeben anzusehen und die Dichte für verschiedene Werte von \\(\\mu\\) und \\(\\sigma^2\\) abzutragen. Der Einfachheit halber gehen wir davon aus, dass \\(\\sigma^2\\) gegeben sei und wir \\(\\mu\\) nicht kennen. Eine mögliche Fragestellung ist jetzt, für einen beobachteten Wert \\(x\\), welcher Wert von \\(\\mu\\) ist am plausibelsten?\nTragen wir dazu verschiedene Dichtewerte für ein gegebenes \\(x\\) in Abhängigkeit von verschiedenen \\(\\mu\\) ab.\nD.h. wir interpretieren die Funktion als:\n\\[\nf(\\mu|x,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2}\\frac{(X - \\mu)^2}{\\sigma^2}\\right)\n\\]\nDiese Funktion wird als die likelihood-Funktion bezeichnet. Das Maximum dieser Funktion kann als derjenige Wert interpretiert werden bei dem derjenige Wert von \\(\\mu\\) die maximal mögliche Dichte einnimmt.\nDie Likelihood-Funktion ist eine Funktion, die die Wahrscheinlichkeit beschreibt, mit der eine gegebene Stichprobe, in Abhängikeit von den Parametern aus einer bestimmten Verteilung stammt. Die Likelihood-Funktion gibt also an, wie gut die beobachteten Daten zu einem bestimmten Satz von Parametern passen.\nFormal wird die Likelihood-Funktion als die gemeinsame Wahrscheinlichkeitsdichte der Stichprobe beschrieben, betrachtet als Funktion der Parameter. Dabei werden die beobachteten Werte als festgelegt und die Parameter als Variablen betrachtet. Die Likelihood-Funktion ist also eine Funktion der Parameter, die die Wahrscheinlichkeit der beobachteten Daten als Funktion dieser Parameter beschreibt. Die Likelihood-Funktion ist dabei keine Dichtefunktion und beschreibt somit keine Wahrscheinlichkeiten. Dementsprechend ist gilt für das Integral der Likelihood-Funktion \\(\\int L(\\mu|X,\\sigma^2) d\\mu \\neq 1\\) bzw. ist \\(=1\\) per Zufall.\nIn unserem Regressionsfall nimmt die Likelihood-Funktion für einen einzelnen Wert die folgende Form an:\n\\[\nL(\\beta_0, \\beta_1, \\sigma^2|y_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2\\sigma^2}\\right)\n\\]\nBei unserer Regressionsanalyse haben wir jedoch nicht nur einen einzigen beobachteten Wert \\((y_i, x_i)\\) sondern \\(N\\) beobachtete Werte. Da die Werte unabhängig voneinander sind (laut der Annahmen), werden die jeweiligen likelihoods miteinander multipliziert. Die resultierende Likelihood-Funktion nimmt dann die folgenden Form an:\n\\[\\begin{align*}\nL(\\beta_0, \\beta_1, \\sigma^2) &= \\prod_{i=1}^{N} f(y_i | x_i; \\beta_0, \\beta_1, \\sigma^2) \\\\\n&= \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2\\sigma^2}\\right) \\\\\n&= \\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\right)^N \\exp\\left(\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\right) \\\\\n&= \\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{N/2} \\exp\\left(\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\right) \\\\\n\\end{align*}\\]\nDie Idee ist jetzt wieder die Gleiche. Wir versuchen das Maximum dieser Funktion zu finden, da die Werte \\(\\beta_0, \\beta_1\\) und \\(\\sigma^2\\) dann so gewählt sind, dass sie die höchste likelihood haben. Der Ansatz erfolgt wieder mechanisch ,indem wie bei der Herleitung der Normalengleichungen, die partiellen Ableitungen berechnet werden, diese gleich Null gesetzt werden und das resultierende Gleichungssystem gelöst wird. Zu beachten hierbei, wir haben in jedem Produktterm die gleichen Parameter \\(\\beta_0, \\beta-1\\) und \\(\\sigma^2\\) und die jeweiligen beobachteten \\((y_i, x_i)\\) Tuple werden als gegeben angesehen.\nUm die Berechnungn zu vereinfachen, bietet sich bei der Likelihoo-Funktion ein Trick an. Es wird nicht Likelihood-Funktion abgeleitet, sondern der Logarithmus der Likelihood-Funktion. D.h. die Funktion wird transformiert. Bei der Logarithmus-Funktion handelt es sich um eine sogenannte bijektive Funktion. Eine bijektive Funktion ist eine Funktion die jedem Element in der Ursprungsmenge genau ein Element in der Zielmenge zuordnet und ebenfalls umgekehrt. Dadurch kommt es zu keinen Kollisionen oder Auslassungen. Einfach gesagt, wenn die Funktion \\(y = f(x) = log(x)\\) ist, dann wird jedem \\(x\\) genau ein \\(y\\) zugeordnet. Bzw. anders herum, wenn ich \\(y\\) kenne, dann kenne ich auch den Wert von \\(x\\) mit \\(f(x) = y\\) bzw. \\(x = f^{-1}(y) = \\exp(y)\\). Dadurch, das die Logarithmus-Funktion bijektiv ist, führt dies dazu, dass das Maximum der ursprünglichen Funktion \\(L(\\beta_0, \\beta_1, \\sigma^2)\\) an der gleichen Stelle auftritt wie bei der transformierten Funktion \\(\\ln L(\\beta_0, \\beta_1, \\sigma^2)\\).\nWenn jetzt die Eigenschaften der Logarithmusfunktion, speziell des natürlichen Logarithmus, beachtet werden, dann wird auch klar, warum es Sinn machen könnte die Likelihood-Funktion mit dem Logarithmus zu transformieren, da aus den Produkten Summen werden mit denen einfacher umgegangen werden kann:\n\\[\\begin{align*}\n\\log(xy) &= \\log(x) + \\log(y) \\\\\n\\log\\left(\\frac{x}{y}\\right) &= \\log(x) - \\log(y) \\\\\n\\log(x^n) &= n\\log(x) \\\\\n\\log(\\exp(x)) &= x \\\\\n\\log(1) &= 0\n\\end{align*}\\]\nDer Logarithmus angewendet auf \\(L(\\beta_0, \\beta_1, \\sigma^2)\\) resultiert dann in der folgenden Funktion:\n\\[\\begin{align*}\n\\ell(\\beta_0, \\beta_1, \\sigma^2) &= \\ln L(\\beta_0, \\beta_1, \\sigma^2) \\\\\n&= \\ln \\left[\\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{N/2} \\exp\\left(-\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\right)\\right] \\\\\n&= \\ln \\left[\\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{N/2} \\right] + \\ln \\left[\\exp\\left(-\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\right)\\right] \\\\\n&= \\frac{N}{2} \\ln \\left[\\left(\\frac{1}{2\\pi \\sigma^2}\\right) \\right] -\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2} \\\\\n&= -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i)^2\n\\end{align*}\\]\nWir die Funktion \\(\\ell(\\beta_0, \\beta_1, \\sigma^2)\\) wieder partiell nach \\(\\beta_0\\) und \\(\\beta_1\\) abgeleitet und gleich Null gesetzt erhalten wir das gleiche Gleichungssystem wie bei den vorhergehenden Herleitungen über die Abweichungen von der Regressionsgeraden. z.B.\n\\[\\begin{align*}\n\\frac{\\partial \\ell(\\beta_0, \\beta_1, \\sigma^2)}{\\partial \\beta_0} &= \\frac{\\partial}{\\partial \\beta_0} -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n&= \\frac{2}{2\\sigma^2}\\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)\n\\end{align*}\\] Wenn dieser Ausdruck gleich Null gesetzt erhalten wir den gleichen Ausdruck wie unter"
  },
  {
    "objectID": "slm_inference.html#konfidenzintervalle-für-die-koeffizienten",
    "href": "slm_inference.html#konfidenzintervalle-für-die-koeffizienten",
    "title": "6  Inferenz",
    "section": "6.4 Konfidenzintervalle für die Koeffizienten",
    "text": "6.4 Konfidenzintervalle für die Koeffizienten\nWie wir im oberen Abschnitt gesehen haben, sind unsere Schätzer für die Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\) mit Unsicherheiten behaftet die sich in Form der Standardfehler ausdrücken. Wir können nun, diese standardfehler wiederum verwenden um Konfidenzintervalle für die Koeffizienten zu bestimmen.\n\\[\\begin{equation}\n\\hat{\\beta_j} \\pm q_{t_{\\alpha/2,df=N-2}} \\times \\hat{\\sigma}_{\\beta_j}\n\\label{eq-slm-inf-conf-0}\n\\end{equation}\\]\nWie in Formel\\(\\eqref{eq-slm-inf-conf-0}\\) zu sehen berechnet sich das Konfidenzintervall nach dem üblichen Muster: Schätzer \\(\\pm\\) Quantile \\(\\times\\) Standardfehler. Im vorliegenden Falle wird die Quantile aus der \\(t\\)-Verteilung mit \\(N-2\\) Freiheitsgarden bestimmt. Wie vorher bereits betont, das Konfidenzintervall erlaubt keine Aussage über die Wahrscheinlichkeit mit der der wahre Koeffizient in dem Intervall liegt, sondern gibt an welche \\(H_0\\)-Hypothesen mit den Daten kompatibel sind. Daher soll in der Ergebnisdokumentation das Konfidenzintervall angegeben und spätenstens in der Diskussion die obere und die untere Schranke diskutiert werden.\nIn R kann das Konfidenzintervall mit der Funktion confint() berechnet und ausgegeben werden.\n\nconfint(mod)\n\n                 2.5 %    97.5 %\n(Intercept) -0.6076488 0.3305767\nv_ms         0.7111082 0.8110957\n\n\nWie die Koeffizienten haben die Konfidenzintervall die gleiche Einheit wie die abhängige Variable und können daher direkt interpretiert werden. Im vorliegenden Fall sollte daher besprochen werden welche Bedeutung ein Koeffizient von \\(\\beta_1 = 0.7\\) bzw. von \\(\\beta_1 = 0.8\\) für die Interpretation des Modell hat.\nNoch einmal zu erwähnen ist, dass die beiden Parameter \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) welche die Regressionsgerade beschreiben, Schätzer für die Parameter aus einer Population sind der die beiden Parameter \\(\\beta_0\\) und \\(\\beta_1\\) den zugrundeliegenden Zusammenhang zwischen den beiden Variablen beschreiben. Diese betrachtung ist parallel zu derjenigen, wenn wir z.B. anhand des Mittelwerts \\(\\bar{x}\\) den währenen Populationsmittelwert \\(\\mu\\) versuchen zu schätzen. D.h. wir haben eine Populationsregressionsgerade, die wir mit Hilfe der Daten versuchen zu schätzen. Die wahre Regressionsgerade werden wird aber niemals mit 100%-iger Sicherheit bestimmen, eben genausowenig wie wir den Populationsmittelwert \\(\\mu\\) nicht mittels \\(\\bar{x}\\) bestimmen können."
  },
  {
    "objectID": "slm_inference.html#weiteres-material",
    "href": "slm_inference.html#weiteres-material",
    "title": "6  Inferenz",
    "section": "6.5 Weiteres Material",
    "text": "6.5 Weiteres Material\nAltman und Krzywinski (2015) und Kutner u. a. (2005, p.40–48)\n\n\n\n\nAltman, Naomi, und Martin Krzywinski. 2015. „Points of Significance: Simple linear regression.“ Nature methods 12 (11).\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York."
  },
  {
    "objectID": "slm_model_fit.html#residuen",
    "href": "slm_model_fit.html#residuen",
    "title": "7  Modellfit",
    "section": "7.1 Residuen",
    "text": "7.1 Residuen\nDazu schauen wir uns zunächst noch einmal an, was überhaupt Residuen \\(e_i\\) sind und gehen noch mal von den grundlegenden Modellannahmen aus (siehe Formel \\(\\eqref{eq-sim-model-lr}\\)).\n\\[\\begin{equation}\ny_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i, \\qquad \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\n\\label{eq-sim-model-lr}\n\\end{equation}\\]\nDas lineare Regressionsmodell geht von einem linearen Zusammenhang in den Koeffizienten zwischen der Variablen \\(x_i\\) und den Variablen \\(y_i\\) aus. Additiv kommt daz ein normalverteilter Fehler \\(\\epsilon_i\\). Die Normalverteilung der \\(\\epsilon_i\\) habem einen Erwartungswert von \\(\\mu = 0\\) und eine Standardabweichung von \\(\\sigma\\). Die Standardabweichung \\(\\sigma\\) ist zunächst unbekannt und muss über die Daten abgeschätzt werden. Dies führt dazu, dass \\(y_i\\) für jeden gegebenen Wert von \\(x_i\\) einer Normalverteilung mit \\(\\mathcal{N}(\\beta_0 + beta_1 x_i, \\sigma^2)\\) folgen und der bereits bekannten graphischen Darstellung (siehe Abbildung 7.1).\n\n\n\n\n\nAbbildung 7.1: Beispiel einer Regressionsgeraden und der Verteilung der Residuen um den Vorhersagewert \\(\\hat{y_i}\\)\n\n\n\n\nFür jeden gegebenen Wert von \\(X\\) sind die \\(Y\\)-Werte Normalverteilt. Die Varianz dieser Normalverteilungen ist gleich \\(\\sigma\\) während der Mittelwert \\(\\mu\\) immer um den Wert der Regressionsgeraden verschoben ist. D.h. die Streuung von \\(\\epsilon_i\\) überträgt sich auf die Streuung von \\(y_i\\) für jeden gegebenen \\(X\\)-Wert. Ohne den zufälligen Einfluss der Fehlerwerte würden wir alle \\(y_i\\)-Werte perfekt auf der Regressionsgeraden erwarten. Dies deutet daher auch schon eine Möglichkeit an die Residuen \\(\\epsilon_i\\) mittels der Daten abzuschätzen. Man verwendet die Abweichungen der beobachteten Werten \\(y_i\\) von den vorhergesagten Werten \\(\\hat{y}_i\\) auf der Regressionsgeraden (siehe Formel \\(\\eqref{eq-sim-model-res-1}\\)).\n\\[\\begin{equation}\n\\hat{\\epsilon}_i = e_i = y_i - \\hat{y_i}\n\\label{eq-sim-model-res-1}\n\\end{equation}\\]\nDiese Abweichungen \\(e_i\\) können als Schätzer \\(\\hat{\\epsilon}_i\\) für die wahren Residuen \\(\\epsilon_i\\) verwendet werden (siehe Abbildung 7.2).\n\n\n\n\n\nAbbildung 7.2: Examplarische Darstellung der Berechnung der Residuen \\(e_i\\) als Abweichung der beobachteten Werte \\(y_i\\) von den vorhergesagten Werten \\(\\hat{y}_i\\)\n\n\n\n\nDa die Normalverteilungen der \\(\\epsilon_i\\) für jeden \\(X\\)-Wert immer gleich sein sollten bis auf die Verschiebung von \\(\\mu_{Y|X}\\), deutet dies ebenfalls eine erste Möglichkeit an, die Modellannahmen graphisch zu überprüfen. Wenn die Residuen \\(e_i\\) geben die vorhergesagten Werte \\(\\hat{y}_i\\) abgetragen werden, dann sollte die Verteilung der Residuen \\(e_i\\) überall nahezu gleich sein, da die Streuung \\(\\sigma\\) unabhängig von der Position auf der Regressionsgerade ist. In R können die Residuen mittels der Funktion residuals() bzw. der Kurzform resid() ermittelt werden. residuals() erwartet als Parameter das gefittete lm()-Objekt.\n\nresiduals(mod)\n\n          1           2           3           4           5           6 \n -9.3009275  -9.3682884 -11.2176585  -5.5721082  -6.3635647  -7.4162019 \n          7           8           9          10          11          12 \n -3.9665569  -8.7152962  -3.8032898  -0.4662810  -2.0491941  -2.1323841 \n         13          14          15          16          17          18 \n  0.1867102  -0.3382894  -2.7300208  -4.0317532  -6.1475804  -0.3782884 \n         19          20          21          22          23          24 \n  1.1267111  -0.4588401  -2.0417532  -2.5546673  -0.2276585   2.3352546 \n         25          26          27          28          29          30 \n -3.2075794   2.7949787   2.9982458   2.4379709   1.1162385   3.1894284 \n         31          32          33          34          35          36 \n  7.8049787  -0.9063196   4.0336013  11.3778918   7.6817897   8.6991516 \n         37          38          39          40          41 \n 12.3052546   7.2595065  20.9634431  -1.3171838  -1.5994700 \n\n\nDie anhand des Modells vorhergesagten Werte \\(\\hat{y_i}\\) werden der Funktion predict() berechnet. Diese Funktion werden wir uns im nächsten Kapitel noch ausführlich betrachten. Als Parameter wird wiederum das gefittete lm()-Modell übergeben.\n\npredict(mod)\n\n       1        2        3        4        5        6        7        8 \n13.35093 14.39829 16.24766 13.61211 14.40356 15.45620 12.02656 16.77530 \n       9       10       11       12       13       14       15       16 \n12.82329 11.49628 13.07919 14.14238 12.82329 13.34829 15.72002 17.04175 \n      17       18       19       20       21       22       23       24 \n19.15758 14.39829 13.87329 15.45884 17.04175 17.57467 16.24766 14.66475 \n      25       26       27       28       29       30       31       32 \n20.20758 15.19502 15.99175 17.57203 18.89376 17.83057 15.19502 23.90632 \n      33       34       35       36       37       38       39       40 \n19.94640 13.61211 17.30821 17.31085 14.66475 20.74049 12.02656 13.32718 \n      41 \n13.60947 \n\n\nBeide Funktionen, resid() und predict() geben die berechneten Werte in der Reihenfolge aus, in der die Originaldaten an lm() übergeben wurde. D.h. \\(e_1\\) und \\(\\hat{y}_1\\) gehören zum ersten \\(X\\)-Wert \\(x_1\\) aus den Originaldaten. Mit Hilfe dieser beiden Variablen kann nun ein Residuenplot erstellt werden (siehe Abbildung 7.3).\n\n\n\n\n\nAbbildung 7.3: Residuenplot der Residuen \\(e_i\\) gegen die vorhergesagten Werte \\(\\hat{y}_i\\)\n\n\n\n\nDer Plot sollte im Optimalfall so aussehen, dass die Residuen \\(e_i\\) gleichmäßig oberhalb und unterhalb um die Nulllinie verteilt sind und keine weiteren Strukturen oder Muster im Zusammenhang mit \\(\\hat{y}_i\\) zu erkennen sind. In dem Residuenplot in Abbildung 7.3 ist zunächst einmal kein größeres Problem zu erkennen, bis auf den einen Wert links oben.\nUm besser zu Verstehen wir Problem aussehen könnten, schauen wir uns zwei Residuenplots an, bei denen eine Struktur zu erkennen ist (siehe Abbildung 7.4)\n\n\n\n\n\n\n\n(a) Parabelförmiger Zusammenhang zwischen \\(e_i\\) und \\(\\hat{y}_i\\)\n\n\n\n\n\n\n\n(b) Ansteigende Streuung mit größer werdendem \\(\\hat{y}_i\\)\n\n\n\n\nAbbildung 7.4: Residuenplots die Probleme anzeigen.\n\n\nIn Abbildung 7.4 (a) ist ein parabelförmiger Zusammenhang zwischen \\(e_i\\) und \\(\\hat{y}_i\\) zu erkennen. Für kleine und große \\(\\hat{y}_i\\) Werte sind die Residuen \\(e_i\\) negativ während für mittlere Werte von \\(\\hat{y}_i\\) die Residuen \\(e_i\\) positiv sind. Diese deutet darauf hin, das zusäztliche Struktur in den Daten nicht im Modell erfasst wird und führt dazu dass die Modellannahmen der Normalverteilung der \\(\\epsilon_i\\) nicht erfüllt sind.\nIn Abbildung 7.4 (b) ist dagegen eine anderes Problem zu beobachten, die Residuen \\(e_i\\) zeigen zwar keine Struktur bezüglich positiv zu negativen Werten, allerdings werden die Abweichung von \\(0\\) mit größer werdenen \\(\\hat{y}_i\\) immer stärker. Dies deutet darauf hin, das die Streuung der Daten nicht gleich ist. Dies wird als Heteroskedastizität bezeichnet und deutet wiederum auf eine Verletzung der Annahmen bei der Homoskedastitzität ausgegangen wird. D.h. die Streuung soll über den gesamten Bereich von \\(\\hat{y}_i\\) gleich bleiben.\n::: {#def homoscedasticity}\n\n7.1.1 Homoskedastizität\nWenn die Varianz der Residuen \\(\\epsilon_i\\) in einem Regressionsmodell unabhängig von der Größe der Vorhersagevariable \\(X_i\\) gleich ist, wird die als Homoskedastizität bezeichnet. Die Streuung der Residuen ist dann für alle Werte \\(X_i\\) gleich. Wenn dies nicht der Fall ist, wird von Heteroskedastizität gesprochen. :::\nEine weitere Möglichkeit die Residuen zu überprüfen ist die Anfertigung von sogenannten qq-Plots. Dies ermöglichen etwas strukturierter die Verteilung der Residuen zu überprüfen.\n\n\n7.1.2 Quantile-Quantile-Plots\nqq-Plot ist die Kurzform von Quantile-Quantile-Plot. D.h. es werden die Quantilen von zwei Variablen gegeneinander abgetragen. Um die Funktionsweise besser zu verstehen schauen wir uns erst einmal ein Spielzeugbeispiel an. In Tabelle 7.1 ist eine kleiner Datensatz mit \\(n = 5\\) Datenpunkten angezeigt.\n\n\n\n\nTabelle 7.1: Spielzeugbeispieldaten mit \\(n=5\\)\n\n\ny\n\n\n\n\n-2.0\n\n\n5.0\n\n\n-1.2\n\n\n0.1\n\n\n7.0\n\n\n\n\n\n\nWir wollen jetzt überprüfen ob dieser Datensatz einer Normalverteilung folgt (Wohlwissend das mit fünf Datenpunkten keine Verteilungsannahme überprüft werden kann). Dazu schauen wir uns zunächst noch einmal die bekannte Standardnormalverteilung \\(\\Phi(z) = \\mathcal{N}(\\mu=0,\\sigma^2=1)\\) an (siehe Abbildung 7.5).\n\n\n\n\n\nAbbildung 7.5: Dichtefunktion der Standardnormalverteilung\n\n\n\n\nIm ersten Schritt unterteilen wir die Standardnormalverteilung \\(\\Phi(z)\\) in \\(n+1 = 6\\) gleich große Flächen. D.h. die durch die Flächen bestimmten Abschnitte haben alle die gleiche Wahrscheinlichkeit (=Fläche unter der Dichtefunktion). Die Flächen werden durch jeweiligen Trennpunkte unterteilt die gleichzeitig die Quantilen sind.\n\n\n\n\n\nAbbildung 7.6: Unterteilung der Standardnormalverteilung in sechs gleich große Flächen\n\n\n\n\nIn unserem Fall haben wir \\(n=5\\) Datenpunkte, unterteilen also unsere Verteilung in \\(6\\) Abschnitte die jeweils eine Fläche von \\(p = \\frac{1}{6} = 0.17\\) haben. D.h. \\(\\frac{1}{6}\\) der Werte von \\(\\Phi(x)\\) liegen links des ersten Trennpunktes, \\(\\frac{2}{6}\\) der Werte von \\(\\Phi(x)\\) liegen links des zweiten Trennpunktes, usw. D.h. die Trennpunkte bestimmen die jeweiligen Quantilen, oder genauer die theoretischen Quantilen die unter der Verteilungsannahme erwartet werden.\nDie Idee hinter dem qq-Plot besteht nun darin, die empirischen Quantilen gegen die theoretischen Quantilen abzutragen (siehe Abbildung 7.7). Wenn die beobachteten Daten aus der gleichen Verteilung wie die theoretische Verteilung stammen, dann sollten die Punkte einer Geraden folgen. Die Steigung der Geraden ist \\(1\\), wenn es sich um die identischen Verteilungen handelt. Wenn die Steigung \\(\\neq1\\) ist, dann kommen die Datenpunkte aus der gleichen Familie sind aber um einen Skalierungsfaktor unterschiedlich bzw. um den Mittelwert verschoben. Die Punkte sollten aber trotzdem auf einer Geraden liegen.\n\n\n\n\n\nAbbildung 7.7: Skizze der theoretischen und der empirischen Verteilung mit unterschiedlicher Skalierung (Faktor \\(2\\times\\)) aber aus der gleichen Verteilungsfamilie. In beiden Graphen ist die gleiche Quartile markiert\n\n\n\n\nUm die empirischen Quartilen zu bestimmen, werden dazu zunächst die beobachteten Datenpunkte aus Tabelle 7.1 aufsteigend nach der Größe sortiert (siehe Tabelle 7.2). Diese Werte können als empirische Quantilen bezeichnet werden. Unter der Annahme, dass die Werte eine repräsentative Stichprobe aus der Verteilung darstellen, erwarten wir, dass wenn wir weitere Werte beobachten würden, etwa \\(\\frac{1}{6}\\) der Werte kleiner als der kleinste Wert wären, \\(\\frac{2}{6}\\) der weiteren Werte kleiner als der 2. kleinste Wert wären und so weiter und so fort.\n\n\nTabelle 7.2: Sortierte Datenwerte des Spielzeugbeispiels\n\n\nkleinster\n2.kleinster\nmittlerer\n2.größter\ngrößter\n\n\n\n\n-2\n-1.2\n0.1\n5\n7\n\n\n\n\nDaher, wenn die beobachteten Werte der angenommenen theoretischen Verteilung folgen, dann sollte ein Graph der empirischen Quartilen gegen die theoretischen Quartilen nahezu (Stichprobenvariabilität) einer Geraden folgen.\n\n\n\n\n\nAbbildung 7.8: Streudiagramm der empirischen Werte gegen die theoretischen Quantilen\n\n\n\n\nIn Abbildung 7.8 sind die empirischen Quartilen gegen die theoretischen Quantilen für unser kleines Beispiel abgetragen. Tatsächlich ist es in diesem Fall schwierig eine Gerade zu erkennen bzw. von einer zu sprechen, da es sich nur um besagte fünf Wert handelt. Nochmals, mit \\(n=5\\) kann eine realistische Verteilungsannahme nicht überprüft werden.\nWenn der Datensatz größer ist, dann eignet sich ein qq-Plot allerdings sehr gut Abweichungen zu erkennen. In Abbildung 7.9 sind verschiedene Beispiele abgetragen.\n\n\n\n\n\n\n\n(a) Perfekt\n\n\n\n\n\n\n\n(b) Enden schwer\n\n\n\n\n\n\n\n\n\n(c) Enden leicht\n\n\n\n\n\n\n\n(d) Rechtsschief\n\n\n\n\nAbbildung 7.9: Beispiele für verschiedenen qq-Plots\n\n\nIn Abbildung 7.9 (a) ist ein perfekter Zusammenhang zwischen den empirischen und den theoretischen Quantilen abgebildet. In diesem Falle wurden synthetisch für 50 normalverteiltet Zufallsdaten ein qq-Plot erstellt. Es ist zu sehen, das tatsächlich eine Gerade den Zusammenhang beschreibt. In Abbildung 7.9 (b) ist dagegen ein Zusammenhang abgetragen, bei dem die empirischen und die theoretische Verteilung nicht zusammenpassen. In diesem Fall sind die haben die Randwerte der empirischen Vereteilung eine höhere Wahrscheinlichkeit als die unter der theoretischen Verteilung zu erwarten ist. D.h. extreme Werte kommen in der beobachteten Verteilung öfter in der theoretischen Verteilung vor. Dies deutet darauf hin, dass die Streuung der Daten möglicherweise nicht korrekt modelliert wurde. In diesem Fall, wird von einer tail heavy Verteilung gesprochen.\nIn Abbildung 7.9 (c) ist der gegenteilige Effekt abgetragen. Hier hat die theoretische Verteilung mehr Wahrscheinlichkeitsmasse in den Randzonen als die empirische Verteilung. Die beobachtete Verteilung ist tail light. Entsprechend ist in Abbildung 7.9 (d) ein Beispiel abgebildet, bei dem nur eine der Randzonen zu viel Wahrscheinlichkeitsmasse besitzt. Da die theoretische Verteilung wiederum die Normalverteilung ist und diese Symmetrisch ist, deutet diese darauf hin, das die empirische Verteilung ähnlich wie in Abbildung 7.9 (b) in der rechten Randzone zu viele Werte hat und daher Rechtsschief ist.\nFür unsere Daten ergibt sich das folgende qq-Diagramm (siehe Abbildung 7.10)\n\n\n\n\n\nAbbildung 7.10: QQ-Diagramm der Residuen des ADAS-ADCS-Modells\n\n\n\n\nDer Graph sieht zunächst einmal gar nicht so schlecht aus. Allerdings deutet die Abweichung rechts oben darauf hin, das möglicherweise die Streuung nicht korrekt abgeschätzt wurde. Insbesondere ist ein Wert zu sehen, der im Verhältnis zu den anderen Werten schon relativ weit von der Gerade weg ist. Daher ist es hier angezeigt, diesen Wert noch einmal genauer zu untersuchen.\n\n\n7.1.3 qq-Plot in R\nIn R gibt es zwei direkte Methoden einen qq-Plot zu erstellen. Mittels des Standardgrafiksystem können mit den Funktionen qqnorm() und qqline() qq-Plots mit der dazugehörigen Gerade erstellt werden. Für das ggplot()-System stehen die geoms geom_qq() und geom_qq_line() zur Verfügung. Wichtig ist hierbei, das in aes() der Parameter sample definiert werden muss. Für unser Spielzeugbeispiel sieht dies folgendermaßen aus:\n\ndf_toy &lt;- tibble::tibble(y = c(-2, 5, -1.2, 0.1, 7))\nggplot(df_toy, aes(sample=y)) +\n  geom_qq() +\n  geom_qq_line()\n\n\n\n\nAbbildung 7.11: qq-Plot der Spielzeugdaten mittels ggplot()\n\n\n\n\n\n\n7.1.4 Standardisierte Residuen\nEine Möglichkeit so einen Wert zu untersuchen, ist abzuschätzen wie ungewöhnlich der zu dem Residuen \\(e_i\\) gehörende \\(y_i\\)-Wert ist. Ein Problem der einfachen Residuen \\(e_i\\) ist, dass diese laut der Modellannahmen die gleiche Varianz \\(\\sigma^2\\) haben sollten. Allerdings, auf Grund der Art, wie die \\(e_i\\) berechnet werden, folgt die Randbedingung, dass die Summe der \\(e_i\\) gleich Null ist, \\(\\sum_{i=1}^n e_i = 0\\). Dies führt dazu, dass die einfachen Residuen nicht unanbhängig voneinander sind und nicht immer Homoskedastizität besitzen. Daher gibt es eine weitere Art Residuen anhand des Modell zu berechnen, die nicht unter diesen Beschränkungen leiden. Dies sind die standardisierten Residuen \\(e_{Si}\\). Dazu müssen wir uns zunächst mit Hebelwerte \\(h_i\\) beschäftigen.\n\n7.1.4.1 Hebelwerte\nWenn ein Modell an die Daten gefittet wird, dann haben nicht alle Werte den gleichen Einfluss auf die Modellparameter. Manche Werte üben einen stärkeren Einfluss auf das Modell aus als andere Werte. In Abbildung 7.12 ist ein Beispiel abgebildet für einen Datensatz bei dem ein einzelner Punkt einen übermäßig großen Einfluss auf das Modell ausübt.\n\n\n\n\n\nAbbildung 7.12: Beispiel für einen Datenpunkt mit einem großen Einfluss auf das Modell. Die resultierenden Regressionsgeraden sind mit dem Punkt (rot) und ohne den Punkt (grün) abgetragen.\n\n\n\n\nDer einzelen Punkt rechts oben in Abbildung 7.12 hat einen großen Einfluss auf die resultierende Regressionsgerade wie in der Abbildung zu sehen ist. Der Einfluss ist zum Teil durch den großen Abstand des \\(x_i\\)-Wertes vom Mittelwert der \\(x_i\\)-Werte \\(\\bar{x}\\) bestimmt. Der Einfluss jedes einzelnen \\(x\\)-Wertes wird mittels der sogenannten Hebelwerte \\(h_i\\) bestimmt. Die genaue Berechnung der Hebelwerte \\(h_i\\) ist für das weitere Verständnis allerdings nicht wichtig, sondern mehr das Verständnis des Konzepts. Die Hebelwerte \\(h_i\\) können Werte in \\(h_i \\in \\[1/n,1\\]\\) annehmen. In R können die Hebelwerte mit der Funktion hatvalues() berechnet werden.\nTragen wir in die Grafik die Hebelwerte in die Grafik Abbildung 7.12 ein (siehe Abbildung 7.13), dann ist zu sehen, dass der abgesetzte Wert auch den größten Hebelwert hat.\n\n\n\n\n\nAbbildung 7.13: Beispiel für einen Datenpunkt mit einem großen Einfluss auf das Modell. Die Werte geben die jeweiligen Hebelwerte \\(h_i\\) der Datenpunkte wieder.\n\n\n\n\nEine Daumenregel für die Hebelwerte ist der Schwellenwert von \\((2k+2)/n\\), wobei \\(k\\) die Anzahl der unabhängigen Variablen ist. Für den Beispieldatensatz in Abbildung 7.13 würde sich daher ein Wert von \\((2\\cdot 1+2)/30 = 0.13\\) ergeben. Entsprechend wäre der abgesetzte Wert mit einem Hebelwert von \\(h_i = 0.54\\) als problematisch einzustufen.\nNach diesem kurzen Exkurs zu den Hebelwerten \\(h_i\\), schauen wir uns für unsere weitere Betrachtung der Residuen zunächst den Zusammenhang zwischen der Varianz der Residuen in der Population \\(\\sigma^2\\) und der Varianz der geschätzten Residuen \\(\\sigma^2(\\hat{\\epsilon}_i) = \\sigma^2(e_i)\\) an. Es gilt:\n\\[\\begin{equation}\n\\sigma^2(e_i) = \\sigma^2 (1 - h_i)\n\\label{eq-slm-model-vare_i}\n\\end{equation}\\]\nD.h. wenn ein Datenpunkt \\(x_i\\) einen kleineren Einfluss auf das Modell ausübt und dementsprechend einen kleinen Hebelwert \\(h_i\\), dann wird die Varianz für diesen Wert nahezu korrekt eingeschätzt. Hat der Wert \\(x_i\\) allerdings, einen großen Hebelwert \\(h_i\\), führt die dazu, dass die Varianz für diesen Wert stärker unterschützt wird. Dieser Zusammenhang kann dazu benutzt werden standardisierte Residuen zu erstellen.\n\\[\\begin{equation}\ne_{Si} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_i}}\n\\label{eq-slm-model-stresid}\n\\end{equation}\\]\nDie standardisierten Residuen \\(e_{Si}\\) haben dazu die Eigenschaft, dass sie eine Varianz und damit Standardabweichung von \\(\\sigma^2(e_{Si}) = 1\\) haben, also Standardnormalverteilt \\(\\Phi(z)\\) sein sollten. Dadurch können Abweichungen von den Modellannahmen leichter Identifiziert werden, da die Skala normiert ist. In R kann die standardiserten Residuen \\(e_{Si}\\) mittels der Funktion rstandard() berechnet werden. Eine Standardgrafik zum inspizieren der standardisierten Residuen ist wiederum eine Abbildung der \\(e_{Si}\\) gegen die \\(\\hat{y}_i\\).\n\n\n\n\n\nAbbildung 7.14: Grafik der standardisierten Residuen \\(e_{Si}\\) gegen die Vorhersagewerte \\(\\hat{y}_i\\) für das ADL-Modell.\n\n\n\n\nDie Abbildung 7.14 sieht relativ ähnlich zu Abbildung 7.3 aus. Durch die Änderung der Skala ist jetzt aber leichter abschätzbar ob die Verteilung der erwarteten Normalverteilung folgt. D.h. etwa \\(\\frac{2}{3}\\) der Werte sollten zwischen \\(-1\\) und \\(1\\) liegen und etwa \\(95\\%\\) zwischen \\(-2\\) und \\(2\\). Bis auf den einen Punkt oben rechts, sieht alles soweit unauffällig aus.\n\n\n\n7.1.5 Studentized Residuals\nDie letzte Art von Residuen sind die sogenannten Studentized Residuals \\(e_{Ti}\\), die mittels der folgenden Formel berechnet werden.\n\\[\\begin{equation}\ne_{Ti} = \\frac{e_i}{\\hat{\\sigma}_{(-i)}\\sqrt{1-h_i}}\n\\label{eq-slm-model-rstudent}\n\\end{equation}\\]\nDie Formel \\(\\eqref{eq-slm-model-rstudent}\\) ist sehr ähnlich zu derer für die standardisierten Residuen, der einzige Unterschied ist der Term \\(\\hat{\\sigma}_{(-i)}\\). Dieser bezeichnet die Residualvarianz wenn dass Modell ohne den Datenpunkt \\(i\\) gefittet wird. D.h. wie stark verändert sich die Schätzung der Varianz wenn ein Datenpunkt weggelassen wird. Normalerweise sollte eine einzelner Punkt keinen übermäßigen Einfluss auf die geschätzte Varianz haben, daher können die Studentized Residuals dazu verwendet werden problematische Datenpunkte zu identifizieren. Wenn die tatsächlichen Residuen einer Normalverteilung folgen, dann kann gezeigt werden, dass die Studentized Residuals einer \\(t\\)-Verteilung mit \\(N-k-2\\) Freiheitsgeraden folgen. Daher könnte sogar ein formaler statistischer Test durchgeführt werden. In R können die Studentized Residuals \\(e_{Ti}\\) mittels der Funktion rstudent() berechnet werden und werden entsprechend den anderen Residuen in dem üblichen Graphen gegen die vorhergesagten Werte \\(\\hat{y}_i\\) abgetragen.\n\n\n\n\n\nAbbildung 7.15: Graph der Studentized Residuals \\(S_{Ti}\\) gegen die vorhergesagten Werte \\(\\hat{y}_i\\) vor das adl-Modell\n\n\n\n\n\n\n7.1.6 Übersicht über die Residuenarten\nIn Tabelle 7.3 sind noch einmal die drei Arten von Residuen aufgelistet.\n\n\nTabelle 7.3: Übersicht über verschiedene Arten von Residuen\n\n\n\n\n\n\n\nTyp\nBerechnung\nZiel\n\n\n\n\nEinfache Residuen\n\\(e_i = y_i - \\hat{y}_i\\)\nVerteilungsannahme\n\n\nStandardisierte Residuen\n\\(e_{Si} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_i}}\\)\nVerteilungsannahme\n\n\nStudentized Residuen\n\\(e_{Ti} = \\frac{e_i}{\\hat{\\sigma}_{(-i)}\\sqrt{1-h_i}}\\)\nEinfluss auf Modell\n\n\n\n\n\n\n7.1.7 summary()\nNach dieser Betrachtung der Residuen, die nach jedem Modellfit inspiziert werden sollten um zu überprüfen ob die Modellannahmen angemessen sind schauen wir uns noch einmal kurz die Ausgabe von summary() an.\n\n\n\nCall:\nlm(formula = adcs ~ adas, data = adl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2177  -3.8033  -0.4663   2.7950  20.9634 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.5445     4.3052   6.166 3.05e-07 ***\nadas         -0.2638     0.1015  -2.599   0.0131 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.516 on 39 degrees of freedom\nMultiple R-squared:  0.1477,    Adjusted R-squared:  0.1258 \nF-statistic: 6.757 on 1 and 39 DF,  p-value: 0.01312\n\n\nNach der Wiedergabe des gefitten Modells erfolgt direkt eine Zusammenfassung der Residuen über Minimum und Maximum, Q1 und Q3 und den Median. Jetzt sollte daher auch besser nachvollziehbar sein, warum es sinnvoll ist diese Statistiken über die Residuen direkt anzugeben. Die beiden Extremwerte geben einen ersten Überblick auf mögliche Ausreißer, während die erste Quartile Q1 und die dritte Quartile Q3 möglich Asymmetrien in der Verteilung der Residuen anzeigen. Laut der Annahem der Residuen als Normalverteilt mit \\(\\mu = 0\\), sollten diese beiden Werte etwa gleich weit von Null entfernt sein. Dementsprechend sollte der Median nahe an Null dran sein. Was nah ist, kommt dabei immer auf die Einheit der abhängigen Variablen an, wenn der Abstand in Kilometern ist kann ein kleiner Wert schon problematisch sein, während wenn eine Sprungweite in Mikrometern angeben wird eine großer Wert unbedenklich sein kann. Der Schätzerwert für \\(\\sigma\\) selbst, wir unten mit Residual standard error angegeben.\nIm vorliegenden Fall des Modells für die adl-Daten ist der Median dementsprechend doch etwas weit von Null entfernt und der geschätzte Residualfehler \\(\\hat{\\sigma} = 6.52\\) ebenfalls relativ groß. \\(\\hat{\\sigma}\\) kann mittels der Funktion sigma() erhalten werden.\n\n\n7.1.8 Zum Nachlesen\nKutner u. a. (2005, p.100–114) Altman und Krzywinski (2016b) Fox (2011, p.285–296)"
  },
  {
    "objectID": "slm_model_fit.html#quantile-quantile-plots",
    "href": "slm_model_fit.html#quantile-quantile-plots",
    "title": "7  Modellfit",
    "section": "7.3 Quantile-Quantile-Plots",
    "text": "7.3 Quantile-Quantile-Plots\nqq-Plot ist die Kurzform von Quantile-Quantile-Plot. D.h. es werden die Quantilen von zwei Variablen gegeneinander abgetragen. Um die Funktionsweise besser zu verstehen schauen wir uns erst einmal ein Spielzeugbeispiel an. In Tabelle 7.1 ist eine kleiner Datensatz mit \\(n = 5\\) Datenpunkten angezeigt.\n\n\n\n\nTabelle 7.1: Spielzeugbeispieldaten mit \\(n=5\\)\n\n\ny\n\n\n\n\n-2.0\n\n\n5.0\n\n\n-1.2\n\n\n0.1\n\n\n7.0\n\n\n\n\n\n\nWir wollen jetzt überprüfen ob dieser Datensatz einer Normalverteilung folgt (Wohlwissend das mit fünf Datenpunkten keine Verteilungsannahme überprüft werden kann). Dazu schauen wir uns zunächst noch einmal die bekannte Standardnormalverteilung \\(\\Phi(z) = \\mathcal{N}(\\mu=0,\\sigma^2=1)\\) an (siehe Abbildung 7.5).\n\n\n\n\n\nAbbildung 7.5: Dichtefunktion der Standardnormalverteilung\n\n\n\n\nIm ersten Schritt unterteilen wir die Standardnormalverteilung \\(\\Phi(z)\\) in \\(n+1 = 6\\) gleich große Flächen. D.h. die durch die Flächen bestimmten Abschnitte haben alle die gleiche Wahrscheinlichkeit (=Fläche unter der Dichtefunktion). Die Flächen werden durch jeweiligen Trennpunkte unterteilt die gleichzeitig die Quantilen sind.\n\n\n\n\n\nAbbildung 7.6: Unterteilung der Standardnormalverteilung in sechs gleich große Flächen\n\n\n\n\nIn unserem Fall haben wir \\(n=5\\) Datenpunkte, unterteilen also unsere Verteilung in \\(6\\) Abschnitte die jeweils eine Fläche von \\(p = \\frac{1}{6} = 0.17\\) haben. D.h. \\(\\frac{1}{6}\\) der Werte von \\(\\Phi(x)\\) liegen links des ersten Trennpunktes, \\(\\frac{2}{6}\\) der Werte von \\(\\Phi(x)\\) liegen links des zweiten Trennpunktes, usw. D.h. die Trennpunkte bestimmen die jeweiligen Quantilen, oder genauer die theoretischen Quantilen die unter der Verteilungsannahme erwartet werden.\nDie Idee hinter dem qq-Plot besteht nun darin, die empirischen Quantilen gegen die theoretischen Quantilen abzutragen (siehe Abbildung 7.7). Wenn die beobachteten Daten aus der gleichen Verteilung wie die theoretische Verteilung stammen, dann sollten die Punkte einer Geraden folgen. Die Steigung der Geraden ist \\(1\\), wenn es sich um die identischen Verteilungen handelt. Wenn die Steigung \\(\\neq1\\) ist, dann kommen die Datenpunkte aus der gleichen Familie sind aber um einen Skalierungsfaktor unterschiedlich bzw. um den Mittelwert verschoben. Die Punkte sollten aber trotzdem auf einer Geraden liegen.\n\n\n\n\n\nAbbildung 7.7: Skizze der theoretischen und der empirischen Verteilung mit unterschiedlicher Skalierung (Faktor \\(2\\times\\)) aber aus der gleichen Verteilungsfamilie. In beiden Graphen ist die gleiche Quartile markiert\n\n\n\n\nUm die empirischen Quartilen zu bestimmen, werden dazu zunächst die beobachteten Datenpunkte aus Tabelle 7.1 aufsteigend nach der Größe sortiert (siehe Tabelle 7.2). Diese Werte können als empirische Quantilen bezeichnet werden. Unter der Annahme, dass die Werte eine repräsentative Stichprobe aus der Verteilung darstellen, erwarten wir, dass wenn wir weitere Werte beobachten würden, etwa \\(\\frac{1}{6}\\) der Werte kleiner als der kleinste Wert wären, \\(\\frac{2}{6}\\) der weiteren Werte kleiner als der 2. kleinste Wert wären und so weiter und so fort.\n\n\nTabelle 7.2: Sortierte Datenwerte des Spielzeugbeispiels\n\n\nkleinster\n2.kleinster\nmittlerer\n2.größter\ngrößter\n\n\n\n\n-2\n-1.2\n0.1\n5\n7\n\n\n\n\nDaher, wenn die beobachteten Werte der angenommenen theoretischen Verteilung folgen, dann sollte ein Graph der empirischen Quartilen gegen die theoretischen Quartilen nahezu (Stichprobenvariabilität) einer Geraden folgen.\n\n\n\n\n\nAbbildung 7.8: Streudiagramm der empirischen Werte gegen die theoretischen Quantilen\n\n\n\n\nIn Abbildung 7.8 sind die empirischen Quartilen gegen die theoretischen Quantilen für unser kleines Beispiel abgetragen. Tatsächlich ist es in diesem Fall schwierig eine Gerade zu erkennen bzw. von einer zu sprechen, da es sich nur um besagte fünf Wert handelt. Nochmals, mit \\(n=5\\) kann eine realistische Verteilungsannahme nicht überprüft werden.\nWenn der Datensatz größer ist, dann eignet sich ein qq-Plot allerdings sehr gut Abweichungen zu erkennen. In Abbildung 7.9 sind verschiedene Beispiele abgetragen.\n\n\n\n\n\n\n\n(a) Perfekt\n\n\n\n\n\n\n\n(b) Enden schwer\n\n\n\n\n\n\n\n\n\n(c) Enden leicht\n\n\n\n\n\n\n\n(d) Rechtsschief\n\n\n\n\nAbbildung 7.9: Beispiele für verschiedenen qq-Plots\n\n\nIn Abbildung 7.9 (a) ist ein perfekter Zusammenhang zwischen den empirischen und den theoretischen Quantilen abgebildet. In diesem Falle wurden synthetisch für 50 normalverteiltet Zufallsdaten ein qq-Plot erstellt. Es ist zu sehen, das tatsächlich eine Gerade den Zusammenhang beschreibt. In Abbildung 7.9 (b) ist dagegen ein Zusammenhang abgetragen, bei dem die empirischen und die theoretische Verteilung nicht zusammenpassen. In diesem Fall sind die haben die Randwerte der empirischen Vereteilung eine höhere Wahrscheinlichkeit als die unter der theoretischen Verteilung zu erwarten ist. D.h. extreme Werte kommen in der beobachteten Verteilung öfter in der theoretischen Verteilung vor. Dies deutet darauf hin, dass die Streuung der Daten möglicherweise nicht korrekt modelliert wurde. In diesem Fall, wird von einer tail heavy Verteilung gesprochen.\nIn Abbildung 7.9 (c) ist der gegenteilige Effekt abgetragen. Hier hat die theoretische Verteilung mehr Wahrscheinlichkeitsmasse in den Randzonen als die empirische Verteilung. Die beobachtete Verteilung ist tail light. Entsprechend ist in Abbildung 7.9 (d) ein Beispiel abgebildet, bei dem nur eine der Randzonen zu viel Wahrscheinlichkeitsmasse besitzt. Da die theoretische Verteilung wiederum die Normalverteilung ist und diese Symmetrisch ist, deutet diese darauf hin, das die empirische Verteilung ähnlich wie in Abbildung 7.9 (b) in der rechten Randzone zu viele Werte hat und daher Rechtsschief ist.\nFür unsere Daten ergibt sich das folgende qq-Diagramm (siehe Abbildung 7.10)\n\n\n\n\n\nAbbildung 7.10: QQ-Diagramm der Residuen des ADAS-ADCS-Modells\n\n\n\n\nDer Graph sieht zunächst einmal gar nicht so schlecht aus. Allerdings deutet die Abweichung rechts oben darauf hin, das möglicherweise die Streuung nicht korrekt abgeschätzt wurde. Insbesondere ist ein Wert zu sehen, der im Verhältnis zu den anderen Werten schon relativ weit von der Gerade weg ist. Daher ist es hier angezeigt, diesen Wert noch einmal genauer zu untersuchen.\n\n7.3.1 qq-Plot in R\nIn R gibt es zwei direkte Methoden einen qq-Plot zu erstellen. Mittels des Standardgrafiksystem können mit den Funktionen qqnorm() und qqline() qq-Plots mit der dazugehörigen Gerade erstellt werden. Für das ggplot()-System stehen die geoms geom_qq() und geom_qq_line() zur Verfügung. Wichtig ist hierbei, das in aes() der Parameter sample definiert werden muss. Für unser Spielzeugbeispiel sieht dies folgendermaßen aus:\n\ndf_toy &lt;- tibble::tibble(y = c(-2, 5, -1.2, 0.1, 7))\nggplot(df_toy, aes(sample=y)) +\n  geom_qq() +\n  geom_qq_line()\n\n\n\n\nAbbildung 7.11: qq-Plot der Spielzeugdaten mittels ggplot()"
  },
  {
    "objectID": "slm_model_fit.html#übersicht-residuen",
    "href": "slm_model_fit.html#übersicht-residuen",
    "title": "7  Modellfit",
    "section": "7.4 Übersicht Residuen",
    "text": "7.4 Übersicht Residuen\n\nÜbersicht über verschiedene Arten von Residuen1\n\n\n\n\n\n\n\nTyp\nBerechnung\nZiel\n\n\n\n\nEinfache Residuen\n\\(e_i = y_i - \\hat{y}_i\\)\nVerteilungsannahme\n\n\nStandardisierte Residuen\n\\(e_{Si} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_i}}\\)\nVerteilungsannahme\n\n\nStudentized Residuen\n\\(e_{Ti} = \\frac{e_i}{\\hat{\\sigma}_{(-i)}\\sqrt{1-h_i}}\\)\nEinfluss auf Modell"
  },
  {
    "objectID": "slm_model_fit.html#residuen-in-r-berechnen-mit-residuals-und-freunden",
    "href": "slm_model_fit.html#residuen-in-r-berechnen-mit-residuals-und-freunden",
    "title": "7  Modellfit",
    "section": "7.5 Residuen in R berechnen mit residuals() und Freunden",
    "text": "7.5 Residuen in R berechnen mit residuals() und Freunden\n\nrstandard(mod)[1:5] # standardisierte Residuen\n\n         1          2          3          4          5 \n-1.4592936 -1.4598906 -1.7440573 -0.8724351 -0.9916310 \n\nrstudent(mod)[1:5] # studentized Residuen\n\n         1          2          3          4          5 \n-1.4814779 -1.4821191 -1.7928881 -0.8697060 -0.9914135"
  },
  {
    "objectID": "slm_model_fit.html#residuen-in-r-inspizieren",
    "href": "slm_model_fit.html#residuen-in-r-inspizieren",
    "title": "7  Modellfit",
    "section": "7.6 Residuen in R inspizieren",
    "text": "7.6 Residuen in R inspizieren\n\ny_hat &lt;- predict(mod)\nplot(y_hat, rstandard(mod))\nplot(y_hat, rstudent(mod))"
  },
  {
    "objectID": "slm_model_fit.html#diagnoseplot---einfache-residuen-hatepsilon_i-sim-haty_i",
    "href": "slm_model_fit.html#diagnoseplot---einfache-residuen-hatepsilon_i-sim-haty_i",
    "title": "7  Modellfit",
    "section": "7.7 Diagnoseplot - Einfache Residuen \\(\\hat{\\epsilon_i} \\sim \\hat{y_i}\\)",
    "text": "7.7 Diagnoseplot - Einfache Residuen \\(\\hat{\\epsilon_i} \\sim \\hat{y_i}\\)\n\n\n\n\n\nStreudiagramm der Residuen \\(\\hat{\\epsilon_i}\\) gegen die Vorhersagewerte \\(\\hat{y}_i\\)"
  },
  {
    "objectID": "slm_model_fit.html#diagnoseplot---standardisierte-residuen-hatepsilon_si-sim-haty_i",
    "href": "slm_model_fit.html#diagnoseplot---standardisierte-residuen-hatepsilon_si-sim-haty_i",
    "title": "7  Modellfit",
    "section": "7.8 Diagnoseplot - Standardisierte Residuen \\(\\hat{\\epsilon}_{Si} \\sim \\hat{y_i}\\)",
    "text": "7.8 Diagnoseplot - Standardisierte Residuen \\(\\hat{\\epsilon}_{Si} \\sim \\hat{y_i}\\)\n\n\n\n\n\nStreudiagramm der standardisierten Residuen \\(\\hat{\\epsilon}_{Si}\\) gegen die Vorhersagewerte \\(\\hat{y}_i\\)"
  },
  {
    "objectID": "slm_model_fit.html#diagnoseplot---studentized-residuen-hatepsilon_ti-sim-haty_i",
    "href": "slm_model_fit.html#diagnoseplot---studentized-residuen-hatepsilon_ti-sim-haty_i",
    "title": "7  Modellfit",
    "section": "7.9 Diagnoseplot - Studentized Residuen \\(\\hat{\\epsilon}_{Ti} \\sim \\hat{y_i}\\)",
    "text": "7.9 Diagnoseplot - Studentized Residuen \\(\\hat{\\epsilon}_{Ti} \\sim \\hat{y_i}\\)\n\n\n\n\n\nStreudiagramm der studentized Residuals \\(\\hat{\\epsilon}_{Ti}\\) gegen die Vorhersagewerte \\(\\hat{y}_i\\)"
  },
  {
    "objectID": "slm_model_fit.html#diagnoseplot---qq-diagramm",
    "href": "slm_model_fit.html#diagnoseplot---qq-diagramm",
    "title": "7  Modellfit",
    "section": "7.10 Diagnoseplot - QQ-Diagramm",
    "text": "7.10 Diagnoseplot - QQ-Diagramm\n\n\n\n\n\nQQ-Diagramm der Residuen des ADAS-ADCS-Modells\n\n\n\n\n2"
  },
  {
    "objectID": "slm_model_fit.html#summary",
    "href": "slm_model_fit.html#summary",
    "title": "7  Modellfit",
    "section": "7.4 summary()",
    "text": "7.4 summary()\nNach dieser Betrachtung der Residuen, die nach jedem Modellfit inspiziert werden sollten um zu überprüfen ob die Modellannahmen angemessen sind schauen wir uns noch einmal kurz die Ausgabe von summary() an.\n\n\n\nCall:\nlm(formula = adcs ~ adas, data = adl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2177  -3.8033  -0.4663   2.7950  20.9634 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.5445     4.3052   6.166 3.05e-07 ***\nadas         -0.2638     0.1015  -2.599   0.0131 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.516 on 39 degrees of freedom\nMultiple R-squared:  0.1477,    Adjusted R-squared:  0.1258 \nF-statistic: 6.757 on 1 and 39 DF,  p-value: 0.01312\n\n\nNach der Wiedergabe des gefitten Modells erfolgt direkt eine Zusammenfassung der Residuen über Minimum und Maximum, Q1 und Q3 und den Median. Jetzt sollte daher auch besser nachvollziehbar sein, warum es sinnvoll ist diese Statistiken über die Residuen direkt anzugeben. Die beiden Extremwerte geben einen ersten Überblick auf mögliche Ausreißer, während die erste Quartile Q1 und die dritte Quartile Q3 möglich Asymmetrien in der Verteilung der Residuen anzeigen. Laut der Annahem der Residuen als Normalverteilt mit \\(\\mu = 0\\), sollten diese beiden Werte etwa gleich weit von Null entfernt sein. Dementsprechend sollte der Median nahe an Null dran sein. Was nah ist, kommt dabei immer auf die Einheit der abhängigen Variablen an, wenn der Abstand in Kilometern ist kann ein kleiner Wert schon problematisch sein, während wenn eine Sprungweite in Mikrometern angeben wird eine großer Wert unbedenklich sein kann. Der Schätzerwert für \\(\\sigma\\) selbst, wir unten mit Residual standard error angegeben.\nIm vorliegenden Fall des Modells für die adl-Daten ist der Median dementsprechend doch etwas weit von Null entfernt und der geschätzte Residualfehler \\(\\hat{\\sigma} = 6.52\\) ebenfalls relativ groß. \\(\\hat{\\sigma}\\) kann mittels der Funktion sigma() erhalten werden."
  },
  {
    "objectID": "slm_model_fit.html#neue-idee-zu-residuen",
    "href": "slm_model_fit.html#neue-idee-zu-residuen",
    "title": "7  Modellfit",
    "section": "7.8 Neue Idee zu Residuen",
    "text": "7.8 Neue Idee zu Residuen\n\n\n\n\n\nSpielzeugbeispiel mit Residuen \\(\\hat{\\epsilon}_i = e_i = y_i - \\hat{y}_i\\)"
  },
  {
    "objectID": "slm_model_fit.html#zum-nacharbeiten",
    "href": "slm_model_fit.html#zum-nacharbeiten",
    "title": "7  Modellfit",
    "section": "7.4 Zum Nacharbeiten",
    "text": "7.4 Zum Nacharbeiten\nAltman und Krzywinski (2016a) Fox (2011, p.294–302)\n\n7.4.1 Weiterführendes\nYoung (2019)\n\n\n\n\nAltman, Naomi, und Martin Krzywinski. 2016a. „Points of significance: Analyzing outliers: influential or nuisance“. Nature Methods 13 (4): 281–82.\n\n\n———. 2016b. „Points of significance: regression diagnostics“. Nature Methods 13 (5): 385–86.\n\n\nFox, John. 2011. An R companion to applied regression. 2. Aufl. SAGE Publication Inc., Thousand Oaks.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York.\n\n\nYoung, Alwyn. 2019. „Channeling fisher: Randomization tests and the statistical insignificance of seemingly significant experimental results“. The Quarterly Journal of Economics 134 (2): 557–98."
  },
  {
    "objectID": "slm_model_fit.html#hebelwerte",
    "href": "slm_model_fit.html#hebelwerte",
    "title": "7  Modellfit",
    "section": "7.5 Hebelwerte",
    "text": "7.5 Hebelwerte\n\n\n\n\n\nStreudiagramm der ADCS-MCI-ADL scores gegen ADAS-cos scores\n\n\n\n\n\n\n\n\n\nHebelwerte der jeweiligen \\(x_i\\)s\n\n\n\n\n\n\n\n\n\nHebelwerte der jeweiligen Datenpunkte\n\n\n\n\n\nÜbersicht über verschiedene Arten von Residuen1\n\n\n\n\n\n\n\nTyp\nBerechnung\nZiel\n\n\n\n\nEinfache Residuen\n\\(e_i = y_i - \\hat{y}_i\\)\nVerteilungsannahme\n\n\nStandardisierte Residuen\n\\(e_{Si} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_i}}\\)\nVerteilungsannahme\n\n\nStudentized Residuen\n\\(e_{Ti} = \\frac{e_i}{\\hat{\\sigma}_{(-i)}\\sqrt{1-h_i}}\\)\nEinfluss auf Modell"
  },
  {
    "objectID": "slm_model_fit.html#dffits",
    "href": "slm_model_fit.html#dffits",
    "title": "7  Modellfit",
    "section": "7.6 DFFITS",
    "text": "7.6 DFFITS\nMit Hilfe der Hebelwerte lassen sich verschiedene Maße erstellen um den Einfluss von Datenpunkten auf das Modell zu überprüfen. Ein Maß wird als bezeichnet (siehe Gleichung 7.1)\n\\[\n(DFFITS)_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{\\sqrt{\\hat{\\sigma}^2h_i}}\n\\tag{7.1}\\]\nIm Zähler kommen vin Gleichung 7.1 zweimal vorhergesagte \\(y\\)-Werte vor. \\(\\hat{y}_i\\) ist dabei der ganz normale Vorhersagewert der uns mittlerweile schon mehrfach begegnet ist. Der zweite Wert \\(\\hat{y}_{i(i)}\\) bezeichnet den vorhergesagten Wert aus dem Modell aus dem der Wert \\(y_i\\) weggelassen wurde. D.h, dass Modell ist mit einem Wert weniger gefittet worden. Daher misst die Differenz \\(\\hat{y}_i - \\hat{y}_{i(i)}\\) den Unterschied in den Vorhersagewerte zwischen zwei Modellen bei denen einmal der Wert \\(y_i\\) zum fitten verwendet wurde und einmal wenn \\(y_i\\) nicht zum fitten verwendet wurde. Umso größer der Unterschied zwischen diesen beiden Werte umso größer ist der Einfluss des Wertes \\(y_i\\) auf den Modellfit. Den Nenner von Gleichung 7.1 lassen wir mal fallen, da es sich dabei nur um einen Normierungswert handelt. Dementsprechend, wird mittels DFFITS für jeden Datenpunkt ein Wert ermittelt und umso größer dieser Wert ist umso größer ist der Einfluss des jeweiligen Datenpunktes auf den Modellfit.\nIm idealen Fall sollte alle Datenpunkt ungefähr den gleichen Einfluss haben und einzelne Datenpunkte die einen übermäßig großen Einfluss auf das Modell haben sollten noch einmal genauer inspiziert werden.\n\n\n\n\n\n\nTipp\n\n\n\nAls Daumenregel, kann für kleine bis mittlere Datensätze ein DFFITS von \\(\\approx 1\\) auf Probleme hindeuten, während bei großen Datensätzen \\(\\approx 2\\sqrt{k/N}\\) als Orientierungshilfe verwendet werden kann (k := Anzahl der Prediktoren, N := Stichprobengröße).\n\n\n\n\n\n\n\n\nWarnung\n\n\n\nWenn ein Wert außerhalb der Daumenregel liegt, heißt das nicht, dass er automatisch ausgeschlossen werden muss/soll, sondern lediglich inspiziert werden sollte und das Modell mit und ohne diesen Wert interpretiert werden sollte.\n\n\nIn R können die DFFITS werden mittels der dffits()-Funktion berechnet werden. Als Parameter erwartet dffits() das gefittete lm()-Objekt. Ähnlich wie bei den Residuen, werden die DFFITS-Werte gegen die vorhergesagten \\(y_i\\)-Werte graphisch abgetragen um die Wert zu inspizieren und Probleme in der Modellspezifikation zu identifizieren.\n\nplot(adl$y_hat, dffits(mod),\n     ylim=c(-2,2),\n     xlab=expression(hat(y)[i]),\n     ylab='DFFIT-Wert')\nabline(h=c(-1,1), col='red', lty=2)\n\n\n\n\nAbbildung 7.16: Beispiel für DFFITS gegen \\(\\hat{y}_i\\)\n\n\n\n\nIn Abbildung 7.16 sind die DFFITS-Werte gegen die vorhergesagten Werte \\(\\hat{y}_i\\) abgetragen und zusätzlich die Daumenregel \\(\\pm1\\) eingezeichnet. Hier ist ein Wert nur gerade so außerhalb des vorgeschlagenen Bereichs. Hier könnte daher sich dieser Datenpunkt noch einmal genauer angeschaut werden, ob bei Ausschluß des Wertes es zu einer qualitativ anderen Interpretation der Daten kommt oder ob bespielsweise Übertragungsfehler für diesen Wert vorliegen oder sonstige Gründe."
  },
  {
    "objectID": "slm_model_fit.html#cooks-abstand",
    "href": "slm_model_fit.html#cooks-abstand",
    "title": "7  Modellfit",
    "section": "7.3 Cooks-Abstand",
    "text": "7.3 Cooks-Abstand\nEin Maß um den Einfluss von einzelnen Datenpunkten auf die Vorhersagewerte \\(\\hat{y}_i\\) über alle Werte abzuschätzen.\n\\[\nD_i = \\frac{\\sum_{j=1}^N(\\hat{y_j} - \\hat{y}_{j(i)})}{k\\hat{\\sigma}^2}\n\\]\n\n7.3.1 Daumenregel\n\\(D_i &gt; 1\\)\n\n\n7.3.2 In R\ncooks.distance()"
  },
  {
    "objectID": "slm_model_fit.html#cooks-abstand-plot",
    "href": "slm_model_fit.html#cooks-abstand-plot",
    "title": "7  Modellfit",
    "section": "7.3 Cooks-Abstand plot",
    "text": "7.3 Cooks-Abstand plot\n\n\n\n\n\nCook’s \\(D_i\\) gegen \\(\\hat{y}_i\\)"
  },
  {
    "objectID": "slm_model_fit.html#dfbetas",
    "href": "slm_model_fit.html#dfbetas",
    "title": "7  Modellfit",
    "section": "7.4 DFBETAS",
    "text": "7.4 DFBETAS\nEin Maß für die Veränderung der \\(\\beta\\)-Koeffizienten durch einzelne Datenpunkte \\(i\\).\n\\[\n(DFBETAS)_{k(i)} = \\frac{\\hat{\\beta}_k - \\hat{\\beta}_{k(i)}}{\\sqrt{\\hat{\\sigma}^2c_{kk}}}\n\\]\n\n7.4.1 Daumenregel\nFür kleine bis mittlere Datensätze \\(\\approx 1\\)\nFür große Datensätze \\(\\approx 2/\\sqrt{N}\\)\n\n\n7.4.2 In R\ndfbeta()1"
  },
  {
    "objectID": "slm_model_fit.html#dfbetas-1",
    "href": "slm_model_fit.html#dfbetas-1",
    "title": "7  Modellfit",
    "section": "7.3 DFBETAS",
    "text": "7.3 DFBETAS\n\n\n\n\n\nDFBETA-Werte für \\(\\beta_0\\) und \\(\\beta_1\\) gegen \\(\\hat{y}_i\\)"
  },
  {
    "objectID": "slm_model_fit.html#zusammenfassung",
    "href": "slm_model_fit.html#zusammenfassung",
    "title": "7  Modellfit",
    "section": "7.3 Zusammenfassung",
    "text": "7.3 Zusammenfassung\n\n\nTabelle 7.4: Übersicht über die verschiedene Einflussmaße zur Bewertung der Modellgüte\n\n\nTyp\nVeränderung\nDaumenregel\n\n\n\n\n\\((DFFITS)_i\\)\nVorhersagewert i\n\\(2\\sqrt{k/N}\\)\n\n\nCook\nDurchschnittliche Vorhersagewerte\n\\(&gt;1\\)\n\n\n\\((DFBETAS)_{k(i)}\\)\nKoeffizient i\n\\(2\\sqrt{N}\\)\n\n\n\\(e_{Ti}\\)\nResiduum i\nt-Verteilung(n-k-2)"
  },
  {
    "objectID": "slm_model_fit.html#diagnoseplots-in-r-mit-plotmod",
    "href": "slm_model_fit.html#diagnoseplots-in-r-mit-plotmod",
    "title": "7  Modellfit",
    "section": "7.4 Diagnoseplots in R mit plot(mod)",
    "text": "7.4 Diagnoseplots in R mit plot(mod)\n\nplot(mod)"
  },
  {
    "objectID": "slm_model_fit.html#zum-nacharbeiten-1",
    "href": "slm_model_fit.html#zum-nacharbeiten-1",
    "title": "7  Modellfit",
    "section": "7.17 Zum Nacharbeiten",
    "text": "7.17 Zum Nacharbeiten\nAltman und Krzywinski (2016a) Fox (2011, p.294–302)\n\n7.17.1 Weiterführendes\nYoung (2019)\n\n\n\n\nAltman, Naomi, und Martin Krzywinski. 2016a. „Points of significance: Analyzing outliers: influential or nuisance“. Nature Methods 13 (4): 281–82.\n\n\n———. 2016b. „Points of significance: regression diagnostics“. Nature Methods 13 (5): 385–86.\n\n\nFox, John. 2011. An R companion to applied regression. 2. Aufl. SAGE Publication Inc., Thousand Oaks.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York.\n\n\nYoung, Alwyn. 2019. „Channeling fisher: Randomization tests and the statistical insignificance of seemingly significant experimental results“. The Quarterly Journal of Economics 134 (2): 557–98."
  },
  {
    "objectID": "slm_model_fit.html#footnotes",
    "href": "slm_model_fit.html#footnotes",
    "title": "7  Modellfit",
    "section": "",
    "text": "Es wird eine Matrize mit \\(k\\)-Spalten zurückgegeben.↩︎"
  },
  {
    "objectID": "slm_prediction.html#vorhergesagte-werte-haty_i",
    "href": "slm_prediction.html#vorhergesagte-werte-haty_i",
    "title": "8  Vorhersage",
    "section": "8.1 Vorhergesagte Werte \\(\\hat{y}_i\\)",
    "text": "8.1 Vorhergesagte Werte \\(\\hat{y}_i\\)\nWenn ein einfaches lineares Modell gefittet wurde ist eine zentrale Frage welche Vorhersagen anhand des Modell getroffen werden können. Die Vorhersagen \\(\\hat{y}_i\\) liegen auf der vorhergesagten Regressionsgerade und berechnen sich nach dem Modell für einen gegeben \\(x\\)-Wert.\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_0} x\n\\]\nWie schon mehrfach besprochen unterliegt die Regressionsgerade inherent der Unsicherheit bezüglich der geschätzen Modellkoeffizienten \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\). Diese Unsicherheit überträgt sich auf die geschätzen Werte \\(\\hat{y}_i\\) und muss daher bei deren Interpretation berücksichtigt werden.\nIn Abbildung 8.1 sind die bereits behandelten Sprungdaten gegen die Anlaufgeschwindigkeiten zusammen mit der Regressionsgeraden und vorhergesagten Werten (rot) abgetragen.\n\n\n\n\n\nAbbildung 8.1: Vorhersagewerte \\(\\hat{y}_i\\) (rote Punkte) für die Sprungdaten.\n\n\n\n\nIn R können die vorhergesagten Werte des mittels lm() gefitteten Modells mit der Hilfsfunktion predict() bestimmt werden. Wenn der Funktion predict() keine weiteren Parameter außer dem lm-Objekt übergeben werden, berechnet predict() die vorhergesagten Werte \\(\\hat{y}_i\\) für alle die \\(x\\)-Werte die auch zum fitten des Modells benutzt wurden. Die Reihenfolge der Werte \\(\\hat{y}_i\\) enspricht dabei den Werten im Original-data.frame().\n\npredict(mod)[1:5] \n\n       1        2        3        4        5 \n4.523537 4.725140 4.856256 4.761778 5.416207 \n\n\nWir haben uns hier nur die ersten fünf Werte ausgeben lassen, da nur demonstriert werden soll wie die predict()-Funktion angewendet werden kann. Um eine Anwendung zu geben, so können mittels predict() die Residuen auch von Hand ohne die resid()-Funktion erhalten werden.\n\n(jump$jump_m - predict(mod))[1:5]\n\n          1           2           3           4           5 \n-0.16267721 -0.41248842 -0.29359256 -0.01047071  0.09927500 \n\nresid(mod)[1:5]\n\n          1           2           3           4           5 \n-0.16267721 -0.41248842 -0.29359256 -0.01047071  0.09927500 \n\n\nWiederum nur zur Demonstration die ersten fünf Wert um die Äquivalenz der beiden Methoden zu demonstrieren.\nMeistens liegt das Interesse jedoch weniger auf den vorhergesagten Werten \\(\\hat{y}_i\\) für die gemessenen Werte, sondern es sollen Werte vorhergesagt werden für \\(x\\)-Werte die nicht im Datensatz enthalten sind. Operational ändert sich nichts, es wird immer noch das gefittete Modell verwendetet und es müssen lediglich neue \\(x\\)-Werte übergeben werden.\nIn R kann dies mittels des zweite Parameter in predict() erreicht werden. Soll zum Beispiel die Sprungweite für eine Anlaufgeschwindigkeit von \\(v = 11.5[m/s]\\) berechnen werden, muss zunächst ein neues tibble() erstellt werden, welches den gewünschten \\(x\\)-Wert enthält. Dabei muss der Spaltenname in dem neuen tibble() demjenigen im Original-tibble() entsprechen. Ansonsten funktioniert die Anwendung von predict() nicht.\n\ndf &lt;- tibble(v_ms = 11.5)\ndf\n\n# A tibble: 1 × 1\n   v_ms\n  &lt;dbl&gt;\n1  11.5\n\n\nDieses tibble() kann nun zusammen mit dem lm()-Objekt an predict() übergeben werden.\n\npredict(mod, newdata = df)\n\n       1 \n8.614136 \n\n\nD.h., bei einer Anlaufgeschwindigkeit von \\(v = 11.5[m/s]\\) ist anhand des Modells eine Sprungweite von \\(8.6m\\) zu erwarten."
  },
  {
    "objectID": "slm_prediction.html#unsicherheit-in-der-vorhersage",
    "href": "slm_prediction.html#unsicherheit-in-der-vorhersage",
    "title": "8  Vorhersage",
    "section": "8.2 Unsicherheit in der Vorhersage",
    "text": "8.2 Unsicherheit in der Vorhersage\nWie schon angesprochen ist unser Modell natürlich mit Unsicherheiten behaftet. Diese drücken sich in den Standardfehler für die beiden Koeffizienten \\(\\hat{\\beta_0}\\) und \\(\\hat{\\beta_1}\\) (siehe Tabelle 8.1).\n\n\n\n\nTabelle 8.1: Modellparameter und Standardfehler\n\n\n\nSchätzer\n\\(s_e\\)\n\n\n\n\n(Intercept)\n-0.14\n0.23\n\n\nv_ms\n0.76\n0.02\n\n\n\n\n\n\nDer vorhergesagte Wert \\(\\hat{y}\\) ist daher für sich alleine ist noch nicht brauchbar, da auch Informationen über dessen Unsicherheit notwendig sind um die Ergebnisse korrekt zu interpretieren.\nEs können zwei unterschiedliche Anwendungsfälle voneinander unterschieden werden.\n\nDer mittlere, erwartete Wert \\(\\hat{\\bar{y}}_{neu}\\)\nDie Vorhersage eines einzelnen Wertes \\(\\bar{y}_{neu}\\)\n\nIm konkreten Fall werden damit zwei unterschiedliche Fragestellungen beantwortet. Im 1. Fall lautet die Frage, ich habe eine Trainingsgruppe und möchte wissen was der mittlere Wert der Gruppe anhand des Modells ist, wenn alle eine bestimmte Anlaufgeschwindigkeit \\(v_{neu}\\) haben. Im 2. Fall lautet die Frage welche Weite eine einzelne Athletin für die Anlaufgeschwindigkeit \\(v_{neu}\\) springen sollte. In beiden Fällen werden keiner genau den Wert des Regressionsmodells treffen, aber im 1. Fall der Gruppe werden sich Streuungen nach oben bzw. nach unten gegenseitig im Schnitt ausbalancieren während im 2. Fall der einzelnen Athletin dies nicht der Fall ist. Daher hat die Vorhersage im 2. Fall eine höhere Unsicherheit. Diese Unterschied sollte sich dementsprechend in den Varianzen der beiden Vorhersagen wiederspiegeln.\nWie bereits erwähnt, der vorhergesagte Wert \\(\\hat{y}_{neu}\\) ist in beiden Fällen gleich und entsprecht der oben beschriebenen Methode anhand des Modell \\(y_{neu} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times x_{\\text{neu}}\\).\nFür den erwarteten Mittelwert errechnet sich die Varianz nach:\n\\[\\begin{equation}\nVar(\\hat{\\bar{y}}_{neu}) = \\hat{\\sigma}^2 \\left[\\frac{1}{n} + \\frac{(x_{neu} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\\right] = \\hat{\\sigma}_{\\hat{\\bar{y}}_{neu}}^2\n\\end{equation}\\]\nDas dazugehörige Konfidenzintervall errechnet sich danach mittels:\n\\[\\begin{equation}\n\\hat{\\bar{y}}_{neu} \\pm q_{t(1-\\alpha/2;n-2)} \\times \\hat{\\sigma}_{\\hat{\\bar{y}}_{neu}}\n\\end{equation}\\]\nDie Varianz für die Vorhersage eines einzelnen Wertes errechnet sich:\n\\[\\begin{equation}\nVar(\\hat{y}_{neu}) = \\hat{\\sigma}^2 \\left[1 + \\frac{1}{n} + \\frac{(x_{neu} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\\right] = \\hat{\\sigma}^2 + \\hat{\\sigma}_{\\hat{\\bar{y}}_{neu}}^2 = \\hat{\\sigma}_{\\hat{y}_{neu}}^2\n\\end{equation}\\]\nWas wiederum zu dem folgenden Konfidenzintervall führt:\n\\[\\begin{equation}\n\\hat{y}_{neu} \\pm q_{t(1-\\alpha/2;n-2)} \\times \\hat{\\sigma}_{\\hat{y}_{neu}}\n\\end{equation}\\]\nIn beiden Fällen ist der Term\n\\[\n\\frac{(x_{neu} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\n\\]\nenthalten. Anhand des Zählers kann abgeleitet werden, dass die Unsicherheit der Vorhersage mit dem Abstand vom Mittelwert der \\(x\\)-Werte zunimmt. Rein heuristisch macht dies Sinn, da davon ausgegangen werden kann, dass um den Mittelwert der \\(x\\)-Werte auch die meiste Information über \\(y\\) vorhanden ist und dementsprechend umso weiter die Werte sich vom \\(\\bar{x}\\) entfernen die Information abnimmt. Im Nenner ist wiederum wie auch beim Standardfehler \\(\\sigma_{\\beta_1}\\) des Steigungskoeffizienten \\(\\beta_1\\) zu sehen, dass die Varianz abnimmt mit der Streuung der \\(x\\)-Werte. Daher, wenn eine Vorhersage in einem bestimmten Bereich von \\(x\\)-Werten durchgeführt werden soll, dann sollte darauf geachtet werden möglichst diesen Bereich auch zu samplen um die Unsicherheit so klein wie möglich zu halten."
  },
  {
    "objectID": "slm_prediction.html#vorhersagen-in-r-mit-predict",
    "href": "slm_prediction.html#vorhersagen-in-r-mit-predict",
    "title": "8  Vorhersage",
    "section": "8.3 Vorhersagen in R mit predict()",
    "text": "8.3 Vorhersagen in R mit predict()\n\n8.3.1 Erwarteter Mittelwert\n\ndf &lt;- data.frame(v_ms = 11.5) # oder tibble(v_ms = 11.5)\npredict(mod, newdata = df, interval = 'confidence')\n\n       fit      lwr      upr\n1 8.614136 8.482039 8.746234\n\n\n\n\n8.3.2 Individuelle Werte\n\npredict(mod, newdata = df, interval = 'prediction')\n\n       fit      lwr      upr\n1 8.614136 8.118445 9.109827"
  },
  {
    "objectID": "slm_prediction.html#konfidenzintervalle-graphisch",
    "href": "slm_prediction.html#konfidenzintervalle-graphisch",
    "title": "8  Vorhersage",
    "section": "8.4 Konfidenzintervalle graphisch",
    "text": "8.4 Konfidenzintervalle graphisch\n\n\n\n\n\nWeiterführende Literatur sind Kutner u. a. (2005)"
  },
  {
    "objectID": "slm_prediction.html#r2-und-root-mean-square",
    "href": "slm_prediction.html#r2-und-root-mean-square",
    "title": "8  Vorhersage",
    "section": "8.5 \\(R^2\\) und Root-mean-square",
    "text": "8.5 \\(R^2\\) und Root-mean-square"
  },
  {
    "objectID": "slm_prediction.html#einfaches-modell",
    "href": "slm_prediction.html#einfaches-modell",
    "title": "8  Vorhersage",
    "section": "8.6 Einfaches Modell",
    "text": "8.6 Einfaches Modell\n\nmod0 &lt;- lm(y ~ x, simple)\nsummary(mod0)\n\n\nCall:\nlm(formula = y ~ x, data = simple)\n\nResiduals:\n      1       2       3       4 \n-0.5817  0.9898 -0.2345 -0.1736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   1.8414     0.7008   2.628    0.119\nx             0.4574     0.3746   1.221    0.346\n\nResidual standard error: 0.8376 on 2 degrees of freedom\nMultiple R-squared:  0.4271,    Adjusted R-squared:  0.1406 \nF-statistic: 1.491 on 1 and 2 DF,  p-value: 0.3465"
  },
  {
    "objectID": "slm_prediction.html#nochmal-abweichungen",
    "href": "slm_prediction.html#nochmal-abweichungen",
    "title": "8  Vorhersage",
    "section": "8.7 Nochmal Abweichungen",
    "text": "8.7 Nochmal Abweichungen\n\nGesamtvarianz: \\[\nSSTO := \\sum_{i=1}^N (y_i - \\bar{y})^2\n\\]\nRegressionsvarianz: \\[\nSSR :=\\sum_{i=1}^N(\\hat{y}_i - \\bar{y})^2\n\\]\nResidualvarianz: \\[\nSSE := \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\\]\n\n\n\n\n\n\nMinimalmodell der Abweichungen"
  },
  {
    "objectID": "slm_prediction.html#verhältnis-von-ssr-zu-ssto",
    "href": "slm_prediction.html#verhältnis-von-ssr-zu-ssto",
    "title": "8  Vorhersage",
    "section": "8.8 Verhältnis von \\(SSR\\) zu \\(SSTO\\)",
    "text": "8.8 Verhältnis von \\(SSR\\) zu \\(SSTO\\)\n\n\n\n\n\nPerfekter Zusammenhang\n\n\n\n\n\\[\n\\frac{SSR}{SSTO} = 1\n\\]\n\n\n\n\n\nKein Zusammenhang\n\n\n\n\n\\[\n\\frac{SSR}{SSTO} = 0\n\\]"
  },
  {
    "objectID": "slm_prediction.html#determinationskoeffizient-r2",
    "href": "slm_prediction.html#determinationskoeffizient-r2",
    "title": "8  Vorhersage",
    "section": "8.9 Determinationskoeffizient \\(R^2\\)",
    "text": "8.9 Determinationskoeffizient \\(R^2\\)\nEs gilt: \\(SSTO = SSR + SSE\\)\n\\[\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO} \\in [0,1]\n\\] 1\n\n8.9.1 Korrigierter Determinationskoeffizient \\(R_a^2\\)\n\\[\nR_a^2 = 1 - \\frac{\\frac{SSE}{n-p}}{\\frac{SSTO}{n-1}} = 1 - \\frac{n-1}{n-p}\\frac{SSE}{SSTO}\n\\]\n\n\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York."
  },
  {
    "objectID": "slm_prediction.html#footnotes",
    "href": "slm_prediction.html#footnotes",
    "title": "8  Vorhersage",
    "section": "",
    "text": "Bei der einfachen Regression gilt: \\(r_{xy} = \\pm\\sqrt{R^2}\\)↩︎"
  },
  {
    "objectID": "mlm_title.html",
    "href": "mlm_title.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "Im folgenden wird das Modell der einfachen linearen Regression erweitert indem zusätzliche Terme in das Modell aufgenommen werden. Die Prinzipien bleiben dabei jedoch weitestgehendst gleich und können direkt auf den komplizierteren Fall der multiplen Regression übertragen werden. Im Laufe der Erweiterung des Modells wird sich dabei wird herausstellen, dass neben mehreren kontinuierlichen Variablen auch nominale Faktoren in das Modell intergriert werden können. Daraus entsteht ein sehr flexibler Modellapparat, der in den verschiedensten Zusammenhängen angewendet werden kann."
  },
  {
    "objectID": "mlm_basics.html#bedeutung-der-koeffizienten-bei-der-multiplen-regression",
    "href": "mlm_basics.html#bedeutung-der-koeffizienten-bei-der-multiplen-regression",
    "title": "9  Einführung",
    "section": "9.1 Bedeutung der Koeffizienten bei der multiplen Regression",
    "text": "9.1 Bedeutung der Koeffizienten bei der multiplen Regression\nUm die Bedeutung der Regressionskoeffzienten bei der multiple Regression besser zu verstehen ist es von Vorteil sich noch einmal die Bedeutung der Koeffizienten im einfachen Regressionsmodell zu vergegenwärtigen (siehe Abbildung 9.1).\n\n\n\n\n\nAbbildung 9.1: Beispiel für eine einfache Regression und der resultierenden Regressiongeraden\n\n\n\n\nBei der einfachen Regression haben mittels der Methode der kleinsten Quadrate eine Regressiongerade durch unsere Punktwolke gelegt. Dabei haben wir die Regressionsgerade so gewählt, dass die senkrechten Abstände der beobachteten Punkte von der Regressionsgerade minimiert werden bzw. die Abstände zwischen denen auf der Gerade liegenden, vorhergesagten Werte \\(\\hat{y}_i\\) und den beobachteten Wert \\(y_i\\).\nWenn wir nun den Übergang von einer Prädiktorvariablenzum nächstkomplizierteren Fall nehmen mit zwei Prädiktorvariablen \\(x_1\\) und \\(x_2\\), dann wäre eine mögliche Darstellungsform der Daten eine Punktwolke im dreidimensionalen Raum (siehe Abbildung 9.2 (a)).\n\n\n\n\n\n\n\n(a) 3D Punktwolke\n\n\n\n\n\n\n\n(b) 3D Punktwolke mit gefitteter Ebene\n\n\n\n\nAbbildung 9.2: Punktwolken bei der multiple Regression\n\n\nDa jetzt eine einzelne Gerade nicht mehr in der Lage ist die Daten zu fitten, ist die nächst Möglichkeit eine Ebene die in die Punktwolke gelegt wird (siehe Abbildung 9.2 (b)). Dies ermöglicht dann genau die gleiche Herangehensweise wie bei der einfachen linearen Regression anzuwenden. Als Zielgröße wird aus den möglichen Ebenen diejenigen gesucht deren vorhergesagten, auf der Ebene liegenden Punkte \\(\\hat{y}_i\\) die geringsten senkrechten Abstand zu den beobachteten Punkten \\(y_i\\) haben. Anders, wir suchen diejenigen Ebene durch die Punktwolke deren Summe der quadrierten Residuen \\(e_i = y_i - \\hat{y}_i\\) minimal ist.\nDiese Herangehensweise hat den Vorteil, dass sie zum einem die einfache lineare Regression als Spezialfall mit \\(K=1\\) beinhaltet und sich beliebig erweitern lässt mit der Einschränkung, dass bei \\(K&gt;2\\) die dreidimenionale Darstellung mittels einer Grafik nicht mehr möglich ist. Das Prinzip der Minimierung der Abweichungen von \\(\\hat{y}_i\\) zu \\(y\\) bleibt aber immer erhalten. Zusammenfassend hat dieser Ansatz somit die folgenden Vorteile:\n\nDie Berechnungen bleiben alle gleich\nAbweichungen \\(\\hat{\\epsilon_i}\\) sind jetzt nicht mehr Abweichungen von einer Gerade sondern von einer \\(K\\)-dimensionalen Hyperebene. Die Eigenschaften der Residuen bleiben aber alle erhalten.\nDie Modellannahmen bleiben gleich: Unabhängige \\(y_i\\) und \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) iid\nInferenz für die Koeffizienten mittels \\(t_k = \\frac{\\hat{\\beta}_k}{s_k} \\sim t(N-K-1)\\) (Konfidenzintervall dito)\nKonzepte für die Vorhersage bleiben erhalten\nModelldiagnosetools bleiben alle erhalten\n\nAls nächster Schritt versuchen wir nun die Interpretation der Koeffizienten im multiplen Regressionsmodell besser zu verstehen."
  },
  {
    "objectID": "mlm_basics.html#einfaches-beispiel",
    "href": "mlm_basics.html#einfaches-beispiel",
    "title": "9  Einführung",
    "section": "9.2 Einfaches Beispiel",
    "text": "9.2 Einfaches Beispiel\n\\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon_i \\\\\n\\beta_0 &= 1 ,\\beta_1 = 3, \\beta_2 = 0.7 \\\\\n\\epsilon_i &\\sim N(0,\\sigma = 0.5)\n\\end{align*}\\]\n\nN &lt;- 50 # Anzahl Datenpunkte\nbeta_0 &lt;- 1\nbeta_1 &lt;- 3\nbeta_2 &lt;- 0.7\nsigma &lt;- 0.5\nset.seed(123)\ndf &lt;- tibble(\n  x1 = runif(N, -2, 2),\n  x2 = runif(N, -2, 2),\n  y = beta_0 + beta_1*x1 + beta_2*x2 + \n    rnorm(N, 0, sigma)) \n\n\n\n\n\n\nEinfacher Zusammenhang y~x1\n\n\n\n\n\n\n\n\n\nEinfacher Zusammenhang y~x2"
  },
  {
    "objectID": "mlm_basics.html#wie-sieht-der-fit-aus",
    "href": "mlm_basics.html#wie-sieht-der-fit-aus",
    "title": "9  Einführung",
    "section": "9.3 Wie sieht der Fit aus?",
    "text": "9.3 Wie sieht der Fit aus?\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.20883 -0.26741 -0.00591  0.27315  1.01322 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.07674    0.06552   16.43  &lt; 2e-16 ***\nx1           2.96537    0.05604   52.91  &lt; 2e-16 ***\nx2           0.70815    0.05961   11.88 9.27e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4604 on 47 degrees of freedom\nMultiple R-squared:  0.9849,    Adjusted R-squared:  0.9842 \nF-statistic:  1529 on 2 and 47 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "mlm_basics.html#was-bedeuten-die-einzelnen-koeffizienten",
    "href": "mlm_basics.html#was-bedeuten-die-einzelnen-koeffizienten",
    "title": "9  Einführung",
    "section": "9.4 Was bedeuten die einzelnen Koeffizienten?",
    "text": "9.4 Was bedeuten die einzelnen Koeffizienten?\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n1.077\n0.066\n\n\nx1\n2.965\n0.056\n\n\nx2\n0.708\n0.060\n\n\n\n\n\nDer Unterschied in der abhängigen Variablen, wenn zwei Objekte sich in \\(x_i\\) um eine Einheit unterscheiden und die paarweise gleichen Werte in den verbleibenden \\(x_j, j \\neq i\\) annehmen."
  },
  {
    "objectID": "mlm_basics.html#was-bedeuten-die-koeffizienten-in-kombination",
    "href": "mlm_basics.html#was-bedeuten-die-koeffizienten-in-kombination",
    "title": "9  Einführung",
    "section": "9.5 Was bedeuten die Koeffizienten in Kombination?",
    "text": "9.5 Was bedeuten die Koeffizienten in Kombination?\n\n9.5.1 Full model\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n1.077\n0.066\n\n\nx1\n2.965\n0.056\n\n\nx2\n0.708\n0.060\n\n\n\n\n\n\n\n9.5.2 um x2 bereinigt\n\nmod_x1_x2 &lt;- lm(x1 ~ x2, df)\nres_mod_x1_x2 &lt;- resid(mod_x1_x2)\nmod_x1_res &lt;- lm(y ~ res_mod_x1_x2, df)\n\n\n\n              Estimate Std. Error t value\n(Intercept)       1.25       0.16    7.61\nres_mod_x1_x2     2.97       0.14   20.97\n\n\n\n\n9.5.3 um x1 bereinigt\n\nmod_x2_x1 &lt;- lm(x2 ~ x1, df)\nres_mod_x2_x1 &lt;- resid(mod_x2_x1)\nmod_x2_res &lt;- lm(y ~ res_mod_x2_x1, df)\n\n\n\n              Estimate Std. Error t value\n(Intercept)       1.25       0.51    2.44\nres_mod_x2_x1     0.71       0.47    1.51"
  },
  {
    "objectID": "mlm_basics.html#was-bedeuten-die-koeffizienten-in-kombination-1",
    "href": "mlm_basics.html#was-bedeuten-die-koeffizienten-in-kombination-1",
    "title": "9  Einführung",
    "section": "9.6 Was bedeuten die Koeffizienten in Kombination?",
    "text": "9.6 Was bedeuten die Koeffizienten in Kombination?\n\n\\(\\hat{\\beta}_1\\): Wenn ich \\(x_2\\) weiß, welche zusätzlichen Informationen bekomme ich durch \\(x_1\\)\n\\(\\hat{\\beta}_2\\): Wenn ich \\(x_1\\) weiß, welche zusätzlichen Informationen bekomme ich durch \\(x_2\\)\n\nIn Beispiel nicht problematisch, weil nach Konstruktion \\(x_1\\) und \\(x_2\\) unabhängig voneinander sind:\n\nround(cor(df),3)\n\n      x1    x2     y\nx1 1.000 0.078 0.969\nx2 0.078 1.000 0.289\ny  0.969 0.289 1.000"
  },
  {
    "objectID": "mlm_basics.html#added-variable-plots",
    "href": "mlm_basics.html#added-variable-plots",
    "title": "9  Einführung",
    "section": "9.7 Added-variable plots",
    "text": "9.7 Added-variable plots\n\n\n\n\n\nZusammenhang zwischen y und x2 bereinigt um den Einfluß von x1."
  },
  {
    "objectID": "mlm_basics.html#added-variable-plots-mit-caravplots",
    "href": "mlm_basics.html#added-variable-plots-mit-caravplots",
    "title": "9  Einführung",
    "section": "9.8 Added-variable plots mit car::avPlots()",
    "text": "9.8 Added-variable plots mit car::avPlots()\n\ncar::avPlots(mod, ~x2)"
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-ich-einen-prädiktor-weg-lasse",
    "href": "mlm_basics.html#was-passiert-wenn-ich-einen-prädiktor-weg-lasse",
    "title": "9  Einführung",
    "section": "9.9 Was passiert wenn ich einen Prädiktor weg lasse?",
    "text": "9.9 Was passiert wenn ich einen Prädiktor weg lasse?\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n1.077\n0.066\n\n\nx1\n2.965\n0.056\n\n\nx2\n0.708\n0.060\n\n\n\n\n\n\ncoef(lm(y ~ x1, df))\n\n(Intercept)          x1 \n   1.007466    3.017589 \n\ncoef(lm(y ~ x2, df))\n\n(Intercept)          x2 \n  1.3377771   0.9555316 \n\n\nIn unserem Beispiel wieder nicht viel, da die Variablen unabhängig (orthogonal) voneinander sind."
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren",
    "href": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren",
    "title": "9  Einführung",
    "section": "9.10 Was passiert wenn Prädiktoren stark miteinander korrelieren?",
    "text": "9.10 Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n\n\nAusschnitt von Körperfettdaten\n\n\ntriceps\nthigh\nmidarm\nbody_fat\n\n\n\n\n19.5\n43.1\n29.1\n11.9\n\n\n24.7\n49.8\n28.2\n22.8\n\n\n30.7\n51.9\n37.0\n18.7\n\n\n29.8\n54.3\n31.1\n20.1\n\n\n19.1\n42.2\n30.9\n12.9\n\n\n25.6\n53.9\n23.7\n21.7\n\n\n\n\n\n1"
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-1",
    "href": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-1",
    "title": "9  Einführung",
    "section": "9.11 Was passiert wenn Prädiktoren stark miteinander korrelieren?",
    "text": "9.11 Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\nGGally::ggpairs(bodyfat) + theme(text = element_text(size = 10))\n\n\n\n\nKorrelationsmatrize"
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-2",
    "href": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-2",
    "title": "9  Einführung",
    "section": "9.12 Was passiert wenn Prädiktoren stark miteinander korrelieren?",
    "text": "9.12 Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n# Alle drei Prädiktoren\nmod_full &lt;- lm(body_fat ~ triceps + thigh + midarm, bodyfat)\n# ohne Arm\nmod_wo_midarm &lt;- lm(body_fat ~ triceps + thigh, bodyfat)\n# Ohne Oberschenkel\nmod_wo_thigh &lt;- lm(body_fat ~ triceps + midarm, bodyfat)\n# Ohne Triceps\nmod_wo_triceps &lt;- lm(body_fat ~ thigh + midarm, bodyfat)"
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-3",
    "href": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-3",
    "title": "9  Einführung",
    "section": "9.13 Was passiert wenn Prädiktoren stark miteinander korrelieren?",
    "text": "9.13 Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n\n\nfull model\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n117.085\n99.782\n\n\ntriceps\n4.334\n3.016\n\n\nthigh\n-2.857\n2.582\n\n\nmidarm\n-2.186\n1.595\n\n\n\n\n\n\n\n\nw/o midarm\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n-19.174\n8.361\n\n\ntriceps\n0.222\n0.303\n\n\nthigh\n0.659\n0.291\n\n\n\n\n\n\n\n\nw/o thigh\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n6.792\n4.488\n\n\ntriceps\n1.001\n0.128\n\n\nmidarm\n-0.431\n0.177\n\n\n\n\n\n\n\n\nw/o triceps\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n-25.997\n6.997\n\n\nthigh\n0.851\n0.112\n\n\nmidarm\n0.096\n0.161"
  },
  {
    "objectID": "mlm_basics.html#multikollinearität",
    "href": "mlm_basics.html#multikollinearität",
    "title": "9  Einführung",
    "section": "9.14 Multikollinearität2",
    "text": "9.14 Multikollinearität2\n\nGroße Änderungen in den Koeffizienten wenn Prädiktoren ausgelassen/eingefügt werden\nKoeffizienten haben eine andere Richtung als erwartet\nHohe (einfache) Korrelationen zwischen Prädiktoren\nBreite Konfidenzintervalle für “wichtige” Prädiktoren \\(b_j\\)\n\n\\[\n\\widehat{\\text{Var}}(b_j) = \\frac{\\hat{\\sigma}^2}{(n-1)s_j^2}\\frac{1}{1-R_j^2}\n\\]\n\\(R_j^2\\) = Multipler Korrelationskoeffizient der Prädiktoren auf Prädiktorvariable \\(j\\)."
  },
  {
    "objectID": "mlm_basics.html#variance-inflation-factor-vif",
    "href": "mlm_basics.html#variance-inflation-factor-vif",
    "title": "9  Einführung",
    "section": "9.15 Variance Inflation Factor (VIF)",
    "text": "9.15 Variance Inflation Factor (VIF)\n\\[\n\\text{VIF}_j = \\frac{1}{1-R_j^2}\n\\]\n\n\n\n\n\n\nTipp\n\n\n\nWenn VIF &gt; 10 ist, dann deutet dies auf hohe Multikollinearität hin.\n\n\n3"
  },
  {
    "objectID": "mlm_basics.html#variance-inflation-factor-vif-1",
    "href": "mlm_basics.html#variance-inflation-factor-vif-1",
    "title": "9  Einführung",
    "section": "9.16 Variance Inflation Factor (VIF)",
    "text": "9.16 Variance Inflation Factor (VIF)\n\ncar::vif(mod_full) \n\n triceps    thigh   midarm \n708.8429 564.3434 104.6060 \n\n\n4\nÜblicherweise wird der größte Wert betrachtet um die Multikollinearität zu bewerten."
  },
  {
    "objectID": "mlm_basics.html#wenn-prädiktoren-sich-gegenseitig-maskieren",
    "href": "mlm_basics.html#wenn-prädiktoren-sich-gegenseitig-maskieren",
    "title": "9  Einführung",
    "section": "9.17 Wenn Prädiktoren sich gegenseitig maskieren5",
    "text": "9.17 Wenn Prädiktoren sich gegenseitig maskieren5\n\n\n\n\n\nx_pos maskiert den Einfluss von x_neg"
  },
  {
    "objectID": "mlm_basics.html#wenn-prädiktoren-sich-gegenseitig-maskieren-1",
    "href": "mlm_basics.html#wenn-prädiktoren-sich-gegenseitig-maskieren-1",
    "title": "9  Einführung",
    "section": "9.18 Wenn Prädiktoren sich gegenseitig maskieren",
    "text": "9.18 Wenn Prädiktoren sich gegenseitig maskieren\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n0.235\n0.135\n\n\nx_pos\n0.218\n0.147\n\n\n\n\n\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n0.228\n0.116\n\n\nx_neg\n-0.618\n0.103\n\n\n\n\n\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n0.135\n0.096\n\n\nx_pos\n0.850\n0.123\n\n\nx_neg\n-0.976\n0.099"
  },
  {
    "objectID": "mlm_basics.html#multiple-regression",
    "href": "mlm_basics.html#multiple-regression",
    "title": "9  Einführung",
    "section": "9.19 Multiple Regression",
    "text": "9.19 Multiple Regression\nAus der einfachen Regression\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\nwird\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_K x_{Ki} + \\epsilon_i\n\\]\nmit K Prädiktorvariablen und Multikollinearität."
  },
  {
    "objectID": "mlm_basics.html#zum-nacharbeiten",
    "href": "mlm_basics.html#zum-nacharbeiten",
    "title": "9  Einführung",
    "section": "9.20 Zum Nacharbeiten",
    "text": "9.20 Zum Nacharbeiten\nAltman und Krzywinski (2015) Kutner u. a. (2005, p.278–288) Fox (2011, p.325–327)\n\n\n\n\nAltman, Naomi, und Martin Krzywinski. 2015. „Points of significance: Multiple linear regression“. Nature Methods 12 (12): 1103–4.\n\n\nFox, John. 2011. An R companion to applied regression. 2. Aufl. SAGE Publication Inc., Thousand Oaks.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York.\n\n\nMcElreath, Richard. 2016. Statistical rethinking, A Bayesian Course with Examples in R and Stan. 1. Aufl. Boca Raton: CRC Press."
  },
  {
    "objectID": "mlm_basics.html#footnotes",
    "href": "mlm_basics.html#footnotes",
    "title": "9  Einführung",
    "section": "",
    "text": "Beispiel nach Kutner u. a. (2005)↩︎\ninformell nach Kutner u. a. (2005, pp. 407)↩︎\nManchmal wird auch Tolerance = \\(\\frac{1}{VIF}\\) betrachtet.↩︎\ncar::vif berechnet generalized variance inflation factor wenn Prädiktoren Faktoren oder Polynome sind (Fox 2011.) ↩︎\nadaptiert nach McElreath (2016)↩︎"
  },
  {
    "objectID": "mlm_interactions.html#beispieldaten",
    "href": "mlm_interactions.html#beispieldaten",
    "title": "10  Interaktionseffekte",
    "section": "10.1 Beispieldaten1",
    "text": "10.1 Beispieldaten1\n\n\n\nBeispieldaten (synthetisch)\n\n\nVelocity[m/s]\nbody mass[kg]\narm span[cm]\n\n\n\n\n185.42\n68.71\n20.14\n\n\n184.08\n73.85\n21.29\n\n\n200.74\n89.43\n27.57\n\n\n170.34\n84.97\n19.88\n\n\n176.89\n82.40\n20.51\n\n\n200.68\n91.57\n29.22"
  },
  {
    "objectID": "mlm_interactions.html#beispieldaten---deskriptiv",
    "href": "mlm_interactions.html#beispieldaten---deskriptiv",
    "title": "10  Interaktionseffekte",
    "section": "10.2 Beispieldaten - Deskriptiv",
    "text": "10.2 Beispieldaten - Deskriptiv\n\n\n\nDeskriptive Statistik der Handballdaten\n\n\n\nMean\nStd.Dev\nMin\nMax\n\n\n\n\narm_span\n184.3\n7.7\n169.4\n200.7\n\n\nbody_mass\n77.5\n10.3\n58.0\n101.1\n\n\nvel\n21.9\n2.3\n18.5\n29.2"
  },
  {
    "objectID": "mlm_interactions.html#beispieldaten-1",
    "href": "mlm_interactions.html#beispieldaten-1",
    "title": "10  Interaktionseffekte",
    "section": "10.3 Beispieldaten",
    "text": "10.3 Beispieldaten\n\n\n\n\n\nGeschwindigkeit gegen Körpergewicht\n\n\n\n\n\n\n\n\n\nGeschwindigkeit gegen Armspannweite"
  },
  {
    "objectID": "mlm_interactions.html#beispieldaten---startmodell",
    "href": "mlm_interactions.html#beispieldaten---startmodell",
    "title": "10  Interaktionseffekte",
    "section": "10.4 Beispieldaten - Startmodell",
    "text": "10.4 Beispieldaten - Startmodell\n\\[\nY_{i} = \\beta_0 + \\beta_1 \\times \\textrm{bm}_i + \\beta_2 \\times \\textrm{as}_i + \\epsilon_i\n\\]\n\nmod_1 &lt;- lm(vel ~ body_mass + arm_span, handball)\n\n\n\n\nModell 1\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n-1.768\n7.632\n-0.232\n0.818\n\n\nbody_mass\n0.077\n0.033\n2.359\n0.024\n\n\narm_span\n0.096\n0.044\n2.192\n0.035\n\n\n\\(\\hat{\\sigma}\\)\n1.996"
  },
  {
    "objectID": "mlm_interactions.html#modellfit",
    "href": "mlm_interactions.html#modellfit",
    "title": "10  Interaktionseffekte",
    "section": "10.5 Modellfit",
    "text": "10.5 Modellfit\n\n\n\n\n\n3D Streudiagramm"
  },
  {
    "objectID": "mlm_interactions.html#zentrierung",
    "href": "mlm_interactions.html#zentrierung",
    "title": "10  Interaktionseffekte",
    "section": "10.6 Zentrierung",
    "text": "10.6 Zentrierung\n\nhandball &lt;- dplyr::mutate(handball,\n                          body_mass_c = body_mass - mean(body_mass),\n                          arm_span_c = arm_span - mean(arm_span))\n\n\n\n\nDeskriptive Statistik\n\n\n\nMean\nStd.Dev\n\n\n\n\narm_span\n184.29\n7.72\n\n\narm_span_c\n0.00\n7.72\n\n\nbody_mass\n77.46\n10.26\n\n\nbody_mass_c\n0.00\n10.26\n\n\nvel\n21.85\n2.31"
  },
  {
    "objectID": "mlm_interactions.html#modell-mit-zentrierten-variablen",
    "href": "mlm_interactions.html#modell-mit-zentrierten-variablen",
    "title": "10  Interaktionseffekte",
    "section": "10.7 Modell mit zentrierten Variablen",
    "text": "10.7 Modell mit zentrierten Variablen\n\nmod_2 &lt;- lm(vel ~ body_mass_c + arm_span_c, handball)\n\n\n\n\nModell 2\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n21.852\n0.316\n69.247\n&lt;0.001\n\n\nbody_mass_c\n0.077\n0.033\n2.359\n0.024\n\n\narm_span_c\n0.096\n0.044\n2.192\n0.035\n\n\n\\(\\hat{\\sigma}\\)\n1.996"
  },
  {
    "objectID": "mlm_interactions.html#residuen-im-zentrierten-additiven-modell",
    "href": "mlm_interactions.html#residuen-im-zentrierten-additiven-modell",
    "title": "10  Interaktionseffekte",
    "section": "10.8 Residuen im zentrierten, additiven Modell",
    "text": "10.8 Residuen im zentrierten, additiven Modell\n\n\n\n\n\nResiduenplot"
  },
  {
    "objectID": "mlm_interactions.html#added-variable-plot",
    "href": "mlm_interactions.html#added-variable-plot",
    "title": "10  Interaktionseffekte",
    "section": "10.9 Added-variable plot",
    "text": "10.9 Added-variable plot\n\n\n\n\n\nAbbildung 10.1: Added-variable Graph mit car::avPlots()"
  },
  {
    "objectID": "mlm_interactions.html#was-passiert-wenn-die-effekte-nicht-mehr-nur-additiv-sind",
    "href": "mlm_interactions.html#was-passiert-wenn-die-effekte-nicht-mehr-nur-additiv-sind",
    "title": "10  Interaktionseffekte",
    "section": "10.10 Was passiert wenn die Effekte nicht mehr nur additiv sind?",
    "text": "10.10 Was passiert wenn die Effekte nicht mehr nur additiv sind?\n\n\n\n\n\nUnterteilung von Körpergewicht und Armspannweite in Kategorien"
  },
  {
    "objectID": "mlm_interactions.html#was-passiert-wenn-die-effekte-nicht-mehr-nur-additiv-sind-1",
    "href": "mlm_interactions.html#was-passiert-wenn-die-effekte-nicht-mehr-nur-additiv-sind-1",
    "title": "10  Interaktionseffekte",
    "section": "10.11 Was passiert wenn die Effekte nicht mehr nur additiv sind?",
    "text": "10.11 Was passiert wenn die Effekte nicht mehr nur additiv sind?\n\n10.11.1 Neues Modell mit Interaktionen:\n\\[\nY_{i} = \\beta_0 + \\beta_1 \\times \\textrm{bm}_i + \\beta_2 \\times \\textrm{as}_i + \\beta_3 \\times \\textrm{bm}_i \\times \\textrm{as}_i + \\epsilon_i\n\\]"
  },
  {
    "objectID": "mlm_interactions.html#modellierung",
    "href": "mlm_interactions.html#modellierung",
    "title": "10  Interaktionseffekte",
    "section": "10.12 Modellierung",
    "text": "10.12 Modellierung\n\nmod_3 &lt;- lm(vel ~ body_mass_c * arm_span_c, handball) \n\n\n\n\nModell 3\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n21.346\n0.143\n149.296\n&lt;0.001\n\n\nbody_mass_c\n0.119\n0.015\n8.133\n&lt;0.001\n\n\narm_span_c\n0.083\n0.019\n4.380\n&lt;0.001\n\n\nbody_mass_c:arm_span_c\n0.021\n0.002\n12.633\n&lt;0.001\n\n\n\\(\\hat{\\sigma}\\)\n0.868\n\n\n\n\n\n\n\n\n2"
  },
  {
    "objectID": "mlm_interactions.html#einfache-steigungen-in-vergleich",
    "href": "mlm_interactions.html#einfache-steigungen-in-vergleich",
    "title": "10  Interaktionseffekte",
    "section": "10.13 Einfache Steigungen in Vergleich",
    "text": "10.13 Einfache Steigungen in Vergleich\n\n\n\n\n\nModell ohne Interaktionen\n\n\n\n\n\n\n\n\n\nModell mit Interaktionen"
  },
  {
    "objectID": "mlm_interactions.html#interaktionen-sind-symmetrisch",
    "href": "mlm_interactions.html#interaktionen-sind-symmetrisch",
    "title": "10  Interaktionseffekte",
    "section": "10.14 Interaktionen sind symmetrisch",
    "text": "10.14 Interaktionen sind symmetrisch\n\n\n\n\n\nVeränderung mit der Körpergewicht\n\n\n\n\n\n\n\n\n\nVeränderung mit dem Armspannweite"
  },
  {
    "objectID": "mlm_interactions.html#warum-das-model-sinn-macht",
    "href": "mlm_interactions.html#warum-das-model-sinn-macht",
    "title": "10  Interaktionseffekte",
    "section": "10.15 Warum das Model Sinn macht",
    "text": "10.15 Warum das Model Sinn macht\n\n\n\n\n\nVeränderung mit dem Körpergewicht\n\n\n\n\n\n\n\nEinfache Steigungen\n\n\narm span\\centered\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\n\n\n\n10\n22.18\n0.33\n\n\n0\n21.35\n0.12\n\n\n-10\n20.51\n-0.09"
  },
  {
    "objectID": "mlm_interactions.html#warum-das-modell-sinn-macht",
    "href": "mlm_interactions.html#warum-das-modell-sinn-macht",
    "title": "10  Interaktionseffekte",
    "section": "10.16 Warum das Modell Sinn macht",
    "text": "10.16 Warum das Modell Sinn macht\n\n\n\nEinfache Steigungen\n\n\narm span\\centered\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\n\n\n\n10\n22.18\n0.33\n\n\n0\n21.35\n0.12\n\n\n-10\n20.51\n-0.09\n\n\n\n\n\n\n\n\nModellkoeffizienten\n\n\n\nbetas\n\n\n\n\nb0\n21.35\n\n\nbm_c\n0.12\n\n\nas_c\n0.08\n\n\nbm_c:as_c\n0.02"
  },
  {
    "objectID": "mlm_interactions.html#interpretation-der-koeffizienten",
    "href": "mlm_interactions.html#interpretation-der-koeffizienten",
    "title": "10  Interaktionseffekte",
    "section": "10.17 Interpretation der Koeffizienten",
    "text": "10.17 Interpretation der Koeffizienten\n\\[\nY = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2 + \\epsilon_i\n\\]\n\n\\(b_0\\): (y-Achsenabschnitt) der Wert von \\(\\hat{Y}\\) wenn \\(x_1 = 0\\) und \\(x_2 = 0\\) gilt.\n\\(b_1\\): Der Unterschied in \\(\\hat{Y}\\) wenn zwei Objekte sich in \\(x_1\\) um eine Einheit unterscheiden und \\(x_2 = 0\\) ist.\n\\(b_2\\): Der Unterschied in \\(\\hat{Y}\\) wenn zwei Objekte sich in \\(x_2\\) um eine Einheit unterscheiden und \\(x_1 = 0\\) ist.\n\\(b_3\\): (Interaktionskoeffizient) Die Veränderung des Effekts von \\(x_1\\) auf \\(\\hat{Y}\\) wenn \\(x_2\\) um eine Einheit größer wird bzw. genau andersherum für \\(x_2\\)."
  },
  {
    "objectID": "mlm_interactions.html#aus-der-ebene-wird-eine-gekrümmte-fläche",
    "href": "mlm_interactions.html#aus-der-ebene-wird-eine-gekrümmte-fläche",
    "title": "10  Interaktionseffekte",
    "section": "10.18 Aus der Ebene wird eine gekrümmte Fläche",
    "text": "10.18 Aus der Ebene wird eine gekrümmte Fläche\n\n\n\n\n\n3D Streudiagramm des Interaktionsmodells"
  },
  {
    "objectID": "mlm_interactions.html#residuenvergleich",
    "href": "mlm_interactions.html#residuenvergleich",
    "title": "10  Interaktionseffekte",
    "section": "10.19 Residuenvergleich",
    "text": "10.19 Residuenvergleich\n\n\n\n\n\nResiduen im additiven Modell\n\n\n\n\n\n\n\n\n\nResiduen im Interaktionsmodell"
  },
  {
    "objectID": "mlm_interactions.html#residuenvergleich---qq-plot",
    "href": "mlm_interactions.html#residuenvergleich---qq-plot",
    "title": "10  Interaktionseffekte",
    "section": "10.20 Residuenvergleich - qq-Plot",
    "text": "10.20 Residuenvergleich - qq-Plot\n\n\n\n\n\nadditives Modell\n\n\n\n\n\n\n\n\n\nInteraktionsmodell"
  },
  {
    "objectID": "mlm_interactions.html#take-away",
    "href": "mlm_interactions.html#take-away",
    "title": "10  Interaktionseffekte",
    "section": "10.21 Take-away",
    "text": "10.21 Take-away\nInteraktionsmodell\n\nErhöht die Flexibilität des linearen Modells.\nBei Interaktionen hängt der Einfluss der einzelnen Variablen immer von den Werten der anderen Variablen ab.\nAchtung: Interpretation der einfachen Haupteffekte nicht mehr möglich bzw. sinnvoll!"
  },
  {
    "objectID": "mlm_interactions.html#zuschlag",
    "href": "mlm_interactions.html#zuschlag",
    "title": "10  Interaktionseffekte",
    "section": "10.22 Zuschlag",
    "text": "10.22 Zuschlag\nWas passiert im Interaktionsmodell mit den Koeffizienten wenn die \\(x_{ki}\\)s zentriert werden?\n\\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1 (x_{1i} - \\bar{x}_1) + \\beta_2 (x_{2i} - \\bar{x}_2) + \\beta_3 (x_{1i}-\\bar{x}_1)(x_{2i}-\\bar{x}_2) \\\\\n&= \\beta_0 + \\beta_1 x_{1i} - \\beta_1 \\bar{x}_1 + \\beta_2 x_{2i} - \\beta_2 \\bar{x}_2 + \\beta_3 x_{1i} x_{2i} - \\beta_3 x_{1i} \\bar{x}_2 - \\beta_3 \\bar{x}_1 x_{2i} + \\beta_3 \\bar{x}_1 \\bar{x}_2 \\\\\n&= \\beta_0 - \\beta_1 \\bar{x}_1 - \\beta_2 \\bar{x}_2 + \\beta_3 \\bar{x}_1 \\bar{x}_2 + \\beta_1 x_{1i}- \\beta_3 \\bar{x}_2 x_{1i} + \\beta_2 x_{2i} - \\beta_3 \\bar{x}_1 x_{2i} + \\beta_3 x_{1i} x_{2i} \\\\\n&= \\underbrace{\\beta_0 - \\beta_1 \\bar{x}_1 - \\beta_2 \\bar{x}_2 + \\beta_3 \\bar{x}_1 \\bar{x}_2}_{\\beta_0} + \\underbrace{(\\beta_1 - \\beta_3 \\bar{x}_2) x_{1i}}_{\\beta_1 x_{1i}} + \\underbrace{(\\beta_2 - \\beta_3 \\bar{x}_1) x_{2i}}_{\\beta_2 x_{2i}} + \\beta_3 x_{1i} x_{2i}\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_interactions.html#zum-nacharbeiten",
    "href": "mlm_interactions.html#zum-nacharbeiten",
    "title": "10  Interaktionseffekte",
    "section": "10.23 Zum Nacharbeiten",
    "text": "10.23 Zum Nacharbeiten\nKutner u. a. (2005, p.306–313)\n\n\n\n\nDebanne, Thierry, und Guillaume Laffaye. 2011. „Predicting the throwing velocity of the ball in handball with anthropometric variables and isotonic tests“. Journal of Sports Sciences 29 (7): 705–13.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York."
  },
  {
    "objectID": "mlm_interactions.html#footnotes",
    "href": "mlm_interactions.html#footnotes",
    "title": "10  Interaktionseffekte",
    "section": "",
    "text": "Debanne und Laffaye (2011)↩︎\nA*B wird von R ausmultipliziert in A + B + A:B. Hätte auch lm(vel ~ body_mass_c + arm_span_c + body_mass_c:arm_span_c) verwenden können.↩︎"
  },
  {
    "objectID": "mlm_dummy_coding.html#beispiel-körpergröße-bei-frauen-und-männern",
    "href": "mlm_dummy_coding.html#beispiel-körpergröße-bei-frauen-und-männern",
    "title": "11  Integration von nominale Variablen",
    "section": "11.1 Beispiel: Körpergröße bei Frauen und Männern",
    "text": "11.1 Beispiel: Körpergröße bei Frauen und Männern\n\n\n\n\n\nSimulierte Daten: Verteilung von Körpergrößen nach Geschlecht"
  },
  {
    "objectID": "mlm_dummy_coding.html#datensatz",
    "href": "mlm_dummy_coding.html#datensatz",
    "title": "11  Integration von nominale Variablen",
    "section": "11.2 Datensatz",
    "text": "11.2 Datensatz\n\n\n\nAusschnitt aus den Daten\n\n\ncm\ngender\n\n\n\n\n174.4\nm\n\n\n177.7\nm\n\n\n195.6\nm\n\n\n171.3\nf\n\n\n164.0\nf\n\n\n176.0\nf"
  },
  {
    "objectID": "mlm_dummy_coding.html#nominale-variablen-in-r",
    "href": "mlm_dummy_coding.html#nominale-variablen-in-r",
    "title": "11  Integration von nominale Variablen",
    "section": "11.3 Nominale Variablen in R",
    "text": "11.3 Nominale Variablen in R\nNominale Variablen werden in R als factor() dargestellt.\n\ngender &lt;- factor(c(0,0,1,1),\n                 levels = c(0,1),\n                 labels = c('m','f'))\ngender\n\n[1] m m f f\nLevels: m f\n\n\n1"
  },
  {
    "objectID": "mlm_dummy_coding.html#t-test-in-r-mit-t.test",
    "href": "mlm_dummy_coding.html#t-test-in-r-mit-t.test",
    "title": "11  Integration von nominale Variablen",
    "section": "11.4 t-Test in R mit t.test()",
    "text": "11.4 t-Test in R mit t.test()\n\nt.test(cm ~ gender, data=height, var.equal=T)\n\n\n\n\n Two Sample t-test\n\ndata: cm by gender\nt = -4.57, df = 58, p-value = &lt;0.001\nd = -10.75, s_e = 2.35\n95 percent confidence interval\n[-15.45, -6.04]"
  },
  {
    "objectID": "mlm_dummy_coding.html#modellformulierung-beim-t-test-n_w-n_m",
    "href": "mlm_dummy_coding.html#modellformulierung-beim-t-test-n_w-n_m",
    "title": "11  Integration von nominale Variablen",
    "section": "11.5 Modellformulierung beim t-Test \\((n_w = n_m)\\)",
    "text": "11.5 Modellformulierung beim t-Test \\((n_w = n_m)\\)\n\\[\\begin{align*}\nY_{if} &= \\mu_{f} + \\epsilon_{if}, \\quad \\epsilon_{if} \\sim \\mathcal{N}(0,\\sigma^2) \\\\\nY_{im} &= \\mu_{m} + \\epsilon_{im}, \\quad \\epsilon_{im} \\sim \\mathcal{N}(0,\\sigma^2)\n\\end{align*}\\]\n\n11.5.1 Hypothesen\n\\[\\begin{align*}\nH_0&: \\delta = 0 \\\\\nH_1&: \\delta \\neq 0\n\\end{align*}\\]\n\n\n11.5.2 Teststatistik\n\\[\nt = \\frac{\\bar{y}_m - \\bar{y}_w}{\\sqrt{\\frac{s_m^2 + s_w^2}{2}}\\sqrt{\\frac{2}{n}}}\n\\]\n\n\n11.5.3 Referenzverteilung\n\\[\nt \\sim t_{df=2n-2}\n\\]\n\n\n\n\n\nt-Verteilung mit \\(df=58\\)"
  },
  {
    "objectID": "mlm_dummy_coding.html#kann-ich-aus-dem-t-test-ein-lineares-modell-machen",
    "href": "mlm_dummy_coding.html#kann-ich-aus-dem-t-test-ein-lineares-modell-machen",
    "title": "11  Integration von nominale Variablen",
    "section": "11.6 Kann ich aus dem t-Test ein lineares Modell machen?",
    "text": "11.6 Kann ich aus dem t-Test ein lineares Modell machen?\n\n11.6.1 t-Test\n\\[\\begin{align*}\nY_{if} &= \\mu_{f} + \\epsilon_{if}, \\quad \\epsilon_{if} \\sim \\mathcal{N}(0,\\sigma^2) \\\\\nY_{im} &= \\mu_{m} + \\epsilon_{im}, \\quad \\epsilon_{im} \\sim \\mathcal{N}(0,\\sigma^2) \\\\\nt &= \\frac{\\bar{y}_m - \\bar{y}_w}{\\sqrt{\\frac{s_m^2 + s_w^2}{2}}\\sqrt{\\frac{2}{n}}} \\\\\nt &\\sim t_{df=2n-2}\n\\end{align*}\\]\n\n\n11.6.2 Lineares Modell\n\\[\\begin{align*}\nY_i &= \\beta_0 + \\beta_1 \\times x_i + \\epsilon_i \\\\\n\\Delta_m &= \\mu_m - \\mu_f \\\\\nY_i &= \\beta_0 + \\beta_1 \\times x_{??} + \\epsilon_i \\\\\nY_i &= \\mu_f + \\Delta_{m} \\times x_{??} + \\epsilon_i\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#dummy--oder-indikatorkodierung",
    "href": "mlm_dummy_coding.html#dummy--oder-indikatorkodierung",
    "title": "11  Integration von nominale Variablen",
    "section": "11.7 Dummy- oder Indikatorkodierung",
    "text": "11.7 Dummy- oder Indikatorkodierung\n\\[\\begin{align*}\nY_i &= \\mu_f + \\Delta_{m} \\times x_{1i} + \\epsilon_i \\\\\n\\Delta_m &= \\mu_m - \\mu_f \\\\\nx_1 &=\n\\begin{cases}\n0\\text{ wenn weiblich}\\\\\n1\\text{ wenn männlich}\n\\end{cases}\n\\end{align*}\\]\nFür eine nominale Variable wird eine Indikatorvariablen (Dummyvariable) definiert. Über diese Indikatorvariable kann die Zugehörigkeit eines Messwerts \\(Y_i\\) zu einer Faktorstufe \\(k\\) bestimmt werden. Eine Faktorstufe ist dabei immer die Referenzstufe bei der die Indikatorvariable gleich \\(0\\) ist."
  },
  {
    "objectID": "mlm_dummy_coding.html#einfach-mal-stumpf-in-lm-eingeben",
    "href": "mlm_dummy_coding.html#einfach-mal-stumpf-in-lm-eingeben",
    "title": "11  Integration von nominale Variablen",
    "section": "11.8 Einfach mal stumpf in lm() eingeben",
    "text": "11.8 Einfach mal stumpf in lm() eingeben\n\nmod &lt;- lm(cm ~ gender, height)\n\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n168.783\n1.663\n101.477\n&lt;0.001\n\n\ngenderm\n10.746\n2.352\n4.568\n&lt;0.001\n\n\n\n\n\n2"
  },
  {
    "objectID": "mlm_dummy_coding.html#vergleich-der-konfidenzintervalle",
    "href": "mlm_dummy_coding.html#vergleich-der-konfidenzintervalle",
    "title": "11  Integration von nominale Variablen",
    "section": "11.9 Vergleich der Konfidenzintervalle",
    "text": "11.9 Vergleich der Konfidenzintervalle\n\n11.9.1 Lineares Modell\n\nconfint(mod)\n\n                2.5 %    97.5 %\n(Intercept) 165.45401 172.11276\ngenderm       6.03713  15.45403\n\n\n\n\n11.9.2 t-Test\n\nt.test(cm ~ gender,\n       data = height,\n       var.equal=T)$conf\n\n[1] -15.45403  -6.03713\nattr(,\"conf.level\")\n[1] 0.95\n\n\n3"
  },
  {
    "objectID": "mlm_dummy_coding.html#auf-welchen-werten-wird-ein-lineares-modell-gerechnet",
    "href": "mlm_dummy_coding.html#auf-welchen-werten-wird-ein-lineares-modell-gerechnet",
    "title": "11  Integration von nominale Variablen",
    "section": "11.10 Auf welchen Werten wird ein lineares Modell gerechnet???",
    "text": "11.10 Auf welchen Werten wird ein lineares Modell gerechnet???\n\n\n\nRepräsentation der Faktorvariablen\n\n\ncm\ngender\n\\(x_1\\)\n\n\n\n\n174.40\nm\n1\n\n\n177.70\nm\n1\n\n\n195.59\nm\n1\n\n\n160.05\nf\n0\n\n\n164.92\nf\n0\n\n\n154.35\nf\n0"
  },
  {
    "objectID": "mlm_dummy_coding.html#residuen",
    "href": "mlm_dummy_coding.html#residuen",
    "title": "11  Integration von nominale Variablen",
    "section": "11.11 Residuen",
    "text": "11.11 Residuen\n\n\n\n\n\nResiduen"
  },
  {
    "objectID": "mlm_dummy_coding.html#wens-interessiert---t-wert",
    "href": "mlm_dummy_coding.html#wens-interessiert---t-wert",
    "title": "11  Integration von nominale Variablen",
    "section": "11.12 Wen’s interessiert - t-Wert",
    "text": "11.12 Wen’s interessiert - t-Wert\nSeien beide Gruppen gleich groß (\\(n\\)) mit \\(N = n_m + n_w = 2 \\times n\\). Der t-Wert für \\(\\beta_1\\) berechnet sich aus \\(t = \\frac{b_1}{s_b}\\) mit:\n\\[\ns_b = \\sqrt{\\frac{\\sum_{i=1}^N (y_i - \\bar{y})^2}{N-2}\\frac{1}{\\sum_{i=1}^N(x_i-\\bar{x})^2}}\n\\] Dadurch, das die \\(x_i\\) entweder gleich \\(0\\) oder \\(1\\) sind, ist \\(\\bar{x}=0.5\\) und die Abweichungsquadrate im zweiten Term sind alle gleich \\(\\frac{1}{4}\\).\n\\[\n\\sum_{i=1}^N(x_i - \\bar{x})^2=\\sum_{i=1}^N\\left(x_i - \\frac{1}{2}\\right)^2 = \\sum_{i=1}^N\\frac{1}{4}=\\frac{N}{4}=\\frac{2n}{4}=\\frac{n}{2}\n\\]\nDer ersten Term kann mit etwas Algebra und der Definition für die Stichprobenvarianz \\(s^2\\) auf die gewünschte Form gebracht werden.\n\\[\n\\frac{\\sum_{i=1}^N(y_i-\\hat{y})^2}{N-2}=\\frac{\\sum_{i=1}^n(\\overbrace{y_{im} - \\bar{y}_m}^{Männer})^2+\\sum_{i=1}^n(\\overbrace{y_{iw}-\\bar{y}_w}^{Frauen})^2}{2(n-1)}=\\frac{(n-1)s_m^2+(n-1)s_w^2}{2(n-1)}=\\frac{s_m^2+s_w^2}{2}\n\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#wens-interessiert---beta_1-mu_w---mu_m",
    "href": "mlm_dummy_coding.html#wens-interessiert---beta_1-mu_w---mu_m",
    "title": "11  Integration von nominale Variablen",
    "section": "11.13 Wen’s interessiert - \\(\\beta_1 = \\mu_w - \\mu_m\\)",
    "text": "11.13 Wen’s interessiert - \\(\\beta_1 = \\mu_w - \\mu_m\\)\nMit \\(s_x^2 = \\frac{N\\frac{1}{4}}{N-1} = \\frac{N}{4(N-1)}\\) \\[\\begin{align*}\n    b_1 &= \\frac{cov(x,y)}{s_x^2} \\\\\n    &= \\frac{\\sum_{i=1}^N(y_i - \\bar{y})(x_i - \\bar{x})}{N-1} \\frac{4(N-1)}{N} \\\\\n    &= 4\\frac{\\sum_{i=1}^n(y_{im}-\\bar{y})\\frac{-1}{2}+\\sum(y_{iw}-\\bar{y})\\frac{1}{2}}{N} \\\\\n    &= \\frac{4}{2}\\frac{\\sum_{i=1}^n(y_{iw}-\\bar{y}) - \\sum_{i=1}^n(y_{im}-\\bar{y})}{2n} \\\\\n    &= \\frac{\\sum_{i=1}^n y_{iw}}{n} - \\frac{n\\bar{y}}{n} - \\frac{\\sum_{i=1}^n y_{im}}{n} + \\frac{n\\bar{y}}{n} \\\\\n    &= \\bar{y}_w - \\bar{y}_m = \\Delta\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#wens-interessiert---beta_0-mu_m",
    "href": "mlm_dummy_coding.html#wens-interessiert---beta_0-mu_m",
    "title": "11  Integration von nominale Variablen",
    "section": "11.14 Wen’s interessiert - \\(\\beta_0 = \\mu_m\\)",
    "text": "11.14 Wen’s interessiert - \\(\\beta_0 = \\mu_m\\)\nMit \\(b_1 = \\Delta = \\bar{y}_w - \\bar{y}_m\\): \\[\\begin{align*}\nb_0 &= \\bar{y} - \\Delta \\times \\bar{x} \\\\\n&= \\frac{\\sum_{i=1}^N y_i}{N} - \\Delta \\times \\frac{1}{2} \\\\\n&= \\frac{\\sum_{i=1}^n y_{im} + \\sum_{i=1}^n y_{iw}}{2n} - \\frac{1}{2}(\\bar{y}_w - \\bar{y}_m)  \\\\\n&= \\frac{1}{2}\\frac{\\sum_{i=1}^ny_{im}}{n} + \\frac{1}{2}\\frac{\\sum_{i=1}^ny_{iw}}{n} - \\frac{1}{2}\\bar{y}_w + \\frac{1}{2}\\bar{y}_m \\\\\n&= \\frac{1}{2}\\bar{y}_m + \\frac{1}{2}\\bar{y}_w - \\frac{1}{2}\\bar{y}_w + \\frac{1}{2}\\bar{y}_m \\\\\n&= \\bar{y}_m\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#können-auch-mehr-als-zwei-stufen-verwendet-werden",
    "href": "mlm_dummy_coding.html#können-auch-mehr-als-zwei-stufen-verwendet-werden",
    "title": "11  Integration von nominale Variablen",
    "section": "11.15 Können auch mehr als zwei Stufen verwendet werden?",
    "text": "11.15 Können auch mehr als zwei Stufen verwendet werden?\n\n\n\n\n\nEin Reaktionszeitexperiment mit vier Stufen A, B, C und D"
  },
  {
    "objectID": "mlm_dummy_coding.html#deskriptive-daten",
    "href": "mlm_dummy_coding.html#deskriptive-daten",
    "title": "11  Integration von nominale Variablen",
    "section": "11.16 Deskriptive Daten",
    "text": "11.16 Deskriptive Daten"
  },
  {
    "objectID": "mlm_dummy_coding.html#reaktionszeitexperiment-als-lineares-modell",
    "href": "mlm_dummy_coding.html#reaktionszeitexperiment-als-lineares-modell",
    "title": "11  Integration von nominale Variablen",
    "section": "11.17 Reaktionszeitexperiment als lineares Modell",
    "text": "11.17 Reaktionszeitexperiment als lineares Modell\n\n11.17.1 Modell\n\\[\ny_i = \\mu_A + \\Delta_{B-A} x_1 + \\Delta_{C-A} x_2 + \\Delta_{D-A} x_3 + \\epsilon_i\n\\]\n\n\n11.17.2 Dummyvariablen"
  },
  {
    "objectID": "mlm_dummy_coding.html#nochmal-allgemeiner",
    "href": "mlm_dummy_coding.html#nochmal-allgemeiner",
    "title": "11  Integration von nominale Variablen",
    "section": "11.18 Nochmal allgemeiner",
    "text": "11.18 Nochmal allgemeiner\nMit \\(K\\) Faktorstufen werden (K-1) Dummyvariablen \\(x_1, x_2, \\ldots, x_{K-1}\\) benötigt. Eine Stufe wird als Referenz definiert. Die \\(x_1\\) bis \\(x_{K-1}\\) kodieren die Abweichungen der anderen Stufen von dieser Stufe.4"
  },
  {
    "objectID": "mlm_dummy_coding.html#reaktionszeitexperiment-mit-lm",
    "href": "mlm_dummy_coding.html#reaktionszeitexperiment-mit-lm",
    "title": "11  Integration von nominale Variablen",
    "section": "11.19 Reaktionszeitexperiment mit lm()",
    "text": "11.19 Reaktionszeitexperiment mit lm()\n\nmod &lt;- lm(rt ~ group, data)\n\n\n\n\nModellfit\n\n\n\n$\\hat{\\beta}$\n$s_e$\nt\np\n\n\n\n\n(Intercept)\n509.526\n10.235\n49.784\n&lt;0.001\n\n\ngroupB\n90.150\n14.474\n6.228\n&lt;0.001\n\n\ngroupC\n197.414\n14.474\n13.639\n&lt;0.001\n\n\ngroupD\n295.561\n14.474\n20.420\n&lt;0.001"
  },
  {
    "objectID": "mlm_dummy_coding.html#ausblick",
    "href": "mlm_dummy_coding.html#ausblick",
    "title": "11  Integration von nominale Variablen",
    "section": "11.20 Ausblick",
    "text": "11.20 Ausblick\n\nanova(mod)\n\n\n\n\nANOVA-Tabelle\n\n\n\nDf\nSSQ\nMSQ\nF\np\n\n\n\n\ngroup\n3\n988935.1\n329645.04\n157.35\n&lt;0.001\n\n\nResiduals\n76\n159221.0\n2095.01"
  },
  {
    "objectID": "mlm_dummy_coding.html#kombination-von-kontinuierlichen-und-nominalen-variablen",
    "href": "mlm_dummy_coding.html#kombination-von-kontinuierlichen-und-nominalen-variablen",
    "title": "11  Integration von nominale Variablen",
    "section": "11.21 Kombination von kontinuierlichen und nominalen Variablen",
    "text": "11.21 Kombination von kontinuierlichen und nominalen Variablen\n\n\n\n\n\nHypothetische Leistungsentwicklung in Abhängigkeit vom Alter und Gender"
  },
  {
    "objectID": "mlm_dummy_coding.html#modellansatz",
    "href": "mlm_dummy_coding.html#modellansatz",
    "title": "11  Integration von nominale Variablen",
    "section": "11.22 Modellansatz",
    "text": "11.22 Modellansatz\n\nAus gender (K = 2) wird eine Dummyvariable\nFrauen werden (zufällig) als Referenz genommen\n\n\\[\\begin{align*}\nY_i &= \\beta_{ta = 0,x_1=0} + \\Delta_m \\times x_1 + \\beta_{ta} \\times ta + \\epsilon_i \\\\\nx_1 &=\n\\begin{cases}\n0\\text{ wenn weiblich}\\\\\n1\\text{ wenn männlich}\n\\end{cases} \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#modellieren-mit-lm",
    "href": "mlm_dummy_coding.html#modellieren-mit-lm",
    "title": "11  Integration von nominale Variablen",
    "section": "11.23 Modellieren mit lm()",
    "text": "11.23 Modellieren mit lm()\n\nmod &lt;- lm(perf ~ gender_f + ta, lew)\n\n\n\n\nModellfit\n\n\n\n$\\hat{\\beta}$\n$s_e$\n\n\n\n\n(Intercept)\n41.181\n1.083\n\n\ngender\\_fm\n-10.877\n0.805\n\n\nta\n1.927\n0.145\n\n\n$\\hat{\\sigma}$\n2.845"
  },
  {
    "objectID": "mlm_dummy_coding.html#die-resultierenden-graden",
    "href": "mlm_dummy_coding.html#die-resultierenden-graden",
    "title": "11  Integration von nominale Variablen",
    "section": "11.24 Die resultierenden Graden",
    "text": "11.24 Die resultierenden Graden\n\n\n\n\n\nLeistungsentwicklung in Abhängigkeit vom Alter und Gender"
  },
  {
    "objectID": "mlm_dummy_coding.html#interaktion-zwischen-kontinuierlichen-und-nominalen-variablen",
    "href": "mlm_dummy_coding.html#interaktion-zwischen-kontinuierlichen-und-nominalen-variablen",
    "title": "11  Integration von nominale Variablen",
    "section": "11.25 Interaktion zwischen kontinuierlichen und nominalen Variablen",
    "text": "11.25 Interaktion zwischen kontinuierlichen und nominalen Variablen\n\n\n\n\n\nLeistungsentwicklung in Abhängigkeit vom Alter und Gender"
  },
  {
    "objectID": "mlm_dummy_coding.html#ansatz-für-ein-interaktionsmodell",
    "href": "mlm_dummy_coding.html#ansatz-für-ein-interaktionsmodell",
    "title": "11  Integration von nominale Variablen",
    "section": "11.26 Ansatz für ein Interaktionsmodell",
    "text": "11.26 Ansatz für ein Interaktionsmodell\nDas vorhergehendes Modell wird um einen Interaktionsterm erweitert.\n\\[\ny_i = \\beta_{ta=0,x_1=0} + \\Delta_m \\times x_1 + \\beta_{ta} \\times ta + \\beta_{ta \\times gender} \\times x_1 \\times ta + \\epsilon_i\n\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#interaktionsmodell-mit-lm",
    "href": "mlm_dummy_coding.html#interaktionsmodell-mit-lm",
    "title": "11  Integration von nominale Variablen",
    "section": "11.27 Interaktionsmodell mit lm()",
    "text": "11.27 Interaktionsmodell mit lm()\n\nmod &lt;- lm(perf ~ gender_f * ta, lew)\n\n\n\n\nModellfit\n\n\n\n$\\hat{\\beta}$\n$s_e$\n\n\n\n\n(Intercept)\n31.354\n1.370\n\n\ngender\\_fm\n8.575\n2.010\n\n\nta\n1.763\n0.195\n\n\ngender\\_fm:ta\n2.362\n0.290\n\n\n$\\hat{\\sigma}$\n2.828"
  },
  {
    "objectID": "mlm_dummy_coding.html#regressionsgeraden",
    "href": "mlm_dummy_coding.html#regressionsgeraden",
    "title": "11  Integration von nominale Variablen",
    "section": "11.28 Regressionsgeraden",
    "text": "11.28 Regressionsgeraden\n\n\n\n\n\nLeistungsentwicklung in Abhängigkeit vom Alter und Gender"
  },
  {
    "objectID": "mlm_dummy_coding.html#zum-nacharbeiten",
    "href": "mlm_dummy_coding.html#zum-nacharbeiten",
    "title": "11  Integration von nominale Variablen",
    "section": "11.29 Zum Nacharbeiten",
    "text": "11.29 Zum Nacharbeiten\nKutner u. a. (2005, p.313–319) \n\n\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York."
  },
  {
    "objectID": "mlm_dummy_coding.html#footnotes",
    "href": "mlm_dummy_coding.html#footnotes",
    "title": "11  Integration von nominale Variablen",
    "section": "",
    "text": "Viele Funktionen in R transformieren eine Vektor mit Zeichenketten in einen factor() um. z.B. factor(c('m','m','f','f'))↩︎\nR gibt die Faktorstufe nach dem Namen des Faktors an. Im Beispiel steht genderm für Stufe m im Faktor gender.↩︎\nMit t.test()$conf.int kann auf das berechnete Konfidenzintervall zugegriffen werden.↩︎\nDiese Art der Kodierung wird auch als treatment Kodierung bezeichnet.↩︎"
  },
  {
    "objectID": "mlm_hierarchies.html#einfaches-modell",
    "href": "mlm_hierarchies.html#einfaches-modell",
    "title": "12  Modellhierarchien",
    "section": "12.1 Einfaches Modell",
    "text": "12.1 Einfaches Modell\nWir beginnen mit einem einfachen Modell, das wiederum nur eine \\(x\\) und eine \\(y\\)-Variable hat.\n\nmod0 &lt;- lm(y ~ x, simple)\nsummary(mod0)\n\n\nCall:\nlm(formula = y ~ x, data = simple)\n\nResiduals:\n      1       2       3       4 \n-0.5817  0.9898 -0.2345 -0.1736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   1.8414     0.7008   2.628    0.119\nx             0.4574     0.3746   1.221    0.346\n\nResidual standard error: 0.8376 on 2 degrees of freedom\nMultiple R-squared:  0.4271,    Adjusted R-squared:  0.1406 \nF-statistic: 1.491 on 1 and 2 DF,  p-value: 0.3465\n\n\nHier ist jetzt zunächst einmal nichts Neues. Setzen wir uns aber noch einmal genauer mit den Abweichungen mit den Residuen auseinander. In der Besprechung des Determinationskoeffizienten \\(R^2\\) haben wir schon die Unterteilung der Quadratsummen kennengelernt. Hier hatten wird auf die Aufteilung der Varianz von \\(Y\\) SSTO in die beiden Komponenten Regressionsvarianz \\(SSR\\) und Fehlervarianz \\(SSE\\). Es gilt.\n\\[\\begin{equation}\nSSTO = SSR + SSE\n\\end{equation}\\]\nDie Fehlerquadratsumme SSE ist definiert nach:\n\\[\\begin{equation}\nSSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\end{equation}\\]\nAls die Summe der quadrierten Abweichungen der vorhergesagten Werte \\(\\hat{y}_i\\) von den tatsächlich beobachteten Werten \\(y_i\\).\nUm die Werte \\(\\hat{y}_i\\) berechnen zu können, benötigen wir unser Modell. Dieses Modell hat zwei Parameter, die beiden Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\). Die Anzahl der Parameter in dem Modell wird meistens per Konvention mit dem Symbol \\(p\\) bezeichnet. In unserem Fall ist gilt daher \\(p = 2\\). Die Anzahl der Parameter \\(p\\) ist verknüpft mit den sogenannten Freiheitsgraden \\(df\\) (degrees of freedom). Die Freiheitsgrade von SSE berechnen sich nach \\(N-p\\), wobei \\(N\\) die Anzahl der Beobachtungen, der Datenpunkte ist. In unseren Fall also \\(N-2\\)\n\\[\\begin{equation}\ndf_E := n - p\n\\end{equation}\\]\nDie Frheitsgerade bestimmen die effektive Anzahl der Beobachtungen die zur Verfügung stehen um die Varianz \\(\\sigma^2\\) des Modells abzuschätzen. Dadurch, dass zwei Parameter anhand der Daten für das Modell bestimt werden, fallen zwei Datenpunkt raus als unabhängige Informationsquellen. Anders ausgedrückt, wenn ich die beiden Modellparameter, in unseren Fall \\(\\beta_0\\) und \\(\\beta_1\\) kenne, dann sind nur noch \\(N-2\\) Datenpunkt frei variierbar. Sobald ich die Werte von \\(N-2\\) Datenpunkten und die beiden Parameter kenne, kann ich die verbleibenden beiden Werte berechnen. Daher die Begriff der Freiheitsgrade.\nWir nun SSE durch die Anzahl der Freiheitsgerade teilen, dann lässt sich zeigen, das dieser Wert ein erwartungstreuer Schätzer für die Residualvarianz \\(\\sigma^2\\) in Bezug auf die Verteilungsannahme zu den \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) der Daten ist. Das Verhältnis von SSE zu \\(df\\) wird als Mean squared error MSE bezeichnet.\n\\[\\begin{equation}\nMSE = \\frac{SSE}{df_{E}} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p} = \\hat{\\sigma}^2\n\\end{equation}\\]\nIm ersten Moment erscheint diese Begründung etwas undurchsichtig, aber tatsächlich ist diese Formel schon eine alte Bekannte die uns in Form der Stichprobenvarianz begegnet ist.\nWenn wir eine Stichprobe der Größe \\(N\\) mit Werten \\(y_i\\) haben, dann haben wir gelernt die Varianz mittels der Formel:\n\\[\\begin{equation}\n\\hat{\\sigma}^2 = s^2 = \\frac{1}{n-1}\\sum_{i=1}^2(y_i - \\bar{y})^2\n\\end{equation}\\]\nzu berechnen. Was bei dieser Formel schon immer etwas quer gesessen hat, ist der Nenner mit \\(N-1\\) anstatt einfach \\(N\\) wie wir es vom Mittelwert kennen. Aber, um die Varianz zu berechnen benötigen wir einen Parameter, eben den Mittelwert. Dies führt wiederum dazu, dass nur \\(N-1\\) Werte frei variiert werden können und wir sobald wir, neben dem Mittelwert \\(\\bar{y}\\), \\(N-1\\) Werte kennen, den verbleibenden Wert berechnen können."
  },
  {
    "objectID": "mlm_hierarchies.html#einfaches-modell-1",
    "href": "mlm_hierarchies.html#einfaches-modell-1",
    "title": "12  Modellhierarchien",
    "section": "12.2 Einfaches Modell",
    "text": "12.2 Einfaches Modell\n\nmod0 &lt;- lm(y ~ x, simple)\nsummary(mod0)\n\n\nCall:\nlm(formula = y ~ x, data = simple)\n\nResiduals:\n      1       2       3       4 \n-0.5817  0.9898 -0.2345 -0.1736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   1.8414     0.7008   2.628    0.119\nx             0.4574     0.3746   1.221    0.346\n\nResidual standard error: 0.8376 on 2 degrees of freedom\nMultiple R-squared:  0.4271,    Adjusted R-squared:  0.1406 \nF-statistic: 1.491 on 1 and 2 DF,  p-value: 0.3465\n\n\nHier ist jetzt zunächst einmal nichts Neues. Setzen wir uns aber noch einmal genauer mit den Abweichungen mit den Residuen auseinander. In der Besprechung des Determinationskoeffizienten \\(R^2\\) haben wir schon die Unterteilung der Quadratsummen kennengelernt. Hier hatten wird auf die Aufteilung der Varianz von \\(Y\\) SSTO in die beiden Komponenten Regressionsvarianz \\(SSR\\) und Fehlervarianz \\(SSE\\). Es gilt.\n\\[\\begin{equation}\nSSTO = SSR + SSE\n\\end{equation}\\]\n\n12.2.1 Sum of squares of error\n\\[\nSSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nTypischerweise beinhaltet ein Modell zum berechnen der \\(\\hat{y}_i\\) verschiedene Parameter. Bei der einfachen Regression zum Beispiel \\(\\beta_0\\) und \\(\\beta_1\\) (#Modellparameter \\(p\\) = 2) .\n\n\n12.2.2 Freiheitsgrade (degrees of freedom) von SSE\n\\[\ndfE := n - p\n\\]\nDie effektive Anzahl der Beobachtungen um die Varianz \\(\\sigma^2\\) abzuschätzen."
  },
  {
    "objectID": "mlm_hierarchies.html#abweichungen-noch-mal",
    "href": "mlm_hierarchies.html#abweichungen-noch-mal",
    "title": "12  Modellhierarchien",
    "section": "12.3 Abweichungen … noch mal",
    "text": "12.3 Abweichungen … noch mal\n\n12.3.1 Sum of squares of error\n\\[\nSSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nTypischerweise beinhaltet ein Modell zum berechnen der \\(\\hat{y}_i\\) verschiedene Parameter. Bei der einfachen Regression zum Beispiel \\(\\beta_0\\) und \\(\\beta_1\\) (#Modellparameter \\(p\\) = 2) .\n\n\n12.3.2 Freiheitsgrade (degrees of freedom) von SSE\n\\[\ndfE := n - p\n\\]\nDie effektive Anzahl der Beobachtungen um die Varianz \\(\\sigma^2\\) abzuschätzen."
  },
  {
    "objectID": "mlm_hierarchies.html#mse-als-schätzer-für-sigma2",
    "href": "mlm_hierarchies.html#mse-als-schätzer-für-sigma2",
    "title": "12  Modellhierarchien",
    "section": "12.3 MSE als Schätzer für \\(\\sigma^2\\)",
    "text": "12.3 MSE als Schätzer für \\(\\sigma^2\\)\n\n12.3.1 Mean squared error MSE\n\\[\nMSE = \\frac{SSE}{dfE} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p}\n\\]\nAls Schätzer \\(\\hat{\\sigma}^2\\) für \\(\\sigma^2\\) aus \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\)\n\n\n12.3.2 Parallel zur Berechnung der Stichprobenvarianz\n\\[\n\\hat{\\sigma}^2 = s^2 = \\frac{1}{n-1}\\sum_{i=1}^2(y_i - \\bar{y})^2\n\\]\nwo \\(s^2\\) ein Schätzer für die Varianz von \\(y\\) ist."
  },
  {
    "objectID": "mlm_hierarchies.html#genereller-linearer-modell-testansatz",
    "href": "mlm_hierarchies.html#genereller-linearer-modell-testansatz",
    "title": "12  Modellhierarchien",
    "section": "12.2 Genereller Linearer Modell Testansatz1",
    "text": "12.2 Genereller Linearer Modell Testansatz1\n\n12.2.1 Idee\nWir bauen uns eine Teststatistik die die Verbesserung in der Vorhersage (\\(=\\) Reduktion der Fehlervarianz) als Metrik verwendet. Modelle werden in eine Hierarchie gesetzt mit einfacheren Modellen untergeordnet zu komplexeren Modellen.\n\n\n12.2.2 Leitfrage:\nBringt mir die Aufnahme Modellparameter eine in der Vorhersage von Y bzw. bezüglich der Aufklärung der Varianz in Y?"
  },
  {
    "objectID": "mlm_hierarchies.html#genereller-linearer-modell-testansatz---full-model",
    "href": "mlm_hierarchies.html#genereller-linearer-modell-testansatz---full-model",
    "title": "12  Modellhierarchien",
    "section": "12.3 Genereller Linearer Modell Testansatz - Full model",
    "text": "12.3 Genereller Linearer Modell Testansatz - Full model\nBeispiel einfache lineare Regression\n\n12.3.1 Volles Modell\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\n\\]\n\n\n12.3.2 Residualvarianz SSE(F)\n\\[\n\\textrm{SSE(F)}  = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n [y_i - (\\beta_0 + \\beta_1 x_i)]^2\n\\]\nmit \\(p = 2, dfE(F) = n - 2\\)"
  },
  {
    "objectID": "mlm_hierarchies.html#genereller-linearer-modell-testansatz---reduced-model",
    "href": "mlm_hierarchies.html#genereller-linearer-modell-testansatz---reduced-model",
    "title": "12  Modellhierarchien",
    "section": "12.4 Genereller Linearer Modell Testansatz - Reduced model",
    "text": "12.4 Genereller Linearer Modell Testansatz - Reduced model\n\n12.4.1 Reduziertes Modell\n\\[\nY_i = \\beta_0 + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\n\\]\n\n\n12.4.2 Residualvarianz SSE(R)\n\\[\n\\textrm{SSE(R)} = \\sum_{i=1}^n (y_i - \\beta_0)^2 = \\sum_{i=1}^n(y_i - \\bar{y})^2 = \\textrm{SSTO}\n\\]\nmit \\(p = 1, dfE(R) = n - 1\\)\nIm Allgemeinen gilt: \\(SSE(F) \\leq SSE(R)\\)"
  },
  {
    "objectID": "mlm_hierarchies.html#link-reduziertes-modell-und-stichprobenvarianz",
    "href": "mlm_hierarchies.html#link-reduziertes-modell-und-stichprobenvarianz",
    "title": "12  Modellhierarchien",
    "section": "12.5 Link: Reduziertes Modell und Stichprobenvarianz",
    "text": "12.5 Link: Reduziertes Modell und Stichprobenvarianz\n\\[\\begin{align*}\nSSE &= \\sum_{i=1}^n(y_i - \\beta_0)^2 = \\sum_{i=1}^n (y_i^2 - 2y_i\\beta_0 + \\beta_0^2) \\\\\n0 &= \\frac{\\mathrm{d}}{\\mathrm{d} \\beta_0}\\sum_{i=1}^n (y_i^2 - 2y_i\\beta_0 + \\beta_0^2) \\\\\n0 &= \\sum_{i=1}^n (-2y_i + 2 \\beta_0) = -2\\sum_{i=1}^n y_i + 2\\sum_{i=1}^n \\beta_0\\\\\nn\\beta_0 &= \\sum_{i=1}^n y_i \\\\\n\\beta_0 &= \\frac{\\sum_{i=1}^n y_i}{n} = \\bar{y} \\rightarrow \\frac{SSE}{n-1} = \\hat{\\sigma}^2 = s^2\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_hierarchies.html#genereller-linearer-modell-testansatz-1",
    "href": "mlm_hierarchies.html#genereller-linearer-modell-testansatz-1",
    "title": "12  Modellhierarchien",
    "section": "12.6 Genereller Linearer Modell Testansatz",
    "text": "12.6 Genereller Linearer Modell Testansatz\nAnnahme: Das reduzierte Modell ist korrekt. Dann sollte \\[\n\\textrm{SSE(R)} - \\textrm{SSE(F)}\n\\] eher klein sein (Beide Modelle haben einen gleich guten fit).\nAnnahme: Das reduzierte Modell ist falsch: Dann sollte \\[\n\\textrm{SSE(R)} - \\textrm{SSE(F)}\n\\] eher groß sein (Das reduzierte Modell kann die Daten nicht so gut fitten wie das komplizierte Modell)"
  },
  {
    "objectID": "mlm_hierarchies.html#genereller-linearer-modell-testansatz---teststatistik",
    "href": "mlm_hierarchies.html#genereller-linearer-modell-testansatz---teststatistik",
    "title": "12  Modellhierarchien",
    "section": "12.7 Genereller Linearer Modell Testansatz - Teststatistik",
    "text": "12.7 Genereller Linearer Modell Testansatz - Teststatistik\nWenn das reduzierte Modell korrekt ist, dann lässt sich zeigen, dass: \\[\nMS_{\\textrm{test}} = \\frac{\\textrm{SSE(R)} - \\textrm{SSE(F)}}{\\textrm{dfE(R)} - \\textrm{dfE(F)}}\n\\] ein Schätzer für die Varianz \\(\\sigma^2\\) (\\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\)) ist.\nWenn das reduzierte Modell korrekt ist, dann ist auch das volle Modell korrekt. Daher ist dann:\n\\[\n\\textrm{MSE(F)} = \\frac{\\textrm{SSE(F)}}{\\textrm{dfE(F)}}\n\\] auch ein Schätzer für \\(\\sigma^2\\)"
  },
  {
    "objectID": "mlm_hierarchies.html#f-wert-als-teststatistik",
    "href": "mlm_hierarchies.html#f-wert-als-teststatistik",
    "title": "12  Modellhierarchien",
    "section": "12.8 F-Wert als Teststatistik",
    "text": "12.8 F-Wert als Teststatistik\n\\[\nF = \\frac{MS_{\\textrm{test}}}{MSE(F)}= \\frac{\\frac{\\textrm{SSE(R)} - \\textrm{SSE(F)}}{\\textrm{dfE(R)} - \\textrm{dfE(F)}}}{ \\frac{\\textrm{SSE(F)}}{\\textrm{dfE(F)}}}\n\\]"
  },
  {
    "objectID": "mlm_hierarchies.html#verteilung-der-f-statistik",
    "href": "mlm_hierarchies.html#verteilung-der-f-statistik",
    "title": "12  Modellhierarchien",
    "section": "12.9 Verteilung der F-Statistik",
    "text": "12.9 Verteilung der F-Statistik\n\\[\nF = \\frac{MS_{\\textrm{test}}}{MSE(F)}  \\sim F(\\textrm{dfE(R)}-\\textrm{dfE(F)},\\textrm{dfE(F)})\n\\]\n\n\n\n\n\nBeispiele für die F-Verteilung mit verschiedenen Freiheitsgraden \\(df_1, df_2\\)"
  },
  {
    "objectID": "mlm_hierarchies.html#hypothesentest-mit-f-wert",
    "href": "mlm_hierarchies.html#hypothesentest-mit-f-wert",
    "title": "12  Modellhierarchien",
    "section": "12.10 Hypothesentest mit F-Wert",
    "text": "12.10 Hypothesentest mit F-Wert\n\n\n\n\n\nF-Verteilung mit \\(df_1 = 5, df_2 = 10\\) und kritischem Wert bei \\(\\alpha=0.05\\)\n\n\n\n\n2"
  },
  {
    "objectID": "mlm_hierarchies.html#teilziel",
    "href": "mlm_hierarchies.html#teilziel",
    "title": "12  Modellhierarchien",
    "section": "12.11 Teilziel",
    "text": "12.11 Teilziel\n\nDurch den Vergleich von Modellen kann die Verbesserung/Verschlechterung der Modellvorhersage statistisch Überprüft werden\nAlternativ: Brauchen ich zusätzliche Parameter oder reicht mir das einfache Modell?"
  },
  {
    "objectID": "mlm_hierarchies.html#beispiel-candy-problem",
    "href": "mlm_hierarchies.html#beispiel-candy-problem",
    "title": "12  Modellhierarchien",
    "section": "12.12 Beispiel: Candy-Problem",
    "text": "12.12 Beispiel: Candy-Problem\n\n\n\n\n\nZusammenhang zwischen der Präferenz für ein Bonbon und dem Süßgrad für verschiedene Weichheitsgrade"
  },
  {
    "objectID": "mlm_hierarchies.html#modelle-als-hierarchien-auffassen",
    "href": "mlm_hierarchies.html#modelle-als-hierarchien-auffassen",
    "title": "12  Modellhierarchien",
    "section": "12.13 Modelle als Hierarchien auffassen",
    "text": "12.13 Modelle als Hierarchien auffassen\n\n12.13.1 Full model\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i}x_{2i} + \\epsilon_i\n\\]\n\n\n12.13.2 Hierarchie\n\\[\\begin{align*}\nm_0&: y_i = \\beta_0 + \\epsilon_i \\\\\nm_1&: y_i = \\beta_0 + \\beta_1 x_{1i} + \\epsilon_i \\\\\nm_2&: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\\\\nm_3&: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i}x_{2i} + \\epsilon_i\n\\end{align*}\\]\nEs gilt: \\(m_0 \\subseteq m_1 \\subseteq m_2 \\subseteq m_3\\)"
  },
  {
    "objectID": "mlm_hierarchies.html#modelle-als-hierarchien-auffassen-in-r",
    "href": "mlm_hierarchies.html#modelle-als-hierarchien-auffassen-in-r",
    "title": "12  Modellhierarchien",
    "section": "12.14 Modelle als Hierarchien auffassen in R",
    "text": "12.14 Modelle als Hierarchien auffassen in R\nIn R:\n\nmod_0 &lt;- lm(like ~ 1, candy)\nmod_1 &lt;- lm(like ~ sweetness, candy)\nmod_2 &lt;- lm(like ~ sweetness + moisture, candy)\nmod_3 &lt;- lm(like ~ sweetness * moisture, candy)"
  },
  {
    "objectID": "mlm_hierarchies.html#vergleich-m_0-gegen-m_1",
    "href": "mlm_hierarchies.html#vergleich-m_0-gegen-m_1",
    "title": "12  Modellhierarchien",
    "section": "12.15 Vergleich \\(m_0\\) gegen \\(m_1\\)",
    "text": "12.15 Vergleich \\(m_0\\) gegen \\(m_1\\)\n\\[\\begin{align*}\nm_0: y_i &= \\beta_0 + \\epsilon_i \\\\\nm_1: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_0, mod_1)\n\n\n\n\nVergleich der Modellfits\n\n\nModel\nResDF\nDF\nSS\nF\np-val\n\n\n\n\nModel 1: like ~ 1\n77\n\n\n\n\n\n\nModel 2: like ~ sweetness\n76\n1\n9039.35\n30\n0"
  },
  {
    "objectID": "mlm_hierarchies.html#vergleich-m_1-gegen-m_2",
    "href": "mlm_hierarchies.html#vergleich-m_1-gegen-m_2",
    "title": "12  Modellhierarchien",
    "section": "12.16 Vergleich \\(m_1\\) gegen \\(m_2\\)",
    "text": "12.16 Vergleich \\(m_1\\) gegen \\(m_2\\)\n\\[\\begin{align*}\nm_1: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\epsilon_i \\\\\nm_2: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_1, mod_2)\n\n\n\n\nVergleich der Modellfits\n\n\n\n\n\n\n\n\n\n\nModel\nResDF\nDF\nSS\nF\np-val\n\n\n\n\nModel 1: like ~ sweetness\n76\n\n\n\n\n\n\nModel 2: like ~ sweetness + moisture\n75\n1\n16787.44\n206.06\n0"
  },
  {
    "objectID": "mlm_hierarchies.html#vergleich-m_2-gegen-full-model-m_3",
    "href": "mlm_hierarchies.html#vergleich-m_2-gegen-full-model-m_3",
    "title": "12  Modellhierarchien",
    "section": "12.17 Vergleich \\(m_2\\) gegen full model \\(m_3\\)",
    "text": "12.17 Vergleich \\(m_2\\) gegen full model \\(m_3\\)\n\\[\\begin{align*}\nm_2: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i  \\\\\nm_3: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_2, mod_3)\n\n\n\n\nVergleich der Modellfits\n\n\n\n\n\n\n\n\n\n\nModel\nResDF\nDF\nSS\nF\np-val\n\n\n\n\nModel 1: like ~ sweetness + moisture\n75\n\n\n\n\n\n\nModel 2: like ~ sweetness * moisture\n74\n1\n5886.88\n1951.72\n0"
  },
  {
    "objectID": "mlm_hierarchies.html#vergleich-full-model-m_3-gegen-minmales-modell-m_0",
    "href": "mlm_hierarchies.html#vergleich-full-model-m_3-gegen-minmales-modell-m_0",
    "title": "12  Modellhierarchien",
    "section": "12.18 Vergleich full model \\(m_3\\) gegen minmales Modell \\(m_0\\)",
    "text": "12.18 Vergleich full model \\(m_3\\) gegen minmales Modell \\(m_0\\)\n\\[\\begin{align*}\nm_0: y_i &= \\beta_0 + \\epsilon_i  \\\\\nm_3: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_0, mod_3)\n\n\n\n\nVergleich der Modellfits\n\n\n\n\n\n\n\n\n\n\nModel\nResDF\nDF\nSS\nF\np-val\n\n\n\n\nModel 1: like ~ 1\n77\n\n\n\n\n\n\nModel 2: like ~ sweetness * moisture\n74\n3\n31713.67\n3504.75\n0"
  },
  {
    "objectID": "mlm_hierarchies.html#in-summary-m_3-gegen-m_0",
    "href": "mlm_hierarchies.html#in-summary-m_3-gegen-m_0",
    "title": "12  Modellhierarchien",
    "section": "12.19 In summary() \\(m_3\\) gegen \\(m_0\\)",
    "text": "12.19 In summary() \\(m_3\\) gegen \\(m_0\\)\n\n\n\nCall:\nlm(formula = like ~ sweetness * moisture, data = candy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5340 -1.0283  0.0897  1.3006  3.0182 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         2.253912   0.882917   2.553   0.0127 *  \nsweetness           0.043886   0.044919   0.977   0.3318    \nmoisture           -0.009216   0.064548  -0.143   0.8869    \nsweetness:moisture  0.159277   0.003605  44.178   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.737 on 74 degrees of freedom\nMultiple R-squared:  0.993, Adjusted R-squared:  0.9927 \nF-statistic:  3505 on 3 and 74 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "mlm_hierarchies.html#eine-nominale-variable-mit-vier-stufen",
    "href": "mlm_hierarchies.html#eine-nominale-variable-mit-vier-stufen",
    "title": "12  Modellhierarchien",
    "section": "12.20 Eine nominale Variable mit vier Stufen",
    "text": "12.20 Eine nominale Variable mit vier Stufen\n\n\n\n\n\nEin Reaktionszeitexperiment mit vier Stufen A, B, C und D"
  },
  {
    "objectID": "mlm_hierarchies.html#früher---analysis-of-variance-anova-bzw.-aov",
    "href": "mlm_hierarchies.html#früher---analysis-of-variance-anova-bzw.-aov",
    "title": "12  Modellhierarchien",
    "section": "12.21 Früher - Analysis of Variance (ANOVA bzw. AOV)",
    "text": "12.21 Früher - Analysis of Variance (ANOVA bzw. AOV)\n\\[\\begin{align*}\ns_{zwischen}^2 &= \\frac{1}{K-1}\\sum_{j=1}^K N_j (\\bar{x}_{j.}-\\bar{x})^2 \\\\\ns_{innerhalb}^2 &= \\frac{1}{N-K}\\sum_{j=1}^K\\sum_{i=1}^{N_j}(x_{ji}-\\bar{x}_{j.})^2 = \\frac{1}{N-K}\\sum_{j=1}^K(N_j-1)s_j^2 \\\\\nF &= \\frac{\\hat{\\sigma}_{zwischen}^2} {\\hat{\\sigma}_{innerhalb}^2} \\sim F(K-1,N-K)\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_hierarchies.html#anova-in-r",
    "href": "mlm_hierarchies.html#anova-in-r",
    "title": "12  Modellhierarchien",
    "section": "12.22 ANOVA in R",
    "text": "12.22 ANOVA in R\n\nmod_aov &lt;- aov(rt ~ group, rt_tbl)\nsummary(mod_aov)\n\n\n\n\nAusgabe mit aov()\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ngroup\n3\n988935.1\n329645\n157.3\n0\n\n\nResiduals\n76\n159221.0\n2095"
  },
  {
    "objectID": "mlm_hierarchies.html#ansatz-mittels-modellhierarchien",
    "href": "mlm_hierarchies.html#ansatz-mittels-modellhierarchien",
    "title": "12  Modellhierarchien",
    "section": "12.23 Ansatz mittels Modellhierarchien",
    "text": "12.23 Ansatz mittels Modellhierarchien\n\n12.23.1 Full model\n\\[\ny_i = \\beta_0 + \\beta_{\\Delta_{B-A}} x_1 + \\beta_{\\Delta_{C-A}} x_2 + \\beta_{\\Delta_{D-A}} x_3 + \\epsilon_i\n\\]\n\n\n12.23.2 Reduced model\n\\[\ny_i = \\beta_0 + \\epsilon_i\n\\]\nWenn das reduced model die Daten gleich gut fittet wie das full model \\(\\Rightarrow\\) Information über das Treatment verbessert meine Vorhersage von \\(y_i\\) nicht."
  },
  {
    "objectID": "mlm_hierarchies.html#model-fit---full-model",
    "href": "mlm_hierarchies.html#model-fit---full-model",
    "title": "12  Modellhierarchien",
    "section": "12.24 Model fit - Full model",
    "text": "12.24 Model fit - Full model\n\nmod &lt;- lm(rt ~ group, rt_tbl)\n\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n509.526\n10.235\n49.784\n&lt;0.001\n\n\ngroupB\n90.150\n14.474\n6.228\n&lt;0.001\n\n\ngroupC\n197.414\n14.474\n13.639\n&lt;0.001\n\n\ngroupD\n295.561\n14.474\n20.420\n&lt;0.001"
  },
  {
    "objectID": "mlm_hierarchies.html#anova-mit-nur-einem-modell",
    "href": "mlm_hierarchies.html#anova-mit-nur-einem-modell",
    "title": "12  Modellhierarchien",
    "section": "12.25 anova() mit nur einem Modell",
    "text": "12.25 anova() mit nur einem Modell\n\nanova(mod)\n\n\n\n\nÄquivalent zum Vergleich full gegen reduced model\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ngroup\n3\n988935.1\n329645\n157.3\n0\n\n\nResiduals\n76\n159221.0\n2095"
  },
  {
    "objectID": "mlm_hierarchies.html#zum-nacharbeiten",
    "href": "mlm_hierarchies.html#zum-nacharbeiten",
    "title": "12  Modellhierarchien",
    "section": "12.26 Zum Nacharbeiten",
    "text": "12.26 Zum Nacharbeiten\nChristensen (2018, p.57–64) \n\n\n\n\nChristensen, Ronald. 2018. Analysis of variance, design, and regression: Linear modeling for unbalanced data. CRC Press.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York."
  },
  {
    "objectID": "mlm_hierarchies.html#footnotes",
    "href": "mlm_hierarchies.html#footnotes",
    "title": "12  Modellhierarchien",
    "section": "",
    "text": "Kutner u. a. (2005), p.72↩︎\nIn R: df(), pf(), qf(), rf()↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatur",
    "section": "",
    "text": "Altman, Douglas G, and J Martin Bland. 1995. “Statistics Notes:\nAbsence of Evidence Is Not Evidence of Absence.” Bmj 311\n(7003): 485.\n\n\nAltman, Naomi, and Martin Krzywinski. 2015a. “Points of\nSignificance: Multiple Linear Regression.” Nature\nMethods 12 (12): 1103–4.\n\n\n———. 2015b. “Points of Significance: Simple Linear\nRegression.” Nature Methods 12 (11).\n\n\n———. 2016a. “Points of Significance: Analyzing Outliers:\nInfluential or Nuisance.” Nature Methods 13 (4): 281–82.\n\n\n———. 2016b. “Points of Significance: Regression\nDiagnostics.” Nature Methods 13 (5): 385–86.\n\n\nChristensen, Ronald. 2018. Analysis of Variance, Design, and\nRegression: Linear Modeling for Unbalanced Data. CRC Press.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. 2nd ed. Routledge.\n\n\nCumming, Geoff. 2013. Understanding the New Statistics: Effect\nSizes, Confidence Intervals, and Meta-Analysis. Routledge.\n\n\nDebanne, Thierry, and Guillaume Laffaye. 2011. “Predicting the\nThrowing Velocity of the Ball in Handball with Anthropometric Variables\nand Isotonic Tests.” Journal of Sports Sciences 29 (7):\n705–13.\n\n\nFox, John. 2011. An r Companion to Applied Regression. 2nd ed.\nSAGE Publication Inc., Thousand Oaks.\n\n\nGross, Benedict, Joe Harris, and Emily Riehl. 2019. Fat Chance:\nProbability from 0 to 1. Cambridge University Press.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li.\n2005. Applied Linear Statistical Models. 5th ed. McGraw-Hill\nIrwin New York.\n\n\nMcElreath, Richard. 2016. Statistical Rethinking, a Bayesian Course\nwith Examples in r and Stan. 1st ed. Boca Raton: CRC Press.\n\n\nSpiegelhalter, David. 2019. The Art of Statistics: Learning from\nData. Penguin UK.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA\nStatement on p-Values: Context, Process, and Purpose.” Taylor\n& Francis.\n\n\nWild, Christopher J, and Georg AF Seber. 2000. Chance Encounters: A\nFirst Course in Data Analysis and Inference. Wiley Press.\n\n\nWilkinson, G. N., and C. E. Rogers. 1973. “Symbolic Description of\nFactorial Models for Analysis of Variance.” Applied\nStatistics 22 (3): 392–99.\n\n\nYoung, Alwyn. 2019. “Channeling Fisher: Randomization Tests and\nthe Statistical Insignificance of Seemingly Significant Experimental\nResults.” The Quarterly Journal of Economics 134 (2):\n557–98."
  },
  {
    "objectID": "slm_model_fit.html#homoskedastizität",
    "href": "slm_model_fit.html#homoskedastizität",
    "title": "7  Modellfit",
    "section": "7.2 Homoskedastizität",
    "text": "7.2 Homoskedastizität\nWenn die Varianz der Residuen \\(\\epsilon_i\\) in einem Regressionsmodell unabhängig von der Größe der Vorhersagevariable \\(X_i\\) gleich ist, wird die als Homoskedastizität bezeichnet. Die Streuung der Residuen ist dann für alle Werte \\(X_i\\) gleich. Wenn dies nicht der Fall ist, wird von Heteroskedastizität gesprochen. :::\nEine weitere Möglichkeit die Residuen zu überprüfen ist die Anfertigung von sogenannten qq-Plots. Dies ermöglichen etwas strukturierter die Verteilung der Residuen zu überprüfen."
  },
  {
    "objectID": "slm_model_fit.html#standardisierte-residuen",
    "href": "slm_model_fit.html#standardisierte-residuen",
    "title": "7  Modellfit",
    "section": "7.4 Standardisierte Residuen",
    "text": "7.4 Standardisierte Residuen\nEine Möglichkeit so einen Wert zu untersuchen, ist abzuschätzen wie ungewöhnlich der zu dem Residuen \\(e_i\\) gehörende \\(y_i\\)-Wert ist. Ein Problem der einfachen Residuen \\(e_i\\) ist, dass diese laut der Modellannahmen die gleiche Varianz \\(\\sigma^2\\) haben sollten. Allerdings, auf Grund der Art, wie die \\(e_i\\) berechnet werden, folgt die Randbedingung, dass die Summe der \\(e_i\\) gleich Null ist, \\(\\sum_{i=1}^n e_i = 0\\). Dies führt dazu, dass die einfachen Residuen nicht unanbhängig voneinander sind und nicht immer Homoskedastizität besitzen. Daher gibt es eine weitere Art Residuen anhand des Modell zu berechnen, die nicht unter diesen Beschränkungen leiden. Dies sind die standardisierten Residuen \\(e_{Si}\\). Dazu müssen wir uns zunächst mit Hebelwerte \\(h_i\\) beschäftigen.\n\n7.4.1 Hebelwerte\nWenn ein Modell an die Daten gefittet wird, dann haben nicht alle Werte den gleichen Einfluss auf die Modellparameter. Manche Werte üben einen stärkeren Einfluss auf das Modell aus als andere Werte. In Abbildung 7.12 ist ein Beispiel abgebildet für einen Datensatz bei dem ein einzelner Punkt einen übermäßig großen Einfluss auf das Modell ausübt.\n\n\n\n\n\nAbbildung 7.12: Beispiel für einen Datenpunkt mit einem großen Einfluss auf das Modell. Die resultierenden Regressionsgeraden sind mit dem Punkt (rot) und ohne den Punkt (grün) abgetragen.\n\n\n\n\nDer einzelen Punkt rechts oben in Abbildung 7.12 hat einen großen Einfluss auf die resultierende Regressionsgerade wie in der Abbildung zu sehen ist. Der Einfluss ist zum Teil durch den großen Abstand des \\(x_i\\)-Wertes vom Mittelwert der \\(x_i\\)-Werte \\(\\bar{x}\\) bestimmt. Der Einfluss jedes einzelnen \\(x\\)-Wertes wird mittels der sogenannten Hebelwerte \\(h_i\\) bestimmt. Die genaue Berechnung der Hebelwerte \\(h_i\\) ist für das weitere Verständnis allerdings nicht wichtig, sondern mehr das Verständnis des Konzepts. Die Hebelwerte \\(h_i\\) können Werte in \\(h_i \\in \\[1/n,1\\]\\) annehmen. In R können die Hebelwerte mit der Funktion hatvalues() berechnet werden.\nTragen wir in die Grafik die Hebelwerte in die Grafik Abbildung 7.12 ein (siehe Abbildung 7.13), dann ist zu sehen, dass der abgesetzte Wert auch den größten Hebelwert hat.\n\n\n\n\n\nAbbildung 7.13: Beispiel für einen Datenpunkt mit einem großen Einfluss auf das Modell. Die Werte geben die jeweiligen Hebelwerte \\(h_i\\) der Datenpunkte wieder.\n\n\n\n\nEine Daumenregel für die Hebelwerte ist der Schwellenwert von \\((2p+2)/n\\), wobei \\(p\\) die Anzahl der unabhängigen Variablen ist. Für den Beispieldatensatz in Abbildung 7.13 würde sich daher ein Wert von \\((2\\cdot 1+2)/30 = 0.13\\) ergeben. Entsprechend wäre der abgesetzte Wert mit einem Hebelwert von \\(h_i = 0.54\\) als problematisch einzustufen.\nNach diesem kurzen Exkurs zu den Hebelwerten \\(h_i\\), schauen wir uns für unsere weitere Betrachtung der Residuen zunächst den Zusammenhang zwischen der Varianz der Residuen in der Population \\(\\sigma^2\\) und der Varianz der geschätzten Residuen \\(\\sigma^2(\\hat{\\epsilon}_i) = \\sigma^2(e_i)\\) an. Es gilt:\n\\[\\begin{equation}\n\\sigma^2(e_i) = \\sigma^2 (1 - h_i)\n\\label{eq-slm-model-vare_i}\n\\end{equation}\\]\nD.h. wenn ein Datenpunkt \\(x_i\\) einen kleineren Einfluss auf das Modell ausübt und dementsprechend einen kleinen Hebelwert \\(h_i\\), dann wird die Varianz für diesen Wert nahezu korrekt eingeschätzt. Hat der Wert \\(x_i\\) allerdings, einen großen Hebelwert \\(h_i\\), führt die dazu, dass die Varianz für diesen Wert stärker unterschützt wird. Dieser Zusammenhang kann dazu benutzt werden standardisierte Residuen zu erstellen.\n\\[\\begin{equation}\ne_{Si} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_i}}\n\\label{eq-slm-model-stresid}\n\\end{equation}\\]\nDie standardisierten Residuen \\(e_{Si}\\) haben dazu die Eigenschaft, dass sie eine Varianz und damit Standardabweichung von \\(\\sigma^2(e_{Si}) = 1\\) haben, also Standardnormalverteilt \\(\\Phi(z)\\) sein sollten. Dadurch können Abweichungen von den Modellannahmen leichter Identifiziert werden, da die Skala normiert ist. In R kann die standardiserten Residuen \\(e_{Si}\\) mittels der Funktion rstandard() berechnet werden. Eine Standardgrafik zum inspizieren der standardisierten Residuen ist wiederum eine Abbildung der \\(e_{Si}\\) gegen die \\(\\hat{y}_i\\).\n\n\n\n\n\nAbbildung 7.14: Grafik der standardisierten Residuen \\(e_{Si}\\) gegen die Vorhersagewerte \\(\\hat{y}_i\\) für das ADL-Modell.\n\n\n\n\nDie Abbildung 7.14 sieht relativ ähnlich zu Abbildung 7.3 aus. Durch die Änderung der Skala ist jetzt aber leichter abschätzbar ob die Verteilung der erwarteten Normalverteilung folgt. D.h. etwa \\(\\frac{2}{3}\\) der Werte sollten zwischen \\(-1\\) und \\(1\\) liegen und etwa \\(95\\%\\) zwischen \\(-2\\) und \\(2\\). Bis auf den einen Punkt oben rechts, sieht alles soweit unauffällig aus."
  },
  {
    "objectID": "slm_model_fit.html#zum-nachlesen",
    "href": "slm_model_fit.html#zum-nachlesen",
    "title": "7  Modellfit",
    "section": "7.5 Zum Nachlesen",
    "text": "7.5 Zum Nachlesen\nKutner u. a. (2005, p.100–114) Altman und Krzywinski (2016b) Fox (2011, p.285–296)"
  },
  {
    "objectID": "slm_model_fit.html#studentized-residuals",
    "href": "slm_model_fit.html#studentized-residuals",
    "title": "7  Modellfit",
    "section": "7.2 Studentized Residuals",
    "text": "7.2 Studentized Residuals\nDie letzte Art von Residuen sind die sogenannten Studentized Residuals \\(e_{Ti}\\), die mittels der folgenden Formel berechnet werden.\n\\[\\begin{equation}\ne_{Ti} = \\frac{e_i}{\\hat{\\sigma}_{(-i)}\\sqrt{1-h_i}}\n\\label{eq-slm-model-rstudent}\n\\end{equation}\\]\nDie Formel \\(\\eqref{eq-slm-model-rstudent}\\) ist sehr ähnlich zu derer für die standardisierten Residuen, der einzige Unterschied ist der Term \\(\\hat{\\sigma}_{(-i)}\\). Dieser bezeichnet die Residualvarianz wenn dass Modell ohne den Datenpunkt \\(i\\) gefittet wird. D.h. wie stark verändert sich die Schätzung der Varianz wenn ein Datenpunkt weggelassen wird. Normalerweise sollte eine einzelner Punkt keinen übermäßigen Einfluss auf die geschätzte Varianz haben, daher können die Studentized Residuals dazu verwendet werden problematische Datenpunkte zu identifizieren. Wenn die tatsächlichen Residuen einer Normalverteilung folgen, dann kann gezeigt werden, dass die Studentized Residuals einer \\(t\\)-Verteilung mit \\(N-k-2\\) Freiheitsgeraden folgen. Daher könnte sogar ein formaler statistischer Test durchgeführt werden. In R können die Studentized Residuals \\(e_{Ti}\\) mittels der Funktion rstudent() berechnet werden und werden entsprechend den anderen Residuen in dem üblichen Graphen gegen die vorhergesagten Werte \\(\\hat{y}_i\\) abgetragen.\n\n\n\n\n\nAbbildung 7.15: Graph der Studentized Residuals \\(S_{Ti}\\) gegen die vorhergesagten Werte \\(\\hat{y}_i\\) vor das adl-Modell"
  },
  {
    "objectID": "slm_model_fit.html#übersicht-über-die-residuenarten",
    "href": "slm_model_fit.html#übersicht-über-die-residuenarten",
    "title": "7  Modellfit",
    "section": "7.3 Übersicht über die Residuenarten",
    "text": "7.3 Übersicht über die Residuenarten\nIn Tabelle 7.3 sind noch einmal die drei Arten von Residuen aufgelistet.\n\n\nTabelle 7.3: Übersicht über verschiedene Arten von Residuen\n\n\n\n\n\n\n\nTyp\nBerechnung\nZiel\n\n\n\n\nEinfache Residuen\n\\(e_i = y_i - \\hat{y}_i\\)\nVerteilungsannahme\n\n\nStandardisierte Residuen\n\\(e_{Si} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_i}}\\)\nVerteilungsannahme\n\n\nStudentized Residuen\n\\(e_{Ti} = \\frac{e_i}{\\hat{\\sigma}_{(-i)}\\sqrt{1-h_i}}\\)\nEinfluss auf Modell"
  },
  {
    "objectID": "slm_model_fit.html#einflussmetriken",
    "href": "slm_model_fit.html#einflussmetriken",
    "title": "7  Modellfit",
    "section": "7.2 Einflussmetriken",
    "text": "7.2 Einflussmetriken\nIm folgenden schauen wir uns verschiedene Einflussmetriken auf das Modell an. Die Idee ist dabei die gleiche wie schon bei den Studentized Residuals. Der Einfluss der Datenpunkte auf den Modellfit wird interpretiert indem ein Modell mit und ein Modell ohne den jeweiligen Datenpunkt gefittet wird. Der Einfluss auf verschiedene Modellparameter wird dann bestimmt und dementsprechend als möglicherweise bedenklich eingestuft. In den Meisten dieser Ansatz kommen die Hebelwerte \\(h_i\\) die wir bereits kennegelernt haben.\n\n7.2.1 DFFITS\nDas erst Maß, daß wir uns anschauen ist DFFITS . Dabei wird für jeden Datenpunkt getrennt ein Wert berechnet. Formal:\n\\[\\begin{equation}\n(DFFITS)_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{\\hat{\\sigma}\\sqrt{h_i}}\n\\label{eq-slm-model-dffits}\n\\end{equation}\\]\nIm Zähler kommen von Formel\\(\\eqref{eq-slm-model-dffits}\\) kommen zweimal die vorhergesagte \\(y\\)-Werte vor. \\(\\hat{y}_i\\) ist dabei der ganz normale Vorhersagewert der uns mittlerweile schon mehrfach begegnet ist. Der zweite Wert \\(\\hat{y}_{i(i)}\\) bezeichnet den vorhergesagten Wert aus dem Modell aus dem der Wert \\(y_i\\) weggelassen wurde. D.h, dass Modell ist mit einem Wert weniger gefittet worden. Daher misst die Differenz \\(\\hat{y}_i - \\hat{y}_{i(i)}\\) den Unterschied in den Vorhersagewerte zwischen den zwei Modellen bei denen einmal der Wert \\(y_i\\) zum fitten verwendet wurde und einmal wenn \\(y_i\\) weggelassen wurde. Umso größer der Unterschied zwischen diesen beiden Werte umso größer ist der Einfluss des Wertes \\(y_i\\) auf den Modellfit. Im Nenner von Formel\\(\\eqref{eq-slm-model-dffits}\\) wird wieder ein ähnlicher Normierungswert wie bei den Studentizied Residuals angewendet. Insgesamt, wird mittels DFFITS daher für jeden Datenpunkt ein Wert ermittelt und umso größer dieser Wert ist umso größer ist der Einfluss des jeweiligen Datenpunktes auf den Modellfit.\nIm idealen Fall sollte alle Datenpunkt ungefähr den gleichen Einfluss haben und einzelne Datenpunkte die einen übermäßig großen Einfluss auf das Modell haben sollten noch einmal genauer inspiziert werden.\n\n\n\n\n\n\nTipp\n\n\n\nAls Daumenregel, kann für kleine bis mittlere Datensätze ein DFFITS von \\(\\approx 1\\) auf Probleme hindeuten, während bei großen Datensätzen \\(\\approx 2\\sqrt{k/N}\\) als Orientierungshilfe verwendet werden kann (k := Anzahl der Prediktoren, N := Stichprobengröße).\n\n\n\n\n\n\n\n\nWarnung\n\n\n\nWenn ein Wert außerhalb der Daumenregel liegt, heißt das nicht, dass er automatisch ausgeschlossen werden muss/soll, sondern lediglich inspiziert werden sollte und das Modell mit und ohne diesen Wert interpretiert werden sollte.\n\n\nIn R können die DFFITS werden mittels der dffits()-Funktion berechnet werden. Als Parameter erwartet dffits() das gefittete lm()-Objekt. Ähnlich wie bei den Residuen, werden die DFFITS-Werte gegen die vorhergesagten \\(y_i\\)-Werte graphisch abgetragen um die Wert zu inspizieren und Probleme in der Modellspezifikation zu identifizieren.\n\n\n\n\n\nAbbildung 7.16: Graph der DFFITS-Werte gegen \\(\\hat{y}_i\\) für das adl-Modell.\n\n\n\n\nIn Abbildung 7.16 sind die DFFITS-Werte gegen die vorhergesagten Werte \\(\\hat{y}_i\\) abgetragen und zusätzlich die Daumenregel \\(\\pm1\\) eingezeichnet. Hier ist ein Wert nur gerade so außerhalb des vorgeschlagenen Bereichs. Hier könnte daher sich dieser Datenpunkt noch einmal genauer angeschaut werden, ob bei Ausschluß des Wertes es zu einer qualitativ anderen Interpretation der Daten kommt oder ob bespielsweise Übertragungsfehler für diesen Wert vorliegen oder sonstige Gründe.\n\n\n7.2.2 Cook-Abstand\nWährend DFFITS den Einfluss des Datenpunktes \\(i\\) auf den jeweiligen Datenpunkt abschätzt, wird bei dem sogenanten Cook-Abstand der Einfluss des \\(i\\)-ten Datenpunktes auf alle \\(n\\) vorhergesagten Werte \\(\\hat{y}_i\\). Formal:\n\\[\\begin{equation}\nD_i = \\frac{\\sum_{j=1}^N(\\hat{y_j} - \\hat{y}_{j(i)})}{k\\hat{\\sigma}^2}\n\\label{eq-slm-model-cook}\n\\end{equation}\\]\nHier bedeutet die Syntax \\(\\hat{y}_{j(i)}\\) der vorhergesagte Wert für den Datenpunkt \\(j\\) wenn der \\(i\\)-te Datenpunkt ausgelassen wird. In R können die Cook-Abstände mit Hilfe der Funktion cooks.distance() berechnet werden.\n\n\n\n\n\n\nTipp\n\n\n\nEine Daumenregel um einen möglichen Ausreißer zu identifzieren kann über \\(D_i &gt; 1\\) abgeschätzt werden.\n\n\nIn Abbildung 7.17 ist wiederum der übliche Graph gegen die vorhergesagten Werte \\(\\hat{y}_i\\) zu sehen. Anhand der abgebildeten Wert ist keiner der Datenpunkte als problematisch zu identifizieren.\n\n\n\n\n\nAbbildung 7.17: Cook’s \\(D_i\\) gegen \\(\\hat{y}_i\\) für das adl-Modell.\n\n\n\n\n\n\n7.2.3 DFBETAS\nAls letztes Maß schauen wir uns noch DFBETAS an. DFBETAS berechnet ein Maß für die Veränderung der \\(\\beta\\)-Koeffizienten auf Grund der einzelnen Datenpunkte \\(i\\). D.h. es wird jetzt nicht nur ein Wert für jeden Wert berechnet, sondern ein Wert für den jeden Datenpunkt und jeden \\(\\beta\\)-Koeffizienten. In unseren Fall mit einem y-Achsenabschnitt \\(\\beta_0\\) und einem Steigungskoeffizienten \\(\\beta_1\\) werden entsprechend \\(2 \\times x\\) Werte berechnet. Formal:\n\\[\\begin{equation}\n(DFBETAS)_{k(i)} = \\frac{\\hat{\\beta}_k - \\hat{\\beta}_{k(i)}}{\\sqrt{\\hat{\\sigma}^2c_{kk}}}\n\\label{eq-slm-model-dfbetas}\n\\end{equation}\\]\nWie aus Formel \\(\\eqref{eq-slm-model-dfbetas}\\) ersichtlich wird, wird die Veränderungen der Koeffizienten \\(\\beta_i\\) bei weglassen des \\(i\\)-ten Datenpunktes abgeschätzt. Den Wert \\(c_{kk}\\) lassen wir unberücksichtigt, da er wiederum nur einen Normierungsfaktor darstellt.\n\n\n\n\n\n\nTipp\n\n\n\nAls Daumenregel gilt für kleine bis mittlere Datensätze \\(\\approx 1\\), bzw. für große Datensätze \\(\\approx 2/\\sqrt{N}\\)\n\n\nWiederum gibt es eine spezielle Funktion in R um die DFBETAS zu berechnen dfbeta(). Dabei ist jedoch zu beachten das eine Matrize mit \\(k\\)-Spalten von dfbeta() zurück gegeben wird. Jede Spalte gibt den Wert für den jeweiligen \\(\\beta\\)-Koeffizienten an.\n\n\n\n\n\nAbbildung 7.18: DFBETA-Werte für \\(\\beta_0\\) und \\(\\beta_1\\) gegen \\(\\hat{y}_i\\)\n\n\n\n\nIn Abbildung 7.18 sind die DFBETAS für die beiden Koeffizienten \\(\\hat{beta}_0\\) und \\(\\hat{beta}_1\\) abgetragen. Hier ist zu sehen, dass die Wert für den Steigunsgkoeffizienten \\(\\beta_1\\) alle als unproblematisch anzusehen sind, während in Bezug auf \\(\\beta_0\\) ein paar wenige Fälle eine weiter Inspektion nach sich ziehen könnten. Allerdings sollte berücksichtigt werden, dass der y-Achsenabschnitt sehr stark durch die Verteilung der Datenpunkte in Bezug auf die \\(x\\)-Werte beeinflusst ist, da der Mittelwert der \\(x\\)-Werte bei \\(\\bar{x} = 41.2\\) liegt.\n\n\n7.2.4 Zusammenfassung\nIn Tabelle 7.4 sind noch einmal die verschiedenen Methoden tabellarisch dargestellt.\n\n\nTabelle 7.4: Übersicht über die verschiedene Einflussmaße zur Bewertung der Modellgüte\n\n\nTyp\nVeränderung\nDaumenregel\n\n\n\n\n\\((DFFITS)_i\\)\nVorhersagewert i\n\\(2\\sqrt{k/N}\\)\n\n\nCook\nDurchschnittliche Vorhersagewerte\n\\(&gt;1\\)\n\n\n\\((DFBETAS)_{k(i)}\\)\nKoeffizient i\n\\(2\\sqrt{N}\\)\n\n\n\\(e_{Ti}\\)\nResiduum i\nt-Verteilung(n-k-2)"
  },
  {
    "objectID": "slm_model_fit.html#diagnoseplots-in-r",
    "href": "slm_model_fit.html#diagnoseplots-in-r",
    "title": "7  Modellfit",
    "section": "7.3 Diagnoseplots in R",
    "text": "7.3 Diagnoseplots in R\nDa die Diagnose eines gefitten Modell in jedem Fall durchgeführt werden sollte, gibt es mit plot(mod) einen short-cut um eine Reihe von Diagnoseplots einfach erstellen zu können.\n\nplot(mod)\n\n\n\n\n\n\n\n7.3.1 Zum Nacharbeiten\nAltman und Krzywinski (2016a), Fox (2011, p.294–302), Young (2019)\n\n\n\n\nAltman, Naomi, und Martin Krzywinski. 2016a. „Points of significance: Analyzing outliers: influential or nuisance“. Nature Methods 13 (4): 281–82.\n\n\n———. 2016b. „Points of significance: regression diagnostics“. Nature Methods 13 (5): 385–86.\n\n\nFox, John. 2011. An R companion to applied regression. 2. Aufl. SAGE Publication Inc., Thousand Oaks.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York.\n\n\nYoung, Alwyn. 2019. „Channeling fisher: Randomization tests and the statistical insignificance of seemingly significant experimental results“. The Quarterly Journal of Economics 134 (2): 557–98."
  }
]