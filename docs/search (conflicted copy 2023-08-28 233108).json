[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skriptum - Fortgeschrittene Statistik",
    "section": "",
    "text": "Vorwort\nDies ist das Skriptum für den Master-Statistikkurse Fortgeschrittene Statistik und ist die Vorlage für die Kurse LTC4 und SBG4. Es werden in den Kursen nicht alle Themen des Skriptums behandelt. Das Skriptum befindet sich derzeit noch in einem frühen Stadium, so dass die Inhalte noch nicht vollständig ausgearbeitet sind."
  },
  {
    "objectID": "r_intro.html#konzeption-und-programmierparadigmen",
    "href": "r_intro.html#konzeption-und-programmierparadigmen",
    "title": "1  Eine Übersicht über R",
    "section": "1.1 Konzeption und Programmierparadigmen",
    "text": "1.1 Konzeption und Programmierparadigmen\nBei der Programmiersprache R handelt es sich um eine interpretierte Programmiersprache. Inpretiert bedeutet, dass R Programmierbefehle direkt ausführt, intepretiert. Dies steht im Gegensatz zu kompilierten Programmiersprachen (z.B. C++) bei denen gesamte Programm zunächst von einem Compiler in Maschinenbefehle übersetzt wird. Wenn eine Programmiersprache interpretiert wird, hat dies den Vorteil, dass die Arbeit besser interaktiv durchgeführt werden kann. D.h. als Anwenderin können, je nach Bedarf, einzelne Befehle oder größere zusammenhängende Befehle, an R übergeben werden und sie werden direkt ausgeführt und R liefert das Ergebnis zurück. Insbesondere bei der Datenverarbeitung vereinfacht diese Vorgehensweise die Arbeit ungemein. So können Daten per trial-and-error schnell angepasst, transformiert oder graphisch bzw. deskriptiv dargestellt werden. Der Hauptnachteil von interpretierten Programmiersprachen ist, dass durch die Abarbeitung der einzelnen Befehle bestimmte Ausführungsoptimierungen durch R nicht angewendet werden können. Dies hat zur Folge, dass die Ausführungszeit, d.h. die Zeit die ein Programm benötigt um seine Aufgaben zu erledigen im Vergleich zu kompilierten Programmiersprachen zum Teil deutlich länger sein kann. In R kann dieser Nachteil jedoch durch die Einbindung von bestimmten Paketen (was ein Paket ist werden wir gleich sehen), die in anderen Programmiersprachen erstellt wurden und entsprechend optimiert wurden, in vielen Fällen umgegangen werden."
  },
  {
    "objectID": "r_intro.html#ressourcen-und-hilfestellungen-zu-r-finden",
    "href": "r_intro.html#ressourcen-und-hilfestellungen-zu-r-finden",
    "title": "1  Eine Übersicht über R",
    "section": "1.2 Ressourcen und Hilfestellungen zu R finden",
    "text": "1.2 Ressourcen und Hilfestellungen zu R finden\nWar es in den Anfangszeiten von R noch teilweise schwierig Hilfe bei auftretenden Problemen zu bekommen, hat sich dies in den letzten 10-15 Jahren glücklicherweise dramatisch geändert. Durch die etwas unglückliche Namensgebung R gab es beispielweise zunächst Probleme bei der Suche nach Problemlösungen, da die Suchmaschinen mit dem Buchstaben R wenig anfangen konnten. Im Internet finden sich jetzt allerdings zahllose überaus aktive Communities rund um R, mit ausführlichen Blogs, Podcasts, Youtube-Sammlungen und Programmierhilfen für alle möglichen Fragen und Probleme. Zwei sehr gute Quellen sind Stack Overflow bei denen oft schon mittels eines Google ein Treffer gefunden wird oder ChatGPT. Zudem ist die Menge an Büchern um und über R in den letzten Jahren geradezu explodiert. Mit eigenen Serien zur Datenanalyse mit R (Springer Use R!, CRC The R-Series) und zahllosen weiteren wissenschaftlichen Büchern mit mindestens Code-Beispielen in R bis hin zu frei verfügbaren Sammlungen hochqualitativer, wissenschaftlicher Bücher zu R (bookdown.org) lassen sich heutzutage relativ niedrigschwellig sehr gute R-skills aufbauen."
  },
  {
    "objectID": "r_intro.html#r-community-und-pakete",
    "href": "r_intro.html#r-community-und-pakete",
    "title": "1  Eine Übersicht über R",
    "section": "1.3 R Community und Pakete",
    "text": "1.3 R Community und Pakete\nEine treibende Kraft bei der Weiterentwicklung und Weiterverbreitung von R ist die riesige Gemeinschaft von Anwenderinnen und Programmiererinnen. Dadurch das R im Kern eine vollständige Programmiersprache ist, kann die Funktionalität von R ständig erweitert und individuellen Bedürfnissen angepasst werden. Neue Funktionalität wird in R im Rahmen von sogenannten Paketen (alternativ Bibliotheken) gebündelt. Durch diese Pakete können neue Befehle durch neudefinierte Funktionen in R zugänglich gemacht werden.\nR Pakete werden über das Comprehensive R Archive Network (kurz CRAN) verteilt. CRAN ist ein internationales Netzwerk von Webservern auf denen R Pakete gespeichert werden und das das einfache Herunterladen aus R heraus ermöglicht. Die auf CRAN gespeicherte Pakete folgen alle einer streng definierten Struktur und durchlaufen eine Qualitätskontrolle. Die Weiterentwicklung bzw. weitere Anpassung wird durch sogenannte Maintainer (Entwickler) sichergestellt. Während die Anzahl an Zusatzpaketen am Anfang von R noch relativ übersichtlich war, liegt die derzeitige Anzahl an R Paketen auf CRAN bei 19900 (Stand 08.2023) mit Tendenz steigend. Da sich Datenanalysen über verschiedenen Disziplinen und Anwendungsfälle im Grund genommen immer wieder ähneln besteht daher eine hohe Wahrscheinlichkeit das auch für ungewöhnliche Anwendungsfälle bereits bestehende Pakete und Zusatzfunktionen in R zur Verfügung stehen. Daher besteht für den Großteil von Anwenderinnen oftmals gar nicht mehr die Notwendigkeit kompliziertere Programmieraufgaben selbst durchzuführen. Sondern, durch die Suche nach einem geeigneten Paket, können auftauchende Problem schnell gelöst werden. Dies führt ebenfalls dazu, dass die Einstiegshürde für den Umgang mit R sehr niedrig ist."
  },
  {
    "objectID": "r_intro.html#r-und-r-studio",
    "href": "r_intro.html#r-und-r-studio",
    "title": "1  Eine Übersicht über R",
    "section": "1.4 R und R-Studio",
    "text": "1.4 R und R-Studio\nHeutzutage wird R praktisch nur noch über die Entwicklungsumgebung R-Studio verwendet. R-Studio stellt dabei eine ganze Reihe von Werkzeugen zur Verfügung die den Umgang mit R deutlich vereinfachen. R tritt dann eigentlich nur noch im Hintergrund bei der Ausführung von Befehlen in Aktion. Vor allem der Editor um Skripte zu erstellen und die einfache Möglichkeit Projekte anzulegen vereinfachen die Arbeit mit R am Anfang doch deutlich.\nIm Rahmen des vorliegenden Kurses werden wir entsprechend auch auschließlich mit R-Studio arbeiten und Vanilla-R ignorieren, bzw. wenn wir R schreiben eigentlich immer R-Studio meinen. Für euch ist es allerdings wichtig zu wissen, dass R installiert sein muss damit R-Studio auch wirklich funktionsfähig ist."
  },
  {
    "objectID": "r_intro.html#weiterführendes",
    "href": "r_intro.html#weiterführendes",
    "title": "1  Eine Übersicht über R",
    "section": "1.5 Weiterführendes",
    "text": "1.5 Weiterführendes\nDie im weiteren Verlauf des Skripts kommenden Beispiele und Erklärungen dienen nur um einen allerersten Überblick über die Arbeit mit R zu gewinnen. D.h. die Inhalte beschränken sich auf die Konzepte, die für den Umgang mit R im späteren Verlauf dieses Statistikkurses benötigt werden. Wie schnell ersichtlich werden wird, besteht eine der Herausforderungen den Umgang mit R zu erlernen, darin die notwendigen Befehle und Funktionen zu kennen. Daher werden sich die ersten Schritte mit R ähnlich dem Erlernen einer neuen Sprache anfühlen. Das Schöne ist jedoch, R eignet sich hervorragend als erste Programmiersprache. In der heutigen Zeit in jedem Fall eine gute Investition in die eigene Zukunft.\nWer im weiteren Verlauf merkt, dass es ihr Spaß macht mit R zu arbeiten und gerne noch weitere Fertigkeiten für den Umgang mit R erlernen möchte, der findet mittlerweile zahlreiche sehr gute Quellen im Internet. Wer lieber was in der Hand haben möchte, für den Einstieg sind die beiden klassischen Büchern Chambers (2008) und Dalgaard (2008) empfehlenswert von denen es auch immer günstige gebrauchte Versionen gibt. Etwas modernere Einführungen sind in Peng (2016) und Wickham, Çetinkaya-Rundel, und Grolemund (2023), zu finden, wobei es beiden Bücher auch als freie online-Versionen gibt (peng, wickham). Überhaupt eignet sich die Seite bookdown.org als gute Anlaufstelle für verregnete Tage am Schreibtisch.\n\n\n\n\nChambers, John M. 2008. Software for data analysis: programming with R. Bd. 2. 1. Springer.\n\n\nDalgaard, Peter. 2008. Introductory statistics with R. Springer.\n\n\nPeng, Roger D. 2016. R programming for data science. Leanpub Victoria, BC, Canada.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, und Garrett Grolemund. 2023. R for data science. \" O’Reilly Media, Inc.\"."
  },
  {
    "objectID": "r_kickoff.html#variablen-in-r",
    "href": "r_kickoff.html#variablen-in-r",
    "title": "2  R Kickoff",
    "section": "2.1 Variablen in R",
    "text": "2.1 Variablen in R\nNehmen wir, wir wollen das Ergebnis der letzten komplexen Berechnung, in irgendeiner Form weiter verwenden. Die im Beispiel berechnete 9 steht jetzt allerdings für die weitere Bearbeitung nicht mehr zur Verfügung. R hat die REPL ausgeführt, die Berechnung der 9, und da jetzt mit dieser Ausgabe nichts weiteres durchgeführt wurde, ist die Ausgabe auch nirgends gespeichert worden. Die Ausgabe bzw. das Ergebnis eines Befehlt wird als Rückgabewert bezeichnet. Um den Rückgabewert eines Ausdrucks weiter zu bearbeiten, muss dieser Wert in irgendeiner Form R zugänglich gemacht werden, d.h. der Rückgabewert muss irgendwie gespeichert werden. Um Werte weiter verwenden zu kommen, wird den Werten daher ein Bezeichner, ein Name, zugewiesen. Es wird eine Variable eingeführt. Erfahrungsgemäß stellt dieses Konzept eine erste Größe Hürde im Umgang mit Rda, das diese Konzept beispielweise in Tabellenkalkulationsprogrammen, bei denen die Berechnungen scheinbar direkt auf den zu sehenden Daten stattfindet, nicht vorhanden ist. Letztendlich ist eine Variable aber nichts als ein Wert in R dem ein Name zugewiesen wurde.\nUm in R einem Ausdruck bzw. dessen Rückgabewert einen Namen zuzuweisen wird ein spezieller in R definierter Befehl verwendet, der Zuweisungsoperator &lt;-. Möchte ich beispielsweise das Ergebnis der „komplexen“ Berechnung 3 * 3 später weiter verwenden, dann verwende ich den Zuweisungsoperator -&gt; um dem Rückgabewert einen Namen zu geben.\n\n&gt; wert_1 &lt;- 3 * 3\n\nR jetzte keinen Ausdruck mehr zurück, da das Ergebnis des Zuweisungsoperator die Zuweisung eines Namens ist, was keinen Wert zurückgibt mehr ergibt. Intern hat R die Berechnung durchgeführt und dem Rückgabewert den Namen wert_1 zugewiesen. Der Name ist dabei, im Rahmen bestimmter Konventionen vollkommen willkührlich und R hätte mich nicht daran gehindert wert_2, wert_123, thomas, steffie oder einen anderen Namen zu verwenden.\nDies Frage stellt sich jetzt natürlich, wie komme ich meinen berechneten Wert wieder heran. Einfach indem ich den Bezeichner, den Namen, wert_1 an R übergebe.\n\n&gt; wert_1\n\n[1] 9\n\n\nIn R-Studio, gibt es oben-rechts einen Reiter mit der Aufschrift Environment, hier sollte jetzt auch ein Eintrag zu finden sein mit dem Bezeichner wert_1 und dem angehängten Wert \\(9\\).\nDieser Prozess funktionert genau gleich mit einem komplexeren Ausdruck wie 2 + 2 * 4.\n\n&gt; x &lt;- 2 + 2 * 4\n\nAufruf des Bezeichners x von auf der Kommandozeile führt dann entsprechend wieder zu der Ausgabe des Wertes.\n\n&gt; x\n\n[1] 10\n\n\nKonzeptionell stellt sich der Vorgang der Zuweisung in etwa so dar. Im internen Speicher von R wird der Wert \\(10\\) an einer passen Stelle abgespeichert und in einer Tabelle wird ein Eintrag mit dem Bezeichner x zusammen mit der Adresse des Wertes \\(10\\) abgelegt. Wenn R dann auf den Bezeichner x trifft, dann schaut es in der Tabelle nach, an welcher Stellt sich der Wert befindet und gibt diesen aus. Dies hat zur Folge, dass Bezeichner genauso wieder in Ausdrücken verwendet werden können wie Werte. R ersetzt den jeweiligen Bezeichner mit dem hinterlegten Wert und führt den Ausdruck aus.\n\n&gt; x + wert_1\n\n[1] 19\n\n\nR ersetzt den Bezeichner x mit dem Wert \\(10\\) und den Bezeichner wert_1 mit dem Wert \\(9\\) und addiert die beiden Werte zusammen.\nIn dieser Vorgehensweise besteht ein grundlegender Unterschied zur der Arbeitsweise mit Tabellenprogrammen bei denen immer direkt auf den jeweiligen Zellen gearbeitet wird. In R werden Berechnungen, die Rückgabewerte von Ausdrücken, Bezeichner zugewiesen und können dann in späteren Ausdrücken (Befehlen) wieder aufgerufen werden. Anders herum, wenn Zwischenergebnisse keinen Bezeichner haben, können sie auch nicht wiederverwendet werden.\nZwei weitere Erläuterungen zu den bisherigen Beispielen sind notwendig. In den bisherigen Ausdrücken sind Leerzeichen zwischen die einzelnen Teile der Ausdrücke gesetzt worden. Diese Leerzeichen dienen lediglich der Leserlichkeit und haben keinen Einfluss auf die Evaluierung des Ausdrucks durch R. Daher sind die Ausdrücke 2 + 2 * 4 und 2+2*4 äquivalent und führen zum gleichen Ergebnis. Bei der Ausgabe des Wertes ist wahrscheinlich auch aufgefallen, das R nicht den Wert \\(16\\) berechnet hat, der korrekt wäre, wenn die Evaluierung des Ausdrucks streng von links nach rechts durchgeführt wird 2 + 2 * 4 = 4 * 4 = 16. R hat jedoch die korrekte mathematischen Regel Punkt-vor-Strich angewendet und ist daher zum korrekten Ergebnis 10 gekommen.\n\n\n\n\n\n\nWichtig\n\n\n\nIn R wird bei Bezeichnern zwischen Groß- und Kleinschreibung unterscheidet. Daher führt der Aufruf des Bezeichners X zu einem Fehler.\n\n&gt; X\n\nError in eval(expr, envir, enclos): Objekt 'X' nicht gefunden\n\n\nDie Fehlermeldung von R gibt auch direkt an, was das Problem ist, das nämlich kein Objekt mit der Bezeichnung X gefunden werden kann.\n\n\n\n\n\n\n\n\nTipp\n\n\n\nDas Auftreten von Fehler führt bei R Neueinsteigerinnen oft zu großer Verwirrung ist aber im Programmieralltag ein vollkommen normales Ereignis und sollte daher niemanden aus der Ruhe bringen. Im vorliegenden Fall bemängelt R lediglich das es den Bezeichner X nicht finden kann und dementsprechend nicht weiß wie es weiter verfahren soll.\n\n\nDamit haben wir jetzt das erste große Konzept in R, bzw. in allen Programmiersprachen, das der Variable kennengelernt. Als nächstes wenden wir uns dem Konzept der Funktionen in R zu."
  },
  {
    "objectID": "r_kickoff.html#funktionen-in-r",
    "href": "r_kickoff.html#funktionen-in-r",
    "title": "2  R Kickoff",
    "section": "2.2 Funktionen in R",
    "text": "2.2 Funktionen in R\nEin gutes Template für Funktionen in Programmiersprachen sind Funktionen aus der Mathematik.\n\\[\\begin{equation*}\ny = f(x)\n\\end{equation*}\\]\nWir haben eine Funktion \\(f()\\), dieser Funktion übergeben wir ein Argument (auch Parameter) \\(x\\). Die Funktion \\(f()\\) macht dann etwas mit diesem Parameter und gibt einen Rückgabewert \\(y\\) zurück. Sei zum Beispiel die folgenden Funktion definiert.\n\\[\\begin{equation*}\nf(x) = x^2\n\\end{equation*}\\]\nD.h. der Funktion \\(f()\\) wird der Parameter \\(x\\) übergeben. Dieser Wert wird anschließend quadriert und das Ergebnis der Berechnung wird zurück gegeben.\nIn R werden Funktionen nach dem Muster (,,…,) aufgerufen (die Zeichen &lt;&gt; stehen für einen beliebigen Bezeichner). D.h. sobald ein rundes Klammerpaar auf einen Bezeichner folgt, geht R davon aus, dass die Anwenderin eine Funktion aufrufen möchte. Über ,,…, können der Funktion durch Komma getrennte Parameter übergeben werden. Die Anzahl der Parameter hängt dabei von der Definition der Funktion ab. In der Mathematik wäre ein Beispiel eine Funktion mit zwei Argumenten.\n\\[\\begin{equation*}\nf(x,y) = x^2 + y^2\n\\end{equation*}\\]\n\n2.2.1 Funktionen anwenden\nEin einfaches Beispiel ist die Anwendung der Wurzelfunktion auf einen numerischen Wert. In R gibt es schon eine vordefinierte Funktion mit dem Bezeichner sqrt(), welche die Wurzel des übergebenen Parameters berechnet.\n\n&gt; y &lt;- 9\n&gt; sqrt(y)\n\n[1] 3\n\n\nIm Beispiel wird zunächst dem Wert \\(9\\) der Bezeichner y zugewiesen. Dieser wird dann an die Wurzelfunktion sqrt() übergeben.\nEin etwas näher an der Anwendung liegendes Beispiel wäre beispielsweise die Berechnung des Mittelwerts oder die Summe der Datenreihe \\((3, 5, 7)\\) sein. In R wird eine solche geordnete Reihe von Zahlen als Vektor repräsentiert. Um einen solchen Vektor zu erstellen wird wiederum eine Funktion c() (für concatenation) verwendet.\n\n&gt; z &lt;- c(3, 5, 7)\n\nMit dieser Anweisung hat R einen Vektor mit den drei Einträgen erstellt. Wir können jetzt den Mittelwert \\(\\bar{z} = \\frac{1}{3}\\sum_{i=1}^3z_i\\) mittels der Funktion mean() berechnen.\n\n&gt; mean(z)\n\n[1] 5\n\n\nVielleicht interessiert uns jetzt nicht der Mittelwert sondern die Summe \\(\\bar{z} = \\sum_{i=1}^3z_i\\). Dafür können wir die sum() Funktion verwenden.\n\n&gt; sum(z)\n\n[1] 15\n\n\nDies sind jetzt nur einige wenige Beispiele und einer der Skills im Umgang mit R besteht darin die Namen der Funktion sich zu merken. Dies kann am schnellesten durch den täglichen Umgang mit R erlernt werden. Am Besten ab heute.\n\n\n2.2.2 Funktionen in R-Paketen\nSollte sich der Fall ergeben, dass keine geeignete Funktion mit R mitgeliefert wird, dann können Zusatzfunktionen mittels sogenannter Pakete installiert werden. Ein Paket kann dabei als eine Sammlung von Funktionen und Anweisungen angesehen werden mit deren Hilfe die Funktionalität von R erweitert werden kann. Daher wird zunächst Information darüber benötigt, in welchem Paket die gewünschte Funktionalität vorhanden ist. Hierfür reicht meistens eine kurze Suche mittels Google aus.\nIst das Paket nun bekannt, dann sind zwei Schritte zunächst durchzuführen. Wobei der 1. Schritt nur beim ersten Mal durchgeführt werden muss. Zunächst muss das benötigte Paket in der lokalen, d.h. derjenigen auf dem Rechner laufenden, R-Umgebung installiert werden, wenn es noch nicht bereits vorher installiert wurde (in R-Studio: Reiter unten-links Packages).\nEin Paket kann mittels des Befehlt install.packages() installiert werden. Der Name des Paket muss in Gänsefüßchen an die Funktion übergeben werden. Wollen wir beispielsweise das Paket performance installieren, führen wir den folgenden Befehl aus.\n\n&gt; install.packages(\"performance\")\n\nR kontaktiert im Hintergrund den CRAN-Server und lädt das Paket sowie benötigte Abhängigkeiten automatisch herunter. Wenn alles gut läuft, dann ist das Paket nun in der lokalen Umgebung installiert. Die Funktionalität des Paket steht jedoch noch nicht direkt zur Verfügung! Sondern, das Paket muss mit einem weiteren Befehl in die derzeit aktive Arbeitsumgebung geladen werden.\nUm das Paket in die aktive Arbeitsumgebung zu laden wird gibt es zwei Befehle in R, library() und require(). Bei require() überprüft R zunächst ob das Paket schon geladen wurde, während bei library() das Paket einfach geladen wird. Um die Funktionalität von performance jetzt in der aktiven Session zu nutzen, kann dementsprechend library() benutzt werden.\n\n&gt; library(performance)\n\nDieser zweite Schritt des Paket laden, muss jedes mal nach einen Neustart von R wieder durchgeführt werden. Zusatzpakete werden durch R beim Start nicht automatisch geladen.\n\n\n\n\n\n\nTipp\n\n\n\nNeue Pakete müssen nur beim ersten Mal neu installiert werden. Danach immer nur noch entweder mit library() oder require() geladen werden.\n\n\nDer große Vorteil von R, beziehungsweise von jeder vollwertigen Programmiersprache, besteht nun darin, dass problemlos eigene Funktion definiert werden können sollten sich keine bereits fertigen Funktionen finden lassen. Dies führt dazu, dass ständig neue Funktionalität zu R hinzugefügt werden kann und mittlerweile kaum eine Disziplin nicht mit eigenen, spezialisierten Paketen aufwarten kann.\n\n\n2.2.3 Eigene Funktionen schreiben\nIm folgenden wird nur kurz examplarisch das Schreiben eigener Funktionen gezeigt, da damit deutlich tiefer in die Programmierung mit R eingestiegen werden muss, als dass zum jetzigen Zeitpunkt notwendig ist.\nWollen wir zum Beispiel eine Funktion schreiben die den gewichteten Mittelwert aus zwei Werten berechnet mit den Gewichten \\((\\frac{1}{3},\\frac{2}{3})\\):\n\\[\\begin{equation*}\nf(x,y) = \\frac{1}{3}x + \\frac{2}{3}y\n\\end{equation*}\\]\nDann könnten wir das in R wie folgt ausdrücken.\n\n&gt; mein_gewichteter_mittelwert &lt;- function(x,y) { \n+   x/3 + 2*y/3 \n+ }\n\nUm eine eigene Funktion zu defineren, muss das Schlüsselworts function() verwendet werden. Wenn R den Ausdruck function sieht, dann interpretiert es den Ausdruck als Definition einer neuen Funktion. Auf das Schlüsselwort folgen runde Klammern () die die benötigten Parameter der Funktion umschließen. Im Beispiel werden zwei Parameter definiert \\(x\\) und \\(y\\). Die Namen der Parameter sind dabei vollkommen willkürlich und müssen lediglich zu späteren Ausdruck in der Funktion passen. Es folgen dann zwei geschweifte Klammern {} die den sogenannten Funktionskörper definieren. Im Funktionskörper ist die Funktionalität der Funktion hinterlegt. Konkret stehen hier die Ausdrücke mit derer die Funktion ihren Ergebniswert berechnen kann. Um die Funktion aufrufbar zu machen, braucht sie wieder einen Bezeichner. Dieser wird mittels des Zuweisungsoperators &lt;- definiert. In R-Studio sollte in der Environment nach dem Ausführen der Anweisung nun ein Eintrag mit dem Namen mein_gewichteter_mittelwert stehen. Der Name ist wiederum vollkommen willkürlich und es R kontrolliert nicht ob mein Bezeichner semantisch dem entspricht was die Funktion berechnet.\nNachdem ich die Funktion definiert habe, kann ich sie wie jeder andere Funktion aufrufen. Bei Aufrufen der Funktion werden die Parameter entsprechend der übergegebenen Werte in den Klammern im Funktionskörper ersetzt. Und R führt die Anweisungen im Funktionskörper aus. Der Rückgabewert der Funktion ist die letzte Anweisung des Funktionskörpers. Im vorliegenden Beispiel, da es nur eine Anweisung gibt, wird dieser Wert zurück gegeben.\n\n&gt; mein_gewichteter_mittelwert(3,6)\n\n[1] 5\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nWas die letzte Anweisung für den Rückgabewert bedeutet ist etwas einfacher zu sehen, wenn wir die Funktion etwas kompliziert schreiben.\n\n&gt; mein_gewichteter_mittelwert_2 &lt;- function(x,y) { \n+   a &lt;- x/3\n+   b &lt;- 2*y/3\n+   a + b\n+ }\n&gt; mein_gewichteter_mittelwert_2(3,6)\n\n[1] 5\n\n\nHier haben wir mit a und b zunächst zwei Zwischenberechnung durchgeführt und dann in der letzten Anweisung das Ergebnis für den Rückgabewert berechnet.\n\n\nWer tiefer in die Programmierung von Funktion in R einsteigen möchte, sollte sich R4DS und Advanced R anschauen."
  },
  {
    "objectID": "r_kickoff.html#datentypen",
    "href": "r_kickoff.html#datentypen",
    "title": "2  R Kickoff",
    "section": "2.3 Datentypen",
    "text": "2.3 Datentypen\nUm mit R effektiv zu arbeiten ist ein zumindest oberflächliches Verständnis von sogenannten Datentypen notwendig. Konzeptionell sind Datentypen Eigenschaften von Werten die bestimmen welche Operationen mit diesem Wert möglich sind. Der einfachste Datentyp numerisch erlaubt zum Beispiel die üblichen mathematischen Operationen +-/* die wir bereits kennengelernt haben. Eine Zeichenkette \"Haus\" lässt sich dagegen nicht dividieren. Die Werte können auch als Objekte bezeichnet werden, was konzeptionell einfacher nachzuvollziehen. Den Datentypen eines Objektes kann mittels der Funktion typeof() bestimmt werden. Ein Objekt hat einen Typ und einen Wert. Wir beginnen mit den grundlegendsten, den atomaren (atomic), Datentypen: Numerisch, Zeichenketten und logische Werte.\n\n2.3.1 Numerisch\nWie bereits beschreiben, ist einer der einfachsten Datentypen ein numerischer Wert wie 1, 123345 12.3456. Es gibt noch eine Unterscheidung in R zwischen ganzzahligen Werten (integer) und Dezimalzahlen (float), wobei für die meisten Anwendungsfälle die Unterscheidung nicht von großer Bedeutung ist. Das Dezimaltrennzeichen in R ist ein Punkt ..\n\n&gt; dezimal_zahl &lt;- 12.345\n&gt; dezimal_zahl\n\n[1] 12.345\n\n&gt; typeof(dezimal_zahl)\n\n[1] \"double\"\n\n&gt; typeof(12.345)\n\n[1] \"double\"\n\n\nInteger Werte werden R mit einem angehängtem L spezifiziert. Wir kein L angehängt geht R per default bei Zahlen immer von einer Dezimalzahl aus.\n\n&gt; ganz_zahl &lt;- 12345L\n&gt; ganz_zahl\n\n[1] 12345\n\n&gt; typeof(ganz_zahl)\n\n[1] \"integer\"\n\n&gt; typeof(12345)\n\n[1] \"double\"\n\n&gt; typeof(12345L)\n\n[1] \"integer\"\n\n\nDie numerischen Werte erlauben die üblichen mathematischen Operationen.\n\n\n2.3.2 Zeichenketten (Strings)\nDer nächste Datentyp sind Zeichenketten (strings). Zeichenketten repräsentieren Textdaten und werden oft für die Manipulation von Texten und die Verarbeitung von Zeichen verwendet. Zeichenketten können mit einfachen Anführungszeichen (') oder doppelten Anführungszeichen (\") erstellt werden.\n\n&gt; s_1 &lt;- \"Haus\"\n&gt; s_1\n\n[1] \"Haus\"\n\n&gt; typeof(s_1)\n\n[1] \"character\"\n\n\nIn R wird der Typ von Zeichenketten als character bezeichnet.\nBezüglich der Anführungszeiche, besteht semantisch kein Unterschied zwischen den einfachen und den doppelten Anführungszeichen und es ist mehr ein Zeichen persönlicher Präferenz welche Art benutzt werden. Ein Anwendungsfall wo die Art von Bedeutung ist, erfolgt wenn innerhalb der Zeichenketten Anführungszeichen benötigt werden. Dann müssen die äußeren Zeichenketten der jeweils andere Typ sein, da ansonsten R die Zeichenkette nicht als solche erkennt. Also zum Beispiel wenn ich als Zeichenkette den Wert: Er sagte: \"Nein\"!, benötige, verwende ich die einfachen Anführungszeichen um R zu signalisieren das ich eine Zeichenkette benötige.\n\n&gt; s_2 &lt;- 'Er sagte: \"Nein\"!'\n&gt; s_2\n\n[1] \"Er sagte: \\\"Nein\\\"!\"\n\n\nUm Operationen auf Zeichenketten anzuwenden gibt es in R eine ganze Reihe von spezialisierten Funktionen. Möchte ich zum Beispiel einen Teil der Zeichen aus einer Zeichenkette extrahieren, kann ich die substring()-Funktion verwenden.\n\n&gt; s_3 &lt;- \"DasisteinelangeZeichenkette\"\n&gt; substring(s_3, 4, 6)\n\n[1] \"ist\"\n\n\nDer erste Parameter übergibt die Zeichenkette an substring() während der zweiter Parameter den Startposition und der dritte Parameter die Endposition des zu extrahierenden Strings angibt.\nDie Länge einer Zeichenkette kann mit der Funktion nchar() bestimmt werden.\n\n&gt; nchar(s_3)\n\n[1] 27\n\n\nEine Funktion die oft mit Zeichenketten eine Anwendung findet ist die paste() bzw. die Spezialform paste0(). Mit paste() können Zeichenkette zusammengesetzt werden.\n\n&gt; paste('Ich', 'bin', 'ein', 'Berliner')\n\n[1] \"Ich bin ein Berliner\"\n\n\nPer default fügt paste() ein Leerzeichen zwischen die Zeichenketten ein. Dies kann entsprechend angepasst werden.\n\n&gt; paste('Ich', 'bin', 'ein', 'Kölner', sep = '--')\n\n[1] \"Ich--bin--ein--Kölner\"\n\n\nDie Argumente an paste() müssen nicht alle Zeichenketten sein, werden aber dann in Zeichenketten konvertiert.\n\n&gt; paste(3, 2, '-mal', sep='')\n\n[1] \"32-mal\"\n\n\npaste0() ist eine Spezielform von paste() bei der das Argument sep auf \"\" gesetzt ist. Das heißt, die Zeichenketten werden direkt aneinander gehängt.\n\n&gt; paste0('kein','zwischen','raum')\n\n[1] \"keinzwischenraum\"\n\n\n\n\n\n\n\n\nTipp\n\n\n\nDas Paket stringr bietet eine große Sammlung von Funktion die den Umgang und die Manipulation von Zeichenketten stark vereinfachen.\n\n\n\n\n2.3.3 Logische (Boolean) Werte\nDer nächste Datentyp sind sogenannte logische Werte oder Wahrheitswerte. Logische Werte können nur einen von zwei Werten annehmen, entweder WAHR oder FALSCH. Logische Ausdrücke kennt ihr wahrscheinlich aus der Schule in Form von Wahrheitstabellen bei denen boolesche Werte entweder mit und \\(\\cap\\) oder oder \\(\\cup\\) verknüpft werden (siehe Tabelle 2.1).\n\n\nTabelle 2.1: Verknüpfung von Wahrheitswerten mit und (\\(\\cap\\)) bzw. oder (\\(\\cup\\)).\n\n\n\n\n(a) Verknüpfung mit \\(\\cap\\)\n\n\n\\(\\cap\\)\nTRUE\nFALSE\n\n\n\n\nTRUE\nTRUE\nFALSE\n\n\nTRUE\nFALSE\nFALSE\n\n\n\n\n\n\n(b) Verknüpfung mit \\(\\cup\\)\n\n\n\\(\\cup\\)\nTRUE\nFALSE\n\n\n\n\nTRUE\nTRUE\nTRUE\n\n\nTRUE\nTRUE\nFALSE\n\n\n\n\n\n\nIn R werden die beiden Werte mit TRUE und FALSE oder in der abgekürzten Form T und F dargestellt.\n\n&gt; wahr &lt;- TRUE\n&gt; wahr\n\n[1] TRUE\n\n&gt; falsch &lt;- FALSE\n&gt; falsch\n\n[1] FALSE\n\n\nDie beiden Verknüpfungen werden mit & für \\(\\cap\\) und | für \\(\\cup\\).\n\n&gt; wahr & wahr\n\n[1] TRUE\n\n&gt; wahr & falsch\n\n[1] FALSE\n\n&gt; falsch & falsch\n\n[1] FALSE\n\n&gt; wahr | wahr\n\n[1] TRUE\n\n&gt; wahr | falsch\n\n[1] TRUE\n\n&gt; falsch | falsch\n\n[1] FALSE\n\n\nLogische Werte sind vor allem bei der Ablaufkontrolle und beim Indexieren wichtig.\nLogische Werte können in numerische Werte konvergiert werden, dabei wird FALSE zu \\(0\\) und TRUE zu \\(1\\). Mit der Funktion as.numeric() können wir Objekte von einem Typ in einen numerischen Typ konvergieren.\n\n&gt; as.numeric(wahr)\n\n[1] 1\n\n&gt; as.numeric(falsch)\n\n[1] 0\n\n\nDie umgekehrte Richtung, von numerisch zum logischen Wert, kann mittels der Funktion as.logical() durchgeführt werden. Dabei werden alle Werte \\(\\neq0\\) zu WAHR und \\(0\\) zu FALSCH.\n\n&gt; as.logical(123)\n\n[1] TRUE\n\n&gt; as.logical(1.23)\n\n[1] TRUE\n\n&gt; as.logical(0)\n\n[1] FALSE\n\n\n\n\n2.3.4 Vektoren\nVektoren sind ein grundlegender Datentypein R und können sowohl numerische als auch nicht-numerische Werte speichern. Allerdings kann immer nur eine Datentyp in einem Vektor gespeichert werden. Da Vektoren sehr oft bei Datenanalysen verwendet werden und oft zur Datenrepräsentation verwendet werden, werden sie entsprechend direkt in R unterstützt. Etwas allgemeiner betrachtet können Vektoren als eine geordnete Sammlung von Elementen betrachtet werden. Geordnet bedeutet dabei, dass sie ein feststehende Abfolge haben.\nDie direkteste Art einen Vektoren in R zu erstellen, ist mittels der c() Funktion (c von combine). Wollen wir zum Beispiel einen numerischen Vektor mit den folgenden Einträgen erstellen.\n\\[\\begin{equation}\nv_1 = \\begin{pmatrix}3\\\\7\\\\8\\end{pmatrix}\n\\end{equation}\\]\nDann sieht dies in R folgendermaßen aus.\n\n&gt; v_1 &lt;- c(3,7,8)\n&gt; v_1\n\n[1] 3 7 8\n\n\nDas gleiche Prinzip funktioniert auch mit anderen Typen. Beispielweise mit einem Zeichenkettenvektor.\n\n&gt; v_2 &lt;- c('mama','papa','tochter')\n&gt; v_2\n\n[1] \"mama\"    \"papa\"    \"tochter\"\n\n\nOder einem logischen Vektor.\n\n&gt; v_bool &lt;- c(TRUE, TRUE, FALSE, TRUE)\n\nAuf einzelne Elemente oder Teile eines Vektor kann mittels des sogenannten subsettings zugegriffen werden. Jeder Vektor hat eine definierte Länge die wir mit length() bestimmen können.\n\n&gt; length(v_2)\n\n[1] 3\n\n\nMit eckigen Klammern [] können wir Teilelemente des Vektors extrahieren. Die Indizierung geht dabei von \\(1\\) bis $n = $ length(Vektor). Wollen wir zum Beispiel das zweite Element aus \\(v_1\\) extrahieren, dann verwenden wir.\n\n&gt; v_1[2]\n\n[1] 7\n\n\nWenn der Index außerhalb des erlaubten Bereichs ist, dann wird ein NA für (N)ot (A)vailable von R zurückgegeben.\n\n&gt; v_1[10]\n\n[1] NA\n\n\nR erlaubt die Verwendung von Vektoren zum subsetten. Beispielweise wenn ich das 1. und 3. Element aus \\(v_2\\) extrahieren möchte, kann ich einen Vektor mit den beiden Elemente \\(1\\) und \\(3\\), \\(v_{\\text{index}} = (1,3)\\) und übergebe diesen an [].\n\n&gt; v_i &lt;- c(1,3)\n&gt; v_2[v_i]\n\n[1] \"mama\"    \"tochter\"\n\n\nDas funktioniert ebenfalls ohne den Zwischenvektor.\n\n&gt; v_2[c(1,3)]\n\n[1] \"mama\"    \"tochter\"\n\n\nDer zurückgegebene Wert ist dann auch wieder ein Vektor. Wir können dadurch zum Beispiel einen Vektor erstellen, der länger ist als der Ursprungsvektor.\n\n&gt; v_1[c(1,1,2,2,3,3)]\n\n[1] 3 3 7 7 8 8\n\n\nZum subsetten können auch logische Vektoren verwendet werden. Der resultierende Vektor ist ein Vektor mit denjenigen Elementen, für die der logische Vektor den Wert TRUE hat.\n\n&gt; v_bool &lt;- c(TRUE, FALSE, TRUE)\n&gt; v_1[v_bool]\n\n[1] 3 8\n\n\nWird ein negativer Wert an [] übergeben, dann wird dieser bzw. diese Wert(e) von R ausgeschlossen.\n\n&gt; v_1[-2]\n\n[1] 3 8\n\n\nDas funktioniert auch wieder mit Vektoren.\n\n&gt; v_lang &lt;- c(1,2,3,4,5,6,7)\n&gt; v_lang[-c(2,3,6)]\n\n[1] 1 4 5 7\n\n\nAuf numerische Vektoren können die gängigen mathematischen Operatoren (+-*/) angewendet werden. Die Operationen werden elementweise ausgeführt.\n\\[\\begin{equation*}\n\\begin{pmatrix}v_1\\\\v_2\\\\\\vdots\\\\v_n\\end{pmatrix} \\times\n\\begin{pmatrix}a\\\\b\\\\\\vdots\\\\c\\end{pmatrix} =\n\\begin{pmatrix}av_1\\\\bv_2\\\\\\vdots\\\\cv_n\\end{pmatrix} \\cdot\n\\end{equation*}\\]\nDementsprechend können Vektoren auch addiert werden, wenn die Längen gleich sind. Die Addition erfolgt dann ebenfalls elementweise.\n\n&gt; v_3 &lt;- c(1,2,3)\n&gt; v_4 &lt;- c(4,5,6)\n&gt; v_3 + v_4\n\n[1] 5 7 9\n\n\nMultiplikation mit einem Skalar folgt der üblichen Skalarmultiplikation von Vektoren aus der Mathematik.\n\\[\\begin{equation}\n\\begin{pmatrix}3\\\\7\\\\8\\end{pmatrix} \\cdot 3 = \\begin{pmatrix}9\\\\21\\\\24\\end{pmatrix}\n\\end{equation}\\]\n\n&gt; v_1 * 3\n\n[1]  9 21 24\n\n\nEine etwas unglückliche gewählte Operation von R ist, dass auch Vektoren mit unterschiedlichen Längen addiert (subtrahiert, multipliziert, dividiert) werden können. Allerdings nur wenn die Länge des längeren Vektors eine Vielfaches des kürzeren Vektors ist. R wiederholt dann die Abfolge der Elemente des kürzeren Vektors so oft bis die Länge des längeren Vektors erreicht wird.\n\n&gt; v_5 &lt;- c(1,2)\n&gt; v_6 &lt;- c(1,2,3,4,5,6)\n&gt; v_5 + v_6\n\n[1] 2 4 4 6 6 8\n\n\nDieses Verhalten ist wie gesagt etwas unglücklich gewählt, da sich sehr schnell subtile Fehler in den Code einschleichen können. Diese sind erfahrungsgemäß nur sehr schwer wieder ausfindig zu machen. Daher sollte auf diese Feature möglichst verzichtet werden.\nDas subsetting wird auch benutzt, wenn ich einem bestehenden Vektor Werte zuweisen will. Will ich zum Beispiel bei \\(v_1\\) den zweiten Wert von einer \\(2\\) in eine \\(20\\) ändern. Dann verwende ich wiederum [] und weise die entsprechende Anzahl von indizierten Werteplätzen zu.\n\n&gt; v_1[2] &lt;- 20\n&gt; v_1\n\n[1]  3 20  8\n\n\nOder eben mehrere Werte\n\n&gt; v_1[1:2] &lt;- c(10, 100)\n&gt; v_1\n\n[1]  10 100   8\n\n\nIch kann auch nur einen einzelnen Wert zuweisen, der dann an mehrere Stellen geschrieben wird.\n\n&gt; v_1[2:3] &lt;- 1000\n&gt; v_1\n\n[1]   10 1000 1000\n\n\nDas funktioniert auch mit logischen Indexvektoren.\n\n&gt; v_1[c(TRUE,FALSE,TRUE)] &lt;- -13\n&gt; v_1\n\n[1]  -13 1000  -13\n\n\nPer default werden Vektoren in R nicht als Spaltenvektoren wie in der Mathematik angesehen. Das kann manchmal ebenfalls zu Problemen in Rechnungen führen, wenn Formeln eins-zu-eins in R übertragen werden und die Formel von der üblichen Algebra bei Vektoren und Matrizen ausgeht. Dies sollte daher im Hinterkopf behalten werden.\nWenn das Skalarprodukt \\(v_x v_y = \\sum_{i=1}^nv_{xi}v_{yi}\\) zweier Vektoren berechnet werden soll, dann muss ein spezieller Matrizenmultiplikator benutzt werden %*%. In unserem Beispiel mit \\(v_3\\) und \\(v_4\\)\n\\[\\begin{equation*}\nv_3 \\cdot v_4 = 1\\cdot4 + 2\\cdot5 + 3\\cdot6 = 32\n\\end{equation*}\\]\n\n&gt; v_3 %*% v_4\n\n     [,1]\n[1,]   32\n\n\nIn R gibt es verschiedene Funktion um schnell Vektoren mit bestimmten Strukturen zu erstellen. Wird ein Vektor mit aufeinanderfolgenden Ganzzahlen benötigt kann der : Operator verwendet werden. Es wird ein Vektor nach dem Muster a:b = \\((a,a+1,\\ldots,b-1,b)\\) erstellt.\n\n&gt; 1:3\n\n[1] 1 2 3\n\n&gt; 10:15\n\n[1] 10 11 12 13 14 15\n\n\nSoll die Zahlenfolgen nicht ganzzahlig sein sondern ein anderes Interval haben, dann kann die seq() Funktion verwendet werden.\n\n1&gt; seq(from = 1, to = 10, by = 2)\n2&gt; seq(0, 2, 0.25)\n3&gt; seq(0, 2*pi, length.out = 10)\n\n\n1\n\nErstelle eine Sequenz von 1 bis 10 in Intervallen der Größe 2\n\n2\n\nErstelle eines Sequenz von 0 bis 0 in Intervallen der Größe \\(0.25\\)\n\n3\n\nErstelle eiene Sequenz von 0 bis \\(2\\pi\\) die die Länge 10 hat.\n\n\n\n\n[1] 1 3 5 7 9\n[1] 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00\n [1] 0.0000000 0.6981317 1.3962634 2.0943951 2.7925268 3.4906585 4.1887902\n [8] 4.8869219 5.5850536 6.2831853\n\n\nEine weitere Funktion die oft einen Einsatz findet ist die rep() Funktion mit der Vektoren mit bestimmten Wiederholungsmustern erstellt werden können.\n\n1&gt; rep(3, 5)\n2&gt; rep(c(1,2), 3)\n3&gt; rep(c(1,2), each=3)\n\n\n1\n\nWiederhole die Zahl 3 fünfmal.\n\n2\n\nWiederhole den Vektor \\((1,2)\\) dreimal.\n\n3\n\nWiederhole jedes Element des Vektor \\((1,2)\\) dreimal.\n\n\n\n\n[1] 3 3 3 3 3\n[1] 1 2 1 2 1 2\n[1] 1 1 1 2 2 2\n\n\nWie in den Beispielen gezeigt, können relativ unkompliziert Vektoren mit beliebigen Wiederholungen erzeugt werden.\nEine weitere praktische Funktion in diesem Zusammenhang ist die Funktion paste() die wir schon bei den Zeichenketten kennengelernt haben. Werden Vektoren an paste() übergeben, werden die kürzeren Werte so oft wiederholt bis die Länge des längsten Vektors erreicht wird. Dann erfolgt erst die Zusammensetzung der Vektoren wiederum elementweise. Als Rückgabewert wird daher wiederum ein Vektor gegeben.\n\n&gt; paste('Participant', 1:3, sep = '_')\n\n[1] \"Participant_1\" \"Participant_2\" \"Participant_3\"\n\n&gt; paste(c('A','B'), 1:4, sep = 'x')\n\n[1] \"Ax1\" \"Bx2\" \"Ax3\" \"Bx4\"\n\n\nZuletzt, benötige ich einen Vektor einer bestimmen Länge bei dem alle Elemente gleich sind kann ich die Funktionen numeric(), character() und logical() verwenden. Zum Beispiel wenn ich einen numerischen Vektor der Länge \\(11\\) mit nur \\(13\\) als Einträg gebrauche.\n\n&gt; numeric(11) + 13\n\n [1] 13 13 13 13 13 13 13 13 13 13 13\n\n\nDa Vektoren ein zentraler Datentyp in R sind, ist es wichtig sich möglichst eingehend mit den Eigenschaften von Vektoren zu beschäftigen. Es ist praktisch unmöglich in R produktiv zu werden ohne zumindest die Grundeigenschaften von Vektoren verstanden zu haben.\n\n\n2.3.5 Matrizen\nMatrizen sind rechteckige und damit zweidimensionale Datenstrukturen, die aus Zeilen und Spalten bestehen und als Verallgemeinerung von Vektoren zu verstehen sind. Matrizen werden häufig für numerische Berechnungen, wie in der linearen Algebra, verwendet. In R können aber genauso wie Vektoren Werte mit Zeichenketten oder logischen Werten haben. Wir werden uns allerdings auf numerische Matrizen beschränken, da diese am ehesten benötigt werden.\nEine Matrize der Dimension \\(m \\times n\\) wird in der Mathematik wir folgt definiert.\n\\[\\begin{equation}\nA = a_{ij} = \\begin{pmatrix}a_{11} & \\ldots & a_{1n} \\\\\n\\vdots & & \\vdots \\\\\na_{m1} & \\ldots & a_{mn}\n\\end{pmatrix}\n\\end{equation}\\]\nMatrizen werden in R mit der Funktion matrix() erstellt. Der erste Parameter gibt die einzelnen Werte an gefolgt von der Anzahl der Zeilen (nrow) und der Anzahl der Spalten (ncol).\n\n&gt; mat_1 &lt;- matrix(1:4, nrow=2, ncol = 2)\n&gt; mat_1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nUm Elemente in der Matrize zu manipulieren oder zu extrahieren verwendet ihr wieder den subsetting Operator [] mit dem Unterschied dass ihr jetzt zwei Indexe angeben müsst nach dem Muster mat[zeile,spalte]. Wie beim Vektor sind Vektoren, logische Werte und negative Werte erlaubt. Wenn ihr eine Dimension weglasst, werden entsprechend alle Zeilen oder Spalten angesteuert.\n\n&gt; mat_1[1,1]\n\n[1] 1\n\n&gt; mat_1[2,2]\n\n[1] 4\n\n&gt; mat_1[,1]\n\n[1] 1 2\n\n&gt; mat_1[2,]\n\n[1] 2 4\n\n&gt; mat_1[-2,1]\n\n[1] 1\n\n\nÜber das subsetting könnt entsprechend auch die indexierten Werte neu belegen.\n\n&gt; mat_0 &lt;- matrix(0, nr=6, nc = 3)\n&gt; mat_0\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n[4,]    0    0    0\n[5,]    0    0    0\n[6,]    0    0    0\n\n&gt; mat_0[2:3,2] &lt;- 1:2\n&gt; mat_0[5,2:3] &lt;- -13\n&gt; mat_0\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    1    0\n[3,]    0    2    0\n[4,]    0    0    0\n[5,]    0  -13  -13\n[6,]    0    0    0\n\n\nMatrizen können wie Vektoren auf zwei Arten multipliziert werden. Entweder die elementweise Art über * oder Matrizenmultiplikation über %*% nach dem Muster.\n\\[\\begin{gather*}\n\\begin{bmatrix}\n    a_{11} & a_{12} & \\dots  & a_{1n} \\\\\n    a_{21} & a_{22} & \\dots  & a_{2n} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    a_{m1} & a_{m2} & \\dots  & a_{mn}\n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\n    b_{11} & b_{12} & \\dots  & b_{1p} \\\\\n    b_{21} & b_{22} & \\dots  & b_{2p} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    b_{n1} & b_{n2} & \\dots  & b_{np}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    c_{11} & c_{12} & \\dots  & c_{1p} \\\\\n    c_{21} & c_{22} & \\dots  & c_{2p} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    c_{m1} & c_{m2} & \\dots  & c_{mp}\n\\end{bmatrix} \\\\\n\\text{mit } c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \\dots + a_{in}b_{nj} \\text{ für } 1 \\leq i \\leq m \\text{ und } 1 \\leq j \\leq p.\n\\end{gather*}\\]\nEtwas übersichtlicher mit einem einfachen Beispiel.\n\\[\n\\begin{matrix}\n& \\begin{pmatrix}\nb_1 & b_2 \\\\\nb_3 & b_4\n\\end{pmatrix} \\\\\n\\begin{pmatrix}\na_1 & a_2 \\\\\na_3 & a_4 \\\\\n\\end{pmatrix} &\n\\begin{pmatrix}\na_1 \\cdot b_1 + a_2 \\cdot b_3 & a_1\\cdot b_2 + a_2\\cdot b_4 \\\\\na_3 \\cdot b_1 + a_4 \\cdot b_3 & a_3\\cdot b_2 + a_4\\cdot b_4 \\\\\n\\end{pmatrix}\n\\end{matrix}\n\\]\nD.h. die Zeilen von \\(A\\) werden mit den Spalten von \\(B\\) per Skalaprodukt multipliziert. Für die Matrizenmultiplikation müssen die Matrizen \\(A\\) und \\(B\\) konform sein. Die Anzahl der Spalten von \\(A\\) muss gleich der Anzahl der Zeilen von \\(B\\) sein.\n\n&gt; A &lt;- matrix(1:6, nr=2)\n&gt; B &lt;- matrix(1:6, nr=3)\n&gt; A\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n&gt; B\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n&gt; A %*% B\n\n     [,1] [,2]\n[1,]   22   49\n[2,]   28   64\n\n\nUm die Dimension einer Matrize, d.h. die Anzahl der Zeilen und Spalten, zu bestimmen gibt es in R die Funktion dim().\n\n&gt; dim(A)\n\n[1] 2 3\n\n\nDer erste Wert gibt entsprechend die Anzahl der Zeilen an während der zweite Wert die Anzahl der Spalten anzeigt. Wenn nur einer der Werte benötigt wird, gibt es auch die beiden Kurzfunktionen:\n\n&gt; nrow(A)\n\n[1] 2\n\n&gt; ncol(A)\n\n[1] 3\n\n\nAnsonsten funktionieren die mathematischen Operator wie in der Matrizenalgebra zu erwarten ist. Wenn z.B. zwei Matrizen addiert werden sollen, dann müssen die Dimensionen übereinstimmen.\n\n&gt; A + B\n\nError in A + B: nicht passende Arrays\n\n\n\n&gt; D &lt;- matrix(11:16, nr=2)\n&gt; dim(D)\n\n[1] 2 3\n\n&gt; A + D\n\n     [,1] [,2] [,3]\n[1,]   12   16   20\n[2,]   14   18   22\n\n\nMit der Funktion diag() wird eine quadratische Diagonalmatrize erstellt.\n\\[\n\\begin{pmatrix}\nd_{11}& 0      & 0 & \\cdots & 0 \\\\\n0     & d_{22} & 0 &        &\\vdots \\\\\n0     & 0      & \\ddots &   &  \\\\\n\\vdots&        &   &        &  \\\\\n0     &  \\cdots &  &        & d_{nn}\n\\end{pmatrix}\n\\]\n\n1&gt; diag(3)\n2&gt; diag(1:4)\n\n\n1\n\nWenn ein Skalar \\(a\\) übergeben wird, die wird die Einheitsmatrize der Dimension \\(a\\) erstellt.\n\n2\n\nWenn ein Vektor übergeben wird, dann eine Diagonalmatrize mit den Werten des Vektors auf der Hauptdiagonalen erstellt.\n\n\n\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    2    0    0\n[3,]    0    0    3    0\n[4,]    0    0    0    4\n\n\nWenn diag() eine Matrize übergeben wird, dann werden die Elemente auf der Hauptdiagonalen extrahiert.\n\n&gt; E &lt;- diag(1:4)\n&gt; diag(E)\n\n[1] 1 2 3 4\n\n\nEine Matrize kann mit Hilfe der t() Funktion transponiert werden.\n\\[\\begin{align*}\nA^T = a_{ji} = \\begin{pmatrix}a_{11} & \\ldots & a_{m1} \\\\\n\\vdots & & \\vdots \\\\\na_{n1} & \\ldots & a_{mn}\n\\end{pmatrix}\n\\end{align*}\\]\n\n&gt; t(A)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n\nD.h. beim transponieren, werden die Zeilen und Spalten der Matrize vertauscht. Die Determinante einer Matrize kann mit der Funktion det() berechnet werden.\n\n&gt; det(E)\n\n[1] 24\n\n\nMit den Funktionen cbind() und rbind() können zusätzliche Spalten bzw. Zeilen an eine Matrize angehängt werden. Dabei werden die Argumente so wiederholt, dass die Anzahl der Elemente übereinstimmt, bzw. ein Fehler geworfen wenn die Anzahl kein Vielfaches voneinander ist.\n\n&gt; cbind(1:2, E)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    1    0    0    0\n[2,]    2    0    2    0    0\n[3,]    1    0    0    3    0\n[4,]    2    0    0    0    4\n\n&gt; rbind(E,1:2)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    2    0    0\n[3,]    0    0    3    0\n[4,]    0    0    0    4\n[5,]    1    2    1    2\n\n\nUm die Inverse eine quadratischen Matrize zu berechnen bzw. um lineare Gleichungsysteme zu lösen kann die solve() Funktion verwendet werden.\n\n&gt; E_inv &lt;- solve(E)\n&gt; E_inv %*% E\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\n\nUm das Gleichungssystem\n\\[\\begin{align*}\n2x + 3y &= 7 \\\\\n-4x + y &= 2 \\\\\n\\end{align*}\\]\nzu lösen, wird entsprechend der folgende Code verwendet.\n\n&gt; A &lt;- matrix(c(2,-4,3,1), nr=2)\n&gt; y &lt;- c(7,2)\n&gt; x &lt;- solve(A,y)\n&gt; x\n\n[1] 0.07142857 2.28571429\n\n&gt; A %*% x\n\n     [,1]\n[1,]    7\n[2,]    2\n\n\nWeitere Funktionen wie beispielweise die QR-Zerlegung findet ihr in der Dokumentation.\nEine weitere hilfreiche Funktion im Zusammenhang mit der Programmierung mit Matrizen ist die apply() Funktion. Mit apply() kann eine Funktion auf die einzelnen Spalten oder Zeilen einer Matrize angewendet werden.\n\n1&gt; F &lt;- matrix(1, nr = 4, nc = 3)\n&gt; F\n2&gt; apply(F, 1, sum)\n3&gt; apply(F, 2, sum)\n\n\n1\n\nEine Matrize \\(F\\) mit vier Zeilen und drei Spalten die nur Einsen als Einträge hat wird erstellt.\n\n2\n\nDie Summenfunktion wird entlang der ersten Dimension (Zeilen) auf \\(F\\) angewendet.\n\n3\n\nDie Summenfunktion wird entlang der zweiten Dimension (Spalten) auf \\(F\\) angewendet.\n\n\n\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    1    1    1\n[3,]    1    1    1\n[4,]    1    1    1\n[1] 3 3 3 3\n[1] 4 4 4\n\n\n\n\n2.3.6 data.frame() oder tibble()\nDataframes sind eine zentrale Datenstruktur in R da praktisch alle Arten von Daten mittels Dataframes organisiert werden. Konzeptionell sind Dataframes ähnlich zu Matrizen, sie ermöglichen allerdings die Speicherung verschiedener Datentypen (z. B. numerisch, Zeichenfolgen, Faktoren) in verschiedenen Spalten ähnlich wie die Daten in einem Tabellenprogramm organisiert sind. Jede Spalte hat dabei die gleiche Anzahl an Elementen und muss einen eineindeutigen Namen haben.\nKlassischerweise werden Dataframes mittels der Funktion data.frame() erstellt. Eine modernere Version sind die sogenannten tibbles() aus dem Package tibble. Dieses werden hier im Weiteren verwenden, d.h. tibbles() aus Anwendersicht verschiedene Verbesserungen mitbringen. Wir verwenden jedoch weiter den Term Dataframe um den Datentyp zu becshreiben. Daher, wenn wir ein Dataframe mittels tibble() erstellen wollen, müssen wir zunächst das Package tibble() laden.\n\n&gt; library(tibble)\n\nDann können wir einen Dataframe erstellen.\n\n&gt; df_1 &lt;- tibble(a = 1:3, b = c('eins','zwei','drei'))\n&gt; df_1\n\n# A tibble: 3 × 2\n      a b    \n  &lt;int&gt; &lt;chr&gt;\n1     1 eins \n2     2 zwei \n3     3 drei \n\n\nWenn ich mir den Inhalt eines Dataframes ausgeben lasse, dann werden mir die Namen der Spalten angezeigt, in vorliegenden Fall a und b. Gefolgt vom Datentyp hier &lt;int&gt; den wir strenggenommen noch nicht kennengelernt haben aber auch ein numerischer Typ ist, und &lt;chr&gt; was die Kurzform für character und entsprechend Zeichenketten bezeichnet. Danach folgen die Einträge in den beiden Spalten. Standardmäßig werden nur die ersten zehn Zeilen gezeigt, was in diesem Fall nicht weiter auffällt.\nWir können auf die einzelnen Spalten über zwei verschiedene Operatoren zugreifen. Einmal mittels des $ Operator oder über [[]] Im ersten Fall können wir den Namen direkt verwenden.\n\n&gt; df_1$a\n\n[1] 1 2 3\n\n\nBei der doppelten, eckigen Klammer [[]] müssen wir den Namen als eine Zeichenkette, also entweder in ' oder \" einschließen.\n\n&gt; df_1[['a']]\n\n[1] 1 2 3\n\n\nWenn ich eine neue Spalte hinzufügen will, kann ich einfache das Dataframe nehmen und über die Syntax von eben eine neue Spalte definieren.\n\n&gt; df_1$neu &lt;- 11:13\n&gt; df_1\n\n# A tibble: 3 × 3\n      a b       neu\n  &lt;int&gt; &lt;chr&gt; &lt;int&gt;\n1     1 eins     11\n2     2 zwei     12\n3     3 drei     13\n\n\nEntsprechend kann ich über eine Kombination von Spaltennamen und subsetting einzelne Elemente verändern.\n\n&gt; df_1$b[2] &lt;- \"AA\"\n&gt; df_1\n\n# A tibble: 3 × 3\n      a b       neu\n  &lt;int&gt; &lt;chr&gt; &lt;int&gt;\n1     1 eins     11\n2     2 AA       12\n3     3 drei     13\n\n\nDa Daten praktisch immer in Form von Dataframes bearbeitet werden, gibt es eine ganze Reihe von Hilfsfunktionen um mit Dataframes zu arbeiten. Erstellen wir uns zunächst ein etwas größeres Beispiel.\n\n&gt; df_2 &lt;- tibble(\n+   id = paste0('P', 1:10),\n+   gruppe = rep(c('CON','TRT'), each=5),\n+   wert = 21:30\n+ )\n&gt; df_2\n\n# A tibble: 10 × 3\n   id    gruppe  wert\n   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;\n 1 P1    CON       21\n 2 P2    CON       22\n 3 P3    CON       23\n 4 P4    CON       24\n 5 P5    CON       25\n 6 P6    TRT       26\n 7 P7    TRT       27\n 8 P8    TRT       28\n 9 P9    TRT       29\n10 P10   TRT       30\n\n\nMit der Funktion summary() erstellt R eine kurze Zusammenfassung mit deskriptiven Statistiken zum Dataframe.\n\n&gt; summary(df_2)\n\n      id               gruppe               wert      \n Length:10          Length:10          Min.   :21.00  \n Class :character   Class :character   1st Qu.:23.25  \n Mode  :character   Mode  :character   Median :25.50  \n                                       Mean   :25.50  \n                                       3rd Qu.:27.75  \n                                       Max.   :30.00  \n\n\nEine ähnliche Funktion erfüllt die Funktion glimpse() aus dem Package tibble().\n\n&gt; glimpse(df_2)\n\nRows: 10\nColumns: 3\n$ id     &lt;chr&gt; \"P1\", \"P2\", \"P3\", \"P4\", \"P5\", \"P6\", \"P7\", \"P8\", \"P9\", \"P10\"\n$ gruppe &lt;chr&gt; \"CON\", \"CON\", \"CON\", \"CON\", \"CON\", \"TRT\", \"TRT\", \"TRT\", \"TRT\", …\n$ wert   &lt;int&gt; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30\n\n\nDie ersten n Elemente (default n = 6) mit head().\n\n&gt; head(df_2, n = 4)\n\n# A tibble: 4 × 3\n  id    gruppe  wert\n  &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;\n1 P1    CON       21\n2 P2    CON       22\n3 P3    CON       23\n4 P4    CON       24\n\n\nDie letzten n elemente (default n = 6) mit tail().\n\n&gt; tail(df_2, n = 3)\n\n# A tibble: 3 × 3\n  id    gruppe  wert\n  &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;\n1 P8    TRT       28\n2 P9    TRT       29\n3 P10   TRT       30\n\n\nDie Spaltennamen werden mittels names() ausgegeben.\n\n&gt; names(df_2)\n\n[1] \"id\"     \"gruppe\" \"wert\"  \n\n\nDie names() Funktion kann auch verwendet werden um die Spaltennamen zu verändern. Dazu kann entweder subsetting angewendet werden um bestimmte Namen zu verändern oder auch alle auf einmal.\n\n&gt; names(df_2)[3] &lt;- 'value'\n&gt; df_2\n\n# A tibble: 10 × 3\n   id    gruppe value\n   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;\n 1 P1    CON       21\n 2 P2    CON       22\n 3 P3    CON       23\n 4 P4    CON       24\n 5 P5    CON       25\n 6 P6    TRT       26\n 7 P7    TRT       27\n 8 P8    TRT       28\n 9 P9    TRT       29\n10 P10   TRT       30\n\n\n\n&gt; names(df_2) &lt;- c('pid', 'group', 'measurement')\n&gt; df_2\n\n# A tibble: 10 × 3\n   pid   group measurement\n   &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n 1 P1    CON            21\n 2 P2    CON            22\n 3 P3    CON            23\n 4 P4    CON            24\n 5 P5    CON            25\n 6 P6    TRT            26\n 7 P7    TRT            27\n 8 P8    TRT            28\n 9 P9    TRT            29\n10 P10   TRT            30\n\n\nDatenframes sind wie Vektoren zentral in der Arbeit mit R und es ist deshalb wiederum wichtig sich schnell in diesen Datentyp einzuarbeiten. Tatsächlich werden ihr feststellen, dass ihr sehr selten Dataframes von Hand erstellt, sondern meistens liegen die Daten schon in digitaler Form auf eurem Rechner vor und ihr ladet mittels Funktionen, die wir später kennenlernen, die Daten in R wo sie dann als Dataframe repräsentiert werden.\n\n\n2.3.7 Zusammenfassung\nDamit haben wir auch schon die wichtigsten Datentypen in R kennengelernt. In Abbildung 2.1 sind die verschiedenen Datentypen und deren Abhängigkeiten noch einmal abgebildet.\n\n\n\n\n\nflowchart TD\n    A[Dataframe] --&gt; B[Vektor]\n    F[Matrize] --&gt; B\n    B --&gt; C(Numerisch)\n    B --&gt; D(Zeichenkette)\n    B --&gt; E(Logisch)\n\n\nAbbildung 2.1: Hierarchie der Datentypen.\n\n\n\n\nDies war natürlich nur eine Übersicht, aber sollte euch schon relativ weit in der Arbeit mit R bringen."
  },
  {
    "objectID": "r_flowcontrol.html#vergleiche",
    "href": "r_flowcontrol.html#vergleiche",
    "title": "3  Ablaufkontrolle",
    "section": "3.1 Vergleiche",
    "text": "3.1 Vergleiche\n\n1&gt; m &lt;- 1:6\n2&gt; m[m &lt; 3]\n\n\n1\n\nDer Variable m wird mit einem Vektor mit den Werten \\([1,2, \\dots, 6]\\) gefüllt.\n\n2\n\nMittels eines Vergleichs werden alle Werte kleiner \\(3\\) ausgegeben.\n\n\n\n\n[1] 1 2"
  },
  {
    "objectID": "r_flowcontrol.html#bedingte-anweisungen-und-verzweigungen",
    "href": "r_flowcontrol.html#bedingte-anweisungen-und-verzweigungen",
    "title": "3  Ablaufkontrolle",
    "section": "3.2 Bedingte Anweisungen und Verzweigungen",
    "text": "3.2 Bedingte Anweisungen und Verzweigungen\n\n\n\n\nflowchart\nA[condition] --&gt; B[TRUE]\nA[condition] --&gt; C[FALSE]\nB --&gt; D[Ausdruck A]\nC --&gt; E[Ausdruck B]\n\n\n\n\n\nif () {} else {}\n\n1&gt; if (condition) {\n2+   AusdruckA\n3+ } else {\n4+   AusdruckB\n+ }\n\n\n1\n\ncondition wird zu einem truth-Wert (true,false) evaluiert\n\n2\n\nWenn condition TRUE ist, wird AusdruckA ausgeführt.\n\n3\n\nWenn condition FALSE ist, wird der else Zweig ausgeführt.\n\n4\n\nIm else-Zweig wird AusdruckB ausgeführt.\n\n\n\n\nBeispiel\n\n&gt; m &lt;- 0\n&gt; a &lt;- 10\n&gt; b &lt;- 20\n&gt; if (a &lt; b) {\n+   m &lt;- 10\n+ } else {\n+   m &lt;- 20\n+ }\n&gt; m\n\n[1] 10"
  },
  {
    "objectID": "r_flowcontrol.html#schleifen",
    "href": "r_flowcontrol.html#schleifen",
    "title": "3  Ablaufkontrolle",
    "section": "3.3 Schleifen",
    "text": "3.3 Schleifen\n\n\n\n\nstateDiagram-v2\n[*] --&gt; item\nitem --&gt; Vector\nVector --&gt; item\nitem --&gt; Ausdruck\nitem --&gt; [*]\n\n\n\n\n\n\n1&gt; for (item in vector) {\n2+   Ausdruck\n+ }\n\n\n1\n\nIn der Klammer werden zwei Ausdrücke benötigt. item ist ein Zähler der die Einträge des Vektors vector durchläuft.\n\n2\n\nJedes Mal wenn item mit einem neuen Wert belegt worden ist, wird Ausdruck ausgeführt und der jeweilige Wert von item steht im Ausdruck zur Verfügung.\n\n\n\n\nBeispiel\n\n&gt; vec &lt;- c('mama','papa','daughter','son') \n&gt; for (i in 1:4) {\n+   cat(i, ': ', vec[i], '\\n')\n+ }\n\n1 :  mama \n2 :  papa \n3 :  daughter \n4 :  son"
  },
  {
    "objectID": "r_example_workflow.html",
    "href": "r_example_workflow.html",
    "title": "4  Ein Beispielworkflow mit Skripten in R",
    "section": "",
    "text": "Im Folgenden wird ein etwas umfangreicheres Beispiel mit nur kurzen Erklärungen exemplarisch vorgeführt. Eingehende Erläuterungen zu den verwendeten Befehlen kann in R mittels der Hilfedokumentation aufgerufen werden. Dazu wird lediglich ein ? vor den Funktionsnamen gestellt und R bzw. R-Studio öffnet die zu der Funktion gehörende Hilfedatei. Soll zum Beispiel die Hilfedokumentation für die Funktion mean() aufgerufen werden.\n\n&gt; ?mean\n\nEs sei der folgende Datensatz aus Tabelle Tabelle 4.1 gegeben. In zwei unabhängigen Gruppen A und B wurde der Körperfettgehalt bestimmt. Nun soll untersucht werden, ob ein statistisch signifikanter Unterschied zwischen den beiden Gruppen besteht. Dies ist natürlich nur ein synthetisches Beispiel und sollte in dieser Form daher nicht im Rahmen einer tatsächlichen wissenschaftlichen Arbeit durchgeführt werden, sondern dient lediglich der Anschauung wie eine solche Analse in R durchgeführt werden könnte.\n\n\n\n\nTabelle 4.1: Exemplarische Körperfettdaten\n\n\nGroup\nBFP\n\n\n\n\nA\n13.3\n\n\nA\n6.0\n\n\nA\n20.0\n\n\nA\n8.0\n\n\nA\n14.0\n\n\nA\n19.0\n\n\nB\n22.0\n\n\nB\n16.0\n\n\nB\n21.7\n\n\nB\n210.0\n\n\nB\n30.0\n\n\nB\n26.0\n\n\nB\n30.0\n\n\n\n\n\n\nUm eine Datenanalyse durchzuführen, müssen die Daten zunächst in R eingeladen werden. In der Rohform liegen die Daten in Form einer Textdatei vor. Die erste Spalte der Datei zeigt die Gruppenzugehörigkeit an, während die zweite Spalte den jeweiligen Fettgehalt beinhaltet. Die Spalten sind durch ein Komma voneinander getrennt und als Dezimaltrennzeichen, der internationalen Konvention folgend, wird ein Punkt verwendet.\nUm die Daten in R zu laden wird eine spezielle Funktion aus dem Paket readr verwendet\n\n\n\n\n\n\nWarnung\n\n\n\nWenn R mit dem Betriebssystem interagiert solltet ihr immer das Arbeitsverzeichnis (working directory) kennen. Dies bezeichnet das Verzeichnis, das R als sein Ausgangsverzeichnis betrachtet. Dementsprechend sind Pfadangaben entweder in Relation zu dem Arbeitsverzeichnis zu geben oder absolut. Das Arbeitsverzeichnis kann mit der Funktion setwd() (kurz für set working directory) verändert werden (in R-Studio auch über den Menüeintrag Session).\n\n\nNun wird das Paket readr geladen in dem zahlreiche Funktionen zum einlesen verschiedener Dateiarten hinterlegt sind. Wir laden die Datei mittels der Funktion read_csv() ein. read_csv() ist spezialisiert auf das einlesen von Komma-separierten Textfiles (comma-separated-v**alues).\n\n&gt; library(readr)\n&gt; bfp &lt;- read_csv(file = 'bfp_data.txt')\n&gt; bfp\n\nDie Daten stehen nun unter dem Bezeichner bfp in R zur Verfügung. Der Name ist dabei wieder willkürlich gewählt und der Einfachheit halber kurz gehalten.\nDie Daten sind in einem sogenannten data.frame-Objekt (bzw. der neueren Version tibble) abgelegt und können nun weiter verarbeitet werden. Beispielsweise kann mittels der Funktion summary() ein Überblick über deskriptiven Statistiken der Daten erzeugt werden.\n\n&gt; summary(bfp)\n\n    Group                BFP        \n Length:13          Min.   :  6.00  \n Class :character   1st Qu.: 14.00  \n Mode  :character   Median : 20.00  \n                    Mean   : 33.54  \n                    3rd Qu.: 26.00  \n                    Max.   :210.00  \n\n\nHier ist bereits zu sehen, dass einer der Datenpunkt wahrscheinlich fehlerhaft ist, da der Wert &gt; 100 ist, was bei für einen prozentualen Körperfettanteil nicht möglich ist. Im nächsten Schritt sollen die Daten dann graphisch mittels eines Boxplots dargestellt werden. R stellt von Haus aus zahlreiche Funktion zur einfachen graphischen Darstellung bereit. Wir wollen hier aber auf das Paket ggplot2 zurückgreifen, welches die Erstellung moderner Graphiken in Publikationsqualität ermöglicht (Healy, 2018; Wickham, 2016). Vor der Verwendung muss das Paket wiederum zunächst geladen werden.\n\n&gt; library(ggplot2)\n&gt; ggplot(bfp, aes(Group, BFP)) + geom_boxplot()\n\n\n\n\nAbbildung 4.1: Darstellung der Beispieldaten mittels eines Boxplots mit dem problematischen Datenpunkt.\n\n\n\n\nIm Boxplot in Abbildung Abbildung 4.1 ist der problematische Datenpunkt noch klarer ersichtlich und er verhindert gleichzeitig eine Analyse der Daten. Da wir keine weitere Information haben, durch welchen Wert wir den fehlerhaften Wert ersetzen könnten, schließen wir den Datenpunkt der Einfachheit halber aus. Dazu benutzen wir aus dem Paket dplyr die Funktion filter().\n\n&gt; library(dplyr)\n&gt; bfp_clean &lt;- filter(bfp, BFP &lt;= 100)\n&gt; bfp_clean\n\n# A tibble: 12 × 2\n   Group   BFP\n   &lt;chr&gt; &lt;dbl&gt;\n 1 A      13.3\n 2 A       6  \n 3 A      20  \n 4 A       8  \n 5 A      14  \n 6 A      19  \n 7 B      22  \n 8 B      16  \n 9 B      21.7\n10 B      30  \n11 B      26  \n12 B      30  \n\n&gt; ggplot(bfp_clean, aes(Group, BFP)) + geom_boxplot()\n\n\n\n\nAbbildung 4.2: Darstellung der Beispieldaten unter Ausschluss des fehlerhaften Datenpunktes.\n\n\n\n\nDie graphische Darstellung mittels eine Boxplots ist jetzt schon deutlich aussagekräftiger (siehe Abbildung Abbildung 4.2). Ohne jetzt weiter auf statistische Voraussetzungen einzugehen führen wir jetzt einen unabhängigen t-Test für Gruppen mit unterschiedlichen Varianzen. Dazu benutzen wir wieder eine Funktion aus R.\n\n&gt; t.test(BFP ~ Group, data = bfp_clean)\n\n\n    Welch Two Sample t-test\n\ndata:  BFP by Group\nt = -3.4017, df = 9.9886, p-value = 0.006762\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -18.040619  -3.759381\nsample estimates:\nmean in group A mean in group B \n       13.38333        24.28333 \n\n\nWir dieses Beispiel zeigt, lässt sich in R mittels weniger Befehle eine Datenanalyse realisieren. Die Entwickler von R haben dabei darauf geachtet, dass die Namensgebung von Funktionen möglichst nahe an der gewünschten Tätigkeit liegt, so dass einen der englische Begriff meist schnell die Funktion herleiten lässt. Im Beispiel haben wir alle Befehlt direkt auf der Kommandozeile eingegeben und die Daten interaktiv analysiert. Bei einer tatsächlichen Analyse wird die Datenanalyse aus einer Kombination von interaktiven Arbeiten und permanenten Skripten bestehen. Beispielsweise würde diejenigen finalen Befehl die auf die Daten angewendet werden sollen in eine Skriptdatei geschrieben werden, so dass die Analyse zu einem späteren Zeitpunkt wieder aufgegriffen bzw. nachvollziehbar ist. So könnte der gezeigte Workflow in das folgende Skript münden:\n\n# Notwendige Bibliotheken\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Daten einlesen\nbfp &lt;- read_csv(file = 'bfp_data.txt')\n\n# Daten bearbeiten\nbfp_clean &lt;- filter(bfp, BFP &lt;= 100)\n\n# Deskriptiv\nsummary(bfp_clean)\n\n# Graphiken\nggplot(bfp_clean, aes(Group, BFP)) + geom_boxplot()\n\n# Analyse\nt.test(BFP~Group, data = bfp_clean)\n\nDie Abfolge der Befehle in dem Skript sind durch die Verwendung von Kommentaren, die in R mit einem # signalisiert werden, noch besser nachvollziehbar. Dieses Skript könnte zusammen mit den Daten abgespeichert werden und bleibt dann zu jedem Zeitpunkt ausführbar."
  },
  {
    "objectID": "r_basics.html#daten-in-r-einlesen",
    "href": "r_basics.html#daten-in-r-einlesen",
    "title": "5  Einfache Datenbearbeitung und Visualisierung in R",
    "section": "5.1 Daten in R einlesen",
    "text": "5.1 Daten in R einlesen"
  },
  {
    "objectID": "r_basics.html#daten-in-r-prozessieren-mit-tidyverse",
    "href": "r_basics.html#daten-in-r-prozessieren-mit-tidyverse",
    "title": "5  Einfache Datenbearbeitung und Visualisierung in R",
    "section": "5.2 Daten in R prozessieren mit tidyverse()",
    "text": "5.2 Daten in R prozessieren mit tidyverse()"
  },
  {
    "objectID": "r_basics.html#daten-in-r-visualisieren-mit-ggplot",
    "href": "r_basics.html#daten-in-r-visualisieren-mit-ggplot",
    "title": "5  Einfache Datenbearbeitung und Visualisierung in R",
    "section": "5.3 Daten in R visualisieren mit GGplot()",
    "text": "5.3 Daten in R visualisieren mit GGplot()"
  },
  {
    "objectID": "r_literate_programming.html#r-projekte",
    "href": "r_literate_programming.html#r-projekte",
    "title": "6  Literate programming in R",
    "section": "6.1 R Projekte",
    "text": "6.1 R Projekte\nBevor wir jedoch mit Quarto anfangen, lernen wir noch R-Projekte kennen, da diese eine deutliche Vereinfachung des Arbeitsprozesses ermöglichen."
  },
  {
    "objectID": "r_literate_programming.html#quarto-dokumente",
    "href": "r_literate_programming.html#quarto-dokumente",
    "title": "6  Literate programming in R",
    "section": "6.2 Quarto-Dokumente",
    "text": "6.2 Quarto-Dokumente\n\n6.2.1 Markdown\nBei Quarto-Dokumenten handelt es sich um Dateien mit der Endung .qmd, die gleichzeitig Text wie auch R-Code enthalten können. Die Textsegmente sind dabei nicht nur einfache Kommentare (#) im Code, sondern vollständig formatierte Textsegmente, wie wir sie aus Word und ähnlichen Programmen kennen. Die Formatierung der Textkomponenten erfolgt mit der sogenannten Markdown-Syntax. Markdown ist eine einfache Auszeichnungssprache die es erlaubt Formatierungsanweisungen in den Text zu integrieren.\nSoll zum Beispiel eine Wort fett gedruckt werden, dann wird das Wort mit zwei ** eingeschlossen. In Tabelle 6.1 sind ein paar Beispiel angezeigt.\n\n\nTabelle 6.1: Beispiele für Textformatierung mit Markdown\n\n\n\n\n\n\nMarkdown Syntax\nFormatierung\n\n\n\n\n*kursiv*, **fett**, ***fett kursiv***\nkursiv, fett, fett kursiv\n\n\nhochgestellt^2^ / tiefgestellt~2~\nhochgestellt2 / tiefgestellt2\n\n\n~~durchgestrichen~~\ndurchgestrichen\n\n\n\n\nÜberschriften werden in Markdown mittels des # Zeichens definiert. Die Zeile wird dabei mit einem # begonnen, und per default sind sechs Stufen definiert, die dann auch entsprechend abgstuft nummeriert werden. In Tabelle 6.2 sind ein paar Beispiel (ohne die Nummerierung) abgebildet.\n\n\nTabelle 6.2: Beispiele für Überschriftenformatierung mit Markdown\n\n\n\n\n\n\nMarkdown Syntax\nFormatierung\n\n\n\n\n# Überschrift 1\nÜberschrift 1\n\n\n## Überschrift 2\nÜberschrift 2\n\n\n### Überschrift 3\nÜberschrift 3\n\n\n\n\nListen können werden ebenfalls über einfache Formatierungen definiert (siehe Tabelle 6.3).\n\n\nTabelle 6.3: Beispiele für Listenformatierungen mit Markdown\n\n\n\n\n\n\nMarkdown Syntax\nFormatierung\n\n\n\n\n* ungeordnete Liste\n    + Unterelement 1\n    + Unterelement 2\n        - Unterunterelment 1\n\nungeordnete Liste\n\nUnterelement 1\nUnterelement 2\n\nUnterunterelement 1\n\n\n\n\n\n1. geordente Liste\n2. Element 2\n    i) Unterelement 1\n         A.  Unterunterelement 1\n\ngeordenete Liste\nElement 2\n\nUnterelement 1\n\nUnterunterelement 1\n\n\n\n\n\n\n\nDie Formatierungsmöglichkeiten in Quarte sind überaus umfangreich und können daher an dieser Stelle nicht alle zusammen gefasst werden. Da immer wieder neue Features dazu kommen ist die erste Adresse die sehr gute Dokumentation von Quarto.\n\n\n6.2.2 R-Code in Quarto-Dokumenten\nR Code der ausgeführt werden soll wird in ein Quarto-Dokument mittels sogenannter Chunks oder Code blocks integriert.\nCodeblöcke werden mittels spezieller Formatierungszeichen angezeigt. Am Anfang stehen drei backticks (SHIFT+´) gefolgt von {r}, während der Codeblock mittels dreier backticks geschlossen wird. ```{r} R Code). Diese Codeblöcke werden während des rendern, also des Formatierens von Quarto ausgeführt.\n\n\n\n\n\n\nTipp\n\n\n\nIn R-Studio gibt es glücklicherweise einen short-cut um einen Codeblock einzufügen .\n\n\n```{r}\na &lt;- 3\nb &lt;- 4\na + b\n```\nDie Variablen die in den jeweiligen Codeblöcken definiert werden stehen in weiteren nachfolgenden Codeblöcken in dem Quartodokument zur Verfügung.\n\nEinzelne Codeblöcke können in R-Studio getrennt ausgeführt werden. Dazu muss der Cursor in dem Block sein und entweder mit PLAY-Button oben-recht im Block oder mit dem short-cut . Um das ganze Dokument zu rendern kann entweder der Render-Button oberhalb des Dokuments verwendet werden oder wiederum der short-cut .\n\nWenn wir unseres vorhergehendes Workflow-Beispiel nehmen, dann könnte ein gekürztes Quarto-Dokument wie folgt aussehen.\n---\ntitle: \"Analyse des Körperfettgehalts in zwei Gruppen\"\nauthor: \"Martina Muster\"\n---\n# Einleitung\n\nNach Schmidt et al. (2023) ist davon auszugehen, dass eine Manipulation von XYZ dazu führt, dass ... . Diese Hypothese soll anhand einer neuen Stichprobe überprüft werden.\n\n# Methodik\n\n...\n\n# Analyse\n\n## Bibliotheken\n\nFür die Bearbeitung der Daten, werden die folgenden Bibliotheken benötigt.\n```{r}\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n```\n\n## Daten einlesen\n\nDie Rohdaten befinden sich in einer Textdatei mit der Bezeichnung \"bfp_data.txt\". **Achtung**, darauf achten, dass der Dateipfad korrekt ist.\n```{r}\nbfp &lt;- read_csv(file = 'bfp_data.txt')\n```\n\n## Daten bearbeiten\nEin Datenpunkt ist ~fehlerhaft~, und zeigt einen Wert von $BFP &gt; 100$, was nicht physiologisch plausibel. Daher wird dieser Wert ausgeschlossen.\n```{r}\nbfp_clean &lt;- filter(bfp, BFP &lt;= 100)\n```\nDie Daten zur Analyse befinden sich jetzt in der Variablen `bfp_clean`.\n\n# Diskussion\n\n...\n\n# Zusammenfassung\n\n...\nZu Beginn des Quarto-Dokuments findet sich ein Abschnitt der mit --- abgegrenzt ist. Dies ist der sogenannte YAML-Header. YAML ist eine ebenfalls Auszeichnungssprache (yet aanother markup **l*anguage). In dem Header werden verschiedene Einstellung für das Quarto-Dokument eingestellt. Hier können zum Beispiel auch weitere Dokumente wie Zitationsbibliotheken und Zitationsstyle eingebunden werden, da Quarte Referenzen und Zitationen innerhalb der Dokumente unterstützt."
  },
  {
    "objectID": "stats_title.html",
    "href": "stats_title.html",
    "title": "Statistik",
    "section": "",
    "text": "Die erste Frage die sich im Umgang mit der Anwendung von Verfahren der Statistik stellt ist: Wofür benötigen wir Statistik überhaupt?\nBeispielsweise wurden ein Datensatz gesammelt, bei dem zwei Gruppen miteinander verglichen werden, eine Treatmentgruppe (TRT) und eine Kontrollgruppe (CON). In beiden Gruppen wurden jeweils \\(N_i = 20\\) Personen untersucht. Es wurde das folgende Ergebnis erhalten (siehe Abbildung 1).\n\n\n\n\n\nAbbildung 1: Boxplot der Kontroll- und der Treatmentgruppe bezüglich einer abhängigen Variable\n\n\n\n\nOffensichtlich sind die Werte in der Treatmentgruppe deutlich höher als diejenigen in der Kontrollgruppe. Warum ist es nicht ausreichend das offensichtliche zu dokumentieren? Warum ist eine statistische Analyse der Daten notwendig?\nDiese Fragestellung wird in dem folgenden Abschnitt untersucht. Gleichzeitig werden die notwendigen Werkzeuge entwickelt um die verschiendenen Schritte die einer statistische Analyse von Daten zugrundeliegenen zu verstehen und anwenden zu können."
  },
  {
    "objectID": "stats_basics.html#ein-experiment",
    "href": "stats_basics.html#ein-experiment",
    "title": "7  Eine kleine Welt der Unsicherheit",
    "section": "7.1 Ein Experiment",
    "text": "7.1 Ein Experiment\nWir wollen nun eine Krafttrainingsstudie durchführen um die Beinkraft zu erhöhen. Wir haben allerdings nur sehr wenige Ressourcen (bzw. wir sind faul) und können insgesamt nur sechs Messungen durchführen. Aus einem kürzlich durchgeführten Census haben wir aber die Kraftwerte der ganzen Population. Wir stellen die Kraftwerte zunächst mittels einer Tabelle dar (siehe Tabelle 7.1)\n\n\n\n\nTabelle 7.1: Kraftwerte (in Newton) der Lummerländer an der einbeinigen Beinpresse\n\n\nID\nKraft[N]\nID\nKraft[N]\n\n\n\n\nP01\n2414\nP11\n2243\n\n\nP02\n2462\nP12\n2497\n\n\nP03\n2178\nP13\n1800\n\n\nP04\n2013\nP14\n2152\n\n\nP05\n2194\nP15\n2089\n\n\nP06\n2425\nP16\n2090\n\n\nP07\n2305\nP17\n3200\n\n\nP08\n2117\nP18\n2196\n\n\nP09\n2298\nP19\n2485\n\n\nP10\n2228\nP20\n2440\n\n\n\n\n\n\nSelbst bei 20 Werten ist diese Darstellung wenig übersichtlich. Wir könnten zwar Zeile für Zeile durchgehen und nach etwas notieren und suchen würden wir sehen das der Maximalwert bei \\(3200\\)N für P17 und der Minimalwert von Person P13 bei \\(1800\\)N liegt. Aber wirklich einfach ist diese Darstellung nicht. Für solche univariaten Daten (uni = eins) kann eine übersichtlichere Darstellung mittels eines sogenannten Dotplots erreicht werden (siehe Abbildung 7.2).\n\n\n\n\n\nAbbildung 7.2: Dotplot der Lummerlandkraftdaten\n\n\n\n\nHier kann deutlich schneller abgelesen werden was das Minimum und das Maximum der Daten ist, sowie es kann auch direkt abgeschätzt werden in welchem Bereich sich der Großteil der Daten befindet. Allerdings wird durch diese Art der Darstellung die Information über welche Person die jeweiligen Werte besitzt nicht mehr dargestellt. Dies stellt in den meisten Fällen allerdings kein Problem dar, da wir in den meisten Fällen aussagen über die Gruppe und weniger über einzelne Personen machen wollen.\nGehen wir jetzt von der folgenden Fragestellung aus. Wir wollen den Gesundheitsstatus unserer Lummerländer verbessern und wollen dazu ein Krafttraining durchführen. Da evidenzbasiert arbeiten wollen, möchten wir überprüfen ob wirklich ein Verbesserung der Kraft durch das Training stattgefunden hat. Da es sich aber gleichzeitig um unsere selbst geschaffene Welt handelt führen wir natürlich ein perfektes Krafttraining, eine perfekte Intervention, durch. D.h wir stellen uns immer wieder als unwissend da und geben vor das wir gar nicht wissen, das das Training perfekt effektiv ist.\nD.h. wir führen gleichzeitig ein Gedankenexperiment durch. Wir führen ein Krafttraining für die Beine durch. Das Training ist perfekt und verbessert die Kraftleistung um genau \\(+100\\)N. Dieser Kraftzuwachs unabhängig davon welche Person aus unserer Population das Training durchführt (Warum ist das keine realistische Annahme?). Wir wollen zwei Gruppen miteinander vergleichen eine Interventionsgruppe und eine Kontrollgruppe. In beiden Gruppen sollen jeweils \\(n_{\\text{TRT}} = n_{\\text{CON}} = 3\\) TeilnehmerInnen bzw. Teilnehmer einbezogen werden da wir nicht mehr Ressourcen für mehr ProbandInnen haben.\nDie erste Frage die sich nun stellt ist wie wählen wir die sechs Personen aus unserer Population aus und wie teilen wir die sechs Personen in die beiden Gruppen? Nach etwas überlegen kommen wir darauf, dass wir am besten eine zufällige Stichprobe ziehen sollten (Warum?).\n\nDefinition 7.2 (Stichprobe) Eine Stichprobe ist eine Teilmenge der Objekte aus der Population.\n\n\nDefinition 7.3 (Zufallsstichprobe) Eine Zufallsstichprobe ist eine Teilmenge der Objekte aus der Population die zufällig ausgewählt wurde.\n\nDiese sechs Personen, unsere Stichprobe, wird dann wiederum zufällig auf die beiden Gruppen aufgeteilt.\nEin Zufallszahlengenerator hat die Zahlen \\(i = \\{3,7,8,9,10,20\\}\\) gezogen. Die entsprechenden Personen werden aus der Population ausgewählt und wiederum zufällig in die beiden Gruppen aufgeteilt (siehe Tabelle 7.2).\n\n\n\n\nTabelle 7.2: Zufällig ausgewählte Stichprobe der Kontrollgruppe (CON) und der Interventionsgruppe (TRT).\n\n\nID\nKraft[N]\nGruppe\n\n\n\n\nP08\n2117\nCON\n\n\nP09\n2298\nCON\n\n\nP03\n2178\nCON\n\n\nP07\n2305\nTRT\n\n\nP10\n2228\nTRT\n\n\nP20\n2440\nTRT\n\n\n\n\n\n\nMit diesen sechs Personen führen wir jetzt unser Experiment durch. Die drei Personen aus der Kontrollgruppe, unterlaufen im Interventionszeitraum nur ein Stretchtraining während die Interventionsgruppe zweimal die Woche für 12 Wochen unser perfektes Krafttraining durchführt. Nach diesem Zeitraum messen wir alle Personen aus beiden Gruppen und erhalten das folgende Ergebnis (siehe Tabelle 7.3).\n\n\nTabelle 7.3: Ergebnis der Intervention in Experiment 1 für die Kontroll- und die Interventionsgruppe.\n\n\n\n\n(a) Kontrollgruppe\n\n\nID\nKraft[N]\n\n\n\n\nP08\n2117\n\n\nP09\n2298\n\n\nP03\n2178\n\n\n\\(\\bar{K}\\)\n2198\n\n\n\n\n\n\n(b) Interventionsgruppe\n\n\nID\nKraft[N]\n\n\n\n\nP07\n2405\n\n\nP10\n2328\n\n\nP20\n2540\n\n\n\\(\\bar{K}\\)\n2424\n\n\n\n\n\n\nFür beide Gruppen ist jeweils der Mittelwert berechnet worden, um die Wert miteinander vergleichen zu können. Später werden wir noch weitere Maße kennenlernen die es ermöglichen zwei Mengen von Werten miteinander zu vergleichen.\n\nDefinition 7.4 (Mittelwert) Der Mittelwert über \\(n\\) Werte berechnet sich nach der Formel:\n\\[\n\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n}\n\\tag{7.1}\\]\nDer Mittelwert wird mit einem Strich über der Variable dargestellt.\n\nDamit lernen wir direkt auch ein neues Konzept kennen. Nämlich das der Statistik. Ein Wert der auf der erhobenen Stichprobe berechnet wird, wird als Statistik bezeichnet.\n\nDefinition 7.5 (Statistik) Ein auf einer Stichprobe berechnet Wert, wird als Statistik bezeichnet.\n\nUm jetzt Unterschied zwischen den beiden Gruppen zu untersuchen berechnen wir die Differenz D zwischen den beiden Mittelwerten \\(D = \\bar{K}_{\\text{TRT}} - \\bar{K}_{\\text{CON}}\\). Die Differenz kann natürlich auch in die andere Richtung berechnet werden und es würde sich das Vorzeichen ändern. Hier gibt es keine Vorgaben, sondern die Richtung kann frei bestimmt werden. Wenn bekannt ist in welcher Richtung der Unterschied berechnet wird, dann stellt dies keine Problem dar. Im vorliegenden Fall ziehen wir die Interventionsgruppe von der Kontrollgruppe ab, da wir davon ausgehen, dass die Intervention zu einer Krafterhöhung führt und wir dadurch einen positiven Unterschied erhalten (vgl. Gleichung 7.2)\n\\[\nD = 2424N - 2198N = 226 N\n\\tag{7.2}\\]\nDa der Wert D, wiederum auf den Daten der Stichprobe berechnet wird, handelt es sich ebenfalls um eine Statistik.\n\n\n\n\n\nAbbildung 7.3: Dotplot der beiden Stichproben. Senkrechte Striche zeigen die jeweiligen Mittelwerte an.\n\n\n\n\nIn Abbildung 7.3 sind die Werte der beiden Gruppen, deren Mittelwerte \\(\\bar{K}_{\\text{CON}}\\) und \\(\\bar{K}_{\\text{TRT}}\\) und der Unterschied \\(D\\) zwischen diesen abgebildet. Wie erwartet zeigt die Interventionsgruppen den höheren Kraftwert im Vergleich zu der Kontrollgruppe. Allerdings ist der Wert mit \\(D = 226\\) größer als der tatsächliche Zuwachs von \\(\\Delta_{\\text{Training}} = 100\\) (Warum ist das so?).\nDer Unterschied zwischen den beiden Gruppen ist natürlich auch zum Teil auf die Unterschiede die zwischen den beiden Gruppen vor der Intervention bestanden haben zurück zu führen. Was wäre denn passiert, wenn wir eine andere Stichprobe gezogen hätten?\nSei \\(i = \\{12,2,19,4,8,16\\}\\) eine zweite Stichprobe. Dies würde zu den folgenden Werten führen nach der Intervention führen.\n\n\n\n\nTabelle 7.4: Ergebnis der Intervention in Experiment 2 für die Kontroll- und die Interventionsgruppe.\n\n\nID\nKraft[N]\nGruppe\n\n\n\n\nP08\n2117\nCON\n\n\nP09\n2298\nCON\n\n\nP03\n2178\nCON\n\n\nP07\n2405\nTRT\n\n\nP10\n2328\nTRT\n\n\nP20\n2540\nTRT\n\n\n\n\n\n\n\n\n\n\n\nAbbildung 7.4: Dotplot der beiden Stichproben in Experiment 2. Senkrechte Striche zeigen die jeweiligen Mittelwerte an.\n\n\n\n\nIn Abbildung 7.4 sind wiederum die Datenpunkte, Mittelwerte und der Unterschied abgetragen. In diesem Fall ist allerdings die Differenz zwischen den beiden Gruppen genau in der anderen Richtung \\(D = -308\\), so dass die Interpretation des Ergebnisses genau in der anderen Richtung wäre. Nämlich, nicht nur hat das Krafttraining zu keiner Verbesserung in der Kraftfähigkeit geführt, sondern zu einer Verschlechterung!\nEs hätte aber auch sein können, das wir noch eine andere Stichprobe gezogen hätten, z.B. \\(i = \\{6,5,7,20,14,16\\}\\). Dies würde zu dem folgenden Ergebnis führen (siehe Tabelle 7.5).\n\n\n\n\nTabelle 7.5: Mittelwertsdaten aus Experiment 3 und der Unterschied \\(D\\) zwischen den beiden Gruppenmittelwerten\n\n\nGruppe\nKraft[N]\n\n\n\n\nCON\n2308\n\n\nTRT\n2327\n\n\n\\(D\\)\n19\n\n\n\n\n\n\nIn diesem Fall haben wird zwar wieder einen positiven Unterschied zwischen den beiden Gruppen in der zu erwartenden Richtung gefunden. Der Unterschied von \\(D = 19\\) ist allerdings deutlich kleiner als das tatsächlichen \\(\\Delta = 100\\). Daher würden wir möglicherweise das Ergebnis so interpretieren, führen, dass wir das Krafttraining als ineffektiv bewerten würden und keine Empfehlung ausprechen.\nZusammengenommen, ist keines der Ergebnisse 100% korrekt. Entweder der Unterschied zwischen den beiden Gruppen ist deutlich zu groß, oder in der anderen Richtung oder deutlich zu klein. Das Ergebnis des Experiments hängt ursächlich damit zusammen, welche Stichprobe gezogen wird. Diese Einsicht gilt in jedem Fall generell für jedes Ergebnis eines Experiments.\nDas Phänomen, das der Wert der berechneten Statistik zwischen Wiederholungen des Experiments schwankt wird als Stichprobenvariabilität bezeichnet.\n\nDefinition 7.6 (Stichprobenvariabilität) Durch die Anwendung von Zufallsstichproben, variiert eine auf den Daten berechnete Statistik. Die Variabilität wird als Stichprobenvariabilität bezeichnet.\n\nStreng genommen, führt die Stichprobenvariabilität für sich genommen noch nicht dazu, das sich die Statistik zwischen Wiederholungen des Experiments verändert, sondern die zu untersuchenden Werte in der Population müssen selbst auch noch eine Streuung aufweisen. Wenn wir eine Population untersuchen würden, bei der alle Personen die gleiche Beinkraft hätten, würden unterschiedliche Stichproben immer den gleichen Mittelwert haben und wiederholte Durchführung des Experiment würden immer wieder zu dem selben Ergebnis führen. Dieser Fall ist in der Realität aber praktisch nie gegeben und sämtlich Parameter für die wir uns hier interessieren zeigen immer eine natürlich Streuung in der Population. Diese Streuung in der Population führt daher zu dem besagten Ergebnis, das das gleiche Experiment mehrmals wiederholt zu unterschiedlichen Zufallsstichproben führt und dementsprechend immer zu unterschiedlichen Ergebnissen führt.\nDaher ist eine der zentrale Aufgabe der Statistik mit dieser Variabilität umzugehen und die Forscherin trotzdem in die Lage zu versetzen rationale Entscheidungen zu treffen. Eine implizite Kernannahme dabei ist, das wir mit Hilfe von Daten überhaupt etwas über die Welt lernen können. D.h. das uns die Erhebung von Daten überhaupt auch in die Lage versetzt rationale Entscheidungen zu treffen. Entscheidungen wie ein spezialisiertes Krafttraining mit einer klinischen Population durchzführen oder eine bestimmte taktische Variante mit meiner Mannschaft zu trainieren um die Gegner besser auszuspielen. Alle diese Entscheidungen sollten rational vor dem Hintergrund von Variabilität getroffen werden und auch möglichst oft korrekte Entscheidungen zu treffen. Wie wir sehen werden, kann uns die Statistik leider nicht garantieren immer die korrekte Entscheidungen zu treffen. Nochmal auf den Punkt gebracht nach Wild und Seber (2000, p.28)\n\nThe subject matter of statistics is the process of finding out more about the real world by collecting and then making sense of data.\n\nUntersuchen wir jedoch zunächst unsere Einsicht, das Wiederholungen des gleichen Experiments zu unterschiedlichen Ergebnissen führt, weiter. In unserem Beispiel aus Lummerland haben wir nämlich den Vorteil, das uns die Wahrheit bekannt ist. In Abbildung 7.5 ist die Verteilung unsere bisheringen drei \\(D\\)s abgetragen.\n\n\n\n\n\nAbbildung 7.5: Bisherige Verteilung der Unterschiede \\(D\\)\n\n\n\n\nDie drei Werte liegen ja relativ weiter auseiander. Eien Anschlussfrage könnte jetzt sein: “Welche weiteren Werte sind denn überhaupt möglich mit der vorliegenden Population?”."
  },
  {
    "objectID": "stats_basics.html#die-stichprobenverteilung",
    "href": "stats_basics.html#die-stichprobenverteilung",
    "title": "7  Eine kleine Welt der Unsicherheit",
    "section": "7.2 Die Stichprobenverteilung",
    "text": "7.2 Die Stichprobenverteilung\nWir können jetzt ja einfach mal das Experiment anfangen zu wiederholen. In Abbildung 7.6 sind mal 15 verschiedene Stichproben abgetragen. Wir haben in jeder Zeile jeweils sechs TeilnehmerInnen gezogen. Drei für die Kontrollgruppe und drei für die Inervationsgruppe. Für jede dieser Zeilen können wir jeweils den Gruppenmittelwert berechnen und den Unterschied \\(D\\) bestimmen.\n\n\n\n\n\nAbbildung 7.6: Beispiele für verschiedene Möglichkeiten zwei Stichproben mit jeweils \\(n_i = 3\\) aus der Population zu ziehen\n\n\n\n\nWarum eigentlich bei 15 aufhören. Wir haben ja den Vorteil, das unsere Population relativ übersichtlich ist. Vielleicht können wir uns ja noch aus unserer Schulezeit an Kombinatorik erinnern. Da haben wir den Binomialkoeffizienten kennengelernt. Die Anzahl der möglichken Kombination von \\(k\\) Elementen aus einer Menge von \\(n\\) Elementen berechnet sich nach:\n\\[\n\\text{Anzahl} = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\tag{7.3}\\]\nIn unserem Fall wollen wir zunächst sechs Elemente aus \\(N = 20\\) auswählen und dann drei Elemente aus den sechs gezogenen Elementen auswählen um diese entweder der Interventionsgruppe oder der Kontrollgruppe zu zuweisen (Warum brauchen wir uns nur eine Gruppe anzuschauen?). Die Anzahl der möglichen Stichprobenkombinationen ist folglich:\n\\[\n\\text{Anzahl} = \\binom{20}{6}\\binom{6}{3} = 7.752\\times 10^{5}\n\\tag{7.4}\\]\nDas sind jetzt natürlich selbst bei dieser kleinen Population ein große Menge von einzelnen Experimenten, aber dafür sind Computer da, die können alle diese Experiment in kurzer Zeit durchführen. In Abbildung 7.7 ist die Verteilung aller möglichen Experimentausgänge, d.h. alle Differenzen \\(D\\) zwischen der Interventions- und der Kontrollgruppe, abgebildet.\n\n\n\n\n\nAbbildung 7.7: Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe bei einer Intervention mit \\(\\Delta = 100\\) (im Graphen mittels der roten Linie angezeigt).\n\n\n\n\nAuf der x-Achse sind die möglichen Differenzen \\(D\\) abgetragen, während auf der y-Achse die relative Häufigkeit, d.h. die Häufigkeit für einen bestimmten \\(D\\)-Wert geteilt durch die Anzahl \\(7.752\\times 10^{5}\\) aller möglichen Werte. Die Verteilung der D’s wird als Stichprobenverteilung bezeichnet.\n\nDefinition 7.7 Die Stichprobenverteilung kennzeichnet die Verteilung der beobachteten Statistik.\n\nDie Abbildung 7.7 zeigt, dass die überwiegende Anzahl der Ausgänge tatsächlich auch im Bereich von \\(\\Delta = 100\\) liegen. Noch präziser das Maximum der Verteilung, also die höchste relative Häufigkeit liegt genau auf der roten Linie. Dies sollte uns etwas beruhigen, denn es zeigt, das unsere Art der Herangehensweise mittels zweier Stichproben auch tatsächlich in den meisten Fällen einen nahezu korrekten Wert ermittelt. Allerdings zeigt die Stichprobenverteilung auch das Werte am rechten Ende die deutlich zu hoch sind wie auch Werte am linken Ende der Verteilung die deutlich in der falschen Richtung möglich sind. Das bedeutet, wenn wir das Experiment nur einmal durchführen wir uns eigentlich nie sich sein können, welches dieser vielen Experimente wir durchgeführt haben. Es ist zwar warscheinlicher, dass wir eins aus der Mitte der Verteilung durchgeführt haben, einfach da die Anzahl größer ist, aber wir haben keine 100% Versicherung, das wir nicht Pech gehabt haben und das Experiment ganz links mit \\(D = -500\\) oder aber das Experiment ganz rechts mit \\(D = 700\\) durchgeführt haben. Diese Unsicherheit wird leider keine Art von Experiment vollständig auflösen können. Eine weitere Eigenschaft der Verteilung ist ihre Symmetrie bezüglich des Maximums mit abnehmenden relativen Häufigkeiten umso weiter von Maximum \\(D\\) entfernt ist (Warum macht das heuristisch Sinn?).\nDie Darstellungsform von Abbildung 7.7 wird als Histogramm bezeichnet und eignet sich vor allem dazu die Verteilung einer Variablen z.B. \\(x\\) darzustellen. Dazu wird der Wertebereich von \\(x\\) zwischen dem Minimalwert \\(x_{\\text{min}}\\) und dem Maximalwert \\(x_{\\text{max}}\\) in \\(k\\) gleich große Intervalle unterteilt und die Anzahl der Werte innerhalb jedes Intervalls wird abgezählt und durch die Anzahl der Gesamtwerte geteilt um die relative Häufigkeit zu erhalten.\nZum Beispiel für die Werte:\n\\[\nx_i \\in \\{1,1.5,1.8,2.1,2.2,2.7,2.8,3.5,4 \\}\n\\] könnte das Histogram ermittelt werden, indem der Bereich von \\(x_{\\text{min}} = 1\\) bis \\(x_{\\text{max}} = 4\\) in vier Intervalle unterteilt wird und dann die Anzahl der Werte in den jewiligen Intervallen ermittelt wird (siehe Abbildung 7.8). Die ermittelte Anzahl würde dann noch durch die Gesamtanzahl \\(9\\) der Elemente geteilt um die relative Häufigkeit zu berechnen.\n\n\n\n\n\nAbbildung 7.8: Beispiel für die Darstellung eines Histogramms für die Daten \\(x_i\\).\n\n\n\n\nDie Form des Histogramms hängt davon ab wie viele Intervalle verwendet werden, so wird die Auflösung mit mehr Intervallen besser, aber es die Anzahl wird geringer und andersherum wird die Auflösung mit weniger Intervallen geringer aber die Anzahl der Elemente pro Intervall wird größer und somit stabiler. Daher sollte in den meisten praktischen Fällen die Anzahl variiert werden um sicher zu gehen, das nicht nur zufällig eine spezielle Darstellung verwendet wird.\nZurück zu unserer Verteilung von \\(D\\) unter \\(\\Delta = 100\\)N in Abbildung 7.7. Wie schon besprochen sind alle Werte zwischen etwa \\(D = -500N\\) und \\(D = 700\\)N plausibel bzw. möglich. Schauen wir uns doch einmal an, was passiert wenn das Training überhaupt nichts bringen würde und es keine Verbesserung gibt, also \\(\\Delta = 0\\).\n\n\n\n\n\nAbbildung 7.9: Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe wenn \\(\\Delta = 0\\) (rote Linie).\n\n\n\n\nDie Verteilung in Abbildung 7.9 sieht praktisch genau gleich aus, wie diejenige für \\(\\Delta = 100\\). Der einzige Unterschied ist lediglich das sie nach links verschoben ist und zwar scheinbar genau um die \\(100\\)N Unterschied zwischen den beiden \\(\\Delta\\)s. Dies ist letztendlich auch nicht weiter verwunderlich, bei der Berechnung des Unterschied \\(D\\) zwischen den beiden Gruppen kommen in beiden Fällen genau die gleichen Kombination vor. Bei \\(\\Delta = 100\\) wird aber zu der Interventionsgruppe das \\(\\Delta\\) dazuaddiert bevor die Differenz der Mittelwerte berechnet wird. Da aber gilt:\n\\[\nD = \\frac{1}{3}\\sum_{i=1}^3 x_{\\text{KON}i} - \\frac{1}{3}\\sum_{j=1}^3 (x_{\\text{TRT}j} + \\Delta) = \\bar{x}_{\\text{KON}} - \\bar{x}_{\\text{TRT}} + \\Delta\n\\]\nDaher bleibt die Form der Verteilung immer genau gleich und wird lediglich um den Wert \\(\\Delta\\) im Vergleich zur Nullintervention verschoben. Wobei mit Nullintervention Umgangssprachlich die Intervention bezeichnet, bei der nichts passiert also \\(\\Delta = 0\\) gilt."
  },
  {
    "objectID": "stats_basics.html#unsicherheit-in-lummerland",
    "href": "stats_basics.html#unsicherheit-in-lummerland",
    "title": "7  Eine kleine Welt der Unsicherheit",
    "section": "7.3 Unsicherheit in Lummerland",
    "text": "7.3 Unsicherheit in Lummerland\nDas führt jetzt aber zu einem Problem für uns. Gehen wir jetzt nämlich von diesen beiden Annahmen aus, das entweder die Intervention effektiv ist \\(\\Delta = 100\\) gilt oder das die Intervention nichts bringt also \\(\\Delta = 0\\) gilt. Wenn wir diese beiden Verteilungen übereinander legen erhalten wir Abbildung 7.10. Wir haben die Darstellung jetzt etwas verändert und eine Kurve durch die relativen Häufigkeiten gelegt. Dieser Graphen wird jetzt nicht mehr als Histogramm sondern als Dichtegraph bezeichnet.\n\n\n\n\n\nAbbildung 7.10: Verteilung aller möglichen Differenzen zwischen Kontroll- und Interventionsgruppe wenn \\(\\Delta = 0\\) und \\(\\Delta = 100\\).\n\n\n\n\nIn Abbildung 7.10 ist klar zu sehen, dass die beiden Graphen zu großen Teilen überlappen und dazu noch in einem Bereich wo beide Ergebnisse ihrer höchsten relativen Häufigkeiten, also auch die größte Wahrscheinlichkeit haben unter den jeweiligen Annahmen aufzutreten. Unser Problem besteht jetzt darin, dass wir in der Realität gar nicht diese Information haben welchen Effekt unser Training auf die Stichprobe ausführt. Wenn wir dies wüssten, dann müssten wir das Experiment ja gar nicht durchführen. Wir haben im Normalfall nur ein einziges Ergebnis, nämlich den Ausgang unseres einen Experiments.\n\n\n\n\n\nAbbildung 7.11: Zuweisung eines beobachteten Unterschieds \\(D\\) nach einem Experiment\n\n\n\n\nWenn wir jetzt unser Experiment einmal durchgeführt haben und ein einziges Ergebnis für \\(D\\) erhalten haben, sei zum Beispiel \\(D = 50\\) dann haben wir ein Zuweisungsproblem (siehe Abbildung 7.11). Wie weisen wir unser Ergebnis jetzt den beiden möglichen Realität zu? Einmal kann es sein, das das Krafttraining aber auch gar nichts gebracht hat und wir haben lediglich eine der vielen möglichen Stichprobenkombination beobachtet haben die zu einem positiven Wert für \\(D\\) führt. Oder aber das Krafttraining ist effektiv gewesen und hat zu einer Verbesserung von \\(\\Delta = 100\\)N geführt und wir haben lediglich ein Stichprobenkombination aus den vielen möglichen Stichprobenkombination gezogen die zu einem Ergebnis von \\(D = 50\\) führt. Noch mal, in der Realität wissen wir nicht welche der beiden Annahmen korrekt ist und können es auch nie vollständig wissen. Denn egal wie viele Experimente wir machen, wir können immer den zwar unwahrscheinlichen aber nicht unmöglichen Fall haben, das wir nur Werte beispielsweise aus dem linken Teil der Verteilung beobachten. Das heißt wir haben immer mit einer Ungewissheit zu kämpfen. Wir können nicht im Sinne eines Beweises zeigen, das das Training effektiv ist.\nDie Methoden der Statistik liefern uns nun Werkzeuge an die Hand um trotzdem rational zu Entscheiden welche der beiden Annahmen möglicherweise wahrscheinlicher ist. Gleichzeitig ermöglicht uns die Statistik abzuschätzen respektive zu berechnen wie groß die Unsicherheit in dieser Entscheidung ist. Die Statistik sagt dabei immer nur etwas über die beobachteten Daten aus. Die Statistik sagt jedoch nichts über die zugrundeliegenden wissenschaftlichen Theorien aus.\nSchauen wir uns jetzt als vorläufig letzten Punkt an welche Entscheidungsmöglichkeiten wir haben."
  },
  {
    "objectID": "stats_basics.html#eine-entscheidung-treffen",
    "href": "stats_basics.html#eine-entscheidung-treffen",
    "title": "7  Eine kleine Welt der Unsicherheit",
    "section": "7.4 Eine Entscheidung treffen",
    "text": "7.4 Eine Entscheidung treffen\nWir hatten im Beispiel zwei verschiedene Annahmen, einmal das das Training nichts bringt und keine Verbesserung der Kraftfähigkeit folgt \\(\\Delta = 0N\\). Andererseits hatten wir das Beispiel gestartet damit, dass die Kraftfähigkeit um \\(100N\\) zunimmt, also \\(\\Delta = 100N\\). Wie bezeichnen jetzt diese beiden Annahmen als Hypothesen und bezeichnen \\(\\Delta = 0N\\) als die Nullhypothese \\(H_0\\) und \\(\\Delta = 100N\\) als die Alternativhypothese \\(H_1\\).\nWenn wir jetzt das Experiment durchgeführt haben, können wir uns also entweder für die \\(H_0\\) oder die \\(H_1\\) entscheiden. Aus Gründen der Symmetrie ist dies gleichbedeutend wenn wir uns nur auf die \\(H_0\\) fokussieren und entweder die \\(H_0\\) annehmen bzw. beibehalten oder verwerfen also uns gegen \\(H_0\\) entscheiden.\n\n\n\n\nTabelle 7.6: Entscheidungsmöglichkeiten wenn entweder \\(H_0\\) oder \\(H_{1}\\) zutrifft.\n\n\n\n\n\n\n\n\n\nRealität\n\n\n\n\n$H_0$\n$H_1$\n\n\n\n\n$H_0$\nkorrekt\n$\\beta$\n\n\n$H_1$\n$\\alpha$\nkorrekt\n\n\n\n\n\n\n\n\nIn Tabelle 7.6 sind die verschiedenen Entscheidungsmöglichkeiten abgetragen. In der Realität gehen wir, wie gesagt, von zwei Fällen aus. Entweder trifft die \\(H_0\\) oder die \\(H_1\\) zu. Wenn die \\(H_=\\) zutrifft und wir uns für die \\(H_0\\) entscheiden, dann haben wir eine korrekte Entscheidung getroffen. Wenn \\(H_0\\) zutrifft und wir allerdings die \\(H_0\\) ablehnen, also uns für die \\(H_1\\) entscheiden ist unsere Entscheidung falsch und wir begehen einen Fehler. Dieser Fehler wird als Fehler 1. Art bzw. \\(\\alpha\\)-Fehler bezeichnet. Trifft in der Realität dagegen die \\(H_1\\) zu und wir entscheiden uns gegen die \\(H_0\\) und für die \\(H_1\\), dann haben wir wiederum eine korrekte Entscheidung getroffen. Zuletzt, wenn die \\(H_1\\) zutrifft und wir uns aber für die \\(H_0\\) entscheiden, also die \\(H_0\\) beibehalten bzw. uns gegen die \\(H_1\\) entscheiden, treffen wir wieder eine falsche Entscheidung. Dieser Fehler wird als Fehler 2. Art, bzw. \\(\\beta\\)-Fehler bezeichnet.\n\nDefinition 7.8 Wenn eine Entscheidung gegen die \\(H_0\\) getroffen wird, obwohl die \\(H_0\\) korrekt ist, wird dies als \\(\\alpha\\)-Fehler bezeichnet.\n\n\nDefinition 7.9 Wenn eine Entscheidung gegen die \\(H_1\\) getroffen wird, obwohl die \\(H_1\\) korrekt ist, wird dies als \\(\\beta\\)-Fehler bezeichnet.\n\n\n\n\n\nWild, Christopher J, und Georg AF Seber. 2000. Chance encounters: A first course in data analysis and inference. Wiley Press."
  },
  {
    "objectID": "stats_significance.html#wie-treffe-ich-eine-entscheidung",
    "href": "stats_significance.html#wie-treffe-ich-eine-entscheidung",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.1 Wie treffe ich eine Entscheidung?",
    "text": "8.1 Wie treffe ich eine Entscheidung?\nIn unserem kleine Welt Bespiel waren wir in der komfortablen Position, das wir genau wussten was passiert bzw. welcher Prozess unseren beobachteten Datenpunkt erzeugt hat. D.h wir kannten den datengenerieren Prozesses.\n\nDefinition 8.1 (Datengenerierender Prozess (DGP)) Der Prozess in der realen Welt der die beobachteten Daten und damit die daraus folgende Statistik erzeugt wird als datengenerierender Prozess bezeichnet.\n\nLetztendlich zielt unsere Untersuchung, unser Experiment, darauf ab, Informationen über den DGP zu erhalten, weil diese Information uns erlaubt Aussagen über die reale Welt zu treffen. Dabei muss allerdings beachtet werden, dass dieser Prozess in den allermeisten Fällen ein starke Vereinfachung des tatsächlichen Prozesses in der Realität darstellt. Meistens sind die Abläufe in der Realität zu komplex um sie ins Gänze abzubilden. Somit wird fast immer nur ein Modell verwendet.\nZurück zu unseren Problem, wenn wir ein Experiment durchführen, dann haben wir normalerweise nur eine einzige beobachtete Statistik. In unseren bisherigen Beispiel also den berechneten Unterschied \\(D\\) in der Kraftfähigkeit nach der Intervention zwischen der Kontroll- und der Interventionsgruppe.\n\n\n\n\n\nAbbildung 8.1: Beobachteter Unterschied nach der Durchführung unseres Experiments\n\n\n\n\nIn Abbildung 8.1 ist der beobachtete Wert, \\(D = 50\\) abgetragen. Wir wissen von vorne herein, dass dieser Wert beeinflusst ist durch die zufällige Wahl der Stichprobe und die daran geknüpfte Streuung der Werte in der Population. Wie können wir den nun überhaupt eine Aussage treffen darüber, ob das Krafttraining was bringt oder vielleicht nur einen sehr kleinen Effekt zeigt oder möglicherweise sogar schädlich ist also zu einer Abnahme der Kraft führt?\nÜberlegen wir uns zunächst, welche Prozesse unseren beobachteten Wert zustande gebracht haben könnten. Wir haben schon zwei Prozesse kennengelernt, einmal den Prozess mit \\(\\Delta = 100\\) wie auch den Prozess mit \\(\\Delta = 0\\)\n\n\n\n\n\nAbbildung 8.2: Mögliche datengenerierende Prozesse für den beobachteten Unterschied \\(D\\) (rot)\n\n\n\n\nIn Abbildung 8.2 ist wieder unser beobachteter Wert \\(D = 50\\) und die beiden Verteilungen abgetragen. Leider können wir nicht eineindeutig sagen, welche der beiden Verteilungen, bzw. deren zugrundeliegende Prozesse, unseren beobachteten Wert erzeugt haben könnte. Da unser beobachteter Wert \\(D\\) genau zwischen den beiden Maxima der Verteilungen liegt. Etwas motiviertes Starren auf die Abbildung wird uns allerdings auf die Idee bringen, dass der beobachtete Wert nicht nur von diesen beiden Verteilungen erzeugt worden sein muss, sondern durchaus noch mehr Verteilungen in Frage kommen.\n\n\n\n\n\nAbbildung 8.3: Beispiele für weitere mögliche Verteilungen als DGP.\n\n\n\n\nAbbildung 8.3 zeigt, dass selbst die Verteilung mit \\(\\Delta = -250N\\) und \\(\\Delta = 350N\\) nicht unplausibel sind den beobachteten Wert erzeugt zu haben. Warum aber bei diesen fünf Verteilungen aufhören, warum sollte \\(\\Delta\\) nicht \\(-50\\) oder \\(127\\) sein. Und überhaupt, keiner kann behaupten die Natur kennt nur ganzzahlige Werte (siehe \\(\\pi\\)). Warum sollte \\(D\\) also nicht auch \\(123.4567N\\) sein?\nWenn diese Überlegung weitergeführt wird, dann wird schnell klar, dass letztendlich eine unendliche Anzahl von Verteilung in der Lage ist unseren beobachteten Wert plausibel zu generieren. D.h. wir haben ein Experiment durchgeführt und den ganzen Aufwand betrieben und haben wochenlang mit unseren ProbandInnen Krafttraining durchgeführt und sind hinterher eigentlich keinen Schritt weiter da wir immer noch nicht wissen was der datengenerierende Prozess ist. Also können wir selbst nach dem Experiment nicht sagen ob unser Krafttraining tatsächlich wirksam ist.\nZum Glück werden wir später sehen, das unser Unterfangen nicht ganz so aussichtslos ist. Schauen wir uns zum Beispiel die Verteilung für \\(\\Delta = -350N\\) an (Abbildung 8.4).\n\n\n\n\n\nAbbildung 8.4: Verteilung für \\(\\Delta = -350N\\) und der beobachtete Wert \\(D\\)\n\n\n\n\nUnser beobachteter Wert unter der Annahme das \\(\\Delta = -350N\\) ist nicht vollkommen unmöglich, aber so richtig wahrscheinlich erscheint er auch nicht. Der Wert liegt relativ weit am Rand der Verteilung. Die Kurve ist dort schon ziemlich nahe bei Null. D.h. der beobachtete Wert ist zwar durchaus möglich, aber es wäre schon überraschend wenn wir bei einer Durchführung des Experiments ausgerechnet so einen Wert beobachten würden wenn unsere angenommenes \\(\\Delta\\) korrekt ist.\nWenn wir jetzt dagegen von der Annahme ausgehen, dass dem DGP der Wert \\(\\Delta = 50N\\) zugrundeliegen würde, hätten wir die Verteilung in Abbildung 8.5. Zunächst ist dieser Wert möglich unter der Annahme. Zusätzlich liegt der beobachtete Wert mitten drin in dem Teil der Verteilung der auch zu erwarten wäre. D.h. der beobachtete Wert ist durchaus plausibel unter der Annahme und bei der einmaligen Durchführung des Experiments würde uns der beobachtete Wert nicht unbedingt überraschen.\n\n\n\n\n\nAbbildung 8.5: Verteilung für \\(\\Delta = 50N\\) und der beobachtete Wert \\(D\\)\n\n\n\n\nDiesen Ansatz können wir verwenden um mit Hilfe unseres Experiments doch etwas über den DGP auszusagen. Allerdings müssen wir uns noch einmal etwas eingehender mit Verteilungen auseinandersetzen um z.B. genauer zu bestimmen welche Ergebnisse uns überraschen würden. D.h. wir müssen uns erst ein mal ein paar neue Konzepte erarbeiten."
  },
  {
    "objectID": "stats_significance.html#lage--und-skalenparameter",
    "href": "stats_significance.html#lage--und-skalenparameter",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.2 Lage- und Skalenparameter",
    "text": "8.2 Lage- und Skalenparameter\nIn Abbildung 8.3 hatten wir mehrere Verteilungen abgebildet. Die Verteilung haben die gleiche Form sind aber gegeneinander verschoben. D.h. sie unterscheiden sich bezüglich ihrer Position bzw. Lage. Der Parameter der bei einer Verteilungen die Lage steuert ist der sogenannte Erwartungwerts \\(\\mu\\) der auch als Mittelwert bezeichnet wird. Dieser Mittelwert \\(\\mu\\) unterscheidet sich allerdings von dem uns bereits bekannten Mittelwert \\(\\bar{x}\\) in der Stichprobe. In einem späteren Abschnitt werden wir uns genauer anschauen wie der Mittelwert \\(\\mu\\) berechnet wird.\n\n8.2.1 Mittelwert \\(\\mu\\) der Population\nDa der Mittelwert \\(\\mu\\) die Position der Verteilung bestimmt, ist \\(\\mu\\) ein Parameter der Verteilung. Die Beschreibung als Parameter der Verteilung bedeutet somit, dass die Verteilung von \\(\\mu\\) abhängt, oder formaler das die Verteilung eine Funktion von \\(\\mu\\) ist. Wenn wir uns an Funktionen aus der Schule zurück erinnen wo wir Funktionen \\(f\\) von \\(x\\) kennengelernt haben und als \\(f(x)\\) dargestellt haben. Übertragen auf die Verteilung könnte dies mittels \\(f(\\mu)\\) dargestellt werden.\nBetrachten wir zwei Verteilungen die sich bezüglich ihrer Mittelwerte \\(\\mu\\) unterscheiden. Zum Beispiel sei \\(\\mu_1 = 0\\) und \\(\\mu_2 = 3\\). Wie in Abbildung 8.6 zu sehen ist, führt dies dazu, das die beiden Verteilungen gegeneinander verschoben sind.\n\n\n\n\n\nAbbildung 8.6: Verteilungen mit zwei unterschiedlichen Mittelwerten\n\n\n\n\nWie bereits erwähnt, wird der Mittelwert \\(\\mu\\) der Verteilung auch als Erwartungswert bezeichnet. Dies kann dahingehend interpretiert werden, das wenn Stichproben aus dieser Verteilungen gezogen werden, im Mittel der Wert \\(\\mu\\) erwartet werden kann. Soweit ist dies eigentlich noch nichts wirklich Neues, sondern hatten dies schon vorher gesehen, als wir alle möglichen Unterschiede zwischen der Kontrollgruppe und der Interventionsgruppe ermittelt haben. Hier war der Mittelwert der Verteilung genau derjenige Wert von \\(\\Delta\\).\nAn dieser Stelle nochmal der Unterschied zwischen \\(\\mu\\) und \\(\\bar{x}\\). Der Mittelwert \\(\\mu\\) ist eine Eigenschaft der Population, also letztendlich ein Wert den wir niemals kennen werden ohne die gesamte Population zu untersuchen. Der Mittelwert \\(\\bar{x}\\) ist eine Eigenschaft der Stichprobe aus der Population. Also der konkrete Wert den wir anhand der Stichprobe berechnen. In vielen Fällen versuchen wir über \\(\\bar{x}\\) einen Rückschluss auf \\(\\mu\\) zu ziehen.\n\n\n8.2.2 Standardabweichung \\(\\sigma\\) der Population\nAls zweite Eigenschaft von Verteilungen schauen wir uns jetzt die Streuung in der Population an. Die Streuung in der Population wird als Varianz bezeichnet und wird mit dem Symbol \\(\\sigma^2\\) bezeichnet. Schauen wir uns zunächst an, welchen Einfluss \\(\\sigma^2\\) auf die Form der Verteilung hat. In Abbildung 8.7 sind wieder zwei Verteilungen abgetragen. Dieses Mal ist \\(\\mu\\) in beiden Fällen gleich, aber die Varianzen \\(\\sigma^2\\) sind mit \\(\\sigma_1^2 = 2\\) und \\(\\sigma_2^2=1\\) unterschiedlich.\n\n\n\n\n\nAbbildung 8.7: Verteilungen mit unterschiedlichen Varianzen\n\n\n\n\nIn Abbildung 8.7 ist zu sehen, dass beide Verteilungen ihren Mittelpunkt an der gleichen Stelle haben, aber die rote Verteilung mit \\(\\sigma_1^2=2\\) breiter ist als die andere Verteilung. Dies bedeutet das die Werte in der Verteilung stärker um den Mittelwert herum streuen. Wenn wir Werte aus der türkisen Verteilung ziehen, dann sollten diese näher um den Mittelwert \\(\\mu = 0\\) liegen, als dies bei der roten Verteilung der Fall ist.\nDie Varianz \\(\\sigma^2\\) ist ebenfalls wie der Mittelwert ein Parameter der Verteilung. Sie bestimmt die die Form der Verteilung. D.h. wenn wir wieder unsere Schreibweise von eben verwenden und die Funktion \\(f\\) die Verteilung beschreibt, dann gilt \\(f(\\sigma^2)\\) oder eben zusammen mit dem Mittelwert \\(\\mu\\), \\(f(\\mu, \\sigma^2)\\).\nWenn aus der Varianz \\(\\sigma^2\\) die Wurzel gezogen wird, dann wird der resultierende Wert \\(\\sigma\\) als Standardabweichung bezeichnet. Da die Varianz \\(\\sigma^2\\) nur positive Werte annehmen kann, ist die Wurzelfunktion bzw. deren Umkehrung die Quadierung eineindeutig. Wenn wir die Standardabweichung kennen, dann kennen wir auch die Varianz und umgekehrt.\nIn der Stichprobe wird die Standardabweichung meistens mit dem Zeichen \\(s\\) bezeichnet und mittels der folgenden Formel berechnet:\n\\[\\begin{equation}\ns = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}}\n\\label{eq-std}\n\\end{equation}\\]\nD.h. die Standardabweichung ist die mittlere quadrierte Abweichung vom Mittelwert (siehe Formel \\(\\eqref{eq-std}\\)). Die Standardabweichung wird verwendet um die Streuung der Daten zu beschreiben. Die Standardabweichung hat den Vorteil, dass sie die gleiche Einheit hat wie der Mittelwert. Da die Abweichungen quadriert werden, also die quadrierten Einheiten haben, hat die Standardabweichung \\(s\\) die gleiche Einheit wie der Mittelwert \\(\\bar{x}\\). Da die Varianz die quadrierte Standardabweichung ist, hat die Varianz der Stichprobe \\(s^2\\) daher die quadrierten Einheiten.\nWenn wir uns an unsere erstes Beispiel aus der kleinen Welt erinnern, dort hatten wir in der Kontrollgruppe die Personen \\(i = \\{3,8,9\\}\\) gezogen, berechnen wir für diese Stichprobe die Standardabweichung erhalten mit dem Mittelwert \\(\\bar{x} = 2198\\):\n\\[\ns = \\sqrt{\\frac{(2178-2198)^2+(2117-2198)^2+(2298-2198)^2}{2}} = 92\n\\]\nWir erhalten einen Wert von \\(s = 92N\\). Wenn dieser Wert größer wird, dann streuen die Wert entsprechend weiter um den Mittelwert herum und entsprechend verringert sich die Streuung wenn die Standardabweichung \\(s\\) abnimmt.\n\n\n8.2.3 Mittelwert und Standardabweichung in R\nUm den Mittelwert und die Standardabweichung bzw. die Varianz zu berechnen gibt in R entsprechende Funktionen die auf die Namen mean(), sd() und var().\n\nx &lt;- c(1,2,3,4,5)\nmean(x)\n\n[1] 3\n\nsd(x)\n\n[1] 1.581139\n\nvar(x)\n\n[1] 2.5"
  },
  {
    "objectID": "stats_significance.html#entscheidungen-und-mu-und-sigma",
    "href": "stats_significance.html#entscheidungen-und-mu-und-sigma",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.3 Entscheidungen und \\(\\mu\\) und \\(\\sigma\\)",
    "text": "8.3 Entscheidungen und \\(\\mu\\) und \\(\\sigma\\)\nZeichnen wir in eine Verteilung die Standardabweichung ein, ergibt sich folgendes Bild (siehe Abbildung 8.8).\n\n\n\n\n\nAbbildung 8.8: Verteilung mit verschiedenen mehrfachen der Standardabweichung \\(\\sigma\\)$\n\n\n\n\nEin Großteil der Werte liegt in dem Bereich \\(\\mu \\pm 1\\times\\sigma\\). Der Bereich \\(\\mu \\pm 2\\times\\sigma\\) beinhaltet schon fast alle Werte, während der Bereich \\(\\mu \\pm 3\\times\\sigma\\) fast alle Werte. Wenn wir die Verteilung noch etwas weiter nach links und rechts abtragen würden, würden wir sehen, dass auch noch Werte jenseits von \\(\\mu \\pm 3\\times\\sigma\\) liegen, aber nur noch sehr wenige. Diese Einsicht können wir dazu benutzen umgekehrt zu denken, wenn wir annehmen, das unsere Statistik dieser Verteilung folgt, welche Werte würde uns den überraschen. Welche Werte würden wir als Evidenz sehen um zu folgern: Ich glaube nicht, dass die beobachtete Statistik aus der angenommen Verteilung stammt!?\nNun, zum Beispiel wenn der Wert mehr als \\(3\\times\\sigma\\) vom Mittelwert \\(\\mu\\) entfernt ist, dann wäre das zwar nicht unmöglich, aber es wäre schon ziemlich unwahrscheinlich so einen Wert zu beobachten. Vielleicht ist uns das aber ein zu schwer zu erreichender Wert, ein Kompromiss könnte ein Wert jenseits von \\(2\\times\\sigma\\) von \\(\\mu\\) entfernt, könnte auch schon als überraschen bezeichnet werden. Tatsächlich ist, die Wahrscheinlichkeit einen Wert jenseits von \\(2\\times\\sigma\\) zu beobachten etwa 5%. D.h. wir könnten einen Entscheidungsprozess erstellen bei dem wir sagen, wenn wir eine bestimmte Stichprobenverteilung für unsere Statistik annehmen. Wenn wir bei unserer Ausführung einen Wert beobachten der weiter als \\(2\\times\\sigma\\) von \\(\\mu\\) entfernt sind. Dann sind wir überrascht und sehen das als Evidenz gegen die Verteilungsannahme an.\nOder als Liste:\n\nSetze eine Verteilung der Statistik mit definierten \\(\\mu\\) und \\(\\sigma\\) als Annahme an.\nZiehe eine Zufallsstichprobe.\nBerechne die Statistik auf der Stichprobe.\nÜberprüfe wie viele Standardabweichungen \\(\\sigma\\) die Statistik von \\(\\mu\\) entfernt liegt.\n\n\n8.3.1 Detour - Schätzer\nSchauen wir uns noch einmal den Mittelwert \\(\\mu\\) der Population und den Mittelwert \\(\\bar{x}\\) der Stichprobe und deren Zusammenhang an. Der Mittelwert \\(\\bar{x}\\) der Stichprobe wird als sogenannter Schätzer verwendet. Diesen Begriff werden wir später noch genauer untersuchen. Im Moment reicht es sich zu merken, dass ein Schätzer eine Statistik ist, mit der wir einen Parameter der Population, z.B. \\(\\mu\\), abschätzen wollen. Wie schon mehrmals erwähnt, den wahren Wert \\(\\mu\\) aus der Population werden wir mittels unserer Stichprobe niemals 100% korrekt bestimmen wir können aber mittels geschickt gewählter Statistiken Schätzer konstruieren die bestimmte Eigenschaften haben.\nNehmen wir zum Beispiel den Mittelwert \\(\\bar{x}\\). In unserer kleinen Welt kennen wir den Mittelwert \\(\\mu\\) unserer Population. Der Wert beträgt \\(\\mu = 2291.3\\). Schauen wir uns einmal an, was passiert, wenn wir alle möglichen Stichproben der Größe \\(N = 10\\) unserer kleinen Welt bestimmen und die Verteilung der Mittelwert abtragen (siehe Abbildung 8.9).\n\n\n\n\n\nAbbildung 8.9: Verteilung der Mittelwerte von Stichproben der Größe \\(n=10\\), Kleine Welt Population \\(\\mu\\) (rot)\n\n\n\n\nIn Abbildung 8.9 sehen wir, dass im Mittel der Stichprobenmittelwert \\(\\bar{x}\\) tatsächlich um den wahren Populationsmittelwert \\(\\mu\\) herum zentriert ist. Einzelne Ausgänge des Experiments können zwar daneben liegen, der Großteil der Experiment gruppiert sich jedoch um \\(\\mu\\) herum. Der Stichprobenmittelwert \\(\\bar{x}\\) ist daher eine gute Statistik um den tatsächlichen Populationsmittelwert \\(\\mu\\) abzuschätzen."
  },
  {
    "objectID": "stats_significance.html#welche-verteilung-setzen-wir-an",
    "href": "stats_significance.html#welche-verteilung-setzen-wir-an",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.4 Welche Verteilung setzen wir an?",
    "text": "8.4 Welche Verteilung setzen wir an?\nKommen wir aber wieder zurück zu unserem Ausgangsproblem, dass wir anhand unserer beobachteten Stichprobe etwas über die Effektivität der Kraftintervention aussagen wollen. Wie hilft uns jetzt die Kenntnis von Mittelwert \\(\\mu\\) oder \\(\\bar{x}\\) und der Standardabweichung \\(\\sigma\\) bzw. \\(s\\) weiter? Wenn die Verteilung unserer Statistik der Form folgt wie sie bisher jetzt mehrmals beobachtet haben, dann können wir davon ausgehen, dass wenn wir eher Wert in der Nähe des Mittelpunkts erwarten würden. Wie werden selten genau den Mittelpunkt beobachten aber wir würde schon sehr überrascht sein, wenn wir Werte weit ab des Mittelwerts beobachten würden. Ab welcher Weite diese Werte als überraschen eingestuft werden hängt dabei von der Streuung der Verteilung an. Wenn \\(\\sigma\\) groß ist, überraschen uns weit entfernte Werte weniger als wenn \\(\\sigma\\) klein ist.\n\n\n\n\n\nAbbildung 8.10: Welche Verteilung nehmen wir?\n\n\n\n\nSpielen wir verschiedene Möglichkeiten einmal durch. Wir vernachlässigen zunächst einmal \\(\\sigma\\) und konzentrieren uns auf \\(\\mu\\). Wir benötigen eine einzelne Referenzverteilung um unseren beobachteten Wert \\(\\Delta\\), den Unterschied zwischen den beiden Gruppen, mit der Verteilung in Beziehung zu setzen. Wir könnten zum Beispiel sagen, dass wir davon ausgehen, dass der Unterschied zwischen den beiden Gruppen \\(\\Delta_{\\text{wahr}} = 75N\\) ist. D.h. dies wäre der wahre Unterschied zwischen den beiden Gruppen. Wir treffen ihn nicht genau, da wir eine Zufallsstichprobe gezogen haben und die Stichprobenvariabilität dazu führt, dass wir nicht genau den Unterschied treffen. Allerdings, wird wieder einmal etwas starren auf den Wert \\(75N\\) zu der Einsicht führen, dass \\(75\\) vollkommen willkürlich ist. Warum nicht \\(85N\\) oder \\(25\\) oder warum überhaupt ganzzahlig, \\(\\pi\\) ist schließlich auch keine ganzzahlige Zahl, also könnten wir genauso gut \\(74.1234N\\) nehmen. Schnell wird daher klar, dass keine Zahl so richtig gut begründet werden kann. Wir brauchen aber eine Zahl um unseren Apparatus mit Verteilungen ansetzen zu können. Tatsächlich gibt es eine Zahl die zwar auch willkürlich ist, aber doch etwas besser begründet werden kann, nämlich die Zahl \\(\\Delta_{\\text{wahr}} = 0\\). Warum ist der Wert \\(0\\) in diesem Fall speziell. Nun, er bedeutet, dass wir davon ausgehen, dass zwischen den beiden Gruppen kein Unterschied besteht, also die Intervention überhaupt nichts gebracht hat. Dies ist zwar keine wirklich interessante Annahme, aber sie hat trotz ihr Willkürlichkeit doch etwas mehr Gewicht als eine beliebige andere Zahl. Wir bezeichnen diese Annahme jetzt auch noch als die \\(H_0\\)-Hypothese. Die \\(0\\) bei \\(H\\) bedeutet dabei nicht unbedingt, dass die \\(H_0\\) davon ausgeht, dass nicht passiert, sondern nur, das das unsere Ausgangsannahme ist. In vielen Fällen hat die \\(H_0\\) tatsächlich auch die Annahem das nichts passiert, dies muss aber nicht immer der Fall sein. Daher ist unsere Referenzverteilung für die Stichproben in unseren Fall die Hypothese (siehe Formel \\(\\eqref{eq-stats-sig-H0}\\)):\n\\[\\begin{equation}\nH_0: \\Delta = 0\n\\label{eq-stats-sig-H0}\n\\end{equation}\\]\noder graphisch (siehe Abbildung 8.11)\n\n\n\n\n\nAbbildung 8.11: Verteilung wenn nichts passiert mit den beiden Bereichen jenseits von zwei Standardfehlern ausgezeichnet.\n\n\n\n\nDiese Referenzverteilung können wir nun verwenden um eine Entscheidung bezüglich unseres beobachteten Werts zu treffen. Die Streuung in der Referenz- bzw. Stichprobenverteilung wird als Standardfehler bezeichnet im Gegensatz zur Streuung in der Population \\(\\sigma\\) und in der Stichprobe \\(s\\). Letztendlich ist der Standardfehler \\(s_e\\) nichts anderes als die Standardabweichung der Statistik.\n\nDefinition 8.2 (Standardfehler ) Die theoretische Streuung einer berechneten Statistik wird als Standardfehler bezeichnet und mit dem Symbol \\(\\sigma_e\\) gekennzeichnet. Wird dieser Wert anhand der Stichprobe abgeschätzt, dann hat der Standardfehler das Symbol \\(s_e\\)."
  },
  {
    "objectID": "stats_significance.html#statistisch-signifikanter-wert",
    "href": "stats_significance.html#statistisch-signifikanter-wert",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.5 Statistisch signifikanter Wert",
    "text": "8.5 Statistisch signifikanter Wert\nKommen wir nun zu dem wichtigen Konzept des statistisch signifikanten Werts. Im vorhergehend Abschnitt haben wir eine Stichprobenverteilung für unsere Statistik, den Unterschied zwischen den Mittelwerten der beiden Gruppen, hergeleitet. Wir gehen von der Verteilung aus, bei der es keinen Unterschied \\(H_0: \\Delta = 0\\) zwischen den beiden Gruppen gibt. \\(\\Delta=0\\) hat somit die Bedeutung, das das Krafttraining nicht effektiv war. Dazu haben wir als Kriterium hergeleitet, dass wir Werte die mehr als \\(2\\) Standardabweichungen von Mittelwert entfernt sind, als unwahrscheinlich ansehen, da diese Werte etwa eine Wahrscheinlichkeit von \\(5\\%\\) haben. Präziser, Werte die mehr als zwei Standardfehler vom Mittelwertsunterschied \\(\\Delta = 0\\) entfernt sind. Da, unserer angenommener Mittelwertsunterschied, die gemessene Statistik, mit \\(\\Delta = 0\\) zu \\(\\mu = 0\\) wird, bedeutet dies, das wir Werte die entweder kleiner als \\(-2\\times\\) Standardfehler oder größer als \\(2\\times\\) Standardfehler sind, als unwahrscheinlich unter der Annahme von \\(H_0: \\mu = 0\\) betrachten. Als Entscheidungsregel folgt somit:\n\\[\n|\\text{beobachteter Wert }| &gt; 2\\times \\sigma_e \\Rightarrow \\text{ Evidenz gegen } H_0\n\\]\n\n\n\n\n\nflowchart TD\n    A[Statistik T] --&gt; B{Entscheidung: T &gt; 2xs_e}\n    B --&gt; D(Nein)\n    D --&gt; E[H0 beibehalten]\n    B --&gt; F(Ja)\n    F --&gt; G[H0 ablehnen]\n\n\nAbbildung 8.12: Entscheidungsregel zur \\(H_0\\)\n\n\n\n\nIn Abbildung 8.13 ist die Entscheidungsregel noch einmal graphisch dargestellt. Wir bestimmen eine Stichprobenverteilung unter der \\(H_0\\), beispielsweise \\(H_0: \\mu = \\Delta = 0\\) und schneiden nun rechts und links jeweils einen Bereich der Verteilung ab, den wir als unwahrscheinlich unter dieser speziellen \\(H_0\\) ansehen. Diesen Bereich bezeichnen wir als kritischen Bereich. Wenn unser beobachteter Wert im kritischen Bereich liegt, dann sehen wir diese Beobachtung als Evidenz gegen die Korrektheit der Annahme , dass die \\(H_0\\) gilt, an.\n\n\n\n\n\nAbbildung 8.13: Die \\(H_0\\) Verteilung wenn nichts passiert unterteilt in Regionen die zur Entscheidung für die \\(H_0\\) (grün) und gegen die \\(H_0\\) (rot, kritische Regionen) führen.\n\n\n\n\nWenn der Stichprobenwert der Statistik in der kritischen Region auftritt, dann wird von einem statistisch signifikanten Effekt gesprochen. Unter der \\(H_0\\) bin ich überrascht diesen Wert zu sehen! Allerdings, dieser Wert ist nicht unmöglich, sondern lediglich unwahrscheinlich wenn die Annahme \\(H_0\\) korrekt ist. Unwahrscheinlich ist dabei kein absolutes Maß, sondern nur eine willkürliche Festsetzung die wir selbst getroffen haben.\nWir hatten vorhin vorhin gesagt, dass Werte jenseits von \\(2\\times \\sigma_e\\) etwa eine Wahrscheinlichkeit von \\(5\\%\\) unter der \\(H_0\\) haben. Dies Bedeutet, dass die Wahrscheinlichkeit Werte im kritischen Bereich zu beobachten bei etwas \\(5%\\) liegt, wenn die \\(H_0\\) zutrifft. Oder anders, wenn die \\(H_0\\) in der Realität zutrifft, also den DGP korrekt beschreibt, und ich das Experiment \\(100\\times\\) wiederhole, dann würde ich etwa \\(5\\) Experimente erwarten bei denen der beobachtete Wert im kritischen Bereich liegt. Anhand unserer Entscheidungsregel entscheide ich mich in diesen \\(5\\) Fällen nun gegen die \\(H_0\\), obwohl diese zutrifft. D.h. in diesen \\(5\\) Fällen würde mich irren. Daher wird die Wahrscheinlichkeit die ich benutze um einen kritschen Bereich ausweisen als Irrtumswahrscheinlichkeit bezeichnet. Da die Irrtumswahrscheinlichkeit ein zentrales Konzept in der Statistik ist, erhält sie ein eigenes Symbol \\(\\alpha\\).\n\nDefinition 8.3 (Irrtumswahrscheinlichkeit \\(\\alpha\\) ) Die Wahrscheinlichkeit mit der fälschlicherweise eine korrekte \\(H_0\\)-Hypothese abgelehnt wird, wird als Irrtumswahrscheinlichkeit bezeichnet. Die Irrtumswahrscheinlichkeit wird mit Symbol \\(\\alpha\\) bezeichnet und auch als Fehler I. Art bezeichnet.\n\nEines der grundlegenden Probleme, das oftmals nicht beachtet wird bei der Interpretation von statistisch signifikanten Ergebnis bezieht sich darauf, dass ich nicht weiß, welches der \\(100\\) Experimente ich gerade durchgeführt habe. Es ist durchaus möglich, dass ich Pech gehabt habe und ausgerechnet mein Experiment eines der fünf Experimente ist.\nEine weitere Missinterpretation ist der Irrtumswahrscheinlichkeit ist, dass Sie eine Ausage über die Wahrscheinlichkeit des Zutreffens der \\(H_0\\) erlaubt. Die Irrtumswahrscheinlichkeit ermöglichst dies allerdings nicht. Ob die \\(H_0\\) zutrifft hat die Wahrscheinlichkeit entweder \\(P(H_0) = 1\\) oder \\(P(H_0) = 0\\). Entweder sie trifft zu oder eben nicht. Darüber wird hier keine Aussage gemacht, sondern nur ob unter der Annahme das \\(H_0\\) zutrifft, der beobachtete Wert in einem wahrscheinlichen oder einem unwahrscheinlichen Bereich liegt. Nochmal, was wahrscheinlich ist wurde durch eine willkürliche Festlegung bestimmt. Die gewählte Grenze ist keine physikalische Realität!\nKommen wir nun zum nächsten oft missverstandenen Term, dem p-Wert."
  },
  {
    "objectID": "stats_significance.html#der-p-wert",
    "href": "stats_significance.html#der-p-wert",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.6 Der p-Wert",
    "text": "8.6 Der p-Wert\nFangen wir dieses Mal mit der Definition an. Da wir mittlerweile hoffentlich schon einiges an Intuition aufgebaut haben, sollte die Definition einigermaßen verständlich sein.\n\nDefinition 8.4 (p-Wert ) Der p-Wert gibt die Wahrscheinlichkeit für den beobachteten oder einen noch extremeren Wert unter der \\(H_0\\) an.\n\nIn Abbildung 8.14 ist eine Verteilung unter der \\(H_0\\) eingzeichnet, zusammen mit den kritischen Bereichen für gegebenes \\(\\alpha\\) und der beobachtete Wert.\n\n\n\n\n\nAbbildung 8.14: Der beobachtete Wert der Statistik (schwarzer Punkt) zusammen mit der Verteilung unter der \\(H_0\\). Die gelben Flächen zeigen den p-Wert für den Wert der beobachteten Statistik an.\n\n\n\n\nDer p-Wert ist die Wahrscheinlichkeit (gelbe Fläche), unter der \\(H_0\\), für den beobachteten oder einen extremeren Wert. Ein extremere Wert bedeutet in diesem Fall einen größeren Wert, also alle Werte rechts vom beobachteten Wert. Jetzt irritiert allerdings, dass wir auf der linken Seite ebenfalls eine gelbe Fläche haben. Was hier passiert ist, ist dass der beobachtete Wert an \\(\\mu\\) in den anderen kritischen Bereich gespiegelt (salopp) wurde. Jetzt wird wieder das gleiche Prinzip mit dem extremeren Wert angewendet. Hier bedeutet allerdings extremer links vom beobachteten Wert. Wir erhalten dann wieder eine Fläche und somit eine Wahrscheinlichkeit. Die beiden gelben Flächen zusammen ergeben dann den p-Wert. Das die Wahrscheinlichkeit für eine Seite dazugenommen wird bei der wir gar keinen Wert beobachtet haben wird später verständlich wenn wir den unterschied zwischen gerichteten und ungerichteten Hypothesen uns anschauen. Zur Begrifflichkeit extrem nochmal können wir aber schon mal zusammenfassen, dass extrem immer in Bezug auf das \\(\\mu\\) der Stichprobenverteilung zu verstehen ist.\nIn Abbildung 8.15 sind verschiedene Beispiele für beobachtete Werte und den dazugehörenden p-Werten und deren Flächen abgebildet.\n\n\n\n\n\nAbbildung 8.15: Verschiedene P-Werte und die dazugehörenden Flächen.\n\n\n\n\nKönnen wir eigentlich den p-Wert und die Irrtumswahrscheinlichkeit in irgendeiner Form zusammenbringen? Ja, wenn wir wissen, dass die beobachtete Statistik einen p-Wert von kleiner \\(\\alpha\\) hat, dann haben wir automatisch ein statistisch signifikantes Ergebnis. Wenn euch das nicht auf Anhieb einleuchtet, dann schaut euch noch mal Abbildung 8.14 an. Welche Wahrscheinlichkeit hat \\(\\alpha\\) (Tip: Welche Flächen sind das?) und welche Wahrscheinlichkeit hat der p-Wert (Tip: Gelb?).\nDa der p-Wert eines der am meisten missverstandenen Konzepte ist, hier noch mal ein paar Statements und Erklärungen rund um den p-Wert von verschiedenen Autoren und Institutionen.\n“[A] p-value is the probability under a specified statistical model that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” (Wasserstein und Lazar 2016, p.131)\n“[T]he P value is the probability of seeing data that are as weird or more weird than those that were actually observed.” (Christensen 2018, p.38)\n\n8.6.1 Signifikanter Wert - Das Kleingedruckte\n\nVor dem Experiment wird für ein \\(H_0\\) ein \\(\\alpha\\)-Level angesetzt (per Konvention \\(\\alpha=0,05 = 5\\%\\))\nAnhand des \\(\\alpha\\)-Levels können kritische Werte (\\(k_{lower}, k_{upper}\\)) bestimmt werden. Diese bestimmen die Grenzen der kritischen Regionen.\nWenn der gemessene Wert w der Statistik in die kritische Region fällt, also \\(w \\leq k_{lower}\\) oder \\(w \\geq k_{upper}\\) gilt, dann wird von einem statistisch signifikanten Wert gesprochen und die dazugehörige Hypothese wird abgelehnt. Äquivalent: Der p-Wert ist kleiner als \\(\\alpha\\).\nDa in \\(\\alpha\\)-Fällen ein Wert in der kritischen Region auftritt, auch wenn die \\(H_0\\) zutrifft, wird in \\(\\alpha\\)-Fällen ein \\(\\alpha\\)-Fehler gemacht.\nWenn der Wert w der Statistik nicht in den kritischen Regionen liegt, oder gleichwertig der p-Wert größer als \\(\\alpha\\) ist, wird die \\(H_0\\) beibehalten. D.h. nicht, dass kein Effekt vorliegt, sondern lediglich, dass anhand der Daten keine Evidenz diesbezüglich gefunden werden konnte!\nDie statistische Signifikanz sagt nichts über die Wahrscheinlichkeit der Theorie aus!\nEin p-Wert von \\(p = 0.0001\\) heißt nicht, dass mit 99,99% Wahrscheinlichkeit ein Effekt vorliegt!\nStatistisch signifikant heißt nicht automatisch praktisch relevant!\n\nUnd noch ein paar weitere Erklärung für den p-Wert nach Wasserstein und Lazar (2016)\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.\nProper inference requires full reporting and transparency\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\nIn Abbildung 8.16 ist ein kurzer Abschnitt aus Altman und Bland (1995) abgebildet, der noch mal auf eine weitere Missinterpretation eines statisisch signifikanten Ergebnisses eingeht, also wenn p-Wert \\(&lt;\\alpha\\) gilt.\n\n\n\n\n\nAbbildung 8.16: Ausschnitt aus Altman et al. (1995)\n\n\n\n\nKurz gesagt, wenn wir kein statistisches Ergebnisse gefunden haben, bedeutet dies nicht, das es keinen Unterschied gibt. Tatsächlich wird der beobachtete Wert der Statistik praktisch nie exakt \\(=0\\) sein und wir werden daher praktisch immer einen Unterschied finden. Allerdings ist der beobachtete Unterschied nicht überraschend unter der \\(H_0\\) auf Grund der Stichprobenvariabilität. Allerdings gilt trotzdem, die Abwesenheit von Evidenz ist nicht gleichzusetzen mit der Evidenz für Abwesenheit.\nDas gibt uns auch einen schönen Aufschlag für die nächste Etappe. Was passiert eigentlich, wenn die andere Hypothese, also nicht die \\(H_0\\) zutrifft."
  },
  {
    "objectID": "stats_significance.html#was-passiert-wenn-eine-andere-hypothese-zutrifft",
    "href": "stats_significance.html#was-passiert-wenn-eine-andere-hypothese-zutrifft",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.7 Was passiert wenn eine “andere” Hypothese zutrifft?",
    "text": "8.7 Was passiert wenn eine “andere” Hypothese zutrifft?\nIn Abbildung 8.17 ist neben der uns schon bekannten Stichprobenverteilung unter der \\(H_0\\) für unsere kleine Welt eine weitere Verteilung unter einer Alternativhypothese die wir mit \\(H_1\\) bezeichnen. Unter der \\(H_1\\) ist der wahre Unterschied zwischen den beiden Verteilungen \\(\\Delta = 500N\\).\n\n\n\n\n\nAbbildung 8.17: Differenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von \\(\\alpha\\) wenn \\(H_0\\) zutrifft und unter einer Alternativhypothese \\(H_1: \\Delta = 500N\\).\n\n\n\n\nIn Abbildung 8.17 ist zu sehen, dass sich die beiden Verteilungen überschneiden. Der krititische Bereich unter der \\(H_0\\) fängt etwa bei \\(500N\\) an, so dass der Bereich zwischen etwa \\(100-400N\\) unter beiden Verteilungen wohl relativ wahrscheinlich ist. D.h. wenn wir einen Wert in diesem Bereich beobachten würden, dann könnten wir nicht wirklich trennscharf argumentieren aus welcher Stichprobenverteilung der Wert tatsächlich stammt. In Abbildung 8.18 haben wir den Bereich unter der \\(H_1\\) der links des krititschen Bereichs von \\(H_1\\) liegt grün eingefärbt. Da es sich hier wieder um eine Fläche handelt, bestimmt diese Fläche eine Wahrscheinlichkeit. Wie könnte diese Wahrscheinlichkeit verbal beschrieben werden?\n\n\n\n\n\nAbbildung 8.18: Differenzen mit kritischen Regionen (rot) mit einer Wahrscheinlichkeit von \\(\\alpha\\) wenn \\(H_0\\) zutrifft und \\(\\beta\\) (grün) wenn \\(H_1\\) zutrifft.\n\n\n\n\nDie Werte zwischen den beiden krititschen Bereichen beschreiben diejenigen Werte bei denen wir die \\(H_0\\) beibehalten würden, da wir diese Werte nichts als überraschend unter der \\(H_0\\) einschätzen. Dementsprechend, wenn in der Realität allerdings die \\(H_1\\) Hypothese zutreffen würde, würden wir uns irren. Daher beschreibt die grüne Fläche in Abbildung 8.18 ebenfalls Wahrscheinlichkeit sich zu irren. Aber dieses Mal wenn die Alternativhypothese \\(H_1\\) zutrifft. Diese Irrtumswahrscheinlichkeit bezeichent man als \\(\\beta\\)-Wahrscheinlichkeit.\n\nDefinition 8.5 (\\(\\beta\\)-Wahrscheinlichkeit) Die \\(\\beta\\)-Wahrscheinlichkeit beschreibt die Wahrscheinlichkeit sich gegen die Alternativhypothese \\(H_1\\) zu entscheiden wenn diese zutrifft. Die \\(\\beta\\)-Wahrscheinlichkeit wird auch als Fehler II. Art bezeichnet.\n\nWenn wir jetzt das Komplement der grüne Flächen nehmen, dann erhalten wir wiederum eine Wahrscheinlichkeit. Abbildung 8.19 ist diese Fläche blau eingezeichnet.\n\n\n\n\n\nAbbildung 8.19: \\(1-\\beta\\) = Power des Tests (blaue Fläche).\n\n\n\n\nDiese Fläche beschreibt die Wahrscheinlichkeit sich für die Alternativhypothese \\(H_1\\) zu entscheiden, wenn diese auch tatsächlich zutrifft und wird als die Power bezeichnet.\n\nDefinition 8.6 (Power ) Die Power bezeichnet die Wahrscheinlichkeit sich für die Alternativhypothese \\(H_1\\) zu entscheiden, wenn die in der Realität zutrifft.\n\nNochmal zu Abbildung 8.19, die Werte unter der blauen Fläche werden mit einer Wahrscheinlichkeit beobachtet die derjenigen der blauen Fläche entspricht. Jedes Mal wenn so ein Wert eintritt, dann liegt dieser im kritischen Bereich unter der \\(H_0\\) (rote Fläche) und wir entscheiden uns gegen die \\(H_0\\).\nZusammengefasst haben wir die folgende Liste bezüglich der Terme, \\(\\alpha\\), \\(\\beta\\) und Power: - \\(\\alpha\\): Die Wahrscheinlichkeit sich gegen die \\(H_0\\) zu entscheiden, wenn die \\(H_0\\) zutrifft. \\(\\alpha\\)-Level wird vor dem Experiment festgelegt um zu kontrollieren welche Fehlerrate toleriert wird. - \\(\\beta\\): Die Wahrscheinlichkeit sich gegen die \\(H_1\\) zu entscheiden, wenn die \\(H_1\\) zutrifft. - Power := \\(1 - \\beta\\): Die Wahrscheinlichkeit sich für die \\(H_1\\) zu entscheiden, wenn die \\(H_1\\) zutrifft. Sollte ebenfalls vor dem Experiment festgelegt werden.\nFassen wir auch noch mal kurz die möglichen Entscheidungen und Konsequenzen in einer Tabelle zusammen (siehe ?tbl-stats-sigpower).\n\n\nTabelle 8.1: Entscheidungsmöglichen und Fehlerarten\n\n\nEntscheidung\\Realität\n\\(H_0\\)\n\\(H_1\\)\n\n\n\n\n\\(H_0\\)\nkorrekt\n\\(\\beta\\)\n\n\n\\(H_1\\)\n\\(\\alpha\\)\nkorrekt"
  },
  {
    "objectID": "stats_significance.html#wie-können-wir-die-power-erhöhen",
    "href": "stats_significance.html#wie-können-wir-die-power-erhöhen",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.8 Wie können wir die Power erhöhen?",
    "text": "8.8 Wie können wir die Power erhöhen?\n\n\n\n\n\nVerteilungen wenn \\(\\delta\\)=500 und \\(\\delta\\)=0 in unserem kleine Welt Beispiel mit n = 3."
  },
  {
    "objectID": "stats_significance.html#stichprobengröße-von-n-3-auf-n-9-erhöhen",
    "href": "stats_significance.html#stichprobengröße-von-n-3-auf-n-9-erhöhen",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.9 Stichprobengröße von n = 3 auf n = 9 erhöhen?",
    "text": "8.9 Stichprobengröße von n = 3 auf n = 9 erhöhen?\n\n\n\n\n\nStichprobenverteilungen der Differenz unter \\(H_0\\) und \\(H_1:\\delta=500\\)N bei einer Stichprobengröße von n = 9"
  },
  {
    "objectID": "stats_significance.html#standardfehler-1",
    "href": "stats_significance.html#standardfehler-1",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "8.10 Standardfehler",
    "text": "8.10 Standardfehler\nDie Standardabweichung der Stichprobenverteilung wird als Standardfehler \\(s_e\\) bezeichnet1. Der Standardfehler ist nicht gleich der Standardabweichung in der Population bzw. der Stichprobe. Es gilt für den Mittelwert:\n\n\n\n\nAltman, Douglas G, und J Martin Bland. 1995. „Statistics notes: Absence of evidence is not evidence of absence“. Bmj 311 (7003): 485.\n\n\nChristensen, Ronald. 2018. Analysis of variance, design, and regression: Linear modeling for unbalanced data. CRC Press.\n\n\nCohen, Jacob. 1988. Statistical power analysis for the behavioral sciences. 2. Aufl. Routledge.\n\n\nWasserstein, Ronald L, und Nicole A Lazar. 2016. „The ASA statement on p-values: context, process, and purpose“. Taylor & Francis."
  },
  {
    "objectID": "stats_significance.html#footnotes",
    "href": "stats_significance.html#footnotes",
    "title": "8  Statistische Signifikanz, p-Wert und Power",
    "section": "",
    "text": "Der Standardfehler schätzt die Reliabilität der Statistik ab (Cohen (1988))↩︎"
  },
  {
    "objectID": "stats_estimation.html",
    "href": "stats_estimation.html",
    "title": "9  Parameterschätzung",
    "section": "",
    "text": "10 Parameterschätzung"
  },
  {
    "objectID": "stats_estimation.html#problem-bei-einer-dichotomen-betrachtung-der-daten",
    "href": "stats_estimation.html#problem-bei-einer-dichotomen-betrachtung-der-daten",
    "title": "9  Parameterschätzung",
    "section": "10.1 Problem bei einer dichotomen Betrachtung der Daten",
    "text": "10.1 Problem bei einer dichotomen Betrachtung der Daten\n\n\n\n\n\nAbbildung 10.1: Auszug aus Cumming (2013, p.1)"
  },
  {
    "objectID": "stats_estimation.html#wie-groß-ist-der-effekt",
    "href": "stats_estimation.html#wie-groß-ist-der-effekt",
    "title": "9  Parameterschätzung",
    "section": "10.2 Wie groß ist der Effekt?",
    "text": "10.2 Wie groß ist der Effekt?\n\n\n\n\n\nStichprobenverteilungen der Differenz unter \\(H_0\\) und \\(H_1:\\delta=500\\)N bei einer Stichprobengröße von n = 9"
  },
  {
    "objectID": "stats_estimation.html#schätzung-der-populationsparameter",
    "href": "stats_estimation.html#schätzung-der-populationsparameter",
    "title": "9  Parameterschätzung",
    "section": "10.3 Schätzung der Populationsparameter",
    "text": "10.3 Schätzung der Populationsparameter\nKleine Welt: Experiment wird einmal mit n = 9 durchgeführt\n\n10.3.1 Beobachtete Stichprobenkennwerte\n\\[\\begin{align*}\nd = \\bar{x}_{treat} - \\bar{x}_{con} &= 350 \\\\\ns &= 132 \\\\\ns_e &= 44\n\\end{align*}\\]\nWie präzise ist meine Schätzung und welche anderen Unterschiedswerte sind anhand der beobachteten Daten noch plausibel?"
  },
  {
    "objectID": "stats_estimation.html#welche-deltas-sind-plausibel-für-d-350",
    "href": "stats_estimation.html#welche-deltas-sind-plausibel-für-d-350",
    "title": "9  Parameterschätzung",
    "section": "10.4 Welche \\(\\delta\\)s sind plausibel für \\(d = 350\\)?",
    "text": "10.4 Welche \\(\\delta\\)s sind plausibel für \\(d = 350\\)?\n\n\n\n\n\nVerschiedene Verteilungen von Gruppendifferenzen, beobachteter Unterschied (rot)\n\n\n\n\nPlausibel unter einem gegebenem \\(\\alpha\\)-Level!"
  },
  {
    "objectID": "stats_estimation.html#alle-möglichen-deltas-die-plausibel-sind",
    "href": "stats_estimation.html#alle-möglichen-deltas-die-plausibel-sind",
    "title": "9  Parameterschätzung",
    "section": "10.5 Alle möglichen \\(\\delta\\)s die plausibel sind",
    "text": "10.5 Alle möglichen \\(\\delta\\)s die plausibel sind\n\n\n\n\n\nKonfidenzintervall (grün), Populationsparameter \\(\\delta\\) und \\(\\alpha\\)-Level für die beobachtete Differenz (gelb)."
  },
  {
    "objectID": "stats_estimation.html#was-passiert-wenn-ich-das-experiment-ganz-oft-wiederhole",
    "href": "stats_estimation.html#was-passiert-wenn-ich-das-experiment-ganz-oft-wiederhole",
    "title": "9  Parameterschätzung",
    "section": "10.6 Was passiert wenn ich das Experiment ganz oft wiederhole?",
    "text": "10.6 Was passiert wenn ich das Experiment ganz oft wiederhole?\n\n\n\n\n\nSimulation von \\(n = 100\\) Konfidenzintervallen."
  },
  {
    "objectID": "stats_estimation.html#konfidenzintervall---das-kleingedruckte",
    "href": "stats_estimation.html#konfidenzintervall---das-kleingedruckte",
    "title": "9  Parameterschätzung",
    "section": "10.7 Konfidenzintervall - Das Kleingedruckte",
    "text": "10.7 Konfidenzintervall - Das Kleingedruckte\n\nDas Konfidenzintervall für ein gegebenes \\(\\alpha\\)-Niveau gibt nicht die Wahrscheinlichkeit an mit der der wahre Parameter in dem Intervall liegt.\nDas Konfidenzintervall gibt alle mit den Daten kompatiblen Populationsparameter an.\nDas \\(\\alpha\\)-Niveau des Konfidenzintervalls gibt an bei welchem Anteil von Wiederholungen davon auszugehen ist, das das Konfidenzintervall den wahren Populationsparameter enthält."
  },
  {
    "objectID": "stats_estimation.html#konfidenzintervall-herleiten-nach-spiegelhalter2019-p.241",
    "href": "stats_estimation.html#konfidenzintervall-herleiten-nach-spiegelhalter2019-p.241",
    "title": "9  Parameterschätzung",
    "section": "10.8 Konfidenzintervall herleiten nach Spiegelhalter (2019, p.241)",
    "text": "10.8 Konfidenzintervall herleiten nach Spiegelhalter (2019, p.241)\n\nWe use probability theory to tell us, for any particular population parameter, an interval in which we expect the observed statistic to lie with 95% probability.\nThen we observe a particular statistic.\nFinally (and this is the difficult bit) we work out the range of possible population parameters for which our statistic lies in their 95% intervals. This we call a “95% confidence interval”.\nThis resulting confidence interval is given the label “95%” since, with repeated application, 95% of such intervals should contain the true value.1\n\nAll clear? If it isn’t, then please be reassured that you have joined generations of baffled students."
  },
  {
    "objectID": "stats_estimation.html#konfidenzintervall-berechnen-vorschau",
    "href": "stats_estimation.html#konfidenzintervall-berechnen-vorschau",
    "title": "9  Parameterschätzung",
    "section": "10.9 Konfidenzintervall berechnen (Vorschau)",
    "text": "10.9 Konfidenzintervall berechnen (Vorschau)\n\\[\n\\textrm{CI}_{1-\\alpha} = \\bar{x} \\pm z_{\\alpha/2} \\times s_e\n\\]"
  },
  {
    "objectID": "stats_estimation.html#dualität-von-signifikanztests-und-konfidenzintervall",
    "href": "stats_estimation.html#dualität-von-signifikanztests-und-konfidenzintervall",
    "title": "9  Parameterschätzung",
    "section": "10.10 Dualität von Signifikanztests und Konfidenzintervall",
    "text": "10.10 Dualität von Signifikanztests und Konfidenzintervall\nWenn das Konfidenzintervall mit Niveau \\(1-\\alpha\\%\\) die \\(H_0\\) nicht beinhaltet, dann wird auch bei einem Signifikanztest die \\(H_0\\) bei einer Irrtumswahrscheinlichkeit von \\(\\alpha\\) abgelehnt.\n\n\n\n\nCumming, Geoff. 2013. Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis. Routledge.\n\n\nSpiegelhalter, David. 2019. The art of statistics: learning from data. Penguin UK."
  },
  {
    "objectID": "stats_estimation.html#footnotes",
    "href": "stats_estimation.html#footnotes",
    "title": "9  Parameterschätzung",
    "section": "",
    "text": "Strictly speaking, a 95% confidence interval does not mean there is a 95% probability that this particular interval contains the true value […]↩︎"
  },
  {
    "objectID": "stats_distributions.html",
    "href": "stats_distributions.html",
    "title": "10  Theoretische Verteilungen",
    "section": "",
    "text": "11 Verteilungszoo"
  },
  {
    "objectID": "stats_distributions.html#die-verteilung---1.-deep-dive",
    "href": "stats_distributions.html#die-verteilung---1.-deep-dive",
    "title": "10  Theoretische Verteilungen",
    "section": "10.1 Die Verteilung - 1. deep dive",
    "text": "10.1 Die Verteilung - 1. deep dive\nWir versuchen jetzt als erstes zu Verstehen was nochmal genau der Graph der Verteilung bedeutet. Auf der x-Achse werden die verschiedenen möglichen Werte der jeweiligen Statistik abgebildet. In unserem bisherigen Beispiel was das die Unterschiede \\(D\\) zwischen der Kontroll- und der Treatmentgruppe. Der Wert auf der y-Achse was zunächst die relative Häufigkeit was auch Sinn gemacht hatte, da wir nur eine bestimmte endliche Anzahl von möglichen Unterschieden \\(D\\) (ihr erinnert auch an die Zahl) vorliegen hatten. Was passiert aber wenn wir tatsächlich eine kontiuierliche Statistik haben, also eine Statistik die alle Werte innerhalb eines Intervalls einnehmen kann. Um den Fall zu verstehen fangen wir aber erst mal wieder mit einem einfachen Modell an.\n\n10.1.1 Der Münzwurf\nWir fangen mit dem einfachsten Experiment an: dem Münzwurf. Beim Münzwurf haben wir zwei mögliche Ausgänge unseres Experiments, entweder Kopf oder Zahl. Wir gehen von einer perfekten Münze aus, d.h. die Münze ist vollkommen symmetrisch auf beiden System und keine der Seiten ist in irgendeiner Form schwere oder beeinflusst in einer Art den Ausgang.\nWenn wir uns an die Schule zurück erinnern, dann haben wir in Wahrscheinlichkeitstheorie schon mal was gehört, das im Fall gleichwahrscheinlicher Ereignisse die Wahrscheinlichkeit für ein bestimmtes Ereignis, mittels der Anzahl der vorteilhaften Ausgänge geteilt durch die Anzahl der möglichen Ausgänge berechnet wird. Also beim einmaligen Münzwurf haben wir zwei Ausgänge \\(\\{\\text{Kopf}, \\text{Zahl}\\}\\) und jeweils nur vorteilhaften Ausang als entweder Kopf oder Zahl, daher folgt daraus.\n\\[\\begin{align}\nP(\\text{Kopf}) &= \\frac{1}{2} \\\\\nP(\\text{Zahl}) &= \\frac{1}{2}\n\\end{align}\\]\nWenn wir das jetzt als Graphen in Form einer Wahrscheinlichkeitsverteilung abtragen, dann sieht das noch wenig interessant aus (siehe Abbildung 10.1). Das Muster ist aber trotzdem wichtig, damit wir später wissen worauf wir hier eigentlich schauen. Auf der x-Achse haben wir die möglichen Ausgänge, Kopf oder Zahl, und auf der y-Achse haben wir die Wahrscheinlichkeit abgetragen.\n\n\n\n\n\nAbbildung 10.1: Wahrscheinlichkeitsverteilung des einmaligen Münzwurfes\n\n\n\n\nDa sich mit einem Münzwurf aber so wenig anfangen lässt, machen wir das Ganze jetzt etwas komplizierter und schauen uns an, wie unser Experiment aussieht wenn wir zwei Münzwwürfe uns anschauen. Rein operational, wir schmeißen unsere Münze in die Luft, schreiben uns das Ergebnis auf, und machen das Ganze noch ein zweites Mal und schreiben uns das Ergebnis auf. D.h. was auch immer im ersten Durchgang passiert, hat keine Auswirkungen auf das Ergebnis des zweiten Wurfs. Wir könnten auch zwei Münzen nehmen und beide gleichzeitig in die Luft werfen. Das wäre das gleiche Experiment. Welche Ausgänge haben wir jetzt beim zweimaligen Münzwurf? Zunächst einmal haben wir jetzt nicht mehr nur einen einzelnen Ausgang sondern wir haben ein Ausgangstupel, eine Liste mit zwei Elementen. Etwas motiviertes krizteln auf einem Schmierblatt wird wahrscheinlich relativ schnell zu folgender Tabelle führen (siehe Tabelle 10.1)\n\n\nTabelle 10.1: Mögliche Ausgänge bei einem zweimaligen Münzwurf\n\n\nAusgang 1. Wurf\nAusgang 2. Wurf\nTupel\n\n\n\n\nKopf\nKopf\n(Kopf, Kopf)\n\n\nKopf\nZahl\n(Kopf, Zahl)\n\n\nZahl\nKopf\n(Zahl, Kopf)\n\n\nZahl\nZahl\n(Zahl, Zahl)\n\n\n\n\nJetzt können wir uns wieder fragen, was die Wahrscheinlichkeit für die jeweiligen Ereignistupel ist. Eine direkte Methode wäre, wieder mittels der Symmetrie zu argumentieren. Es gibt vier verschiedene Ausgänge von denen jetzt keiner in irgendeiner Weise bevorzugt ist, daraus würde folgen das alle vier Ausgänge eine Wahrscheinlichkeit von \\(P = \\frac{1}{4}\\) haben.\nEine weitere Möglichkeit wäre mit den Wahrscheinlichkeiten aus dem einfachen Wurf an das Problem heran zu gehen. Wir betrachten die beiden Münzwürfe jetzt wieder sequentiell (siehe Abbildung 10.2). Im ersten Schritt können wir entweder Kopf oder Zahl beobachten. Beide Wahrscheinlichkeiten sind \\(P = \\frac{1}{2}\\). Darauf folgend können wir wieder zwei verschiedene Ausgänge beobachten, eben Kopf oder Zahl, wieder mit der Wahrscheinlichkeit \\(P = \\frac{1}{2}\\).\n\n\n\n\n\nflowchart TD\n    A[Start] --&gt; B(Kopf)\n    A --&gt; C(Zahl)\n    B --&gt; D(Kopf)\n    B --&gt; E(Zahl)\n    C --&gt; F(Kopf)\n    C --&gt; G(Zahl)\n\n\nAbbildung 10.2: Auswahlmöglichkeiten beim sequentiellen zweimaligen Münzwurf\n\n\n\n\nDa die Münzwürfe voneinander unabhängig sind und keinen Einfluss aufeinander ausüben, folgt daraus, dass die Wahrscheinlichkeiten für jede spezielle Folge von Kopf oder Zahl sich berechnet nach:\n\\[\nP(\\text{Ausgang}) = P(\\text{1. Wurf}) \\times P(\\text{2. Wurf})\n\\tag{10.1}\\]\nAlso in unseren Fall:\n\\[\nP(\\text{Ausgang}) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\n\\tag{10.2}\\]\nWomit wir wieder beim gleichen Ergebnis wie vorher angekommen sind. Der Vorteil dieser Herangehensweise ist jedoch, dass wir damit eine einfache Möglichkeit gefunden haben das Ergebnis auf mehr als nur zwei Würfe zu verallgemeinern. Nehmen wir zum Beispiel den dreifachen Münzwurf, dann können wir die Wahrscheinlichkeit für die Folge \\(P(\\text{KKZ}) = \\frac{1}{2}\\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8}\\) direkt angeben.\nBleiben wir aber erst noch mal kurz beim zweimaligen Münzwurf und schauen uns die Wahrscheinlichkeitsverteilung an. Hier stoßen wir nämlich auf ein Problem in der Darstellung. Wenn wir bei dem Muster aus Abbildung 10.1 bleiben wollen und auf der x-Achse die möglichen Ergnisse und auf der y-Achse die dazugehörende Wahrscheinlichkeit abtragen wollen, dann ist nicht ganz klar wie wir die Ergebnisse ordnen sollen. Eine mögliche Lösung ist in Abbildung 10.3 zu sehen.\n\n\n\n\n\nAbbildung 10.3: Wahrscheinlichkeitsverteilung des zweimaligen Münzwurfes (K: Kopf, Z: Zahl)\n\n\n\n\nDies ist natürlich nicht die einzige Möglichkeit wie wir die Ereignisse ordenen können sondern wahrscheinlich ist jede der 24 möglichen Anordnungen gleich sinnig. Wir könnten auch beispielsweise nicht mehr die beiden einzelnen Ausgänge als Ereignisse wählen, sondern könnten zum Beispiel nur noch die Anzahl der Köpfe in unseren zwei Würfen zählen. Dies würde zu der folgenden Zuordnung führen (siehe Tabelle 10.2).\n\n\nTabelle 10.2: Zuordnung der Anzahl der Köpfe zu den Ereignissen beim zweimaligen Münzwurf\n\n\nEreignisse\nAnzahl der Köpfe\n\n\n\n\n(Kopf, Kopf)\n2\n\n\n(Kopf, Zahl)\n1\n\n\n(Zahl, Kopf)\n1\n\n\n(Zahl, Zahl)\n0\n\n\n\n\nWir verliegen bei dieser Zuordnung nachtürlich die Information bei welchem Wurf die Zahl beobachtet wurde, aber eigentlich interessiert uns das sowieso nicht so brennend. In der Terminologie der Wahrscheinlichkeitstheorie wird die Anzahl der Köpfe als Zufallsvariable bezeichnet.\n\nDefinition 10.1 (Zufallsvariable) Eine Zufallsvariable ist die Abbildung eines Zufallsereignisses auf eine Zahl.\n\nAnders dargestellt, ist eine Zufallsvariable eine Funktion, die einem Ereignis eine Zahl zuordnet (siehe Abbildung 10.4.\n\n\n\n\n\nflowchart LR \n    A[Ereignis] --&gt; B(Zahl)\n\n\nAbbildung 10.4: Abbildung des Ereignisses auf eine Zahl\n\n\n\n\nWenn wir uns jetzt die Wahrscheinlichkeiten für unsere Zufallsvariable anschauen, dann sehen wir aber, dass wir nicht mehr vier verschiedne Ausgänge haben, sondern nur noch drei und das die gleiche Wahrscheinlichkeit für nicht gleich sind.\n\n\nTabelle 10.3: Wahrscheinlichkeitstabelle für Zufallsvariable “Anzahl der Köpfe beim zweimaligen Münzwurf”.\n\n\n\n\n\n\n\nEreignisse\nZufallsvariale\nWahrscheinlichkeit\n\n\n\n\n(Zahl, Zahl)\nKeine Köpfe\n\\(\\frac{1}{4}\\)\n\n\n(Kopf, Zahl)(Zahl,Kopf)\n1 Kopf\n\\(\\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}\\)\n\n\n(Kopf,Kopf)\n2 Köpfe\n\\(\\frac{1}{4}\\)\n\n\n\n\nJetzt können wir wieder eine Wahrscheinlichkeitsverteilung für unsere Zufallsvariable abtragen (siehe Abbildung 10.5).\n\n\n\n\n\nAbbildung 10.5: Wahrscheinlichkeitsverteilung für die Anzahl der Köpfe beim zweimaligen Münzwurf\n\n\n\n\nNur um nebenbei noch einmal das offensichtliche Anzusprechen. Die Summe aller Wahrscheinlichkeiten aller Ereignisse muss \\(1\\) sein. Das sollte auch direkt einsichtig sein. Wenn ich alle möglichen Ereignisse abfrage also: “Was ist die Wahrscheinlichkeit das ich keine Köpfe, 1 Kopf oder 2 Köpfe beim zweimaligen Münzwurf erhalte”, dann sind das alle möglichen Ausgänge und dementsprechend sollte die Wahrscheinlichkeit dafür “1” sein oder mathematisch ausgedrückt:\n\\[\nP(\\text{0 Köpfe} \\cup \\text{1 Kopf} \\cup \\text{2 Köpfe}) = \\frac{1}{4} + \\frac{1}{2} + \\frac{1}{4} = 1\n\\]\nJetzt gehen wir zum nächst komplizierteren Fall. Die Anzahl der Köpfe bei drei Münzwürfen. Welche Möglichkeiten gibt es hier? Nun bei drei Würfen kann entweder \\(0, 1, 2\\) oder \\(3\\) Kopf auftreten. Wenn wir die Wahrscheinlichkeiten für diese vier Ereignisse berechnen wollen, können wir aber nicht einfache \\(\\frac{1}{4}\\) für jedes Ereignis als Wahrscheinlichkeit ansetzen (Warum?). Schauen wir uns erst einmal wieder die möglichen Tupel, oder auch die Elemenarereignisse, den wir erinnern uns, dass die Anzahl der Köpfe eine Zufallsvariable ist. Also eine Abbildung der 3-fach Tupel auf eine der Zahlen \\(\\{0, 1, 2, 3\\}\\).\n\n\nTabelle 10.4: Abbildung der 3-fach Tupel auf die Anzahl Kopf beim dreifachen Münzwurf\n\n\nElementarereignis\nAnzahl Kopf\n\n\n\n\n(Z,Z,Z)\n\\(0\\)\n\n\n(K,Z,Z)\n\\(1\\)\n\n\n(Z,K,Z)\n\\(1\\)\n\n\n(Z,Z,K)\n\\(1\\)\n\n\n(K,K,Z)\n\\(2\\)\n\n\n(Z,K,K)\n\\(2\\)\n\n\n(K,Z,K)\n\\(2\\)\n\n\n(K,K,K)\n\\(3\\)\n\n\n\n\nDie Elementarereignisse in Tabelle 10.4 sind wieder alle gleichwahrscheinlich, daher können wir jetzt wieder einfache abzählen. Es gibt insgesamt \\(8\\) mögliche Ausgänge, davon haben jeiweils einer \\(0\\)-mal oder \\(3\\)-mal Kopf und jeweils \\(3\\) Ausgänge haben \\(1\\)-mal oder \\(2\\)-mal Kopf. Daraus folgt für die Wahrscheinlichkeitsfunktion (siehe Tabelle 10.5).\n\n\nTabelle 10.5: Wahrscheinlichkeitsfuntion für den dreifachen Münzwurf\n\n\nAnzahl Kopf\nP\n\n\n\n\n\\(0\\)\n\\(\\frac{1}{8}\\)\n\n\n\\(1\\)\n\\(\\frac{3}{8}\\)\n\n\n\\(2\\)\n\\(\\frac{3}{8}\\)\n\n\n\\(3\\)\n\\(\\frac{1}{8}\\)\n\n\n\n\nDas Ganze auch wieder als Graph (siehe ?fig-sts-coin-toss-3)\n\n\n\n\n\nAbbildung 10.6: Wahrscheinlichkeitsverteilung für die Anzahl der Köpfe beim dreimaligen Münzwurf\n\n\n\n\nBleiben wir noch einmal kurz bei dem Beispiel und versuchen uns die Wahrscheinlichkeiten anders herzuleiten. Sollten wir zum Beispiel einmal in die Verlegenheit kommen und 20 Münzwürfe untersuchen wollen, dann wir die Tabelle relative schnell relativ unhandlich.\nSei \\(N\\) die Anzahl der Würfe die wir durchführen. Wenn wir \\(N\\) kennen, wissen wir auch direkt welche möglichen Ausgänge bei dem Experiment möglich sind, nämlich alle Zahlen zwischen \\(0\\) und \\(N\\). \\(0\\) wenn wir kein Kopf geworfen haben, und \\(N\\) wenn wir nur Kopf geworfen haben. Dementsprechend sind alle Zahlen dazwischen auch noch möglich.\nSchauen wir uns jetzt noch mal den dreimaligen Münzwurf an. Wenn wir kein Kopf werfen in \\(3\\) Würfen und betrachten die Würfe wieder sequentiell, dann haben wir \\(\\frac{1}{2}\\) für die erste Zahl, \\(\\frac{1}{2}\\) für die zweite Zahl und \\(\\frac{1}{2}\\) für die dritte Zahl. Also insgesamt \\(P(1 \\text{ Kopf}) = \\frac{1}{2} \\times \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{8}\\). Aber diese Wahrscheinlichkeit hat ja jedes Elementarereignis egal ob es (K,K,K) oder (K,Z,K) oder (Z,Z,K) usw. ist. Jetzt haben wir aber das Problem, das wir für \\(1\\times\\) oder \\(2\\times\\) Kopf nicht nur eine Möglichkeit vorhanden diese Anzahl an Kopf zu beobachten. In Tabelle 10.4 haben wir bereits gezeigt, dass jeweils drei verschiedene Möglichkeiten, Kombination von Kopf und Zahl, möglich sind. D.h. wir haben jetzt ein Abzählproblem. Können wir irgendwie direkt bestimmen wie viele unterschiedliche Möglichkeiten es gibt?\nSchauen wir uns den Fall \\(1\\times\\) Kopf im 3-fach Tupel an. Auf wie viele Arten können wir 3-fach Tupel erzeugen mit nur einem Kopf. Nun, der Kopf ist entweder an der ersten, der zweiten oder der dritten Stelle und die jeweils anderen Position im Tupel sind mit Zahl besetzt. Das hört sich aber ähnlich wie ein Problem an wie wie etwas was wir schon vorher einmal gehört haben. Als wir uns die Anzahl der möglichen Stichproben aus unserer kleinen Welt angeschaut haben. Dort hatten wir das Problem, das wir bestimmen wollten auf wie viele Möglichkeiten wir zwei Stichproben mit jeweils drei Personen aus 20 Personen ziehen können. Dabei sind wir auf den Binomialkoeffizienten gestoßen Gleichung 7.3.\n\\[\n\\text{Anzahl} = \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\nFormal berechnet der Binomialkoeffizient die Möglichkeiten \\(k\\) Objekte aus \\(n\\) Objekten zu ziehen. Wenden wir das mal auf unseren Dreifachwurf an mit \\(n = N = 3\\) und \\(k = 1\\). Ausgeschrieben, auf wie viele Arten können wir \\(1\\times\\) Kopf aus drei Positionen auswählen.\n\\[\n\\text{Kombinationen mit }1\\times\\text{ Kopf} = \\binom{3}{1} = \\frac{3!}{1!(3-1)!} = \\frac{3\\times 2 \\times 1}{1\\times 2 \\times 1} = 3\n\\]\nPasst. Probieren wir das auch direkt mit dem Ereignis \\(2\\times\\) Kopf, also mit \\(N = 3\\) und \\(k = 2\\), aus.\n\\[\n\\text{Kombinationen mit } 2\\times \\text{ Kopf} = \\binom{3}{2} = \\frac{3!}{2!(3-2)!} = \\frac{3\\times 2 \\times 1}{2\\times 1 \\times 1} = 3\n\\]\nPasst auch. Jetzt müssen wir noch nur die beiden Fälle \\(0\\times\\) und \\(3\\times\\) Kopf behandeln. Wenn wir in einem Mathebuch den Binomialkoeffizienten nachschlagen, dann sind dort die beiden folgenden Definition zu finden für die Fälle \\(k=0\\) und \\(k=n\\).\n\\[\\begin{align*}\n\\binom{N}{N} &= 1 \\\\\n\\binom{N}{0} &= 1\n\\end{align*}\\]\nWenn wir diese Definition für die anderen beiden verbleibenden Fälle anwenden, erhalten wir:\n\\[\\begin{align*}\n\\text{Kombinationen mit } 0\\times \\text{ Kopf} &= \\binom{3}{0} = 1 \\\\\n\\text{Kombinationen mit } 3\\times \\text{ Kopf} &= \\binom{3}{3} = 1\n\\end{align*}\\]\nDamit können wir nun für alle möglichen Ausgängen die Anzahl der möglichen Elementarereignisse mittels bestimmen. Allgemein erhalten wir dadurch eine Formel für die Wahrscheinlichkeiten der Ereignisse für den dreifachen Münzwurf.\n\\[\nP(k \\times \\text{Kopf}) = \\binom{3}{k} \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\binom{3}{k} \\left(\\frac{1}{2}\\right)^3\n\\tag{10.3}\\]\nWeil wir natürlich sofort nach einer allgemeinen Lösung streben führen wir jetzt noch ein paar Symbole ein. Die Zufallsvariable, also die Anzahl von Kopf, bezeichnen wir mit dem Großbuchstaben \\(Y\\). Einen speziellen Ausgang bezeichnen wir mit dem Kleinbuchstaben \\(y\\). Damit würden allgemein die Wahrscheinlichkeit für irgend eines der Ereignisse mit \\(Y = y\\) bezeichnen. Und wenn wir sagen wir das Ereignis \\(2\\times\\) Kopf bezeichnen, mit \\(y = 2\\). Also, die Wahrscheinlichkeit für \\(3\\times\\) Kopf mit:\n\\[\nP(Y = 3) = \\binom{3}{3}\\left(\\frac{1}{2}\\right)^3\n\\]\nDie nächste Verallgemeinerung die wir Vornehmen ist dass wir für die Wahrscheinlichkeit das Kopf auftritt das Symbol \\(p\\) benutzen. So könnten wir auch modellieren, wenn wir eine unfaire Münze haben. Wenn jetzt aber \\(p \\neq \\frac{1}{2}\\) gilt, also zum Beispiel die Wahrscheinlichkeit für Kopf \\(p = \\frac{2}{3}\\) wäre, dann ist die Wahrscheinlichkeit für Zahl nicht mehr die Gleiche wie für Kopf. Die Wahrscheinlichkeit für Zahl wäre dann \\(1 - p\\). Wenn wir für die Wahrscheinlichkeit für das Auftreten von Zahl das Symbol \\(q\\) einführen, muss die Wahrscheinlichkeit für Kopf oder Zahl gleich \\(1\\) sein, formal:\n\\[\np + q = 1\n\\] Daraus folgt, dass \\(q = p - 1\\). Wenn wir das auf unseren Münzwurf übertragen, müssen wir das dementsprechend berücksichtigen. Wir können uns aber zunutze machen, dass wir wissen wie viele Würfe durchgeführt wurden, nämlich \\(N\\), und wie viele davon Kopf waren, nämlich \\(y\\). Damit wissen wir automatisch auch die Anzahl von Zahl, \\(N - y\\). Jedes Kopf, hat die Wahrscheinlichkeit \\(p\\) und jede Zahl hat die Wahrscheinlichkeit \\(q = 1 - p\\). Das gilt unabhängig von der Reihenfolge, da z.B. die Wahrscheinlichkeiten \\(KKZK\\) und \\(ZKKK\\) gleich \\(ppqp = qppp\\) sind. Insgesamt haben wir \\(y \\times K\\) und \\((n-y) \\times Z\\) also \\(p^y\\) und \\(q^{n-y}\\). Diesen Zusammenhang können wir in eine Formel stecken.\n\\[\\begin{equation}\nP(Y = y) = \\binom{N}{y}p^y (1-p)^{N-y} = \\binom{N}{y}p^y q^{N-y} \\label{eq-binom-distribution}\n\\end{equation}\\]\nDamit haben wir jetzt auch direkt unsere erste theoretische Verteilung kennengelernt, die in der Statistik eine zentrale Rolle spielt. Die Verteilung in Formel \\(\\eqref{eq-binom-distribution}\\) wird als die Binomialverteilung bezeichnet. Da die Formel \\(\\eqref{eq-binom-distribution}\\) von den Parametern \\(p\\) und \\(n\\) abhängt, wird die Binomialverteilung als eine Familie von Verteilungen bezeichnet.\n\n\n\n\n\n\n\n(a) \\(p = 0.5, n = 10\\)\n\n\n\n\n\n\n\n(b) \\(p = 0.3, n = 7\\)\n\n\n\n\nAbbildung 10.7: Beispiel für verschiedene Binomialverteilungen\n\n\nSchauen wir uns aber noch mal ob wir mit den ganzen Symbolen wirklich unseren dreifachen Münzwurf zurückbekommen. Es gilt \\(N = 3, p = \\frac{1}{2}\\). Daraus folgt das \\(q = 1 - p = 1 - \\frac{1}{2}=\\frac{1}{2}\\). Wenn wir uns noch an \\(x^a x^b = x^{a+b}\\) aus der Schule erinnern folgt:\n\\[\\begin{align*}\nP(Y = 0) &= \\binom{3}{0} \\left(\\frac{1}{2}\\right)^{0}\\left(\\frac{1}{2}\\right)^3 = \\binom{3}{0}\\left(\\frac{1}{2}\\right)^3 = 1 \\left(\\frac{1}{2}\\right)^3 \\\\\nP(Y = 1) &= \\binom{3}{1} \\left(\\frac{1}{2}\\right)^{1}\\left(\\frac{1}{2}\\right)^2 = \\binom{3}{1}\\left(\\frac{1}{2}\\right)^3 = 3 \\left(\\frac{1}{2}\\right)^3 \\\\\nP(Y = 2) &= \\binom{3}{0} \\left(\\frac{1}{2}\\right)^{2}\\left(\\frac{1}{2}\\right)^1 = \\binom{3}{2}\\left(\\frac{1}{2}\\right)^3 = 3 \\left(\\frac{1}{2}\\right)^3 \\\\\nP(Y = 3) &= \\binom{3}{0} \\left(\\frac{1}{2}\\right)^{3}\\left(\\frac{1}{2}\\right)^0 = \\binom{3}{3}\\left(\\frac{1}{2}\\right)^3 = 1 \\left(\\frac{1}{2}\\right)^3 \\\\\n\\end{align*}\\]\nTatsächlich können wir unser Ergebnis von oben wiedergewinnen. Die Funktion der Binomialverteilung (Formel \\(\\eqref{eq-binom-distribution}\\)) wird als Wahrscheinlichkeitsfuntion bezeichnet.\n\nDefinition 10.2 (Wahrscheinlichkeitsfunktion) Eine Wahrscheinlichkeitsfunktion ist eine mathematische Funktion, die die Wahrscheinlichkeiten für alle möglichen Ausgänge eines diskreten Zufallsexperiments angibt. Sie wird auch als diskrete Wahrscheinlichkeitsverteilung bezeichnet. Eine Wahrscheinlichkeitsfunktion ordnet jedem möglichen Ausgang \\(x\\) eines Experiments eine Wahrscheinlichkeit \\(P(X = x)\\) zu. Die Wahrscheinlichkeit liegt zwischen 0 und 1. Die Summe aller Wahrscheinlichkeiten für alle möglichen Ergebnisse muss gleich 1 sein. Eine Wahrscheinlichkeitsfunktion kann als Tabelle oder als Formel dargestellt werden\n\nFür die Eigenschaften einer Verteilung gibt es einer weitere Darstellungsform, die Verteilungsfunktion.\n\nDefinition 10.3 (Verteilungsfunktion) Die Verteilungsfunktion gibt die Wahrscheinlichkeit \\(P\\) an, dass eine Zufallsvariable \\(X\\) einen Wert kleiner oder gleich einem bestimmten Wert \\(x\\) annimmt, formal \\(P(X \\leq x)\\). Sie wird daher auch als kumulative Verteilungsfunktion bezeichnet.\n\nUm die Definition der Verteilungsfunktion leichter nachzuvollziehen schauen wir uns das Ganze graphisch an (siehe Abbildung 10.8).\n\n\n\n\n\n\n\n(a) Wahrscheinlichkeitsfunktion \\(P(X = x)\\)\n\n\n\n\n\n\n\n(b) Verteilungsfunktion \\(P(X \\leq x)\\)\n\n\n\n\nAbbildung 10.8: Zusammenhang zwischen der Wahrscheinlichkeits- und der Verteilungsfunktion bei \\(p = 0.5, n = 10\\)\n\n\nDie Wahrscheinlichkeitsfunktion gibt, wie schon bekannt, die Wahrscheinlichkeit für eine bestimmtes Ereignis an. Zum Beispiel, die Wahrscheinlichkeit bei \\(p = 0.5, n = 10, 5\\times\\) Kopf zu sehen ist etwas unter \\(0.25\\). Wir könnten uns aber auch fragen, was die Wahrscheinlichkeit ist \\(5\\) oder weniger Köpfe zu beobachten. Diese Wahrscheinlichkeit setzt sic zusammen aus \\(P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) + P(X = 5)\\). Genau diesen Wert gibt die Verteilungsfunktion (siehe Abbildung 10.8 (b)) an.\nDie beiden Funktionen sind dabei eineindeutig aufeinander abbildbar. Wenn die Verteilungsfunktion bekannt ist, dann kann daraus die Wahrscheinlichkeitsfunktion berechnet werden und anders herum wenn die Wahrscheinlichkeitsfunktion bekannt ist, dann kann, wie wir eben gesehen haben, die Verteilungsfunktion berechnet werden. Später bei den kontinuierlichen Verteilungen lernen wir noch die Dichtefunktion kennen, welche die Funktion der Wahrscheinlichkeitsfunktion einnimmt.\nFür unser Ausgangsproblem ist jetzt aber mit der Verteilungsfunktion die Möglichkeit gegeben, das wir bestimmte Wahrscheinlichkeitsbereiche unserer Verteilung auszeichnen können. Denn die Wahrscheinlichkeitsfunktion liefert uns die Antwort auf die Frage, welchen Wertebereich wir für eine gegebene Verteilung eher nicht erwarten würden. Schauen wir uns zum Beispiel die Verteilung bei \\(p = 0.5\\) und \\(n = 30\\).\n\n\n\n\n\nAbbildung 10.9: Wahrscheinlichkeitsfunktion bei \\(p = 0.5\\) und \\(n = 30\\)\n\n\n\n\nIn Abbildung 10.9 sehen wir, dass wir zum Beispiel recht überrascht wären, wenn wir bei einem Durchgang von \\(30\\) Münzwürfen einen Wert von z.B. \\(x = 29 \\times\\) Kopf beobachten würden. Es ist nicht unmöglich, aber es wäre schon überraschend. Diesen Grad der Überraschung können wir als Kriterium nehmen, um zu entscheiden ob wir eine bestimmt Beobachtung dazu verwenden würden diese als Evidenz für oder gegen eine bestimmte Verteilungsannahme zu sehen.\nSetzen wir unser Kriterium z.B. bei 2% an. Die Entscheidung wird jetzt folgendermaßen getroffen. Wenn wir einen Wert beobachten der unter der Annahme einer fairen Münze die wir \\(30\\times\\) aus dem Bereich der Werte von \\(\\leq2\\%\\) kommt. Dann sehen wir dies als gegen die Annahme an.\nIm Folgenden werden vier verschiedene Verteilungen noch einmal etwas genauer vorgestellt, da diese Verteilung immer wieder im weiteren Verlauf auftauchen werden. Dies sind die Normalverteilung, die \\(t\\)-Verteilung, die \\(\\chi^2\\)-Verteilung und die \\(F\\)-Verteilung. Dabei ist es, außer bei der Normalverteilung, weniger wichtig sich die Formeln einzuprägen sondern es soll eher darum gehen die Form der Verteilung, den Wertebereich und die Parameter der Verteilung zu kennen. Also zum Beispiel wird die Normalverteilung durch zwei Parameter \\(\\mu\\) und \\(\\sigma^2\\) spezifiziert während die \\(\\chi^2\\)-Verteilung nur über einen einzelnen Parameter den Freiheitsgrad \\(df\\) bestimmt wird. Streng genommen wird auch nicht über vier Verteilungen gesprochen, sondern es handelt sich um jeweils Verteilungsfamilien, da es beispielsweise nicht die eine Normalverteilung gibt, sondern die Form wie eben beschrieben von den beiden Parametern abhängt. Dies gilt in gleich3em Maßen ebenfalls für die anderen behandelten Verteilungen."
  },
  {
    "objectID": "stats_distributions.html#normalverteilung",
    "href": "stats_distributions.html#normalverteilung",
    "title": "10  Theoretische Verteilungen",
    "section": "10.2 Normalverteilung",
    "text": "10.2 Normalverteilung\nBeginnen wir mit der Normalverteilung.\n\\[\nf(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e^{\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)}\n\\]\nDie Normalverteilung ist eine symmetrische Verteilung und hat die uns schon oft begegnete Glockenform (siehe Abbildung 10.10).\n\n\n\n\n\nAbbildung 10.10: Dichtefunktion der Normalverteilung mit den Parametern \\(\\mu = 0\\) und \\(\\sigma = 1\\).\n\n\n\n\nDer Wertebereich der Normalverteilung ist \\(X \\in [-\\infty, \\infty]\\). Das Maximum liegt genau beim Erwartungswert \\(\\mu\\) der dementsprechend die Verteilung in die linken 50% und die rechten 50% unterteilt. Das Abfallen der Flanken wird über die Varianz \\(\\sigma^2\\) geregelt. Wird \\(\\sigma^2\\) größer, fallen die Flanken flacher ab, wird \\(\\sigma^2\\) kleiner, fallen die Flanken schneller ab (siehe Abbildung 10.11).\n\n\n\n\n\n\n\n(a) \\(\\sigma^2 = 1\\)\n\n\n\n\n\n\n\n(b) \\(\\sigma^2 = 2\\)\n\n\n\n\nAbbildung 10.11: Veränderung der Dichtefunktion bei unterschiedlichen Varianzen \\(\\sigma^2\\)\n\n\nDie Standardabweichung kann dazu verwendet werden, die Dichtfunktion in verschiedene Abschnitte zu unterteilen. Es gelten die folgenden Zusammenhänge (siehe Tabelle 10.6):\n\n\nTabelle 10.6: Wahrscheinlichkeiten für verschiedene Intervalle um \\(\\mu\\) in Abhängigkeit von \\(\\sigma\\)\n\n\n\\(x \\in\\)\nP\n\n\n\n\n\\([-\\sigma,\\sigma]\\)\n0.682\n\n\n\\([-2\\sigma,2\\sigma]\\)\n0.955\n\n\n\\([-3\\sigma,3\\sigma]\\)\n0.997\n\n\n\n\nÜbertragen auf den Dichtegraphen folgt (siehe Abbildung 10.12):\n\n\n\n\n\nAbbildung 10.12: Dichtefunktion von \\(\\mathcal{N}(\\mu,\\sigma^2)\\)\n\n\n\n\nWie in Tabelle 10.6 zu sehen ist, hat der Bereich \\([-2\\sigma, 2\\sigma]\\) eine Wahrscheinlichkeit von etwas über \\(0.95\\). Daher, wenn ich einen Bereich um den Erwartungswert \\(\\mu\\) auszeichnen möchte, der genau eine Wahrscheinlichkeit von \\(0.95\\) hat, dann muss \\(\\sigma\\) mit einem kleineren Wert als \\(2\\) multipliziert werden, nämlich \\(1.96\\). Das wird hier noch mal speziell erwähnt, da die Zahl \\(1.96\\) später immer wieder auftaucht. Formal:\n\\[P(x\\in[\\mu-1.96\\sigma, \\mu+1.96\\sigma]) = 0.95\\]\nAnders herum, wenn es darum geht in Konfidenzintervall abzuschätzen, dann funktioniert auch die Faustregel, Teststatistik \\(\\pm 2\\times\\) Standardfehler.\n\n10.2.1 Die Standardnormalverteilung\nEine Sonderrolle in der Familie der Normalverteilungen spielt die Standardnormalverteilung mit \\(\\mu = 0\\) und \\(\\sigma^2 = 1\\). Tatsächlich taucht diese so oft aus, dass die Mathematiker ihr ein eigenes Symbol spendiert haben \\(\\phi(x)\\)\n\\[\n\\phi(x) = \\mathcal{N}(\\mu = 0, \\sigma^2 = 1)\n\\]\nIm Fall der Standardnormalverteilung nehmen Tabelle 10.6 und Abbildung 10.12 besonders einfache Formen an da die Intervalle jeweils \\([-1,1]\\), \\([-2,2]\\) und \\([-3,3]\\) sind (siehe Abbildung 10.13).\n\n\n\n\n\nAbbildung 10.13: Dichtefunktion der Standardnormalverteilung \\(\\phi(x)\\) mit \\(\\mu=0\\) und \\(\\sigma^2=1\\)\n\n\n\n\n\n\n10.2.2 z-Transformation\nEs besteht mittels einer einfachen Möglichkeit jede beliebiege Normalverteilung \\(\\mathcal{N}(\\mu,\\sigma^2)\\) auf die Standardnormalverteilung \\(\\mathcal{N}(0,1)\\) abzubilden. Die Transformation wird als z-Transformation bezeichnet und hat die folgende Form:\n\\[\nz = \\frac{X - \\mu_X}{\\sigma_X}\n\\tag{10.4}\\]\nD.h. der Mittelwert der Verteilung von \\(X\\) wird von X abgezogen und die Differenz wird durch die Standardabweichung der Population \\(\\sigma_X\\) geteilt. Die Umkehrfunktion ist dementsprechend:\n\\[\nX = \\mu_X + z \\sigma_X\n\\tag{10.5}\\]\n\n\n10.2.3 Zentraler Grenzwertsatz\nDie Normalverteilung spielt in der Wahrscheinlichkeitstheorie und der Statistik aus verschiedenen Gründen eine Spezialrolle. Ein Grund dafür ist der sogenannte Zentrale Grenzwertsatz, den wir hier nicht beweisen sondern nur kurz diskutieren.\n\nAussage 10.1 (Zentraler Grenzwertsatz) Seien \\(X_1, X_2, \\ldots, X_n\\) n unabhängige, gleichverteilte Zufallsvariablen mit \\(E[X_i]=\\mu\\) und \\(Var[X_i]=\\sigma^2\\) endlich. \\[\n\\lim_{n\\to\\infty}\\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\ \\rightarrow\\ \\mathcal{N}(\\mu=0,\\sigma^2=1)\n\\]\n\nIn Worten besagt der Zentrale Grenzwertsatz, dass egal welche Ursprungsform die Verteilung einer Zufallsvariablen \\(X\\) hat, wenn die Stichprobengröße gegen unendlich geht, die konvergiert die Differenz des Stichprobenmittelwerts und des Mittelwert der Verteilung geteilt durch den Stichprobenstandardfehler gegen die Standardnormalverteilung. Grenzwertsätz sind manchmal etwas schwierig zu interpretieren, da hier noch keine Aussage gemacht wird, wie groß die Stichprobe sein muss, damit diese Abschätzung valide ist. In der Praxis wird oft ab einer gefühlt großen Stichproben diese Abschätzung als zulässig angesehen."
  },
  {
    "objectID": "stats_distributions.html#t-verteilung",
    "href": "stats_distributions.html#t-verteilung",
    "title": "10  Theoretische Verteilungen",
    "section": "11.1 t-Verteilung",
    "text": "11.1 t-Verteilung\n\n\n\n\n\nBeispiel für verschiedene Dichtefunktionen der t-Verteilung"
  },
  {
    "objectID": "stats_distributions.html#chi2-verteilung",
    "href": "stats_distributions.html#chi2-verteilung",
    "title": "10  Theoretische Verteilungen",
    "section": "11.2 \\(\\chi^2\\)-Verteilung",
    "text": "11.2 \\(\\chi^2\\)-Verteilung\n\n\n\n\n\nBeispiele für verschiedene Dichtefunktion der \\(\\chi^2\\)-Verteilung."
  },
  {
    "objectID": "stats_distributions.html#f-verteilung",
    "href": "stats_distributions.html#f-verteilung",
    "title": "10  Theoretische Verteilungen",
    "section": "11.3 F-Verteilung",
    "text": "11.3 F-Verteilung\n\n\n\n\n\nBeispiele für verschiedene Dichtefunktion der F-Verteilung."
  },
  {
    "objectID": "stats_hypotheses.html#wahrscheinlichkeitstheorie",
    "href": "stats_hypotheses.html#wahrscheinlichkeitstheorie",
    "title": "11  Hypothesen testen",
    "section": "11.1 Wahrscheinlichkeitstheorie",
    "text": "11.1 Wahrscheinlichkeitstheorie"
  },
  {
    "objectID": "stats_hypotheses.html#rechenregeln-zum-erwartungswert-und-der-varianz",
    "href": "stats_hypotheses.html#rechenregeln-zum-erwartungswert-und-der-varianz",
    "title": "11  Hypothesen testen",
    "section": "11.2 Rechenregeln zum Erwartungswert und der Varianz",
    "text": "11.2 Rechenregeln zum Erwartungswert und der Varianz\n\n11.2.1 Der Erwartungwert einer Zufallsvariable\nFür eine diskrete Zufallsvariable \\(X\\) auf einer endlichen Menge \\(\\{x_i, i = 1, \\ldots, n\\}\\) mit \\(n\\) Elementen ist der Erwartungswert definiert mit:\n\\[\\begin{equation}\nE[X] = \\sum_{i=1}^n x_i P(x_i)\n\\label{eq-stats-hypo-expected-01}\n\\end{equation}\\]\nD.h. jedes mögliche Ereignis wird mit seiner Wahrscheinlichkeit multipliziert und die Summe über alle diese Möglichkeiten wird gebildet. Da der Erwartungswert eine zentrale Rolle in der Wahrscheinlichkeitstheorie und der Statistik spielt, hat er ein eigenes Symbol spendiert bekommen \\(\\mu\\). Daher wird uns immer wieder die folgende Schreibweise begegnen:\n\\[\nE[X] = \\mu_X\n\\] Der Erwarungswert der Zufallsvariable \\(X\\) wird mit \\(\\mu_X\\) bezeichnet. Wenn der Zusammenhang klar ist und nur von einer bestimmten Zufallsvariablen gesprochen wird, dann auch nur \\(\\mu\\).\nEs hat sich eingebürgert, die Größe \\(\\mu\\) als den Mittelwert der Population zu bezeichnen auch wenn es sich dabei nicht unbedingt um den Mittelwert handelt wie er üblicherweise verstanden wird und z.B. bei der Stichprobe berechnet wird (\\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)). Bei dem Erwartungswert handelt es sich um den gewichteten Mittelwert. Es wird unterschieden zwischen dem Mittelwert in der Population \\(\\mu\\), der theoretisch ist, dem Mittelwert der Stichprobe \\(\\bar{X}\\) als Zufallswert, ebenfalls theoretisch, und dem tatsächlich beobachtet Mittelwert \\(\\bar{x}\\) in der Stichprobe.\nIm folgenden werden verschiedene Rechenregeln mit dem Erwartungswert aufgelistet. Diesen Regeln werden wir immer wieder begegnen wenn wir später Erwarungswerte für Statistiken berechnen. Die erste Regel bezieht sich darauf, wenn eine Zufallsvariable mit einer Konstanten \\(a\\) multipliziert wird. Konstant heißt, bei \\(a\\) handelt es sich nicht um eine Zufallsvariable und \\(a\\) hat immer den gleichen Wert. Der Erwartungswert berechnet sich dann mittels:\n\\[\\begin{equation}\nE[aX] = \\sum_{i=1}^n a x_i P(x_i) = a \\sum_{i=1}^n x_i P(x_i) = a E[X]\n\\label{eq-stats-hypo-expected-mult}\n\\end{equation}\\]\nIn den meisten Fällen sind wir nicht an einer einzelnen Zufallsvariablen interessiert, sondern, beispielsweise wenn wir eine Stichprobe untersuchen, es liegen mehrere Zufallsvariablen vor. Im einfachsten Fall starten wir mit zwei unabhängigen Zufallsvariablen \\(X\\) und \\(Y\\). Die beiden Variablen können auf der gleichen Ereignismenge definiert sein, können aber auch auf unterschiedlichen Ereignismengen, z.B. \\(\\{x_i, i = 1, \\ldots, n\\}\\) und \\(\\{y_j, j = 1, \\ldots, m\\}\\) definiert sein. Wollen wir den Mittelwert von \\(X\\) und \\(Y\\) berechnen und davon den Erwartungswert berechnen, müssen wir verstehen wie sich die Addition unabhängiger Zufallsvariblen auf den Erwartungswert auswirkt. Tatsächlich ist diese Operation relativ einfach zu verstehen, der Erwartungswert von \\(E[X + Y]\\) berechnet sich mittels:\n\\[\\begin{equation*}\nE[X + Y] = \\sum_{i=1}^n x_i P(x_i) + \\sum_{j=1}^m y_j P(x_j) = E[X] + E[Y]\n\\end{equation*}\\]\nDiese Formel generalisiert für unabhängige \\(X_i, i = 1, \\ldots, n\\) zu:\n\\[\\begin{equation}\nE[X_1 + X_2 + \\ldots + X_n] = E[X_1] + E[X_2] + \\ldots + E[X_n]\n\\label{eq-stats-hypo-expected-add}\n\\end{equation}\\]\nIn Kombination mit der Regel für konstante Terme mit den Konstanten \\(a_1, a_2, \\ldots, a_n\\) folgt:\n\\[\nE[a_1 X_1 + a_2 X_2 + \\ldots + a_n X_n] = a_1 E[X_1] + a_2 E[X_2] + \\ldots + a_n E[X_n]\n\\]\nDer Erwartungswert ist daher ein linearer Operator.\nNur der Vollständigkeit halber auch noch kurz die Definition des Erwartungswertes für reelle Verteilungen, bei der das Summenzeichen mit einem Integral ausgetauscht ist.\n\\[\\begin{equation*}\nE[X] = \\int_{x=-\\infty}^\\infty x f(x) dx\n\\end{equation*}\\]\nDas Integral geht über den gesamten Definitionsbereich von \\(X\\) und die Wahrscheinlichkeitsfunktion wird durch die Dichtefunktion \\(f(x)\\) ausgetauscht. Die Linearität bleibt für alle uns interessierenden Fälle erhalten (keine Panik, es werden im Weiteren keine Integrale berechnet).\n\nBeispiele\nNehmen wir zur Veranschaulichung ein einfaches Beispiel mit einer Zufallsvariable \\(X\\) die nur vier verschiedene Werte annehmen kann, die die folgenden Wahrscheinlichkeiten haben (siehe Tabelle 11.1):\n\n\nTabelle 11.1: Verteilung der Zufallsvariablen \\(X\\)\n\n\nx\n0\n1\n2\n3\n\n\n\n\n\\(P(x)\\)\n\\(\\frac{1}{8}\\)\n\\(\\frac{5}{8}\\)\n\\(\\frac{1}{8}\\)\n\\(\\frac{1}{8}\\)\n\n\n\n\nAnwendung von Formel \\(\\eqref{eq-stats-hypo-expected-01}\\) führt zu der Berechnung des Erwartungswerts \\(E[X]\\) mittels:\n\\[\nE[X] = \\sum_{i=1}^4 x_i P(x_i) = 0 \\cdot \\frac{1}{8}  + 1 \\cdot \\frac{1}{8} + 2\\cdot \\frac{5}{8} + 3\\cdot \\frac{1}{8} = 1.25\n\\]\nHier kann auch eine interessante Eigenschaft des Erwartungswerts beobachtet werden, nämlich das der berechnete Wert gar nicht in der Menge der möglichen Werte der Zufallsvariablen vorkommen muss. In der Ereignismenge von \\(X\\) sind nur ganzzahlige Werte.\nHaben wir eine zweite Zufallsvariable \\(Y\\) mit der Verteilung (siehe Tabelle 11.2)\n\n\nTabelle 11.2: Verteilung der Zufallsvariablen \\(X\\)\n\n\ny\n0\n1\n2\n3\n\n\n\n\n\\(P(y)\\)\n\\(\\frac{2}{8}\\)\n\\(\\frac{2}{8}\\)\n\\(\\frac{1}{8}\\)\n\\(\\frac{3}{8}\\)\n\n\n\n\nMit Formel \\(\\eqref{eq-stats-hypo-expected-01}\\) folgt wiederum:\n\\[\nE[Y] = \\sum_{i=1}^4 y_i P(y_i) = 0 \\cdot \\frac{2}{8} + 1 \\cdot \\frac{2}{8} + 2 \\cdot \\frac{1}{8} + 3 \\cdot \\frac{3}{8} = 1.625\n\\]\nWenn wir eine neue Zufallsvariable \\(Z\\) definieren mit \\(Z = X + Y\\), dann folgt für den Erwarungswert von \\(E[Z] = E[X + Y]\\) mittels \\(\\eqref{eq-stats-hypo-expected-add}\\):\n\\[\nE[Z] = E[X + Y] = E[X] + E[Y] = 1.25 + 1.625 = 2.875\n\\]\nDefinieren wir dagegen \\(Z\\) mit \\(Z := a \\cdot X\\) mit der Konstanten \\(a := 2\\). Dann folgt für den Erwartungswert von \\(E[Z]\\) mit Formel \\(\\eqref{eq-stats-hypo-expected-mult}\\):\n\\[\nE[Z] = E[aX] = aE[X] = 2 \\cdot 1.25 = 2.5\n\\]\nEin ganz anderes Beispiel, welches noch mal den Begriff Erwartungswert veranschaulicht, bezieht sich auf ein Glückspiel mit dem Namen Chuck-a-Luck. Das Beispiel ist Gross, Harris, und Riehl (2019) entnommen. Chucia-Luck wird mit einem Einsatz von 1 € gespielt. Es werden drei Würfel geworfen und die folgende Regeln bestimmen den Gewinn (siehe Tabelle 11.3).\n\n\nTabelle 11.3: Gewinnauschüttung bei Chuck-a-Luck\n\n\nAusgang\nGewinn\n\n\n\n\nkeine 6\n0 EU\n\n\nmin. eine 6\n2 EU\n\n\n3 x 6\n27 EU\n\n\n\n\nDie Frage die sich nun stellt, ist ob dieses Spiel fair ist bzw. lohnt es sich einen 1 € Einsatz zu setzen? Diese Frage kann mit dem Erwartungswert beantwortet werden. Um den Erwartungswert zu berechnen benötigen wir allerdings zunächst die Wahrscheinlichkeiten für die verschiedenen Ausgänge.\nDie Wahrscheinlichkeit keine \\(6\\) zu werfen ist für jeden Würfel einzeln \\(\\frac{5}{6}\\), dementsprechend, da die Würfel unabhängig voneinander sind, kann diese Wahrscheinlichkeit dreimal miteinander multipliziert werden.\n\\[\nP(0 \\times 6) = \\left(\\frac{5}{6}\\right)^3 = \\frac{125}{216} \\approx 0.579\n\\]\nD.h. in knapp 60% der Fälle wird beim dem Spiel kein Gewinn ausgeschüttet. Berechnen wir zunächst den Fall, dass drei Sechsen geworfen werden. Dieser Fall kann parallel zudemjenigen für keine Sechs gelöst werden. Einziger Unterschied, ist die Wahrscheinlichkeit des Ereignisses. Die Wahrscheinlichkeit für einen Würfel eine Sechs zu würfeln ist \\(\\frac{1}{6}\\). Es folgt daher analog:\n\\[\nP(3 \\times 6) = \\left(\\frac{1}{6}\\right)^3 = \\frac{1}{216} \\approx 0.005\n\\]\nD.h. die Wahrscheinlichkeit für \\(3 \\times 6\\) ist gerade einmal ein halbes Prozent. D.h. in 200 Spielen, erwawrten wir dieses Ereignis nur ein einziges Mal.\nLetzlich bleibt noch das Ereignis mindestens eine \\(6\\). Hier nehmen wir das Komplementärereignis zu mindestens eine Sechs. Das Komplementärereignis ist nämlich keine Sechs zu würfeln. Diese Wahrscheinlichkeit ziehen wird dann von 1, dem sicheren Ereignis, ab. Da diese Menge auch die drei Sechsen beinhaltet, für das eine andere Gewinnberechnung gilt, müssen wir dessen Wahrscheinlichkeit noch subtrahieren.\n\\[\nP(\\text{min. eine } 6) = 1 - P(0 \\times 6) - P(3 \\times 6) = \\frac{216}{216} - \\frac{125}{216} - \\frac{1}{216} = \\frac{90}{216} = 0.41\\bar{6}\n\\]\nDie Wahrscheinlichkeit für mindestens eine Sechs ist dementsprechend etwas über 40%. Jetzt wenden wir wieder die Formel für den Erwartungswert an um die zu erwartende Gewinnsumme zu bestimmen. Die Gewinnsumme nimmt jetzt den Wert der Zufallsvariablen ein.\n\\[\nE[X] = \\frac{125}{216}\\times 0 + \\frac{90}{216}\\times 2 + \\frac{1}{216}\\times27 = \\frac{207}{216} \\approx 0.958\n\\]\nIm Mittel erwarten wir bei dem Spiel einen Gewinn von \\(0.958\\)€ bei einem Einsatz von \\(1\\) €. Daher wird im Mittel ein Verlust bei dem Spiel gemacht.\nAls letztes Beispiel betrachten wir den Erwartungswert des Mittelwerts \\(\\bar{x}\\) und wenden dabei die obigen Rechenregeln an.\n\\[\nE[\\bar{x}] = E\\left[\\frac{1}{n}\\sum_{i=1}^n x_i\\right] = \\frac{1}{n}\\sum_{i=1}^n E[x_i] = \\frac{1}{n}\\sum_{i=1}^n \\mu = \\frac{1}{n}n \\mu = \\mu\n\\]\n\n\n\n11.2.2 Varianz"
  },
  {
    "objectID": "stats_hypotheses.html#schätzer",
    "href": "stats_hypotheses.html#schätzer",
    "title": "11  Hypothesen testen",
    "section": "11.3 Schätzer",
    "text": "11.3 Schätzer\nErwartungstreue"
  },
  {
    "objectID": "stats_hypotheses.html#hypothesentestung",
    "href": "stats_hypotheses.html#hypothesentestung",
    "title": "11  Hypothesen testen",
    "section": "11.4 Hypothesentestung",
    "text": "11.4 Hypothesentestung\n\n11.4.1 Der t-Test\nDas Verhältnis einer standardnormalverteilten Variable z und eine \\(\\chi^2\\)-verteilten Variable s folgt einer \\(t\\)-Verteilung.\n\\[\nT = \\frac{\\hat{\\Delta}}{\\hat{s}_e(\\hat{\\delta})} \\sim t\\text{-Verteilung}\n\\]\n\n\n11.4.2 \\(\\chi^2\\)-Test der Varianz\nSei \\(\\hat{\\sigma}^2\\) ein Schätzer für eine Varianz und \\(H_0: \\sigma^2 = \\sigma_0^2\\) die Nullhypothese, dann lässt sich eine Teststatistik über die folgende Formel konstruieren:\n\\[\nT = d \\frac{\\hat{\\sigma}^2}{\\sigma_0^2} \\sim \\chi^2(d\\text{ Freiheitsgrade})\n\\]\n\n\n11.4.3 F-Test von Varianzverhältnissen\nSeien zwei normalverteilte Stichproben gegeben und deren Varianzen über \\(\\hat{\\sigma}_A^2\\) und \\(\\hat{\\sigma}_B^2\\) abgeschätzt werden dann kann eine Teststatisk über die Gleichheit der beiden Varianzen \\(\\sigma_A^2 = \\sigma_B^2\\) über die folgende Formel konstruiert werden.\n\\[\nT = \\frac{\\hat{\\sigma}^2_A}{\\hat{\\sigma}^2_B} \\sim F(df_A, df_B)\n\\]\nDie beiden Varianzen folgen dabei jeweils einer \\(\\chi^2\\) Verteilung mit Freiheistgraden \\(df_A\\) und \\(df_B\\), so dass die Statistik \\(T\\) einer \\(F\\)-Verteilung mit \\((df_A, df_B)\\) Freiheitsgeraden folgt und die \\(H_0\\) lautet \\(H_0: \\frac{\\sigma_A^2}{\\sigma_B^2} = 1\\)\n\n\n\n\nGross, Benedict, Joe Harris, und Emily Riehl. 2019. Fat Chance: Probability from 0 to 1. Cambridge University Press."
  },
  {
    "objectID": "slm_title.html",
    "href": "slm_title.html",
    "title": "Das einfache Regressionmodell",
    "section": "",
    "text": "Wir beginnen nun mit dem einfachen Regressionsmodell. Das Modell knüpft an unsere Vorkenntnisse aus der Schule mit linearen Gleichungen an. Ausgehend von diesem Modell werden schrittweise neue Konzept eingeführt. Diese Herangehensweise hat den Vorteil, dass eine einfaches mentales Template immer wieder auf die neuen Konzepte abgebildet werden kann. Diese stetige Aufbau vollzieht sich über den ersten Teil des einfachen Regressionsmodells und wird dann im folgenden Teil, welcher die multiple Regression behandelt, fortgeführt. Dabei wird auch gezeigt, wie vorher voneinander unabhängig gelernte Methoden, wie die Regression und die ANOVA letztendlich aus dem gleichen Ansatz entstehen und es eigentlich keinen Unterschied zwischen den beiden Ansätzen gibt."
  },
  {
    "objectID": "slm_basics.html#back-to-school",
    "href": "slm_basics.html#back-to-school",
    "title": "12  Einführung",
    "section": "12.1 Back to school",
    "text": "12.1 Back to school\nWir beginnen mit ein Konzept mit dem wir sehr gut umgehen können. Nämlich der Punkt-Steigungsform aus der Schule (siehe Gleichung 12.1).\n\\[\ny = m x + b\n\\tag{12.1}\\]\nWir haben eine abhängige Variable \\(y\\) und eine lineare Formel \\(mx + b\\) die den funktionalen Zusammenhang zwischen den Variablen \\(y\\) und \\(x\\) beschreibt. Um das Ganze einmal konkret zu machen setzen wir \\(m = 2\\) und \\(b = 3\\) fest. Die Formel Gleichung 12.1 wird dann zu:\n\\[\ny = 2 x + 3\n\\tag{12.2}\\]\nUm ein paar Werte für \\(y\\) zu erhalten setzen wir jetzt verschiedene Wert für \\(x\\) ein indem wir \\(x\\) in Einserschritten zwischen \\([0, \\ldots, 5]\\) erhöhen. Um die Werte darzustellen verwenden wir zunächst eine Tabelle (vlg. Tabelle 12.1)\n\n\n\n\nTabelle 12.1: Tabelle der Daten\n\n\nx\ny\n\n\n\n\n0\n3\n\n\n1\n5\n\n\n2\n7\n\n\n3\n9\n\n\n4\n11\n\n\n5\n13\n\n\n\n\n\n\nWenig überraschend nimmt \\(y\\) für den Wert \\(x = 0\\) den Wert \\(3\\) an und z.B. für den Wert \\(x = 3\\) nimmt \\(y\\) den Wert \\(2 \\cdot 3 + 3 = 9\\) an.\nEine andere Darstellungsform ist naturlich eine graphische Darstellung in dem wir die Werte von \\(y\\) gegen \\(x\\) auf einem Graphen abtragen (siehe Abbildung 12.1).\n\n\n\n\n\nAbbildung 12.1: Graphische Darstellung der Daten aus Tabelle 12.1\n\n\n\n\nWiederum wenig überraschen sehen wir einen linearen Zuwachs der \\(y\\)-Wert mit den größerwerdenden \\(x\\)-Werte. Da in der Definition der Formel Gleichung 12.2 nirgends festgelegt wurde, dass diese nur für ganzzahlige \\(x\\)-Werte gilt, haben wir direkt eine Gerade durch die Punkte gelegt. Hier wird auch die Bedeutung von \\(m\\) und \\(b\\) direkt klar. Die Variable \\(m\\) bestimmt die Steigung der Gleichung während \\(b\\) den y-Achsenabschnitt beschreibt.\n\nDefinition 12.1 (\\(y\\)-Achsenabschnitt) Der y-Achsenabschnitt ist der Wert den \\(y\\) einnimmt wenn \\(x\\) den Wert \\(0\\) annimmt. Sei \\(y\\) durch eine lineare Gleichung \\(y = mx + b\\) definiert, dann wird der y-Achsenabschnitt durch den Wert \\(b\\) bestimmt.\n\nDie Variable \\(m\\) dahingehend bestimmt die Steigung der Gerade.\n\nDefinition 12.2 (Steigungskoeffizient) Wenn \\(y\\) durch eine lineare Gleichung \\(y = mx + b\\) definiert ist, dann bestimmt die Variable \\(m\\) die Steiung der dazugehörenden Gerade. D.h. wenn sich die Variable \\(x\\) um einen Einheit vergrößert (verkleinert) wird der Wert von \\(y\\) um \\(m\\) Einheiten größer (kleiner). Gilt \\(m &lt; 0\\) dann umgekehrt.\n\nDiese beiden trivialen Konzepte mit eigenen Definitionen zu versehen erscheint im ersten Moment vielleicht etwas übertrieben. Wie sich allerdings später zeigen wird, sind diese beiden Einsichten immer wieder zentral wenn es um die Interpretation von linearen statistischen Modellen geht.\nSoweit so gut. Führen wir direkt ein paar Symbole ein, die uns später noch behilflich sein werden. Sei jetzt die Menge der \\(x\\)-Werte geben \\(x = [0, 1, 2, 3, 4, 5]\\). Strenggenommen handelt es sich wieder um ein Tupel, da wir jetzt die Reihenfolge nicht mehr ändern. Wir führen nun einen Index \\(i\\) ein, um einzelne Werte in dem Tupel über ihre Position zu bestimmen und wir hängen diesen Index \\(i\\) an \\(x\\) an. Dann wird aus \\(x\\), \\(x_i\\).\n\n\n\n\nTabelle 12.2: \\(x\\)-Werte und ihr Index \\(i\\)\n\n\nIndex \\(i\\)\n\\(x\\)-Wert\n\n\n\n\n1\n0\n\n\n2\n1\n\n\n3\n2\n\n\n4\n3\n\n\n5\n4\n\n\n6\n5\n\n\n\n\n\n\nDamit können wir jetzt einen speziellen Wert zum Beispiel den dritten Wert mit \\(x_3 = 2\\) bestimmen. Wenden wir unseren Index auf unsere Gleichung 12.1 an, folgt daraus, dass \\(y\\) jetzt auch einen Index \\(i\\) erhält.\n\\[\ny_i = m x_i + b \\qquad i \\text{ in } [1,2,3,4,5,6]\n\\]\nWir bezeichnen die beiden Variablen \\(m\\), die Steigung, und \\(b\\), den y-Achsenabschnitt, jetzt auch mit neuen Variablen die auch noch einen Index erhalten. Aus \\(m\\) wird \\(\\beta_1\\) und aus \\(b\\) wird \\(\\beta_0\\). Damit wird der y-Achsenabschnitt mit \\(\\beta_0\\) bezeichnet und die Steigung wird mit \\(\\beta_1\\) bezeichnet. Dann wir aus unserer Gleichung:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i\n\\tag{12.3}\\]\nDas ist immer noch unsere einfache Punkt-Steigungsform, wir haben lediglich den Index \\(i\\) eingeführt um unterschiedliche \\(y-x\\)-Wertepaare zu bezeichnen und wir haben den \\(y\\)-Achsenabschnitt und die Steigung mit neuen Symbolen versehen.\nBei dem bisherigen Zusammenhang handelt es sich um einen funktionalen Zusammenhang zwischen den beiden Variablen \\(x\\) und \\(y\\). Funktional deswegen, weil wir eine definiertes mathematisches Modell angeben können, d.h. wir haben eine mathematische Funktion welche die Beziehung zwischen den beiden Variablen beschreibt. Wenn wir den Wert für \\(x\\) kenne, dann können wir den präzisen Wert für \\(y\\) ausreichen, indem wir ihn in Gleichung 12.1 einsetzen. Aus der Schule kennen wir auch noch die Darstellung \\(y = f(x)\\). Streng genommen ist diese Darstellung für Gleichung 12.1 nicht ausreichend, denn um den Wert für \\(y\\) auszurechnen benötigen wir auch noch Kenntnis über die Werte \\(m\\) und \\(b\\), bzw. in unsere weiteren Darstellung \\(\\beta_0\\) und \\(\\beta_1\\). Daher sollte der Zusammenhang eigentlich mit \\(y = f(x, \\beta_0, \\beta_1)\\) bezeichnet werden. Es gilt aber immernoch, für gegebene \\(x, \\beta_0\\) und \\(\\beta_1\\) ist der Wert für \\(y\\) fest determiniert.\nWenn wir mit realen Daten arbeiten, dann funktioniert dieser Ansatz leider nicht ganz. Selbst wenn wir ein Experiment gleich durchführen werden wir immer etwas unterschiedliche Werte im Sinne der Messungenauigkeit messen. Wenn wir biologische Systeme messen, kommt dazu das diese in den seltensten Fällen zeitstabil sind sondern immer bestimmte Veränderungen von einem Zeitpunkt zum nächsten auftauchen. In Abbildung 12.2 sind Sprungweiten von mehreren Weitspringerinnen gegen die Anlaufgeschwindigkeit abgetragen. Bei der Betrachtung der Daten erscheint ein linearer Zusammenhang zwischen diesen beiden Variablen durchaus als plausibel.\n\n\n\n\n\nAbbildung 12.2: Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite beim Weitsprung\n\n\n\n\nIn Abbildung 12.2 sind zwei Punkte rot markiert. Die beiden Werte haben praktisch die gleichen \\(x\\)-Werte allerdings unterscheiden sich die \\(y\\)-Werte deutlich von einander. Und dies sind nicht die einzigen Beispielpaare bei denen die \\(x\\)-Werte nahe beiandern liegen, während die \\(y\\)-Werte deutlich weiter voneiander entfernt liegen als bei einen funktionalen Zusammenhang nach Gleichung 12.1 zu erwarten wäre. Diese Abweichungen kommen durch zufällige Einflussfaktoren wie eben zum Beispiel die Veränderungen angesprochener biologischer Faktoren, Messunsicherheiten, beim Weitsprung draußen sind auch immer externe Einflüsse mögliche, vielleicht wenn es sich um den gleichen Springer handelt, hat er auch beim zweiten Mal keine Lust mehr gehabt. Wenn die Punkte zwei unterschiedliche Springer sind, dann kommt auch dazu, dass zwei Weitspringer bei identischer Anlaufgeschwindigkeit unterschiedliche Sprungfähigkeiten haben oder auch technisch nicht gleich gesprungen sind und so weiter und so fort. Insgesamt führen alle diese Einflüsse dazu, dass wir nicht mehr einen streng funktionalen Zusammenhang zwischen unseren beiden Variablen \\(x\\) der Anlaufgeschwindigkeit und \\(y\\) der Sprungweite vorfinden. Wie wir mit diesen Einflüssen umgehen ist das zentrale Thema des nächsten Abschnitts und markiert auch unseren Eingang zur einfachen linearen Regression."
  },
  {
    "objectID": "slm_basics.html#die-einfache-lineare-regression",
    "href": "slm_basics.html#die-einfache-lineare-regression",
    "title": "12  Einführung",
    "section": "12.2 Die einfache lineare Regression",
    "text": "12.2 Die einfache lineare Regression\nBleiben wir bei unserem Beispiel aus Abbildung 12.2 und interpretieren das als praktisches Problem. Wir sind eine Weitsprungtrainerin und stehen jetzt vor der Aufgabe in unserem Training etwas zu verändern um die Weitsprungleistung zu verbessern. Wir haben wir haben uns dazu entschlossen am Anlauf etwas zu verbessern wissen jetzt aber nicht ob, das wirklich lohnenswert ist. Von einer befreundeten Trainerin haben wir einen Datensatz bekommen von Anlaufgeschwindigkeiten und den dazugehörigen Sprungweiten. Schauen wir uns zunächst die einmal die Struktur der Daten an.\n\n\n\n\nTabelle 12.3: Ausschnitt der Sprungdaten\n\n\njump_m\nv_ms\n\n\n\n\n4.36\n6.13\n\n\n4.31\n6.39\n\n\n4.56\n6.56\n\n\n4.75\n6.44\n\n\n5.52\n7.30\n\n\n5.63\n7.19\n\n\n5.70\n7.30\n\n\n\n\n\n\nIn Tabelle 12.3 ist ein Ausschnitt Sprungdaten abgebildet. Wir haben eine einfache Struktur der Daten. Wir haben eine Tabelle mit zwei Spalten. jump_m bezeichnet die Sprungweiten und v_ms die Anlaufgeschwindigkeiten. Damit wir die Datenpaare voneinander unterscheiden bzw. identifzieren können führen wir unseren bereits besprochenen Index \\(i\\) und können so einzelne Paare ansprechen.\n\n\n\n\nTabelle 12.4: Ausschnitt der Sprungdaten\n\n\ni\njump_m\nv_ms\n\n\n\n\n1\n4.36\n6.13\n\n\n2\n4.31\n6.39\n\n\n3\n4.56\n6.56\n\n\n4\n4.75\n6.44\n\n\n5\n5.52\n7.30\n\n\n6\n5.63\n7.19\n\n\n7\n5.70\n7.30\n\n\n\n\n\n\nDas waren bisher aber nur Formalitäten. Wir wollen jetzt denn Zusammenhang zwischen den beiden Variablen modellieren. Wir könnten wahrscheinlich auch einfach Pi-mal-Daumen abschätzen wie groß der Zusammenhang ist. Wenn wir jetzt aber einen unserer Läufer haben, der z.B. etwa \\(9m/s\\) anläuft, welchen Vergleichswerte nehmen wir dann aus Abbildung 12.2. Den unteren oder den oberen der beiden roten Werte? Oder vielleicht den Mittelwert? Welchen Wert nehmen wir wenn unserer Athlete \\(9.7m/s\\) anläuft. Da haben wir leider keinen Vergleichswert in unserer Tabelle. Daher wäre es schon ganz praktisch eine Formel nach dem Muster von Gleichung 12.3 zu haben. Wie wir allerdings schon festgestellt haben, geht dies nicht so einfach da wir eben das Problem mit den Einflussfaktoren haben, die dazu führen, dass die Werte eben nicht streng auf eine Gerade liegen. Somit liegt die Herausforderung nun eine Gerade zu finden die möglichst genau die Daten wiederspiegelt.\n\n\n\n\n\nAbbildung 12.3: Mögliche Geraden um den Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite zu modellieren\n\n\n\n\nIn Abbildung 12.3 sind die Daten zusammen mit verschiedenen möglichen Geraden abgebildet. Eine kurze Überlegung macht schnell klar, dass es im Prinzip unendlich viele unterschiedliche Geraden gibt die durch die Datenpunkte gelegt werden können. D.h. es gibt unendlich viele Kombinationen von \\(\\beta_0\\) und \\(\\beta_1\\), die die jeweiligen Geraden bezeichnen. Daher muss jetzt eine Kriterium gefunden werden, welches ermöglicht aus diesen unendlich vielen Geraden eine auszuwählen die im Sinne des Kriterium optimal ist.\nTatsächlich gibt es dort auch verschiedene Möglichkeiten Kriterien anzuwenden, dasjenige dass jedoch am weitesten verbreitet ist aus verschiedenen Gründen sind die quadratierten Abweichungen von der Gerade. Schauen wir uns die Herleitung dazu schrittweise an. In Abbildung 12.4 ist zur Übersicht nur ein Ausschnitt der Daten zusammen mit einer möglichen Gerade eingezeichnet. Die senkrechten Abweichungen der Geraden zu den jeweiligen Datenpunkten sind rot eingezeichnet. Es ist ersichtlich, dass für diese Wahl der Geraden es zwei Punkte gibt die tatsächlich auch ziemlich genau auf der Geraden liegen während die anderen Punkte zum Teil oberhalb bzw. unterhalb der Geraden liegen. Das Kriterium wäre jetzt dementsprechen die jenige Geraden aus den unendlich vielen zu finden, bei der diese Abweichung ein Minimum annehmen.\n\\[\n\\text{min}\\sum_{i=1}^n y_i - (\\beta_0 + \\beta_1 x_i) = \\sum_{i=1}^n y_i - \\beta_0 - \\beta_1 x_i\n\\]\n\n\n\n\n\nAbbildung 12.4: Abweichungen der Gerade von der Datenpunkten für die Daten mit eine Anlaufgeschwindigkeit zwischen \\(8m/s\\) und \\(10m/s\\).\n\n\n\n\nUnglücklicherweise haben die einfachen Abweichungen die unhandliche Eigenschaft, dass dann die Gerade \\(y_i = \\hat{y}\\) optimal ist.\n\\[\n\\sum_{i=1}^n y_i - \\hat{y} = \\sum_i^n y_i - \\sum_{i=1}^n \\hat{y} = \\sum_{i=1}^n y_i - n\\hat{y} = \\sum_{i=1}^n y_i - n\\frac{1}{n}\\sum_{i=1}^n y_i = \\sum_{i=1}^n y_i - \\sum_{i=1}^n y_i= 0\n\\]\nWir können das Kriterium aber auch noch etwas schärfer machen. Wenn wir sagen, dass wir größere Abweichungen stärker gewichten wollen als kleinere Abweichungen. D.h. große Abweichungen zwischen der Gerade und den Datenpunkten sollten stärker berücksichtigt werden, als kleine Abweichungen. Dies können wir erreichen indem wir die Abweichungen noch zusätzlich quadrieren. Dies hat auch noch den Vorteil noch verschiedene andere mathematische Vorteile, unter anderem führt dies dazu, dass wir eine Gerade erhalten, die auch tatsächlich die Steigung der Punkte berücksichtigt und nicht einfache nur eine horizontale Gerade durch die Punkte zeichnet. Dementsprechend erhalten wir die folgende Funktion, die es zu minimieren gilt:\n\\[\n\\text{min} \\sum_{i=1}^n(y_i - (\\beta_0 + \\beta_1 x_i))^2\n\\tag{12.4}\\]\nDie Abweichungen zwischen der zu findenden Gerade und den Datenpunkten werden als Residuen \\(e_i\\) bezeichnet. Dementsprechend ist die Minimierungsgleichung auch als:\n\\[\n\\text{min} \\sum_{i=1}^n e_i^2\n\\] darzustellen, mit \\(e_i := y_i - (\\beta_0 + \\beta_1 x_i)\\). Führen wir noch eine weitere Bezeichnung \\(E\\) ein, mit der wir die Minimierungsfunktion bezeichnen (\\(E\\) nach englisch error).\n\\[\nE = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2\n\\]\nDas Minimum läßt sich finden, indem die partiellen Ableitungen von \\(E\\) nach \\(\\beta_0\\) und \\(\\beta_1\\) berechnet werden und, wie wir es aus der Schule kennen, die Ableitungen gleich Null gesetzt werden.\n\\[\\begin{align*}\n\\frac{\\partial E}{\\partial \\beta_0} &= -2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i) = 0 \\\\\n\\frac{\\partial E}{\\partial \\beta_1} &= -2 \\sum_{i=1}^n x_i (y_i - \\beta_0 - \\beta_1 x_i) = 0\n\\end{align*}\\]\nDiese Gleichungen lassen sich umstellen und nach \\(\\beta_0\\) und \\(\\beta_1\\) auflösen:\n\\[\\begin{align}\n\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1} \\bar{x} \\label{eq-slm-basics-norm1}\n\\end{align}\\]\n\\(\\bar{x}\\) und \\(\\bar{y}\\) sind wieder die Mittelwerte von \\(x_i\\) und \\(y_i\\). Diese beiden Gleichungen werden als die Normalengleichungen bezeichnet.\nWir führen noch einen weiteren Term ein, den vorhergesagten Wert \\(\\hat{y}_i\\) von \\(y_i\\) anhand der Geradengleichung. Das Hütchen über \\(y_i\\) ist dabei immer das Signal dafür, das es sich um einen abgeschätzten Wert handelt. Wenn wir \\(\\beta_0\\) und \\(\\beta_1\\) anhand der Normalengleichung bestimmen, dann sind das mit großer Wahrscheinlichkeit nicht die wahren Werte aus der Population, sondern wir haben sie nur anhand der Daten abgeschätzt. Daher bekommen die berechneten Werte ebenfalls ein Hütchen \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\). Insgesamt nimmt die lineare Geradengleichung dann die folgende Form an:\n\\[\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i\n\\]\nGraphisch sind die \\(\\hat{y}_i\\)s die Werte auf der Geraden für die gegebenen \\(x_i\\)-Werte.\n\n\n\n\n\nAbbildung 12.5: Die vorhergesaten Werte \\(\\hat{y}_i\\) auf der Gerade.\n\n\n\n\nFür den vorliegenden Fall der Weitsprungdaten erhalten wir die Werte für die Koeffizienten nach Einsetzen der beobachteten Werte in Formel \\(\\eqref{eq-slm-basics-norm1}\\) mit \\(\\hat{\\beta}_0 = -0.14\\) und \\(\\hat{\\beta}_1 = 0.76\\). Somit folgt für die Geradengleichung:\n\\[\n\\hat{y}_i = -0.14 + 0.76 \\cdot x_i\n\\]\nWir erhalten die graphische Darstellung der Geradengleichung indem die \\(x_i\\)-Werte eingesetzt werden und eine Gerade durch die Punkte gezogen wird. Oder auch einfacher für den größten und den kleinsten \\(x_i\\)-Wert.\n\n\n\n\n\nAbbildung 12.6: Die Regressionsgerade der Sprungdaten.\n\n\n\n\nUm uns auch zu vergewissern, dass unsere Berechnungen korrekt sind, schauen wir uns noch einmal an, wie sich \\(E\\) verhält, wenn wir unterschiedliche Kombinationen von Werten für \\(\\beta_0\\) und \\(\\beta_1\\) in die lineare Gleichung einsetzen.\n\n\n\n\n\nAbbildung 12.7: Heatmap von \\(log(E)\\) für verschiedene Werte von \\(\\beta_0\\) und \\(\\beta_1\\)\n\n\n\n\nIn Abbildung 12.7 sind verschiedene Werte für \\(E\\) in Form einer heatmap dargestellt. Die Abweichungen wurden \\(log\\)-transformiert (d.h. der Logarithmus der \\(E\\)-Werte wurde berechnet), da sonst die Unterschiede in der diagnaolen Bildrichtung zu schnell wachsen und die Unterschiede nicht mehr so einfach zu erkennen sind. Werte näher an Weiß bedeuten kleine Werte und Werte näher an Rot bedeuten größere Werte von \\(E\\). Das berechnete Paar für \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) mit \\(\\hat{\\beta}_0 = -0.14\\) und \\(\\hat{\\beta}_1 = 0.76\\) ist schwarz eingezeichnet. Die Abbildung zeigt, dass dieses Wertepaar tatsächlich ein Minimum bezüglich der Funktion \\(E\\) ist, da in alle Richtung weg von dem schwarzen Punkt die Werte für \\(E\\) zunehmen. Da wir nur einen Ausschnitt der möglichen Werte sehen, handelt es sich zunächst um eine lokales Minimum aber es lässt sich zeigen, dass es sich dabei auch um ein globales Minimum handelt. Diese Eigenschaft hängt mit der Form der Funktion \\(E\\) zusammen. In Tabelle 12.5 sind beispielhaft ein paar Werte für \\(log(E)\\) für Paare von \\(\\beta_0\\) und \\(\\beta_1\\) angezeigt, die in Abbildung 12.7 gelb eingezeichnet sind.\n\n\n\n\nTabelle 12.5: Werte von \\(log(E)\\) für verschiedenen Kombinationen von \\(\\beta_0\\) und \\(\\beta_1\\).\n\n\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(log(E)\\)\n\n\n\n\n-0.48\n0.67\n70.34\n\n\n-0.44\n0.68\n51.85\n\n\n-0.38\n0.72\n22.04\n\n\n-0.30\n0.75\n6.46\n\n\n-0.22\n0.75\n3.77\n\n\n-0.14\n0.76\n2.41\n\n\n\n\n\n\n\n12.2.1 Schritt-für-Schritt Herleitung der Normalengleichungen\nUm die Herleitung der Normalengleichungen Schritt-für-Schritt nachvollziehen zu können benötigen wir zunächst einmal ein paar algebraische Tricks.\nFür den Mittelwert gilt: \\[\n\\bar{x} = \\frac{1}{n}\\sum x_i \\Leftrightarrow \\sum x_i = n \\bar{x}\n\\]\nBei Summen und konstanten \\(a\\) konstant gilt: \\[\\begin{align}\n    \\sum a &= n a \\\\\n    \\sum a x_i &= a \\sum x_i \\\\\n    \\sum (x_i + y_i) &= \\sum x_i + \\sum y_i\n\\end{align}\\]\nWenn eine Summe abgeleitet wird, kann in die Ableitung in die Summe reingezogen werden. \\[\n\\frac{d}{d x}\\sum f(x) = \\sum\\frac{d}{d x} f(x)\n\\]\nHier ein zwei Umformungen bei Summen und dem Kreuzprodukt bzw. dem Quadrat. \\[\\begin{alignat}{2}\n&& \\sum(x_i-\\bar{x})(y_i-\\bar{y}) \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (x_iy_i-\\bar{x}y_i-x_i\\bar{y}+\\bar{x}\\bar{y}) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_i y_i - \\sum\\bar{x}y_i - \\sum x_i \\bar{y} + \\sum \\bar{x} \\bar{y} \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu&&  \\sum x_iy_i - n\\bar{x}\\bar{y}-n\\bar{x}\\bar{y}+n\\bar{x}\\bar{y} \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_iy_i - n\\bar{x}\\bar{y} \\nonumber\n\\end{alignat}\\] \\[\\begin{alignat}{2}\n&& \\sum(x_i - \\bar{x})^2 \\\\\n\\Leftrightarrow\\mkern40mu && \\sum(x_i^2 - 2 x_i \\bar{x} + \\bar{x}^2) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_i^2 - 2\\bar{x}\\sum x_i + \\sum\\bar{x}^2 \\nonumber\\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_i^2 - 2\\bar{x}n\\bar{x} + n\\bar{x}^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum x_i^2 - n \\bar{x}^2 \\nonumber\n\\end{alignat}\\]\nZurück zu unserem Problem. Es gilt \\(E\\) zu minimieren:\n\\[\\begin{alignat}{2}\n&& E = \\sum e_i^2 = \\sum (y_i - \\hat{y}_i)^2 \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - (\\beta_0 + \\beta_1 x_i))^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2 \\nonumber\n\\end{alignat}\\]\nDie Gleichung hängt von zwei Variablen \\(\\beta_0\\) und \\(\\beta_1\\). Um das Minimum der Gleichung zu erhalten, verfährt man wie in der Schule, indem man die Ableitung gleich Null setzt. Der vorliegenden Fall ist jedoch etwas komplizierter, da die Gleichung von zwei Variablen abhängt. Daher müssen wir die partiellen Ableitungen \\(\\frac{\\partial}{\\partial \\beta_0}\\) und \\(\\frac{\\partial}{\\partial \\beta_1}\\) verwendet. Wir erhalten dadurch ein Gleichungssystem mit zwei Gleichungen (die jeweiligen Ableitungen) in zwei Unbekannten (\\(\\beta_0\\) und \\(\\beta_1\\)). Die Lösung erfolgt, indem zuerst eine Gleichung nach der einen Unbekannten umgestellt wird und das Ergebnis dann in die andere Gleichung eingesetzt wird.\nWir beginnen mit der partiellen Ableitung nach \\(\\beta_0\\) für den y-Achsenabschnitt. (Zurück an die Schule erinnern: Äußere Ableitung mal innere Ableitung)\n\\[\\begin{alignat}{2}\n&& \\frac{\\partial \\sum (y_i - \\beta_0 - \\beta_1 x_i)^2}{\\partial \\beta_0} \\\\\n\\Leftrightarrow\\mkern40mu && \\sum\\frac{\\partial}{\\partial \\beta_0}(y_i - \\beta_0- \\beta_1 x_i)^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum 2(y_i - \\beta_0- \\beta_1 x_i) (-1) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && -2 \\sum (y_i - \\beta_0- \\beta_1 x_i) \\nonumber\n\\end{alignat}\\] Zum minimieren gleich Null setzen. \\[\\begin{alignat}{2}\n&& -2 \\sum (y_i - \\beta_0- \\beta_1 x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0- \\beta_1 x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i - \\sum \\beta_0- \\sum \\beta_1 x_i = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && n \\bar{y} - n \\beta_0- \\beta_1 n \\bar{x} = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\bar{y} - \\beta_0- \\beta_1 \\bar{x} = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\bar{y} - \\beta_1 \\bar{x} = \\beta_0\\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\beta_0= \\bar{y} - \\beta_1 \\bar{x}\n\\end{alignat}\\]\nEs folgt nach dem gleichen Prinzip die Herleitung für die Steigung \\(\\beta_1\\) und indem die Lösung für \\(\\beta_0\\) eingesetzt wird.\n\\[\\begin{alignat}{2}\n&& \\frac{\\partial \\sum (y_i - \\beta_0 - \\beta_1x_i)^2}{\\partial \\beta_1} \\\\\n\\Leftrightarrow\\mkern40mu && \\sum\\frac{\\partial}{\\partial b}(y_i - \\beta_0 - \\beta_1x_i)^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum2(y_i - \\beta_0 - \\beta_1x_i) -x_i \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && -2 \\sum(y_i - \\beta_0 - \\beta_1x_i)x_i\n\\end{alignat}\\] Wiederum gleich Null setzen. \\[\\begin{alignat}{2}\n&& -2 \\sum(y_i - \\beta_0 - \\beta_1x_i)x_i = 0 \\nonumber\\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i - \\beta_0 - \\beta_1x_i)x_i = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (y_i x_i - \\beta_0 x_i - \\beta_1x_i x_i) = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - \\beta_0 \\sum x_i - b\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n \\beta_0 \\bar{x} - \\beta_1\\sum x_i^2 = 0 \\nonumber\n\\end{alignat}\\] Einsetzen der Lösung für \\(\\beta_0\\) führt zu: \\[\\begin{alignat}{2}\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n (\\bar{y} - \\beta_1 \\bar{x}) \\bar{x} - \\beta_1\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n\\bar{y}\\bar{x} + n \\beta_1\\bar{x}^2 - \\beta_1\\sum x_i^2 = 0 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum y_i x_i - n\\bar{y}\\bar{x} = \\beta_1 \\sum x_i^2 - \\beta_1n \\bar{x}^2 \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\sum (x_i-\\bar{x})(y_i-\\bar{y}) = \\beta_1 (\\sum x_i^2 - n\\bar{x}^2) \\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum x_i^2 - n\\bar{x}^2} = \\beta_1\\nonumber \\\\\n\\Leftrightarrow\\mkern40mu && \\beta_1= \\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2} \\nonumber\n\\end{alignat}\\]\nSomit erhält man die beiden Normalengleichungen der Regression.\nÜber diese beiden Gleichungen erhalten wir die gewünschten Koeffizienten \\(\\hat{\\beta_0}\\) und \\(\\hat{\\beta_1}\\). Die Methode wird als die als die Methode der kleinsten Quadrate bezeichnet oder im Englischen Root-Mean-Square (RMS)."
  },
  {
    "objectID": "slm_basics.html#was-bedeuten-die-koeffizienten",
    "href": "slm_basics.html#was-bedeuten-die-koeffizienten",
    "title": "12  Einführung",
    "section": "12.3 Was bedeuten die Koeffizienten?",
    "text": "12.3 Was bedeuten die Koeffizienten?\nGehen wir zurück nun zu unseren Ausgangsproblem der Weitspringer, was haben wir jetzt durch die Berechnung der Gerade eigentlich gewonnen? Dazu müssen wir erst einmal verstehen was die beiden Koeffizienten \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) bedeuten. Wenn wir zurück zu Gleichung 12.1 gehen, haben die beiden Koeffzienten den \\(y\\)-Achsenabschnitt und die Steigung der Geraden beschrieben. In unserem Beispiel haben wir anhand der Daten einen \\(y\\)-Achsenabschnitt \\(\\hat{\\beta}_0\\) von \\(-0.14\\) berechnet. D.h ein Weitspringer der mit einer Anlaufgeschwindigkeit von \\(x = 0\\) anläuft, landet \\(14\\)cm hinter der Sprunglinie. Dies macht offensichtlich nicht viel Sinn (warum?). Der Grund warum hier ein offensichtlich unrealistischere Wert berechnet wurde, werden wir später noch genauer betrachten. Wir können trotzdem zwei Eigenschaften von \\(\\hat{\\beta}_0\\) beobachten. 1) der Koeffizient hat eine Einheit, nämlich die gleiche Einheit wie die Variable \\(y\\). 2) Ob der Wert zu interpretieren ist, hängt von der Verteilung der Daten ab. Schauen wir uns nun den Steigungskoeffizienten \\(\\hat{\\beta}_0\\) an. Der Steigungskoeffizient in Gleichung 12.1 zeigt an, wie sich der \\(y\\)-Wert verändert, wenn sich der \\(x\\)-Wert um einen Einheit verändert. In unserem Fall welcher Unterschied zu erwarten ist zwischen zwei Weitspringern die sich in der Anlaufgeschwindigkeit um eine \\(m/s\\) unterscheiden. D.h. der Steigungskoeffizient ist ebenfalls in der Einheit der \\(y\\)-Variable zu interpretieren.\nUnsere Trainerin kann jetzt die berechnete Gerade dazu nehmen um zu überprüfen ob es sich lohnen würde Trainingszeit in den Anlauf zu stecken und welche Verbesserung dort zu erwarten sind. Allerdings fehlt dazu noch etwas, wir wissen nämlich noch nicht ob die berechnete Gerade auch wirklich die Daten gut wiederspiegelt. Im Beispiel erscheint dies anhand der Grafik als relativ plausibel. Das muss aber nicht immer so sein. Wir können nämlich für alle möglichen Daten eine Gerade berechnen ohne das diese Gerade die Daten wirklich auch nur annährend korrekt wiedergibt. In Formel \\(\\eqref{eq-slm-basics-norm1}\\) steht nirgends für welche Daten die Berechnung nur erlaubt ist.\n\n\n\n\n\nAbbildung 12.8: Gefittete Gerade durch die Daten einer Funktion \\(f(x) = x^3\\).\n\n\n\n\nIn Abbildung 12.8 sind synthetische Daten der Funktion \\(f(x) = x^3\\) abgebildet und die mittels ?eq-slm-basics-norm1 berechneten Gerade eingezeichnet. Die Gerade ist zwar in der Lage die ansteigenden Werte zu modellieren aber eben nicht Schwingungen die durch die kubische Abhängigkeit zustande kommen. Aber, nichts verhindert die Anwendung der Formel auf die Daten.\nDer gleiche Effekt ist auch in Abbildung 12.9 wieder zu beobachten. Hier besteht eine sinusförmige Abhängigkeit zwischen \\(y\\) und \\(x\\). Wir können wieder \\(\\eqref{eq-slm-basics-norm1}\\) anwenden und erhalten auch ein Ergebnis für \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\). Allerdings repräsentiert die Gerade in keinster Weise den tatsächlichen Zusammenhang zwischen den Daten.\n\n\n\n\n\nAbbildung 12.9: Gefittete Gerade durch die Daten einer Funktion \\(f(x) = sin(x) + 3\\).\n\n\n\n\nIm nächsten Kapitel werden wir uns daher damit beschäftigen die Repräsentation der Daten näher zu betrachten und zu präzisieren.\nWir nehmen noch eine weitere Eigenschaft der Gerade mit, die zunächst nichts mit der Interpretation der Koeffizienten zu tun hat, aber später noch mal von Interesse sein wird. Die Gerade hat nämlich die Eigenschaft durch den Punkt (\\(\\bar{x}\\), \\(\\bar{y}\\)) zu gehen. Dies kann daran gesehen werden wenn in die Gleichung \\(\\bar{x}\\) für \\(x_i\\) eingesetzt wird. Anhand der Normalgleichungen kann die Geradengleichung in der Form.\n\\[\ny_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i = \\underbrace{\\bar{y} - \\hat{\\beta}_1 \\bar{x}}_{\\text{Def. }\\hat{\\beta}_0} + \\hat{\\beta}_1 \\cdot x_i\n\\]\nWird jetzt für \\(x_i\\) der Wert \\(\\bar{x}\\) eingesetzt folgt daher.\n\\[\ny_i = \\bar{y} - \\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 \\bar{x} = \\bar{y}\n\\]\nD.h. für den Wert \\(\\bar{x}\\) nimmt die Geradengleichung der Wert \\(\\bar{y}\\) an. Für die Sprungdaten ist die auch noch mal in Abbildung 12.10 graphisch dargestellt.\n\n\n\n\n\nAbbildung 12.10: Regressionsgerade der Sprungdaten und der Punkt \\((\\bar{x}, \\bar{y})\\)\n\n\n\n\nEine Eigenschaft die im weiteren Verständnis immer wieder auftaucht bezieht sich auf die \\(x\\)-Werte. Bei der Regression wird im Allgemeinen davon ausgegangen, dass die beobachteten \\(x\\)-Werte fixiert sind. D.h. trotzdem die \\(x\\)-Werte bei einem Experiment zufällig sein können, werden diese in den nachfolgenden Schritten als fixiert angesehen. Daher ist in der Formel \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) auch nur \\(\\epsilon_i\\) die einzige zufällige Variable."
  },
  {
    "objectID": "slm_basics.html#die-einfache-lineare-regression-in-r",
    "href": "slm_basics.html#die-einfache-lineare-regression-in-r",
    "title": "12  Einführung",
    "section": "12.4 Die einfache lineare Regression in R",
    "text": "12.4 Die einfache lineare Regression in R\nIn R wird eine Regression mit der Funktion lm() berechnet. Die für uns zunächst wichtigsten Parameter von lm() sind der erste Parameter formula und der zweite Parameter data. Mit der Formel wird der Zusammenhang zwischen den Variablen beschrieben, dabei können die Namen bzw. Bezeichner aus dem tibble() benutzt werden, die an den zweiten Parameter data übergeben werden. D.h. die Spaltennamen aus dem tibble() werden in formula verwendet.\nIn unserem Weitsprungbeispiel konnten wir in Tabelle 12.3 sehen, das das tibble() zwei Spalten mit den Namen v_ms, den Anlaufgeschwindigkeiten, und jump_m, den Weitsprungweiten enthielt. Dementsprechend, müssen wir diese beiden Bezeichner in formula verwenden, um unser Regressionsmodell zu beschreiben. Die Form der Modellbeschreibung folgt, dabei einer bestimmten Syntax die wir uns zunächst anschauen müssen. Zentrales Element der Syntax ist das Tilde Zeichen ~ (Win: ALTGR++, MacOS: ___), welches interpretiert wird als modelliert mit. Der Term der auf der linken Seite steht bezeichnet die abhängige Variable während die Terme auf der rechten Seite der Tilde stehen die unabhängige Variablen spezifizieren. Dementsprechend kann der Satz “Y wird mittels X modelliert” in die Formelsyntax mit Y ~ X übersetzt. Die komplette Syntax orientiert sich an eine Arbeit von Wilkinson und Rogers (1973).\nWenn ein konstanter in der Syntax benötigt wird, dann wird dieser mit einer \\(1\\) bezeichnet. Also zum Beispiel wenn wir Gleichung 12.3 modellieren wollen benutzen wir die Syntax y ~ 1 + x. Die beiden Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\) brauchen wir nicht explizit anzugeben, sondern R generiert uns automatisch anhand der Bezeichner Koeffizienten, die allerdings die Namen der Bezeichner bekommen. Dazu kommt noch eine Besonderheit, dass R bei einer Regressionsgleichung automatisch davon ausgeht, dass ein konstanter Term verwendet werden soll, d.h. der Term +1 wird automatisch dazugefüht. Wenn wir ein Modell ohne einen \\(y\\)-Achsenabschnitt fitten wollen, dann müssen wir dies R explizit mitteilen, indem wir -1 der linken Seite hinzufügen, also z.B. y ~ x - 1. Die Syntax generalisiert dann später einfach, wenn zusätzliche Terme in der multiplen Regression benötigt werden, in dem weitere unabhängige Variablen durch + dazugefügt werden. Dementsprechend würde sich die Formel y ~ x_1 + x_1 übersetzen in die abhängige Variable \\(y\\) wird mittels der unabhängigen Variablen x_1 und x_2 und einem konstaten Term modelliert. In Tabelle 12.6 sind weitere Beispiele für die Struktur der Formelsyntax für lm() gezeigt.\n\n\nTabelle 12.6: Formelsyntaxbeispiele für lm() (y-Ab = y-Achsenabshnitt, StKoef = Steigungskoeffizient)\n\n\nModell\nFormel\nErklärung\n\n\n\n\n\\(y=\\beta_0\\)\ny ~ 1\ny-Ab\n\n\n\\(y=\\beta_0+\\beta x\\)\ny ~ x\ny-Ab und StKoef\n\n\n\\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2\\)\ny ~ x1 + x2\ny-Ab und 2 StKoe\n\n\n\n\nWenn wir jetzt also unsere Weitsprungdaten modellieren wollen, verwenden wir die folgenden Befehle.\n\nlm(jump_m ~ v_ms, data = jump)\n\n\nCall:\nlm(formula = jump_m ~ v_ms, data = jump)\n\nCoefficients:\n(Intercept)         v_ms  \n    -0.1385       0.7611  \n\n\nPer default ist das Ergebnis von lm() nicht wirklich besonders hilfreich und es werden nur die beiden berechneten Koeffizienten ausgegeben. Dabei bezeichnet der Term (Intercept) den automatisch dazugefügten konstanten Term in der Formel, sprich den \\(y\\)-Achsenabschnitt \\(\\hat{\\beta}_0\\) und mit v_ms den Steigungskoeffizienten \\(\\hat{\\beta}_1\\). Um aus lm() mehr Informationen heraus zu bekommen, ist es sinnvoll das Ergebnis einen Variable zuzuweisen. In dem vorliegenden Arbeit wird dazu in den meisten Fällen eine Variante des Bezeichners mod benutzt, als Kurzform vom model. Diese Bezeichnung ist aber wie alle Bezeichner in R vollkommen willkürlich und entspringt nur der Tippfaulheit des Autors.\n\nmod &lt;- lm(jump_m ~ v_ms, data = jump)\n\nUm jetzt mehr Informationen aus dem gefitteten lm()-Objekt zu bekommen werden Helferfunktion verwendet. Die wichtigste Funktion ist die summary()-Funktion (?summary.lm).\n\nsummary(mod)\n\n\nCall:\nlm(formula = jump_m ~ v_ms, data = jump)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.44314 -0.22564  0.02678  0.19638  0.42148 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.13854    0.23261  -0.596    0.555    \nv_ms         0.76110    0.02479  30.702   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2369 on 43 degrees of freedom\nMultiple R-squared:  0.9564,    Adjusted R-squared:  0.9554 \nF-statistic: 942.6 on 1 and 43 DF,  p-value: &lt; 2.2e-16\n\n\nHier bekommen wir schon deutlich mehr Informationen mitgeteilt. Als erstes die Formell die wir lm() übergeben haben. Dann folgt ein Abschnitt über die Residuen, gefolgt von den Koeffzienten und im unteren Abschnitt noch weitere Statistiken. Wir konzentrieren uns zunächst einmal nur auf die Tabelle im Abschnitt Coefficients. Hier begegnen uns wieder in der ersten Spalte die Bezeichner für die beiden \\(\\beta\\)s in Form von \\(\\beta_0\\) (Intercept) und \\(\\beta_1\\) v_ms. In der zweiten Spalte daneben stehen die berechneten Koeffizienten die wir jetzt schon mehrmals gesehen haben. Die weiteren Spalten ignorieren wir hier zunächst. Im Laufe der folgenden Kapitel werden wir uns die weiteren Statistiken anschauen und deren Bedeutung verstehen.\nBei der Benutzung von lm() werden uns noch weitere Helferfunktionen begegnen, die den Umgang mit dem gefitteten Modell vereinfachen. Wollen wir zum Beispiel die beiden Koeffiziente aus dem Modell extrahieren können wir dazu die Funktion coefficients() oder auch nur kurz coef() verwenden. Koeffizienten und Standardschätzfehler\n\ncoef(mod)\n\n(Intercept)        v_ms \n -0.1385361   0.7611019 \n\n\nDie Funktion coef() gibt einen Vektor benannten Vektor zurück der entweder über die Bezeichner oder einfach über die Position der Koeffizienten angesprochen werden kann. Möchte ich zum Beispiel den Steigungskoeffizienten verwendent werwende ich:\n\ncoef(mod)[1]\n\n(Intercept) \n -0.1385361 \n\n\noder\n\ncoef(mod)['v_ms']\n\n     v_ms \n0.7611019 \n\n\nEin etwas übersichtlicher Zugang ist wieder zunächst einmal das Ergebnis von coef() einer Variablen zuweisen und diese dann weiter benutzen.\n\njump_betas &lt;- coef(mod)\njump_betas[1]\n\n(Intercept) \n -0.1385361 \n\n\nDie Koeffizienten kann ich zum Beispiel benutzen um die Regressionsgerade in ein Streudiagramm hinzuzufügen (Das tibble() mit den Sprungdaten hat den Bezeichner jump). Entweder mit dem ggplot2() Grafiksystem.\n\nggplot(jump,\n       aes(x = v_ms, y = jump_m)) +\n  geom_abline(intercept = jump_betas[1],\n              slope = jump_betas[2],\n              color = 'red') +\n  geom_point()\n\n\n\n\nOder mit den Standard R-Grafiksystem. Hier kann der Funktion abline() das gefittete lm()-Objekt direkt übergeben werden und die Koeffizienten werden automatisch extrahiert.\n\nplot(jump_m ~ v_ms, data = jump)\nabline(mod, color = 'red')\n\n\n\n\nSchauen wir uns noch mal ein ganz einfaches Beispiel, bei dem wir tatsächlich wissen welcher Zusammenhang zwischen den beiden Variablen. Wir halten das Beispiel ganz einfache und nehmen vier verschiedene \\(x\\)-Werte mit \\(x_i = i\\). Wir setzen \\(\\beta_0 = 1\\) und \\(\\beta_1 = 0.5\\). Wir generieren die vier Werte mit R, speichern diese in einem tibble() mit dem Bezeichner data und berechnen die resultierenden Koeffizienten mittels lm().\n\ndata &lt;- tibble(\n  x = 1:4,\n  y = 1 + 0.5 * x\n) \nmod &lt;- lm(y ~ x, data)\ncoef(mod)\n\n(Intercept)           x \n        1.0         0.5 \n\n\nUnd tatsächlich können wir die korrekten Koeffizienten mittels der einfachen linearen Regression wiedergewinnen. Diesen Ansatz mittels synthetisch generierten Daten die eingeführten Konzepte und Ansätze zu überprüfen werden wir im weiteren Verlauf des Skripts immer wieder anwenden, da er die Möglichkeit bietet relativ einfach und nachvollziehbar das Verhalten verschiedener Ansätze auszutesten.\nZusammenfassend lässt sich sagen, das wir jetzt gelernt haben wie wir ein einfaches Regressionmodell der Form Gleichung 12.3 an einen beliebigen Datensatz fitten können. Die Berechnung der beiden Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\) erfolgt mittels ?eq-slm-basics-norm1. Dabei berechnen wir die Koeffizienten nicht von Hand sondern lassen die von R mittels der lm() durchführen. Die Berechnung ist dabei vollkommen mechanisch und die Koeffizienten per-se sagen nichts darüber aus, ob das lineare Modell die Daten tatsächlich auch widerspiegelt. Dazu müssen wir noch etwas mehr Theorie aufbauen um Aussagen darüber zu treffen ob das Modell adäquat ist. Dies gehen wir in den folgenden Abschnitten und Kapiteln an.\n\n\n\n\nWilkinson, G. N., und C. E. Rogers. 1973. „Symbolic description of factorial models for analysis of variance“. Applied Statistics 22 (3): 392–99."
  },
  {
    "objectID": "slm_inference.html#statistische-überprüfung-von-beta_1-und-beta_0",
    "href": "slm_inference.html#statistische-überprüfung-von-beta_1-und-beta_0",
    "title": "13  Inferenz",
    "section": "13.1 Statistische Überprüfung von \\(\\beta_1\\) und \\(\\beta_0\\)",
    "text": "13.1 Statistische Überprüfung von \\(\\beta_1\\) und \\(\\beta_0\\)\nDer erste Schritt um eine Verteilung zu bekommen ist allerdings, dass wir zunächst einmal eine Zufallsvariable benötigen. Bisher haben wir den Zusammenhang zwischen Variablen über die Formel\n\\[\ny_i = \\beta_0 + \\beta_1 \\cdot x_i\n\\]\nbeschrieben. In dieser Form ist allerdings noch kein zufälliges Element vorhanden. Für ein gegebenes \\(x_i\\) bekommen wir ein genau spezifiziertes \\(y_i\\). Allerdings haben wir bei der Herleitung gesehen, dass die Daten in den seltensten Fällen genau auf der Gerade liegen, sondern wir die Parameter \\(\\hat{\\beta}_0\\) und \\(\\hat{beta}_1\\) so gewählt haben, dass die quadrierten Abweichungen, die Residuen \\(\\epsilon_i\\) minimal werden. Dies Residuen verwenden wir jetzt um eine zufälliges Element in unsere Regression rein zu bekommen. Ein mögliche Annahme ist, das die Residuen beispielsweise Normalverteilt sind.\nWarum könnte dies Sinn machen. In dem vorhergehenden Weitsprungbeispiel haben wir informell hergeleitet, dass die Weitsprungleistung von unzähligen weiteren Faktoren beeinflusst werden kann, welche dazu führen, dass für eine gegebene Anlaufgeschwindigkeit nicht immer die gleiche Weitsprungweite erzielt wird. Generell, ist diese Art der Begründung bei biologischen System meistens plausibel. In vorhergehenden Abschnitt haben wir dazu aber auch noch gesehen, dass die Normalverteilung eben gut geeignet ist, um solche Prozesse, bei denen viele kleine additive Effekt auftreten. Dieser Argumentation folgend ist es plausibel diese Einflüsse auch beim Regressionsfall mittels einer Normalverteilung zu modellieren. Dazu führen wir noch eine weitere Annahme an, nämlich dass diese Einflüsse im Mittel in gleichen Maßen die Werte nach nach oben wie auch nach unten ablenken. D.h. die Werte nach oben und unten von der Regressionsgerade abweichen. Dies erlaubt uns jetzt die Annahme genau zu spezifizieren.\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nD.h also, wir gehen davon aus, dass die Residuen normalverteilt sind, mit einem Mittelwert von \\(\\mu = 0\\) und einer noch näher zu spezifizierenden Varianz \\(\\sigma^2\\). Das führt dann zu der folgenden Formulierung des Regressionsmodells.\n\\[\nY_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\tag{13.1}\\]\n\\(Y\\) wird jetzt groß geschrieben, da es sich um eine Zufallsvariable handelt. Dies führt jetzt dazu, das das Regressionsmodell in zwei Teile unterschieden werden kann. Einmal eine deterministischen Teil \\(\\beta_0 + \\beta_1 \\cdot x\\) und einen stochastischen Teil \\(\\epsilon_i\\). Dies führt dazu, dass \\(Y_i\\) ebenfalls stochastisch ist und zu einer Zufallsvariable wird.\nSchauen wir uns weiter an, wie sich \\(Y_i\\) verhält, wenn wir \\(x_i\\) als Konstante \\(x\\) mit ein bestimmten Wert annehmen. Dann wird aus Gleichung 13.1 \\(Y_i = \\beta_0 + \\beta_i \\cdot x + \\epsilon_i\\). Folglich bleibt der deterministische Teil immer gleich, wird zu einer Konstante. Da \\(\\epsilon_i\\) normalverteilt ist ist \\(Y_i\\) ebenfalls normalverteilt. Der Mittelwert der Normalverteilung von \\(Y_i\\) \\(\\mu_{Y_i}\\) ist allerdings nicht gleich Null, sondern die Normalverteilung von \\(\\epsilon_i\\) wird um die Konstante \\(\\beta_0 + \\beta_1 \\cdot x\\) verschoben (siehe Abbildung 13.1). Das führt dazu, dass \\(Y_i\\) der Verteilung \\(\\mathcal{N}(\\beta_0 + \\beta_1 x)\\) folgt.\n\n\n\n\n\n\n\n(a) Verteilung von \\(\\epsilon_i\\)\n\n\n\n\n\n\n\n(b) Verteilung von \\(Y_i\\)\n\n\n\n\nAbbildung 13.1: Relation der Lageparameter von \\(e_i\\) und \\(Y_i\\)\n\n\nDaraus folgt jetzt aber zusätzlich, dass für jedes gegebenes \\(X\\) die \\(Y\\)-Werte einer Normalverteilung folgen. Lediglich die Verschiebung des Mittelwert der jeweiligen \\(Y\\)-Normalverteilung hängt von \\(X\\) über die Formel \\(\\beta_0 + \\beta_1 \\cdot X\\) zusammen. Formal:\n\\[\nY|X \\sim N(\\beta_0+ \\beta_1 X,\\sigma^2)\n\\]\nDie Schreibweise \\(|X\\) wird übersetzt für gegenbenes \\(X\\) und sagt aus, dass die Verteilung von \\(Y\\) von \\(X\\) abhängt. Es handelt sich dabei um eine bedingte Wahrscheinlichkeit. Die Varianz der jeweiligen \\(Y\\)-Werte ist dabei die zuvor angenommen Varianz der \\(\\epsilon_i\\) also \\(\\sigma^2\\). Eine wichtige Annahme die noch mal betont werden sollte, wir gehen davon aus, dass die einzelnen Punkte unabhängig voneinander sind. Im Weitsprungbeispiel würde dies bedeuten, dass jeder Sprung von einem anderen Athleten kommen muss.\nWenn wir die Verteilungen von \\(Y\\) graphisch führ beispielweise drei verschiedene \\(X\\)-Wert darstellen, dann folgt daraus die folgende Abbildung (siehe Abbildung 13.2). D.h. für jeden \\(X\\)-Wert werden mehrere \\(Y\\)-Werte beobachtet, die jeweils einer Normalverteilung folgen.\n\n\n\n\n\nAbbildung 13.2: Verteilung der Daten für verschiedene \\(x\\)-Werte\n\n\n\n\nIn Abbildung 13.2 ist klar zu sehen, wie für jeden der drei Punkte von \\(X\\) die beobachteten \\(Y\\)-Werte einer Normalverteilung. Die Breite der Verteilung ist an jedem Punkte gleich, nämlich \\(=\\sigma^2\\) während der Mittelwert der Gleichung \\(\\beta_0 + \\beta_1 X\\) folgend entlang der Regressionsgerade verschoben ist.\nWenn wir uns zurück an die Ausführungen zur statistischen Signifikanz erinnern, dann haben wir in dem Zusammenhang vom einem datengenerierenden Prozess gesprochen (Definition 8.1) (DGP). In unserem jetzigen Modell können wir dementsprechend zwei Komponenten als Teile des DGP identizifieren. Entsprechend Gleichung 13.1 besteht der DGP aus dem deterministischen Teil \\(\\beta_0 + \\beta_1 X\\) und dem stochastischen Teil \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\). Diese Einsicht können wir verwenden um die Eigenschaften dieses Modells bezüglich Aussagen über statistische Signifikanz zu untersuchen.\nWir fokussieren uns jetzt auf ein vereinfachtes Modell bei dem wir zusätzlich noch \\(\\beta_0 = 0\\) setzen, und wir uns erst mal nur für die Eigenschaften von \\(\\beta_1\\) interessieren. Gehen wir nun davon aus, dass zwischen \\(X\\) und \\(Y\\) der Zusammenhang \\(\\beta_1 = 1\\) besteht. D.h. wenn \\(X\\) um eine Einheit vergrößert wird, dann wird \\(Y\\) ebenfalls um eine Einheit größer.\n\\[\nY = 0 + 1 \\cdot X + \\epsilon, \\quad \\epsilon\\sim\\mathcal{N}(0,\\sigma^2)\n\\tag{13.2}\\]\nJetzt müssen wir noch einen Wert für \\(\\sigma^2\\) festlegen. Sei dieser einfach einmal \\(\\sigma = \\frac{1}{2}\\). Jetzt können wir R benutzen um Experimente, also Beobachtungen, anhand dieses DGP zu simulieren. Der Einfachheit halber legen wir ein übersichtliches \\(N = 12\\) fest und nehmen uns jeweils drei \\(X\\)-Werte z.B. mit \\(X \\in \\{-1, 0, 1\\}\\), d.h. wir ziehen für jeden \\(X\\)-Wert vier \\(Y\\)-Werte.\n\nN &lt;- 12\nbeta_0 &lt;- 0\nbeta_1 &lt;- 1\nsigma &lt;- 1/2\ndat_sim_1 &lt;- tibble(\n  x_i = rep(-1:1, each=4),\n  y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma)\n)\n\nWenn wir uns die generierten Daten anschauen, dann sehen wir wenig überraschend 12 verschiedene Werte für \\(y_i\\) und jeweils \\(3 \\times 4\\) verschiedene Werte für \\(x_i\\) (siehe Tabelle 13.1).\n\n\n\n\nTabelle 13.1: Eine Simulation des Modells Gleichung 13.2\n\n\nx_i\ny_i\n\n\n\n\n-1\n-1.10\n\n\n-1\n-1.69\n\n\n-1\n-1.14\n\n\n-1\n-1.01\n\n\n0\n-0.21\n\n\n0\n0.09\n\n\n0\n0.60\n\n\n0\n-0.42\n\n\n1\n0.97\n\n\n1\n0.66\n\n\n1\n0.94\n\n\n1\n1.69\n\n\n\n\n\n\nWenn wir die Daten graphisch darstellen erhalten wir (Abbildung 13.3):\n\nggplot(dat_sim_1, aes(x_i, y_i)) + \n  geom_point()\n\n\n\n\nAbbildung 13.3: Streudiagramm der Daten aus Tabelle 13.1\n\n\n\n\nEbenfalls wenig überraschend, die Punkte sind auf den \\(x\\)-Werten \\(-1, 0\\) und \\(1\\) zentriert und liegen nicht alle aufeinander, da sie einer Zufallsstichprobe aus \\(\\mathcal{N}(0, \\frac{1}{4})\\) entspringen.\nJetzt kann ich natürlich für diese Daten unsere Normalengleichungen anwenden und Werte für \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) berechnen. Oder eben direkt in R.\n\nmod_sim_1 &lt;- lm(y_i ~ x_i, dat_sim_1)\ncoef(mod_sim_1)\n\n(Intercept)         x_i \n-0.05046686  1.14951274 \n\n\nWir sehen, dass die berechneten Werte für \\(\\beta_0\\) und \\(\\beta_1\\) schon in der Nähe der tatsächlichen Werte liegen (siehe ?eq-slm-inf-mod-1), aber auf Grund der Stichprobenvariabilität eben nicht genau auf diesen Werten. Was passiert denn jetzt, wenn ich das Ganze noch einmal durchlaufen lassen?\n\ndat_sim_2 &lt;- tibble(\n  x_i = rep(-1:1, each=4),\n  y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma)\n)\nmod_sim_2 &lt;- lm(y_i ~ x_i, dat_sim_2)\ncoef(mod_sim_2)\n\n(Intercept)         x_i \n -0.3645971   1.0616288 \n\n\nWieder wenig überraschend, da jedes Mal wenn ich rnom() eine neue Ziehung aus der Normalverteilung generiert wird, erhalte ich neue Werte für \\(y_i\\) und dementsprechend andere Werte für \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\). Nochmal, warum? Stichprobenvariabilität! Jetzt sind wir wieder bei dem gleichen Prinzip, das wir im Rahmen der kleinen Welt ausgiebig behandelt haben. Schauen wir uns jetzt doch einfach mal was passiert wenn wir die Simulation nicht \\(2\\times\\) sondern z.B. \\(1000\\times\\) durchführen.\n\nN_sim &lt;- 1000\nbeta_1_s &lt;- numeric(N_sim)\nx_i &lt;- rep(-1:1, each=4)\nfor (i in 1:N_sim) {\n  daten_temporaer &lt;- tibble(x_i,\n                            y_i = beta_0 + beta_1 * x_i + rnorm(N, mean = 0, sd = sigma))\n  model_temporaer &lt;- lm(y_i ~ x_i, daten_temporaer)\n  beta_1_s[i] &lt;- coef(model_temporaer)[2]\n}\n\nWir erhalten jetzt einen Vektor beta_1_s mit \\(1000\\) beobachteten \\(\\hat{\\beta}_1\\). Da das etwas viele Werte sind um die uns einzeln anzuschauen, erstellen ein Histogramm der \\(\\hat{\\beta}_1\\)s. (Abbildung 13.4).\n\nhist(beta_1_s, xlab = expression(hat(beta)[1]), main='')\nabline(v = beta_1, col='red', lty=2)\n\n\n\n\nAbbildung 13.4: Histogram der auf den simulierten Daten berechneten \\(\\hat{\\beta}_1\\). Wahrer Wert von \\(\\beta_1\\) rot eingezeichnet.\n\n\n\n\nIn Abbildung 13.4 begegnet uns zunächst einmal wieder unsere altbekannte Glockenkurve. Schön ist, dass deren Mittelwert im Bereich des wahren Werts von \\(\\beta_1\\) liegt und Werte mit größer werdender Abweichung vom wahren Wert in ihrer Häufigkeit abnehmen. Aber die Häufigkeit ist nicht Null, sondern eben nur geringer. Werte in der Nähe von \\(\\beta_1\\) weisen dagegen eine größere Häufigkeit aufweisen. Das sollte uns jetzt auch irgendwie zufrieden stimmen, denn dies bedeutet, dass wir in der Lage sind mit unserem Regressionsmodell im Mittel tatsächlich den korrekten Wert abzuschätzen. Allerdings, wie immer, bei einer einzelnen Durchführung des Experiments können wir alles von perfekt spot-on bis komplett danebenliegen und würden es nicht wissen.\nWir können jetzt aber auch wieder ganz parallel zu unseren Herleitungen in der kleinen Welt einen Entscheidungsprozess spezifizieren. Wenn Abbildung 13.4 den DGP beschreibt und das die Verteilung der zu erwartenden \\(\\hat{\\beta}_1\\) unter dem Modell sind. Bei der Dürchführung eines neuen Experiments, dann würden wir sagen, dass wenn unserer beobachteter Wert in den Rändern der Verteilung von Abbildung 13.4 liegt, das wir eher nicht davon ausgehen, dass unserer neues Experiment den gleichen DGP zugrundeliegen hat. D.h wir definieren uns jetzt Grenzen am oberen und am unteren Rand der Verteilung. Wenn jetzt ein neuer beobachteter Wert entweder unterhalb der unteren Grenze oder oberhalb der oberen Grenze liegt, dann sagen wir: Wir sind jetzt aber sehr überrascht diesen Wert zu sehen, wenn der dem gleichen datengenerierenden Prozess entstammen soll. Daher glauben wir nicht, dass dieses Experiment den gleichen DGP besitzt.\nUm diese Entscheidung treffen zu können, müssen wir also Grenzen definieren. Dazu können wir zunächst einmal einfach die Quantilen der Verteilung nehmen und schneiden z.B. unten \\(2.5\\%\\) und oben \\(2.5\\%\\) ab. So kommen wir dann insgesamt auf \\(5\\%\\), um auf die übliche Irrtumswahrscheinlichkeit von \\(\\alpha = 0.05\\) zu kommen. Dazu benutzen wir R und zwar quantile()-Funktion^[Im folgenden Snippet werden die Werte auf zwei Kommastellen mit round() der besseren Darstellung wegen gerundet).\n\n\n 2.5% 97.5% \n 0.65  1.35 \n\n\nMittels dieser Werte können wir zwei disjunkte Wertmenge definieren, einmal die Werte innerhalb von \\(\\hat{\\beta}_1 \\in [0.65,1.35]\\) bei denen wir nicht überrascht sind, und die unter der Annahme \\(\\beta_1 = 1\\) erwartbar sind und die Werte \\(\\hat{\\beta}_1 \\notin [0.65,1.35]\\) diejenigen Werte die uns überraschen würden unter der Annahme. Ins Histogramm übertragen (siehe Abbildung 13.5).\n\n\n\n\n\nAbbildung 13.5: Histogram der auf den simulierten Daten berechneten \\(\\hat{\\beta}_1\\). Wahrer Wert von \\(\\beta_1\\) rot eingezeichnet und kritische Werte grün.\n\n\n\n\nFühren wir nun ein Experiment noch einmal durch. Wir beobachten einen Wert für \\(\\hat{\\beta}_1\\) von \\(1.46\\). Dieser Wert liegt außerhalb unseres definierten Intervalls \\([0.65, 1.35]\\), daher sehen wir diesen Wert als derart unwahrscheinlich unter dem angenommenen DGP, das wir sagen: Wir glauben nicht, dass diesem Experiment nicht der angenommene DGP zugrunde liegt. Graphisch wieder dargestellt (siehe Abbildung 13.6).\n\n\n\n\n\nAbbildung 13.6: Histogram der auf den simulierten Daten berechneten \\(\\hat{\\beta}_1\\). Wahrer Wert von \\(\\beta_1\\) rot eingezeichnet und kritische Werte grün und der beobachtete Wert als roter Punkt.\n\n\n\n\nDaher würden wir diesen Wert als statisisch signifikant bezeichnen und würden unsere Annahme ablehnen.\nJetzt sind wir aber etwas hin und her zwischen Experiment, Annahmen und Schlussfolgerungen gesprungen. Normalerweise kennen wir die Stichprobenverteilung nicht vor dem Experiment, sondern, wir sind am dem Wert \\(\\beta_1\\) interessiert. Wenn wir den Wert schon wissen würden, dann müssten wir ja gar kein Experiment mehr durchführen. D.h. wir haben eigentlich noch keinen klaren Vorkenntnisse. Mit welcher Annahme gehen wir dann in das Experiment rein? Nun, wir schon bei kleinen Welt Beispiel, starten wir mit der Annahme das zwischen den beiden Variablen kein Zusammenhang besteht. Übertragen auf die Modellparameter also, dass kein linearer Zusammenhang zwischen den beiden Variablen besteht.\n\\[\\begin{align*}\nH_0: \\beta_1 &= 0 \\\\\nH_1: \\beta_1 &\\neq 0\n\\end{align*}\\]\nUm die Stichprobenverteilung unter der \\(H_0\\) formal Herleitung zu können, ist der Erwartungswert von \\(\\hat{\\beta}_1\\) und dessen Standardfehler notwendig. Es lässt sich zeigen, dass die folgenden Zusammenhänge unter den gesetzten Annahmen bestehen:\n\\[\nE[\\hat{\\beta}_0] = \\beta_0\n\\]\nAlso der Schätzer von \\(\\beta_1\\) ist erwartungstreu (biased) und der Standardfehler des Schätzer lässt sich wie folgt bestimmen.\n\\[\n\\sigma_{\\beta_1} = \\sqrt{\\frac{\\sigma^2}{\\sum{(X_i - \\bar{X})^2}}}\n\\tag{13.3}\\]\nHier taucht jetzt zum ersten Mal der Parameter \\(\\sigma^2\\) formal auf. Wo kommt diese Variance her? Sie gehört zu unserer Annahme der Verteilung der \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\). Bisher haben wir aber noch gar keine Möglichkeit kennen gelerntm, diese abzuschätzen. Wieder nach etwas motivierten Starren auf die verschiedenen Formeln, könnte heuristisch plausibel sein, dass die Varianz, also die Streuung der \\(\\epsilon_i\\) mit der Streuung unserer Werte um die Regressionsgerade zusammenhängen könnten. Formal hatten wir diese als Residuen bezeichnet und mit \\(e_i = \\hat{y}_i - y_i\\) bezeichnet. Vormals hatten wir diese Abweichungen als Fehler bezeichnet, aber unter den jetzt eingeführten Annahmen, handelt es sich nicht wirklich um Fehler, sondern die Abweichungen sind eine Folge davon, dass \\(Y_i\\) für jeden Wert von \\(X_i\\) nicht nur einen einzigen Wert hat, sondern eben einer Verteilung folgt \\(Y_i|X_i \\sim \\mathcal{N}(\\beta_0 + beta_1, \\sigma^2)\\) deren Form über die \\(\\epsilon_i\\) bestimmt wird.\nDie \\(e_i\\) sind tatsächlich die Schätzer für die wahren \\(\\epsilon_i\\) also \\(e_i = \\hat{\\epsilon_i} = \\hat{y}_i - y_i\\). Es lässt sich nun wieder zeigen, dass mittels dieser \\(e_i\\) ein erwartungstreuer Schätzer für \\(\\sigma^2\\) erzeugen lässt. Nämlich die mittleren quadrierten Abweichungen (MSE).\n\\[\n\\hat{\\sigma} = \\frac{\\sqrt{\\sum_{i=1}^N e_i^2}}{N-2} = \\frac{\\text{SSE}}{N-2} = \\text{MSE}\n\\tag{13.4}\\]\nDa das später immer wieder auftauchen wird, hier auch noch mal in die zwei Komponenten zerlegt. Der Zähler wird als Summe der quadrierten Abweichungen (SSE) bezeichnet und durch den Term \\(N-2\\), der als Freiheitsgerade bezeichnet wird, geteilt. Dann mit die Formel und deren Bezeichnung mittlere Abweichung zusammenpasst, wäre es schöner wenn die Summe durch die Anzahl \\(N\\) der Terme geteilt wird, allerdings verhält sich das in diesem Fall ähnlich wie bei der Varianz einer Stichprobe wo die Summe auch durch \\(N-1\\) geteilt wird (zur Erinnerung \\(s = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})^2}{N-1}\\)). Jetzt wird dementsprechend nicht durch \\(N-1\\) sondern durch \\(N-2\\) geteilt.\nFür unser Problem der Stichprobenverteilung ist jetzt aber wichtiger, dass wir mittels Gleichung 13.4 den Standardfehler von \\(\\hat{\\beta}_1\\) bestimmen können, indem wir für \\(\\sigma^2\\) das mittels der Daten ermittelte \\(\\hat{\\sigma}^2\\) einsetzen.\n\\[\n\\hat{\\sigma}_{\\beta_1} = \\sqrt{\\frac{\\hat{\\sigma}^2}{\\sum{(X_i - \\bar{X})^2}}}\n\\tag{13.5}\\]\nDies erlaubt uns jetzt nach unserem bereits bekannten Muster eine Teststatistik für die \\(H_0\\) herzuleiten:\n\\[\nt = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\hat{\\sigma}_{\\beta_1}}\n\\]\nUnter der \\(H_0\\) mit \\(\\beta_1 = 0\\) wird daraus\n\\[\nt = \\frac{\\hat{\\beta}_1}{\\hat{\\sigma}_{\\beta_1}}\n\\tag{13.6}\\]\nDiese Teststatistik folgt einer t-Verteilung mit \\(N-2\\) Freiheitsgeraden. Da diese Formel wieder etwas aus der Luft gegriffen erscheint, hier noch mal eine Simulation zusammen mit der theoretischen Testverteilung.\n\nN &lt;- 45\nn_sim &lt;- 1000\nx &lt;- runif(N, -1, 1)\nsigma &lt;- 1\nexperiment &lt;- function() {\n  y &lt;- rnorm(N, mean = 0, sd = sigma)\n  mod &lt;- lm(y~x)\n  b &lt;- coef(mod)[2]\n  c(beta_0 = coef(mod)[1],\n    beta_1 = coef(mod)[2],\n    sigma = sigma(mod))\n}\nbetas &lt;- t(replicate(n_sim, experiment()))\nbetas &lt;- tibble(beta_0 = betas[,1],\n                beta_1 = betas[,2],\n                sigma = betas[,3]) |&gt; \n  mutate(\n   s_e_beta_1 = sqrt(sigma**2/sum( (x - mean(x))**2)),\n   t = beta_1 / s_e_beta_1)\nt_theoretical &lt;- tibble(\n  t = seq(-3, 3, length.out = 150),\n  p = dt(t, N - 2)\n)\n\nggplot(betas, aes(t)) +\n  geom_histogram(aes(y = ..density..), bins = 20) +\n  geom_line(data = t_theoretical, aes(t, p), color = 'red') +\n  labs(x = \"t\", y = 'Relative Häufigkeit') \n\n\n\n\nAbbildung 13.7: Verteilung von t bei 1000 Simulationen unter der Annahme der \\(H_0\\) und die theoretische Verteilung von t (rot).\n\n\n\n\nIn Abbildung 13.7 können wir sehen, dass die theoretische Verteilung in rot die beobachtete Verteilung sehr gut abschätzt.\nIn R kann der Wert \\(\\hat{\\sigma}^2\\) über die Funktion sigma() aus dem gefitteten lm()-Modell extrahiert werden.\n\nsigma(mod)\n\n[1] 0.2369055\n\n\nSchauen wir uns die Stichprobenverteilung von \\(\\hat{\\sigma}^2\\) anhand unserer Simulation an. Es ist wieder zu beobachten, das im Mittel der korrekte, im Modell definierte, Wert von \\(\\sigma = 1\\) beobachtet wird (siehe Abbildung 13.8).\n\n\n\n\n\nAbbildung 13.8: Verteilung von \\(\\hat{\\sigma}\\) in der Simulation und der wahre Wert in rot eingezeichnet\n\n\n\n\nAber wie immer, leider steht uns bei einem realen Experiment diese Information nicht zur Verfügung und wir haben nur einen einzelnen Wert, der alles von komplett daneben bis ziemlich perfekt sein kann.\nSchauen wir uns noch einmal die Ausgabe zu unserem Weitsprungmodell mittels summary() an. Unter Residual Standard Error sehen wir, dass hier \\(\\hat{\\sigma}\\) angegeben wird. Dieser Wert wird auch als mittlerer Schätzfehler bezeichnet und kann als Maß verwendet werden, welche Abweichung das Modell im Mittel hat. Die Einheit sind wieder in den Einheiten der abhängigen Variable, so kann auch schon abgeschätzt werden mit welcher Präzision das Modell die Daten fittet.\n\nsummary(mod)\n\n\nCall:\nlm(formula = jump_m ~ v_ms, data = jump)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.44314 -0.22564  0.02678  0.19638  0.42148 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.13854    0.23261  -0.596    0.555    \nv_ms         0.76110    0.02479  30.702   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2369 on 43 degrees of freedom\nMultiple R-squared:  0.9564,    Adjusted R-squared:  0.9554 \nF-statistic: 942.6 on 1 and 43 DF,  p-value: &lt; 2.2e-16\n\n\nIn unserem Fall beobachten wir \\(0.24m\\). Diesen Wert muss jetzt unsere Trainerin im Sinne der Weitsprungleistung der deren Varianz interpretieren und ein Abschätzung treffen zu können.\nNach der Herleitung der Teststatistik für \\(\\beta_1\\), können wir jetzt auch weitere Teil der Ausgabe von summary() interpretieren. In der Tabelle stehen entsprechend die Standardfehler für \\(\\hat{\\beta}_1\\) und \\(\\hat{\\beta}_0\\). Für \\(\\beta_0\\) wird genau die gleiche Vorgehensweise wie auch bei \\(\\beta_1\\) angewendet. Die Nullhypothese \\(H_0\\) ist hier ebenfalls das der Parameter standardmäßig als Null angesetzt wird. Der Standardfehler von \\(\\beta_0\\) errechnet sich nach:\n\\[\\begin{equation}\n\\sigma^2[\\beta_0] = \\sigma^2\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right)\n\\label{eq-slm-inf-beta0-se}\n\\end{equation}\\]\nAn Formel \\(\\eqref{eq-slm-inf-beta-0-se}\\) ist zu erkennen, dass wenn die \\(X\\)-Werte den Mittelwert \\(0\\) haben, dass \\(\\sigma^2[\\beta_0]\\) gleich dem Standardfehler für den Mittelwert SEM wird. Was auch wiederum Sinn macht, da in diesem Fall \\(\\beta_0 = \\bar{y}\\) gilt.\nDies führt dies zu den beiden zu überprüfenden Hypothesen für \\(\\beta_0\\):\n\\[\\begin{align*}\nH_0: \\beta_0 &= 0 \\\\\nH_1: \\beta_0 &\\neq 0\n\\end{align*}\\]\nDementsprechend überprüft die Hypothesentestung ob der \\(y\\)-Achsenabschnitt gleich Null ist. Hier sollte berücksichtigt werden, dass diese Hypothese in den seltensten Fällen tatsächlich auch von Interesse ist und lediglich besagt, dass entweder der \\(y\\)-Achsenabschnitt durch den Nullpunkt geht, oder dass wenn tatsächlich \\(\\beta_1 = 0\\) gilt, der Mittelwert von \\(y\\) gleich Null ist, was ebenfalls in den seltensten Fällen von Interesse ist.\nDie Spalten 3 und 4 in summary() unter Coefficients: können jetzt interpretiert werden, da es sich hierbei um die \\(t\\)-Teststatistik handelt und den entsprechenden p-Wert unter der jeweiligen \\(H_0\\). Die Hypothesen sind ungerichtet."
  },
  {
    "objectID": "slm_inference.html#herleitung-der-eigenschaften-von-hatbeta_1",
    "href": "slm_inference.html#herleitung-der-eigenschaften-von-hatbeta_1",
    "title": "13  Inferenz",
    "section": "13.2 Herleitung der Eigenschaften von \\(\\hat{\\beta}_1\\)",
    "text": "13.2 Herleitung der Eigenschaften von \\(\\hat{\\beta}_1\\)\nUm den Schätzer \\(\\hat{\\beta}_1\\) für \\(\\beta_1\\) formal herzuleiten. Beginnen wir zunächst mit der folgenden Formel, wobei wir im folgenden den Schätzer mit \\(b_1\\) bezeichnen.\n\\[\nb_1 = \\sum k_i Y_i\n\\tag{13.7}\\]\nD.h. wir zeigen zunächst, dass \\(b_1\\) durch eine lineare Kombination der \\(Y_i\\)-Werte berechnet werden kann. Die Koeffizienten \\(k_i\\) der Summe sind dabei wie folgt definiert:\n\\[\nk_i = \\frac{X_i - \\bar{X}}{\\sum(X_i - \\bar{X})^2}\n\\tag{13.8}\\]\nDer Grund für diese zunächst etwas uneinsichtige Definition wird im weiteren klarer werden. Zunächst haben die \\(k_i\\) verschieldene Eigenschaften die wir uns im Späteren zunutze machen wollen. Zunächst erst einmal noch ein paar Identitäten die wir später auch noch verwenden.\nDie erste Identität bezieht sich auf das Kreuzprodukt der Abweichungen von \\(X_i\\) und \\(Y_i\\) von ihren jeweiligen Mittelwerten.\n\\[\\begin{align*}\n\\sum(X_i-\\bar{X})(Y_i-\\bar{Y}) &= \\sum(X_i - \\bar{X})Y_i  -\\underbrace{\\sum(X_i - \\bar{X})}_{=0}\\bar{Y}  \\\\\n&= \\sum(X_i - \\bar{X})Y_i\n\\end{align*}\\]\nWenn wir in der Formel \\((Y_i-\\bar{Y})\\) durch \\((X_i-\\bar{X})\\) austauschen, folgt noch eine weitere nützliche Identität:\n\\[\n\\sum(X_i-\\bar{X})^2 = \\sum(X_i - \\bar{X})X_i\n\\]\nWerden die jeweiligen \\(k_i\\) mit den dazugehörigen \\(X_i\\) multipliziert und die Definition der \\(k_i\\) (siehe Gleichung 13.8) beachten, erhalten wir:\n\\[\n\\sum k_i X_i = \\frac{\\sum(X_i - \\bar{X})X_i}{\\sum(X_i-\\bar{X})^2} = \\frac{\\sum(X_i-\\bar{X})^2}{\\sum(X_i-\\bar{X})^2} = 1\n\\]\nD.h. Die Summe der \\(k_i X_i\\) ist gleich \\(1\\). Aus der Definition Gleichung 13.8 folgt weiterhin.\n\\[\n\\sum k_i = \\sum \\left(\\frac{X_i-\\bar{X}}{\\sum(X_i-\\bar{X})^2}\\right)= \\frac{\\sum(X_i-\\bar{X})}{\\sum(X_i-\\bar{X})^2} = \\frac{0}{\\sum(X_i-\\bar{X})^2} = 0\n\\]\nD.h. die Summe der \\(k_i\\) ist gleich Null.\nWenn wir jetzt wieder die Definition unseres Schätzer für \\(\\beta_1\\) verwenden (siehe ?eq-slm-basics-norm1). Dann erhalten unter der Verwendung der Identität der Kreuzprodukte den gewünschten Zusammenhang zwischen \\(b_1\\) und \\(Y_i\\).\n\\[\\begin{align*}\nb_1 &= \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} \\\\\n&= \\frac{\\sum(X_i - \\bar{X})Y_i}{\\sum(X_i - \\bar{X})^2} = \\sum k_i Y_i\\\\\n\\end{align*}\\]\nWenden wir jetzt den Erwartungswert auf \\(Y_i\\) an, dann werden die \\(k_i\\) als konstant angesehen und nur die \\(Y_i\\) sind Zufallsvariablen. Da aber \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) gilt und in dieser Formel wiederum nur \\(\\epsilon_i\\) eine Zufallsvariable mit \\(\\beta_0\\) und \\(\\beta_1 X_i\\) konstant ist und zudem die \\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) also \\(E[\\epsilon_i] = 0\\) laut der Annahme gilt, folgt:\n\\[\\begin{align*}\n    E[b_1] &= E\\left[\\sum k_i Y_i\\right] = \\sum k_i E[Y_i] = \\sum k_i (\\beta_0 + \\beta_1 X_i) \\\\\n    &= \\beta_0 \\sum k_i + \\beta_1 \\sum k_i X_i = \\beta_1\n\\end{align*}\\]\nD.h. ?eq-slm-basics-norm1 ist ein erwartungstreuer Schätzer für \\(\\beta_1\\). Das gleiche gilt auch für den Schätzer \\(b_0\\) für \\(\\beta_0\\).\nLeiten wir noch eine weitere Identität über die Summe der \\(k_i^2\\) her:\n\\[\n\\sum k_i^2 = \\sum \\left[\\frac{X_i-\\bar{X}}{\\sum(X_i-\\bar{X})^2}\\right]^2 = \\frac{\\sum(X_i-\\bar{X})^2}{\\left[\\sum(X_i-\\bar{X})^2\\right]^2} = \\frac{1}{\\sum(X_i-\\bar{X})^2}\n\\] Können wir auch noch die Varianz bzw. den Standardfehler unseres Schätzers für \\(\\beta_1\\) herleiten. Es gilt nämlich:\n\\[\\begin{align*}\n    \\sigma^2[b_1] &= \\sigma^2\\left[\\sum k_i Y_i\\right] = \\sum k_i^2 \\sigma^2[Y_i] \\\\\n    &= \\sum k_i^2 \\sigma^2 = \\sigma^2 \\sum k_i^2 \\\\\n    &= \\sigma^2 \\frac{1}{\\sum(X_i-\\bar{X})^2}\n    \\label{eq-slm-inf-beta1-deriv}\n\\end{align*}\\]\nWir erhalten die bereits eingeführte Formel. Wiederum eine Einsicht aus der Herleitung der Formel folgt, dass die Varianz \\(\\sigma^2\\) als konstant angesehen wird, d.h. \\(\\sigma_i^2 = \\sigma^2\\). Dies hat uns erlaubt im zweiten Schritt \\(\\sigma^2\\) aus der Summe heraus zu ziehen. Wenn die Varianz \\(\\sigma^2\\) nicht konstant ist, dann ist der berechnete Standardfehler für \\(\\hat{\\beta}_1 = b_1\\) nicht korrekt.\nEine interessante Eigenschaft des Standardfehler von \\(\\hat{\\beta}_1\\) ist in Formel \\(\\eqref{eq-slm-inf-beta1-deriv}\\) zu sehen. Im Nenner stehen die Abweichungen der \\(X\\)-Werte vom Mittelwert \\(\\hat{X}\\). D.h. wenn die \\(X\\)-Werte weiter auseinander sind, dann führt dies dazu, dass der Standardfehler \\(\\sigma^2[b_1]\\) kleiner wird. Intuitive macht dies auch Sinn, wenn ich eine Gerade bestimmen will, dann ist es einfacher die Gerade anhand weit auseinander liegenden Stütztwerten zu bestimmen im Vergleich zu wenn ich eng beinander liegende \\(X\\)-Werte verwende."
  },
  {
    "objectID": "slm_inference.html#maximum-likelihood-methode-bei-der-einfachen-linearen-regression",
    "href": "slm_inference.html#maximum-likelihood-methode-bei-der-einfachen-linearen-regression",
    "title": "13  Inferenz",
    "section": "13.3 Maximum-likelihood Methode bei der einfachen linearen Regression",
    "text": "13.3 Maximum-likelihood Methode bei der einfachen linearen Regression\nEin anderer Herleitung für \\(\\beta_0\\) und \\(\\beta_1\\) kann über die sogenannten Maximum Likelihood durchgeführt werden. Dabei gehen direkt die Verteilungsannahmen direkt ein.\nFür eine gegebene Zufallsvariable die jeweilige Dichte eines gegebenen Wertes über die Dichtefunktion berechnet werden. Wenn ein Zufallsvariable \\(X\\) einer Normalverteilung folgt, dann wird die Verteilung von \\(X\\) nach der bereits kennengelernte Dichtefunktion der Normalverteilung beschrieben.\n\\[\nf(X|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2}\\frac{(X - \\mu)^2}{\\sigma^2}\\right)\n\\]\nHier wird die Dichte von \\(X\\) als eine Funktion von \\(\\mu\\) und \\(\\sigma^2\\) aufgefasst. Es ist aber auch möglich, die Zufallsvariable \\(X\\) als gegeben anzusehen und die Dichte für verschiedene Werte von \\(\\mu\\) und \\(\\sigma^2\\) abzutragen. Der Einfachheit halber gehen wir davon aus, dass \\(\\sigma^2\\) gegeben sei und wir \\(\\mu\\) nicht kennen. Eine mögliche Fragestellung ist jetzt, für einen beobachteten Wert \\(x\\), welcher Wert von \\(\\mu\\) ist am plausibelsten?\nTragen wir dazu verschiedene Dichtewerte für ein gegebenes \\(x\\) in Abhängigkeit von verschiedenen \\(\\mu\\) ab.\nD.h. wir interpretieren die Funktion als:\n\\[\nf(\\mu|x,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2}\\frac{(X - \\mu)^2}{\\sigma^2}\\right)\n\\]\nDiese Funktion wird als die likelihood-Funktion bezeichnet. Das Maximum dieser Funktion kann als derjenige Wert interpretiert werden bei dem derjenige Wert von \\(\\mu\\) die maximal mögliche Dichte einnimmt.\nDie Likelihood-Funktion ist eine Funktion, die die Wahrscheinlichkeit beschreibt, mit der eine gegebene Stichprobe, in Abhängikeit von den Parametern aus einer bestimmten Verteilung stammt. Die Likelihood-Funktion gibt also an, wie gut die beobachteten Daten zu einem bestimmten Satz von Parametern passen.\nFormal wird die Likelihood-Funktion als die gemeinsame Wahrscheinlichkeitsdichte der Stichprobe beschrieben, betrachtet als Funktion der Parameter. Dabei werden die beobachteten Werte als festgelegt und die Parameter als Variablen betrachtet. Die Likelihood-Funktion ist also eine Funktion der Parameter, die die Wahrscheinlichkeit der beobachteten Daten als Funktion dieser Parameter beschreibt. Die Likelihood-Funktion ist dabei keine Dichtefunktion und beschreibt somit keine Wahrscheinlichkeiten. Dementsprechend ist gilt für das Integral der Likelihood-Funktion \\(\\int L(\\mu|X,\\sigma^2) d\\mu \\neq 1\\) bzw. ist \\(=1\\) per Zufall.\nIn unserem Regressionsfall nimmt die Likelihood-Funktion für einen einzelnen Wert die folgende Form an:\n\\[\nL(\\beta_0, \\beta_1, \\sigma^2|y_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2\\sigma^2}\\right)\n\\]\nBei unserer Regressionsanalyse haben wir jedoch nicht nur einen einzigen beobachteten Wert \\((y_i, x_i)\\) sondern \\(N\\) beobachtete Werte. Da die Werte unabhängig voneinander sind (laut der Annahmen), werden die jeweiligen likelihoods miteinander multipliziert. Die resultierende Likelihood-Funktion nimmt dann die folgenden Form an:\n\\[\\begin{align*}\nL(\\beta_0, \\beta_1, \\sigma^2) &= \\prod_{i=1}^{N} f(y_i | x_i; \\beta_0, \\beta_1, \\sigma^2) \\\\\n&= \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2\\sigma^2}\\right) \\\\\n&= \\left(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\right)^N \\exp\\left(\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\right) \\\\\n&= \\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{N/2} \\exp\\left(\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\right) \\\\\n\\end{align*}\\]\nDie Idee ist jetzt wieder die Gleiche. Wir versuchen das Maximum dieser Funktion zu finden, da die Werte \\(\\beta_0, \\beta_1\\) und \\(\\sigma^2\\) dann so gewählt sind, dass sie die höchste likelihood haben. Der Ansatz erfolgt wieder mechanisch ,indem wie bei der Herleitung der Normalengleichungen, die partiellen Ableitungen berechnet werden, diese gleich Null gesetzt werden und das resultierende Gleichungssystem gelöst wird. Zu beachten hierbei, wir haben in jedem Produktterm die gleichen Parameter \\(\\beta_0, \\beta-1\\) und \\(\\sigma^2\\) und die jeweiligen beobachteten \\((y_i, x_i)\\) Tuple werden als gegeben angesehen.\nUm die Berechnungn zu vereinfachen, bietet sich bei der Likelihoo-Funktion ein Trick an. Es wird nicht Likelihood-Funktion abgeleitet, sondern der Logarithmus der Likelihood-Funktion. D.h. die Funktion wird transformiert. Bei der Logarithmus-Funktion handelt es sich um eine sogenannte bijektive Funktion. Eine bijektive Funktion ist eine Funktion die jedem Element in der Ursprungsmenge genau ein Element in der Zielmenge zuordnet und ebenfalls umgekehrt. Dadurch kommt es zu keinen Kollisionen oder Auslassungen. Einfach gesagt, wenn die Funktion \\(y = f(x) = log(x)\\) ist, dann wird jedem \\(x\\) genau ein \\(y\\) zugeordnet. Bzw. anders herum, wenn ich \\(y\\) kenne, dann kenne ich auch den Wert von \\(x\\) mit \\(f(x) = y\\) bzw. \\(x = f^{-1}(y) = \\exp(y)\\). Dadurch, das die Logarithmus-Funktion bijektiv ist, führt dies dazu, dass das Maximum der ursprünglichen Funktion \\(L(\\beta_0, \\beta_1, \\sigma^2)\\) an der gleichen Stelle auftritt wie bei der transformierten Funktion \\(\\ln L(\\beta_0, \\beta_1, \\sigma^2)\\).\nWenn jetzt die Eigenschaften der Logarithmusfunktion, speziell des natürlichen Logarithmus, beachtet werden, dann wird auch klar, warum es Sinn machen könnte die Likelihood-Funktion mit dem Logarithmus zu transformieren, da aus den Produkten Summen werden mit denen einfacher umgegangen werden kann:\n\\[\\begin{align*}\n\\log(xy) &= \\log(x) + \\log(y) \\\\\n\\log\\left(\\frac{x}{y}\\right) &= \\log(x) - \\log(y) \\\\\n\\log(x^n) &= n\\log(x) \\\\\n\\log(\\exp(x)) &= x \\\\\n\\log(1) &= 0\n\\end{align*}\\]\nDer Logarithmus angewendet auf \\(L(\\beta_0, \\beta_1, \\sigma^2)\\) resultiert dann in der folgenden Funktion:\n\\[\\begin{align*}\n\\ell(\\beta_0, \\beta_1, \\sigma^2) &= \\ln L(\\beta_0, \\beta_1, \\sigma^2) \\\\\n&= \\ln \\left[\\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{N/2} \\exp\\left(-\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\right)\\right] \\\\\n&= \\ln \\left[\\left(\\frac{1}{2\\pi \\sigma^2}\\right)^{N/2} \\right] + \\ln \\left[\\exp\\left(-\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2}\\right)\\right] \\\\\n&= \\frac{N}{2} \\ln \\left[\\left(\\frac{1}{2\\pi \\sigma^2}\\right) \\right] -\\sum_{i=1}^N \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2} \\\\\n&= -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i)^2\n\\end{align*}\\]\nWir die Funktion \\(\\ell(\\beta_0, \\beta_1, \\sigma^2)\\) wieder partiell nach \\(\\beta_0\\) und \\(\\beta_1\\) abgeleitet und gleich Null gesetzt erhalten wir das gleiche Gleichungssystem wie bei den vorhergehenden Herleitungen über die Abweichungen von der Regressionsgeraden. z.B.\n\\[\\begin{align*}\n\\frac{\\partial \\ell(\\beta_0, \\beta_1, \\sigma^2)}{\\partial \\beta_0} &= \\frac{\\partial}{\\partial \\beta_0} -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i)^2 \\\\\n&= \\frac{2}{2\\sigma^2}\\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)\n\\end{align*}\\] Wenn dieser Ausdruck gleich Null gesetzt erhalten wir den gleichen Ausdruck wie unter"
  },
  {
    "objectID": "slm_inference.html#konfidenzintervalle-für-die-koeffizienten",
    "href": "slm_inference.html#konfidenzintervalle-für-die-koeffizienten",
    "title": "13  Inferenz",
    "section": "13.4 Konfidenzintervalle für die Koeffizienten",
    "text": "13.4 Konfidenzintervalle für die Koeffizienten\nWie wir im oberen Abschnitt gesehen haben, sind unsere Schätzer für die Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\) mit Unsicherheiten behaftet die sich in Form der Standardfehler ausdrücken. Wir können nun, diese standardfehler wiederum verwenden um Konfidenzintervalle für die Koeffizienten zu bestimmen.\n\\[\\begin{equation}\n\\hat{\\beta_j} \\pm q_{t_{\\alpha/2,df=N-2}} \\times \\hat{\\sigma}_{\\beta_j}\n\\label{eq-slm-inf-conf-0}\n\\end{equation}\\]\nWie in Formel\\(\\eqref{eq-slm-inf-conf-0}\\) zu sehen berechnet sich das Konfidenzintervall nach dem üblichen Muster: Schätzer \\(\\pm\\) Quantile \\(\\times\\) Standardfehler. Im vorliegenden Falle wird die Quantile aus der \\(t\\)-Verteilung mit \\(N-2\\) Freiheitsgarden bestimmt. Wie vorher bereits betont, das Konfidenzintervall erlaubt keine Aussage über die Wahrscheinlichkeit mit der der wahre Koeffizient in dem Intervall liegt, sondern gibt an welche \\(H_0\\)-Hypothesen mit den Daten kompatibel sind. Daher soll in der Ergebnisdokumentation das Konfidenzintervall angegeben und spätenstens in der Diskussion die obere und die untere Schranke diskutiert werden.\nIn R kann das Konfidenzintervall mit der Funktion confint() berechnet und ausgegeben werden.\n\nconfint(mod)\n\n                 2.5 %    97.5 %\n(Intercept) -0.6076488 0.3305767\nv_ms         0.7111082 0.8110957\n\n\nWie die Koeffizienten haben die Konfidenzintervall die gleiche Einheit wie die abhängige Variable und können daher direkt interpretiert werden. Im vorliegenden Fall sollte daher besprochen werden welche Bedeutung ein Koeffizient von \\(\\beta_1 = 0.7\\) bzw. von \\(\\beta_1 = 0.8\\) für die Interpretation des Modell hat.\nNoch einmal zu erwähnen ist, dass die beiden Parameter \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\) welche die Regressionsgerade beschreiben, Schätzer für die Parameter aus einer Population sind der die beiden Parameter \\(\\beta_0\\) und \\(\\beta_1\\) den zugrundeliegenden Zusammenhang zwischen den beiden Variablen beschreiben. Diese betrachtung ist parallel zu derjenigen, wenn wir z.B. anhand des Mittelwerts \\(\\bar{x}\\) den währenen Populationsmittelwert \\(\\mu\\) versuchen zu schätzen. D.h. wir haben eine Populationsregressionsgerade, die wir mit Hilfe der Daten versuchen zu schätzen. Die wahre Regressionsgerade werden wird aber niemals mit 100%-iger Sicherheit bestimmen, eben genausowenig wie wir den Populationsmittelwert \\(\\mu\\) nicht mittels \\(\\bar{x}\\) bestimmen können."
  },
  {
    "objectID": "slm_inference.html#weiteres-material",
    "href": "slm_inference.html#weiteres-material",
    "title": "13  Inferenz",
    "section": "13.5 Weiteres Material",
    "text": "13.5 Weiteres Material\nAltman und Krzywinski (2015) und Kutner u. a. (2005, p.40–48)\n\n\n\n\nAltman, Naomi, und Martin Krzywinski. 2015. „Points of Significance: Simple linear regression.“ Nature methods 12 (11).\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York."
  },
  {
    "objectID": "slm_model_fit.html#residuen",
    "href": "slm_model_fit.html#residuen",
    "title": "14  Modellfit",
    "section": "14.1 Residuen",
    "text": "14.1 Residuen\nDazu schauen wir uns zunächst noch einmal an, was überhaupt Residuen \\(e_i\\) sind und gehen noch mal von den grundlegenden Modellannahmen aus (siehe Formel \\(\\eqref{eq-sim-model-lr}\\)).\n\\[\\begin{equation}\ny_i = \\beta_0 + \\beta_1 \\cdot x_i + \\epsilon_i, \\qquad \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\n\\label{eq-sim-model-lr}\n\\end{equation}\\]\nDas lineare Regressionsmodell geht von einem linearen Zusammenhang in den Koeffizienten zwischen der Variablen \\(x_i\\) und den Variablen \\(y_i\\) aus. Additiv kommt daz ein normalverteilter Fehler \\(\\epsilon_i\\). Die Normalverteilung der \\(\\epsilon_i\\) habem einen Erwartungswert von \\(\\mu = 0\\) und eine Standardabweichung von \\(\\sigma\\). Die Standardabweichung \\(\\sigma\\) ist zunächst unbekannt und muss über die Daten abgeschätzt werden. Dies führt dazu, dass \\(y_i\\) für jeden gegebenen Wert von \\(x_i\\) einer Normalverteilung mit \\(\\mathcal{N}(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\) folgen und der bereits bekannten graphischen Darstellung (siehe Abbildung 14.1).\n\n\n\n\n\nAbbildung 14.1: Beispiel einer Regressionsgeraden und der Verteilung der Residuen um den Vorhersagewert \\(\\hat{y_i}\\)\n\n\n\n\nFür jeden gegebenen Wert von \\(X\\) sind die \\(Y\\)-Werte Normalverteilt. Die Varianz dieser Normalverteilungen ist gleich \\(\\sigma\\) während der Mittelwert \\(\\mu\\) immer um den Wert der Regressionsgeraden verschoben ist. D.h. die Streuung von \\(\\epsilon_i\\) überträgt sich auf die Streuung von \\(y_i\\) für jeden gegebenen \\(X\\)-Wert. Ohne den zufälligen Einfluss der Fehlerwerte würden wir alle \\(y_i\\)-Werte perfekt auf der Regressionsgeraden erwarten. Dies deutet daher auch schon eine Möglichkeit an die Residuen \\(\\epsilon_i\\) mittels der Daten abzuschätzen. Man verwendet die Abweichungen der beobachteten Werten \\(y_i\\) von den vorhergesagten Werten \\(\\hat{y}_i\\) auf der Regressionsgeraden (siehe Formel \\(\\eqref{eq-sim-model-res-1}\\)).\n\\[\\begin{equation}\n\\hat{\\epsilon}_i = e_i = y_i - \\hat{y_i}\n\\label{eq-sim-model-res-1}\n\\end{equation}\\]\nDiese Abweichungen \\(e_i\\) können als Schätzer \\(\\hat{\\epsilon}_i\\) für die wahren Residuen \\(\\epsilon_i\\) verwendet werden (siehe Abbildung 14.2).\n\n\n\n\n\nAbbildung 14.2: Examplarische Darstellung der Berechnung der Residuen \\(e_i\\) als Abweichung der beobachteten Werte \\(y_i\\) von den vorhergesagten Werten \\(\\hat{y}_i\\)\n\n\n\n\nDa die Normalverteilungen der \\(\\epsilon_i\\) für jeden \\(X\\)-Wert immer gleich sein sollten bis auf die Verschiebung von \\(\\mu_{Y|X}\\), deutet dies ebenfalls eine erste Möglichkeit an, die Modellannahmen graphisch zu überprüfen. Wenn die Residuen \\(e_i\\) geben die vorhergesagten Werte \\(\\hat{y}_i\\) abgetragen werden, dann sollte die Verteilung der Residuen \\(e_i\\) überall nahezu gleich sein, da die Streuung \\(\\sigma\\) unabhängig von der Position auf der Regressionsgerade ist. In R können die Residuen mittels der Funktion residuals() bzw. der Kurzform resid() ermittelt werden. residuals() erwartet als Parameter das gefittete lm()-Objekt.\n\nresiduals(mod)\n\n          1           2           3           4           5           6 \n -9.3009275  -9.3682884 -11.2176585  -5.5721082  -6.3635647  -7.4162019 \n          7           8           9          10          11          12 \n -3.9665569  -8.7152962  -3.8032898  -0.4662810  -2.0491941  -2.1323841 \n         13          14          15          16          17          18 \n  0.1867102  -0.3382894  -2.7300208  -4.0317532  -6.1475804  -0.3782884 \n         19          20          21          22          23          24 \n  1.1267111  -0.4588401  -2.0417532  -2.5546673  -0.2276585   2.3352546 \n         25          26          27          28          29          30 \n -3.2075794   2.7949787   2.9982458   2.4379709   1.1162385   3.1894284 \n         31          32          33          34          35          36 \n  7.8049787  -0.9063196   4.0336013  11.3778918   7.6817897   8.6991516 \n         37          38          39          40          41 \n 12.3052546   7.2595065  20.9634431  -1.3171838  -1.5994700 \n\n\nDie anhand des Modells vorhergesagten Werte \\(\\hat{y_i}\\) werden der Funktion predict() berechnet. Diese Funktion werden wir uns im nächsten Kapitel noch ausführlich betrachten. Als Parameter wird wiederum das gefittete lm()-Modell übergeben.\n\npredict(mod)\n\n       1        2        3        4        5        6        7        8 \n13.35093 14.39829 16.24766 13.61211 14.40356 15.45620 12.02656 16.77530 \n       9       10       11       12       13       14       15       16 \n12.82329 11.49628 13.07919 14.14238 12.82329 13.34829 15.72002 17.04175 \n      17       18       19       20       21       22       23       24 \n19.15758 14.39829 13.87329 15.45884 17.04175 17.57467 16.24766 14.66475 \n      25       26       27       28       29       30       31       32 \n20.20758 15.19502 15.99175 17.57203 18.89376 17.83057 15.19502 23.90632 \n      33       34       35       36       37       38       39       40 \n19.94640 13.61211 17.30821 17.31085 14.66475 20.74049 12.02656 13.32718 \n      41 \n13.60947 \n\n\nBeide Funktionen, resid() und predict() geben die berechneten Werte in der Reihenfolge aus, in der die Originaldaten an lm() übergeben wurde. D.h. \\(e_1\\) und \\(\\hat{y}_1\\) gehören zum ersten \\(X\\)-Wert \\(x_1\\) aus den Originaldaten. Mit Hilfe dieser beiden Variablen kann nun ein Residuenplot erstellt werden (siehe Abbildung 14.3).\n\n\n\n\n\nAbbildung 14.3: Residuenplot der Residuen \\(e_i\\) gegen die vorhergesagten Werte \\(\\hat{y}_i\\)\n\n\n\n\nDer Plot sollte im Optimalfall so aussehen, dass die Residuen \\(e_i\\) gleichmäßig oberhalb und unterhalb um die Nulllinie verteilt sind und keine weiteren Strukturen oder Muster im Zusammenhang mit \\(\\hat{y}_i\\) zu erkennen sind. In dem Residuenplot in Abbildung 14.3 ist zunächst einmal kein größeres Problem zu erkennen, bis auf den einen Wert links oben.\nUm besser zu Verstehen wir Problem aussehen könnten, schauen wir uns zwei Residuenplots an, bei denen eine Struktur zu erkennen ist (siehe Abbildung 14.4)\n\n\n\n\n\n\n\n(a) Parabelförmiger Zusammenhang zwischen \\(e_i\\) und \\(\\hat{y}_i\\)\n\n\n\n\n\n\n\n(b) Ansteigende Streuung mit größer werdendem \\(\\hat{y}_i\\)\n\n\n\n\nAbbildung 14.4: Residuenplots die Probleme anzeigen.\n\n\nIn Abbildung 14.4 (a) ist ein parabelförmiger Zusammenhang zwischen \\(e_i\\) und \\(\\hat{y}_i\\) zu erkennen. Für kleine und große \\(\\hat{y}_i\\) Werte sind die Residuen \\(e_i\\) negativ während für mittlere Werte von \\(\\hat{y}_i\\) die Residuen \\(e_i\\) positiv sind. Diese deutet darauf hin, das zusäztliche Struktur in den Daten nicht im Modell erfasst wird und führt dazu dass die Modellannahmen der Normalverteilung der \\(\\epsilon_i\\) nicht erfüllt sind.\nIn Abbildung 14.4 (b) ist dagegen eine anderes Problem zu beobachten, die Residuen \\(e_i\\) zeigen zwar keine Struktur bezüglich positiv zu negativen Werten, allerdings werden die Abweichung von \\(0\\) mit größer werdenen \\(\\hat{y}_i\\) immer stärker. Dies deutet darauf hin, das die Streuung der Daten nicht gleich ist. Dies wird als Heteroskedastizität bezeichnet und deutet wiederum auf eine Verletzung der Annahmen bei der Homoskedastitzität ausgegangen wird. D.h. die Streuung soll über den gesamten Bereich von \\(\\hat{y}_i\\) gleich bleiben.\n\nDefinition 14.1 (Homoskedastizität) Wenn die Größe der Varianz der Residuen \\(\\epsilon_i\\) in einem Regressionsmodell unabhängig von der Größe der Vorhersagevariable \\(X_i\\) ist, wird dies als Homoskedastizität bezeichnet. Die Streuung der Residuen ist dann für alle Werte \\(X_i\\) gleich. Wenn dies nicht der Fall ist, wird von Heteroskedastizität gesprochen.\n\nDa die Varianz also konstant für alle Werte von \\(X_i\\) ist, trifft dies ebenfalls für die vorhergesagten Werte \\(\\hat{Y}_i\\) zu. Daher werden bei vielen der Plots die Residuen \\(\\hat{\\epsilon}_i\\) gegen die \\(\\hat{Y_i}\\) abgetragen. Dies hat den Vorteil, dass später bei der multiplen Regression die gleiche Art von Graphen benutzt werden kann, um Homoskeastizität zu analysieren.\nEine weitere Möglichkeit die Verteilung der Residuen zu überprüfen ist die Anfertigung von sogenannten qq-Plots. Dies ermöglichen etwas strukturierter die Verteilung der Residuen zu überprüfen.\n\n14.1.1 Quantile-Quantile-Plots\nqq-Plot ist die Kurzform von Quantile-Quantile-Plot. D.h. es werden die Quantilen von zwei Variablen gegeneinander abgetragen. Um die Funktionsweise besser zu verstehen schauen wir uns erst einmal ein Spielzeugbeispiel an. In Tabelle 14.1 ist eine kleiner Datensatz mit \\(n = 5\\) Datenpunkten angezeigt.\n\n\n\n\nTabelle 14.1: Spielzeugbeispieldaten mit \\(n=5\\)\n\n\ny\n\n\n\n\n-2.0\n\n\n5.0\n\n\n-1.2\n\n\n0.1\n\n\n7.0\n\n\n\n\n\n\nWir wollen jetzt überprüfen ob dieser Datensatz einer Normalverteilung folgt (Wohlwissend das mit fünf Datenpunkten keine Verteilungsannahme überprüft werden kann). Dazu schauen wir uns zunächst noch einmal die bekannte Standardnormalverteilung \\(\\Phi(z) = \\mathcal{N}(\\mu=0,\\sigma^2=1)\\) an (siehe Abbildung 14.5).\n\n\n\n\n\nAbbildung 14.5: Dichtefunktion der Standardnormalverteilung\n\n\n\n\nIm ersten Schritt unterteilen wir die Standardnormalverteilung \\(\\Phi(z)\\) in \\(n+1 = 6\\) gleich große Flächen. D.h. die durch die Flächen bestimmten Abschnitte haben alle die gleiche Wahrscheinlichkeit (=Fläche unter der Dichtefunktion). Die Flächen werden durch jeweiligen Trennpunkte unterteilt die gleichzeitig die Quantilen sind.\n\n\n\n\n\nAbbildung 14.6: Unterteilung der Standardnormalverteilung in sechs gleich große Flächen\n\n\n\n\nIn unserem Fall haben wir \\(n=5\\) Datenpunkte, unterteilen also unsere Verteilung in \\(6\\) Abschnitte die jeweils eine Fläche von \\(p = \\frac{1}{6} = 0.17\\) haben. D.h. \\(\\frac{1}{6}\\) der Werte von \\(\\Phi(x)\\) liegen links des ersten Trennpunktes, \\(\\frac{2}{6}\\) der Werte von \\(\\Phi(x)\\) liegen links des zweiten Trennpunktes, usw. D.h. die Trennpunkte bestimmen die jeweiligen Quantilen, oder genauer die theoretischen Quantilen die unter der Verteilungsannahme erwartet werden.\nDie Idee hinter dem qq-Plot besteht nun darin, die empirischen Quantilen gegen die theoretischen Quantilen abzutragen (siehe Abbildung 14.7). Wenn die beobachteten Daten aus der gleichen Verteilung wie die theoretische Verteilung stammen, dann sollten die Punkte einer Geraden folgen. Die Steigung der Geraden ist \\(1\\), wenn es sich um die identischen Verteilungen handelt. Wenn die Steigung \\(\\neq1\\) ist, dann kommen die Datenpunkte aus der gleichen Familie sind aber um einen Skalierungsfaktor unterschiedlich bzw. um den Mittelwert verschoben. Die Punkte sollten aber trotzdem auf einer Geraden liegen.\n\n\n\n\n\nAbbildung 14.7: Skizze der theoretischen und der empirischen Verteilung mit unterschiedlicher Skalierung (Faktor \\(2\\times\\)) aber aus der gleichen Verteilungsfamilie. In beiden Graphen ist die gleiche Quartile markiert\n\n\n\n\nUm die empirischen Quartilen zu bestimmen, werden dazu zunächst die beobachteten Datenpunkte aus Tabelle 14.1 aufsteigend nach der Größe sortiert (siehe Tabelle 14.2). Diese Werte können als empirische Quantilen bezeichnet werden. Unter der Annahme, dass die Werte eine repräsentative Stichprobe aus der Verteilung darstellen, erwarten wir, dass wenn wir weitere Werte beobachten würden, etwa \\(\\frac{1}{6}\\) der Werte kleiner als der kleinste Wert wären, \\(\\frac{2}{6}\\) der weiteren Werte kleiner als der 2. kleinste Wert wären und so weiter und so fort.\n\n\nTabelle 14.2: Sortierte Datenwerte des Spielzeugbeispiels\n\n\nkleinster\n2.kleinster\nmittlerer\n2.größter\ngrößter\n\n\n\n\n-2\n-1.2\n0.1\n5\n7\n\n\n\n\nDaher, wenn die beobachteten Werte der angenommenen theoretischen Verteilung folgen, dann sollte ein Graph der empirischen Quartilen gegen die theoretischen Quartilen nahezu (Stichprobenvariabilität) einer Geraden folgen.\n\n\n\n\n\nAbbildung 14.8: Streudiagramm der empirischen Werte gegen die theoretischen Quantilen\n\n\n\n\nIn Abbildung 14.8 sind die empirischen Quartilen gegen die theoretischen Quantilen für unser kleines Beispiel abgetragen. Tatsächlich ist es in diesem Fall schwierig eine Gerade zu erkennen bzw. von einer zu sprechen, da es sich nur um besagte fünf Wert handelt. Nochmals, mit \\(n=5\\) kann eine realistische Verteilungsannahme nicht überprüft werden.\nWenn der Datensatz größer ist, dann eignet sich ein qq-Plot allerdings sehr gut Abweichungen zu erkennen. In Abbildung 14.9 sind verschiedene Beispiele abgetragen.\n\n\n\n\n\n\n\n(a) Perfekt\n\n\n\n\n\n\n\n(b) Enden schwer\n\n\n\n\n\n\n\n\n\n(c) Enden leicht\n\n\n\n\n\n\n\n(d) Rechtsschief\n\n\n\n\nAbbildung 14.9: Beispiele für verschiedenen qq-Plots\n\n\nIn Abbildung 14.9 (a) ist ein perfekter Zusammenhang zwischen den empirischen und den theoretischen Quantilen abgebildet. In diesem Falle wurden synthetisch für 50 normalverteiltet Zufallsdaten ein qq-Plot erstellt. Es ist zu sehen, das tatsächlich eine Gerade den Zusammenhang beschreibt. In Abbildung 14.9 (b) ist dagegen ein Zusammenhang abgetragen, bei dem die empirischen und die theoretische Verteilung nicht zusammenpassen. In diesem Fall sind die haben die Randwerte der empirischen Vereteilung eine höhere Wahrscheinlichkeit als die unter der theoretischen Verteilung zu erwarten ist. D.h. extreme Werte kommen in der beobachteten Verteilung öfter in der theoretischen Verteilung vor. Dies deutet darauf hin, dass die Streuung der Daten möglicherweise nicht korrekt modelliert wurde. In diesem Fall, wird von einer tail heavy Verteilung gesprochen.\nIn Abbildung 14.9 (c) ist der gegenteilige Effekt abgetragen. Hier hat die theoretische Verteilung mehr Wahrscheinlichkeitsmasse in den Randzonen als die empirische Verteilung. Die beobachtete Verteilung ist tail light. Entsprechend ist in Abbildung 14.9 (d) ein Beispiel abgebildet, bei dem nur eine der Randzonen zu viel Wahrscheinlichkeitsmasse besitzt. Da die theoretische Verteilung wiederum die Normalverteilung ist und diese Symmetrisch ist, deutet diese darauf hin, das die empirische Verteilung ähnlich wie in Abbildung 14.9 (b) in der rechten Randzone zu viele Werte hat und daher Rechtsschief ist.\nFür unsere Daten ergibt sich das folgende qq-Diagramm (siehe Abbildung 14.10)\n\n\n\n\n\nAbbildung 14.10: QQ-Diagramm der Residuen des ADAS-ADCS-Modells\n\n\n\n\nDer Graph sieht zunächst einmal gar nicht so schlecht aus. Allerdings deutet die Abweichung rechts oben darauf hin, das möglicherweise die Streuung nicht korrekt abgeschätzt wurde. Insbesondere ist ein Wert zu sehen, der im Verhältnis zu den anderen Werten schon relativ weit von der Gerade weg ist. Daher ist es hier angezeigt, diesen Wert noch einmal genauer zu untersuchen.\n\n\n14.1.2 qq-Plot in R\nIn R gibt es zwei direkte Methoden einen qq-Plot zu erstellen. Mittels des Standardgrafiksystem können mit den Funktionen qqnorm() und qqline() qq-Plots mit der dazugehörigen Gerade erstellt werden. Für das ggplot()-System stehen die geoms geom_qq() und geom_qq_line() zur Verfügung. Wichtig ist hierbei, das in aes() der Parameter sample definiert werden muss. Für unser Spielzeugbeispiel sieht dies folgendermaßen aus:\n\ndf_toy &lt;- tibble::tibble(y = c(-2, 5, -1.2, 0.1, 7))\nggplot(df_toy, aes(sample=y)) +\n  geom_qq() +\n  geom_qq_line()\n\n\n\n\nAbbildung 14.11: qq-Plot der Spielzeugdaten mittels ggplot()\n\n\n\n\n\n\n14.1.3 Standardisierte Residuen\nEine Möglichkeit so einen Wert zu untersuchen, ist abzuschätzen wie ungewöhnlich der zu dem Residuen \\(e_i\\) gehörende \\(y_i\\)-Wert ist. Ein Problem der einfachen Residuen \\(e_i\\) ist, dass diese laut der Modellannahmen die gleiche Varianz \\(\\sigma^2\\) haben sollten. Allerdings, auf Grund der Art, wie die \\(e_i\\) berechnet werden, folgt die Randbedingung, dass die Summe der \\(e_i\\) gleich Null ist, \\(\\sum_{i=1}^n e_i = 0\\). Dies führt dazu, dass die einfachen Residuen nicht unanbhängig voneinander sind und nicht immer Homoskedastizität besitzen. Daher gibt es eine weitere Art Residuen anhand des Modell zu berechnen, die nicht unter diesen Beschränkungen leiden. Dies sind die standardisierten Residuen \\(e_{Si}\\). Dazu müssen wir uns zunächst mit Hebelwerte \\(h_i\\) beschäftigen.\n\n14.1.3.1 Hebelwerte\nWenn ein Modell an die Daten gefittet wird, dann haben nicht alle Werte den gleichen Einfluss auf die Modellparameter. Manche Werte üben einen stärkeren Einfluss auf das Modell aus als andere Werte. In Abbildung 14.12 ist ein Beispiel abgebildet für einen Datensatz bei dem ein einzelner Punkt einen übermäßig großen Einfluss auf das Modell ausübt.\n\n\n\n\n\nAbbildung 14.12: Beispiel für einen Datenpunkt mit einem großen Einfluss auf das Modell. Die resultierenden Regressionsgeraden sind mit dem Punkt (rot) und ohne den Punkt (grün) abgetragen.\n\n\n\n\nDer einzelen Punkt rechts oben in Abbildung 14.12 hat einen großen Einfluss auf die resultierende Regressionsgerade wie in der Abbildung zu sehen ist. Der Einfluss ist zum Teil durch den großen Abstand des \\(x_i\\)-Wertes vom Mittelwert der \\(x_i\\)-Werte \\(\\bar{x}\\) bestimmt. Der Einfluss jedes einzelnen \\(x\\)-Wertes wird mittels der sogenannten Hebelwerte \\(h_i\\) bestimmt. Die genaue Berechnung der Hebelwerte \\(h_i\\) ist für das weitere Verständnis allerdings nicht wichtig, sondern mehr das Verständnis des Konzepts. Die Hebelwerte \\(h_i\\) können Werte in \\(h_i \\in [1/n,1]\\) annehmen. In R können die Hebelwerte mit der Funktion hatvalues() berechnet werden.\nTragen wir in die Grafik die Hebelwerte in die Grafik Abbildung 14.12 ein (siehe Abbildung 14.13), dann ist zu sehen, dass der abgesetzte Wert auch den größten Hebelwert hat.\n\n\n\n\n\nAbbildung 14.13: Beispiel für einen Datenpunkt mit einem großen Einfluss auf das Modell. Die Werte geben die jeweiligen Hebelwerte \\(h_i\\) der Datenpunkte wieder.\n\n\n\n\nEine Daumenregel für die Hebelwerte ist der Schwellenwert von \\((2k+2)/n\\), wobei \\(k\\) die Anzahl der unabhängigen Variablen ist. Für den Beispieldatensatz in Abbildung 14.13 würde sich daher ein Wert von \\((2\\cdot 1+2)/30 = 0.13\\) ergeben. Entsprechend wäre der abgesetzte Wert mit einem Hebelwert von \\(h_i = 0.54\\) als problematisch einzustufen.\nNach diesem kurzen Exkurs zu den Hebelwerten \\(h_i\\), schauen wir uns für unsere weitere Betrachtung der Residuen zunächst den Zusammenhang zwischen der Varianz der Residuen in der Population \\(\\sigma^2\\) und der Varianz der geschätzten Residuen \\(\\sigma^2(\\hat{\\epsilon}_i) = \\sigma^2(e_i)\\) an. Es gilt:\n\\[\\begin{equation}\n\\sigma^2(e_i) = \\sigma^2 (1 - h_i)\n\\label{eq-slm-model-vare_i}\n\\end{equation}\\]\nD.h. wenn ein Datenpunkt \\(x_i\\) einen kleineren Einfluss auf das Modell ausübt und dementsprechend einen kleinen Hebelwert \\(h_i\\), dann wird die Varianz für diesen Wert nahezu korrekt eingeschätzt. Hat der Wert \\(x_i\\) allerdings, einen großen Hebelwert \\(h_i\\), führt die dazu, dass die Varianz für diesen Wert stärker unterschützt wird. Dieser Zusammenhang kann dazu benutzt werden standardisierte Residuen zu erstellen.\n\\[\\begin{equation}\ne_{Si} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_i}}\n\\label{eq-slm-model-stresid}\n\\end{equation}\\]\nDie standardisierten Residuen \\(e_{Si}\\) haben dazu die Eigenschaft, dass sie eine Varianz und damit Standardabweichung von \\(\\sigma^2(e_{Si}) = 1\\) haben, also Standardnormalverteilt \\(\\Phi(z)\\) sein sollten. Dadurch können Abweichungen von den Modellannahmen leichter Identifiziert werden, da die Skala normiert ist. In R kann die standardiserten Residuen \\(e_{Si}\\) mittels der Funktion rstandard() berechnet werden. Eine Standardgrafik zum inspizieren der standardisierten Residuen ist wiederum eine Abbildung der \\(e_{Si}\\) gegen die \\(\\hat{y}_i\\).\n\n\n\n\n\nAbbildung 14.14: Grafik der standardisierten Residuen \\(e_{Si}\\) gegen die Vorhersagewerte \\(\\hat{y}_i\\) für das ADL-Modell.\n\n\n\n\nDie Abbildung 14.14 sieht relativ ähnlich zu Abbildung 14.3 aus. Durch die Änderung der Skala ist jetzt aber leichter abschätzbar ob die Verteilung der erwarteten Normalverteilung folgt. D.h. etwa \\(\\frac{2}{3}\\) der Werte sollten zwischen \\(-1\\) und \\(1\\) liegen und etwa \\(95\\%\\) zwischen \\(-2\\) und \\(2\\). Bis auf den einen Punkt oben rechts, sieht alles soweit unauffällig aus.\n\n\n\n14.1.4 Studentized Residuals\nDie letzte Art von Residuen sind die sogenannten Studentized Residuals \\(e_{Ti}\\), die mittels der folgenden Formel berechnet werden.\n\\[\\begin{equation}\ne_{Ti} = \\frac{e_i}{\\hat{\\sigma}_{(-i)}\\sqrt{1-h_i}}\n\\label{eq-slm-model-rstudent}\n\\end{equation}\\]\nDie Formel \\(\\eqref{eq-slm-model-rstudent}\\) ist sehr ähnlich zu derer für die standardisierten Residuen, der einzige Unterschied ist der Term \\(\\hat{\\sigma}_{(-i)}\\). Dieser bezeichnet die Residualvarianz wenn dass Modell ohne den Datenpunkt \\(i\\) gefittet wird. D.h. wie stark verändert sich die Schätzung der Varianz wenn ein Datenpunkt weggelassen wird. Normalerweise sollte eine einzelner Punkt keinen übermäßigen Einfluss auf die geschätzte Varianz haben, daher können die Studentized Residuals dazu verwendet werden problematische Datenpunkte zu identifizieren. Wenn die tatsächlichen Residuen einer Normalverteilung folgen, dann kann gezeigt werden, dass die Studentized Residuals einer \\(t\\)-Verteilung mit \\(N-k-2\\) Freiheitsgeraden folgen. Daher könnte sogar ein formaler statistischer Test durchgeführt werden. In R können die Studentized Residuals \\(e_{Ti}\\) mittels der Funktion rstudent() berechnet werden und werden entsprechend den anderen Residuen in dem üblichen Graphen gegen die vorhergesagten Werte \\(\\hat{y}_i\\) abgetragen.\n\n\n\n\n\nAbbildung 14.15: Graph der Studentized Residuals \\(S_{Ti}\\) gegen die vorhergesagten Werte \\(\\hat{y}_i\\) vor das adl-Modell\n\n\n\n\n\n\n14.1.5 Übersicht über die Residuenarten\nIn Tabelle 14.3 sind noch einmal die drei Arten von Residuen aufgelistet.\n\n\nTabelle 14.3: Übersicht über verschiedene Arten von Residuen\n\n\n\n\n\n\n\nTyp\nBerechnung\nZiel\n\n\n\n\nEinfache Residuen\n\\(e_i = y_i - \\hat{y}_i\\)\nVerteilungsannahme\n\n\nStandardisierte Residuen\n\\(e_{Si} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_i}}\\)\nVerteilungsannahme\n\n\nStudentized Residuen\n\\(e_{Ti} = \\frac{e_i}{\\hat{\\sigma}_{(-i)}\\sqrt{1-h_i}}\\)\nEinfluss auf Modell\n\n\n\n\n\n\n14.1.6 Ausgabe von summary() (continued)\nNach dieser Betrachtung der Residuen, die nach jedem Modellfit inspiziert werden sollten um zu überprüfen ob die Modellannahmen angemessen sind schauen wir uns noch einmal kurz die Ausgabe von summary() an.\n\n\n\nCall:\nlm(formula = adcs ~ adas, data = adl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2177  -3.8033  -0.4663   2.7950  20.9634 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.5445     4.3052   6.166 3.05e-07 ***\nadas         -0.2638     0.1015  -2.599   0.0131 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.516 on 39 degrees of freedom\nMultiple R-squared:  0.1477,    Adjusted R-squared:  0.1258 \nF-statistic: 6.757 on 1 and 39 DF,  p-value: 0.01312\n\n\nNach der Wiedergabe des gefitten Modells erfolgt direkt eine Zusammenfassung der Residuen über Minimum und Maximum, Q1 und Q3 und den Median. Jetzt sollte daher auch besser nachvollziehbar sein, warum es sinnvoll ist diese Statistiken über die Residuen direkt anzugeben. Die beiden Extremwerte geben einen ersten Überblick auf mögliche Ausreißer, während die erste Quartile Q1 und die dritte Quartile Q3 möglich Asymmetrien in der Verteilung der Residuen anzeigen. Laut der Annahem der Residuen als Normalverteilt mit \\(\\mu = 0\\), sollten diese beiden Werte etwa gleich weit von Null entfernt sein. Dementsprechend sollte der Median nahe an Null dran sein. Was nah ist, kommt dabei immer auf die Einheit der abhängigen Variablen an, wenn der Abstand in Kilometern ist kann ein kleiner Wert schon problematisch sein, während wenn eine Sprungweite in Mikrometern angeben wird eine großer Wert unbedenklich sein kann. Der Schätzerwert für \\(\\sigma\\) selbst, wir unten mit Residual standard error angegeben.\nIm vorliegenden Fall des Modells für die adl-Daten ist der Median dementsprechend doch etwas weit von Null entfernt und der geschätzte Residualfehler \\(\\hat{\\sigma} = 6.52\\) ebenfalls relativ groß. \\(\\hat{\\sigma}\\) kann mittels der Funktion sigma() erhalten werden.\n\n\n14.1.7 Zum Nachlesen\nZum weiteren Vertiefen der Inhalte findet ihr in Kutner u. a. (2005, p.100–114), Altman und Krzywinski (2016b) und Fox (2011, p.285–296) noch einmal gute Zusammenfassungen."
  },
  {
    "objectID": "slm_model_fit.html#einflussmetriken",
    "href": "slm_model_fit.html#einflussmetriken",
    "title": "14  Modellfit",
    "section": "14.2 Einflussmetriken",
    "text": "14.2 Einflussmetriken\nUm das gefittet Modell zu diagnostizieren reicht es allerdings nicht aus, sich nur die Residuen anzuschauen. Ein weiterer wichtiger Punkt ist die Analyse des Einflusses der einzelnen Datenpunkte auf das Modell. Wenn alles gut läuft sollte es keine einzelnen Datenpunkte geben, die einen übermäßig großen Einfluss auf das Modell ausüben. Anders ausgedrückt, die Anwesenheit bzw. Abwesenheit von einzelnen Datenpunkte sollte nicht dazu führen, dass die Aussage des Modells sich stark verändert. Im folgenden schauen wir uns dazu verschiedene Einflussmetriken die den Einfluss der Datenpunkte auf das Modell abschätzen. Die Idee der Einflussmetriken ist dabei die Gleiche wie schon bei den Studentized Residuals. Der Einfluss der Datenpunkte auf den Modellfit wird interpretiert indem ein Modell mit und ein Modell ohne den jeweiligen Datenpunkt gefittet wird. Der Einfluss auf verschiedene Modellparameter wird dann bestimmt und dementsprechend als möglicherweise bedenklich eingestuft. Die Meisten der im folgenden vorgestellten Ansätze verwenden in der einen oder anderen Form die Hebelwerte \\(h_i\\) die wir bereits kennengelernt haben.\n\n14.2.1 DFFITS (difference in fits)\nDas erst Maß, daß wir uns anschauen ist DFFITS (kurz für difference in fits). Das DFFITS-Maß wird getreent für jeden einzelnen Datenpunkt berechnet und der Einfluss des Datenpunkts auf den gefitteten Werte \\(\\hat{y}_i\\) für den jeweiligen Datenpunkt berechnet. Formal:\n\\[\\begin{equation}\n(DFFITS)_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{\\hat{\\sigma}\\sqrt{h_i}}\n\\label{eq-slm-model-dffits}\n\\end{equation}\\]\nIm Zähler kommen von Formel\\(\\eqref{eq-slm-model-dffits}\\) kommen zweimal die vorhergesagte \\(y\\)-Werte vor. \\(\\hat{y}_i\\) ist dabei der ganz normale Vorhersagewert der uns mittlerweile schon mehrfach begegnet ist. Der zweite Wert \\(\\hat{y}_{i(i)}\\) bezeichnet den vorhergesagten Wert aus dem Modell aus dem der Wert \\(y_i\\) weggelassen wurde. D.h, dass Modell ist mit einem Wert weniger gefittet worden. Daher misst die Differenz \\(\\hat{y}_i - \\hat{y}_{i(i)}\\) den Unterschied in den Vorhersagewerte zwischen den zwei Modellen bei denen einmal der Wert \\(y_i\\) zum fitten verwendet wurde und einmal wenn \\(y_i\\) weggelassen wurde. Umso größer der Unterschied zwischen diesen beiden Werte umso größer ist der Einfluss des Wertes \\(y_i\\) auf den Modellfit. Im Nenner von Formel\\(\\eqref{eq-slm-model-dffits}\\) wird wieder ein ähnlicher Normierungswert wie bei den Studentizied Residuals angewendet. Insgesamt, wird mittels DFFITS daher für jeden Datenpunkt ein Wert ermittelt und umso größer dieser Wert ist umso größer ist der Einfluss des jeweiligen Datenpunktes auf den Modellfit.\nIm idealen Fall sollte alle Datenpunkt ungefähr den gleichen Einfluss haben und einzelne Datenpunkte die einen übermäßig großen Einfluss auf das Modell haben sollten noch einmal genauer inspiziert werden.\n\n\n\n\n\n\nTipp\n\n\n\nAls Daumenregel, kann für kleine bis mittlere Datensätze ein DFFITS von \\(\\approx 1\\) auf Probleme hindeuten, während bei großen Datensätzen \\(\\approx 2\\sqrt{k/N}\\) als Orientierungshilfe verwendet werden kann (k := Anzahl der Prediktoren, N := Stichprobengröße).\n\n\n\n\n\n\n\n\nWarnung\n\n\n\nWenn ein Wert außerhalb der Daumenregel liegt, heißt das nicht, dass er automatisch ausgeschlossen werden muss/soll, sondern lediglich inspiziert werden sollte und das Modell mit und ohne diesen Wert interpretiert werden sollte.\n\n\nIn R können die DFFITS werden mittels der dffits()-Funktion berechnet werden. Als Parameter erwartet dffits() das gefittete lm()-Objekt. Ähnlich wie bei den Residuen, werden die DFFITS-Werte gegen die vorhergesagten \\(y_i\\)-Werte graphisch abgetragen um die Wert zu inspizieren und Probleme in der Modellspezifikation zu identifizieren.\n\n\n\n\n\nAbbildung 14.16: Graph der DFFITS-Werte gegen \\(\\hat{y}_i\\) für das adl-Modell.\n\n\n\n\nIn Abbildung 14.16 sind die DFFITS-Werte gegen die vorhergesagten Werte \\(\\hat{y}_i\\) abgetragen und zusätzlich die Daumenregel \\(\\pm1\\) eingezeichnet. Hier ist ein Wert nur gerade so außerhalb des vorgeschlagenen Bereichs. Hier könnte daher sich dieser Datenpunkt noch einmal genauer angeschaut werden, ob bei Ausschluß des Wertes es zu einer qualitativ anderen Interpretation der Daten kommt oder ob bespielsweise Übertragungsfehler für diesen Wert vorliegen oder sonstige Gründe.\n\n\n14.2.2 Cook-Abstand\nWährend DFFITS den Einfluss des Datenpunktes \\(i\\) auf den jeweiligen Datenpunkt abschätzt, wird bei dem sogenanten Cook-Abstand der Einfluss des \\(i\\)-ten Datenpunktes auf alle \\(n\\) vorhergesagten Werte \\(\\hat{y}_i\\). Formal:\n\\[\\begin{equation}\nD_i = \\frac{\\sum_{j=1}^N(\\hat{y_j} - \\hat{y}_{j(i)})}{k\\hat{\\sigma}^2}\n\\label{eq-slm-model-cook}\n\\end{equation}\\]\nHier bedeutet die Syntax \\(\\hat{y}_{j(i)}\\) der vorhergesagte Wert für den Datenpunkt \\(j\\) wenn der \\(i\\)-te Datenpunkt ausgelassen wird. In R können die Cook-Abstände mit Hilfe der Funktion cooks.distance() berechnet werden.\n\n\n\n\n\n\nTipp\n\n\n\nEine Daumenregel um einen möglichen Ausreißer zu identifzieren kann über \\(D_i &gt; 1\\) abgeschätzt werden.\n\n\nIn Abbildung 14.17 ist wiederum der übliche Graph gegen die vorhergesagten Werte \\(\\hat{y}_i\\) zu sehen. Anhand der abgebildeten Wert ist keiner der Datenpunkte als problematisch zu identifizieren.\n\n\n\n\n\nAbbildung 14.17: Cook’s \\(D_i\\) gegen \\(\\hat{y}_i\\) für das adl-Modell.\n\n\n\n\n\n\n14.2.3 DFBETAS\nAls letztes Maß schauen wir uns noch DFBETAS an. DFBETAS berechnet ein Maß für die Veränderung der \\(\\beta\\)-Koeffizienten auf Grund der einzelnen Datenpunkte \\(i\\). D.h. es wird jetzt nicht nur ein Wert für jeden Wert berechnet, sondern ein Wert für den jeden Datenpunkt und jeden \\(\\beta\\)-Koeffizienten. In unseren Fall mit einem y-Achsenabschnitt \\(\\beta_0\\) und einem Steigungskoeffizienten \\(\\beta_1\\) werden entsprechend \\(2 \\times x\\) Werte berechnet. Formal:\n\\[\\begin{equation}\n(DFBETAS)_{k(i)} = \\frac{\\hat{\\beta}_k - \\hat{\\beta}_{k(i)}}{\\sqrt{\\hat{\\sigma}^2c_{kk}}}\n\\label{eq-slm-model-dfbetas}\n\\end{equation}\\]\nWie aus Formel \\(\\eqref{eq-slm-model-dfbetas}\\) ersichtlich wird, wird die Veränderungen der Koeffizienten \\(\\beta_i\\) bei weglassen des \\(i\\)-ten Datenpunktes abgeschätzt. Den Wert \\(c_{kk}\\) lassen wir unberücksichtigt, da er wiederum nur einen Normierungsfaktor darstellt.\n\n\n\n\n\n\nTipp\n\n\n\nAls Daumenregel gilt für kleine bis mittlere Datensätze \\(\\approx 1\\), bzw. für große Datensätze \\(\\approx 2/\\sqrt{N}\\)\n\n\nWiederum gibt es eine spezielle Funktion in R um die DFBETAS zu berechnen dfbeta(). Dabei ist jedoch zu beachten das eine Matrize mit \\(k\\)-Spalten von dfbeta() zurück gegeben wird. Jede Spalte gibt den Wert für den jeweiligen \\(\\beta\\)-Koeffizienten an.\n\n\n\n\n\nAbbildung 14.18: DFBETA-Werte für \\(\\beta_0\\) und \\(\\beta_1\\) gegen \\(\\hat{y}_i\\)\n\n\n\n\nIn Abbildung 14.18 sind die DFBETAS für die beiden Koeffizienten \\(\\hat{beta}_0\\) und \\(\\hat{beta}_1\\) abgetragen. Hier ist zu sehen, dass die Wert für den Steigunsgkoeffizienten \\(\\beta_1\\) alle als unproblematisch anzusehen sind, während in Bezug auf \\(\\beta_0\\) ein paar wenige Fälle eine weiter Inspektion nach sich ziehen könnten. Allerdings sollte berücksichtigt werden, dass der y-Achsenabschnitt sehr stark durch die Verteilung der Datenpunkte in Bezug auf die \\(x\\)-Werte beeinflusst ist, da der Mittelwert der \\(x\\)-Werte bei \\(\\bar{x} = 41.2\\) liegt.\n\n\n14.2.4 Übersicht über die Einflussmetriken\nIn Tabelle 14.4 sind noch einmal die verschiedenen Methoden tabellarisch dargestellt.\n\n\nTabelle 14.4: Übersicht über die verschiedene Einflussmaße zur Bewertung der Modellgüte\n\n\nTyp\nVeränderung\nDaumenregel\n\n\n\n\n\\((DFFITS)_i\\)\nVorhersagewert i\n\\(2\\sqrt{k/N}\\)\n\n\nCook\nDurchschnittliche Vorhersagewerte\n\\(&gt;1\\)\n\n\n\\((DFBETAS)_{k(i)}\\)\nKoeffizient i\n\\(2\\sqrt{N}\\)\n\n\n\\(e_{Ti}\\)\nResiduum i\nt-Verteilung(n-k-2)\n\n\n\n\nNochmal, die Daumenregeln sind wirklich auch nur Daumenregeln und identifzieren nicht automatisch ein Problem im Datensatz.\n\n\n14.2.5 Zum Nacharbeiten\nNoch mal weitere Informationen findet Ihr in Altman und Krzywinski (2016a), Fox (2011, p.294–302) und Young (2019)."
  },
  {
    "objectID": "slm_model_fit.html#diagnoseplots-in-r",
    "href": "slm_model_fit.html#diagnoseplots-in-r",
    "title": "14  Modellfit",
    "section": "14.3 Diagnoseplots in R",
    "text": "14.3 Diagnoseplots in R\nDa die Diagnose eines gefitten Modell in jedem Fall durchgeführt werden soll und es sich dabei also um eine alltägliche Aufgabe handelt, gibt es mit plot(mod) einen short-cut um eine Reihe von Diagnoseplots direkt erstellen zu können.\n\nplot(mod)\n\n\n\n\n\n\nEine weitere Möglichkeit ist das package performance das zahlreiche Funktion enthält rund um die Analyse von Modellfits (siehe beispielweise performance::check_model()).\n\n\n\n\nAltman, Naomi, und Martin Krzywinski. 2016a. „Points of significance: Analyzing outliers: influential or nuisance“. Nature Methods 13 (4): 281–82.\n\n\n———. 2016b. „Points of significance: regression diagnostics“. Nature Methods 13 (5): 385–86.\n\n\nFox, John. 2011. An R companion to applied regression. 2. Aufl. SAGE Publication Inc., Thousand Oaks.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York.\n\n\nYoung, Alwyn. 2019. „Channeling fisher: Randomization tests and the statistical insignificance of seemingly significant experimental results“. The Quarterly Journal of Economics 134 (2): 557–98."
  },
  {
    "objectID": "slm_prediction.html#vorhergesagte-werte-haty_i",
    "href": "slm_prediction.html#vorhergesagte-werte-haty_i",
    "title": "15  Vorhersage",
    "section": "15.1 Vorhergesagte Werte \\(\\hat{y}_i\\)",
    "text": "15.1 Vorhergesagte Werte \\(\\hat{y}_i\\)\nWenn ein einfaches lineares Modell gefittet wurde ist eine zentrale Frage welche Vorhersagen anhand des Modell getroffen werden können. Die Vorhersagen \\(\\hat{y}_i\\) liegen auf der vorhergesagten Regressionsgerade und berechnen sich nach dem Modell für einen gegeben \\(x\\)-Wert.\n\\[\n\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_0} x\n\\]\nWie schon mehrfach besprochen unterliegt die Regressionsgerade inherent der Unsicherheit bezüglich der geschätzen Modellkoeffizienten \\(\\hat{\\beta}_0\\) und \\(\\hat{\\beta}_1\\). Diese Unsicherheit überträgt sich auf die geschätzen Werte \\(\\hat{y}_i\\) und muss daher bei deren Interpretation berücksichtigt werden.\nIn Abbildung 15.1 sind die bereits behandelten Sprungdaten gegen die Anlaufgeschwindigkeiten zusammen mit der Regressionsgeraden und vorhergesagten Werten (rot) abgetragen.\n\n\n\n\n\nAbbildung 15.1: Vorhersagewerte \\(\\hat{y}_i\\) (rote Punkte) für die Sprungdaten.\n\n\n\n\nIn R können die vorhergesagten Werte des mittels lm() gefitteten Modells mit der Hilfsfunktion predict() bestimmt werden. Wenn der Funktion predict() keine weiteren Parameter außer dem lm-Objekt übergeben werden, berechnet predict() die vorhergesagten Werte \\(\\hat{y}_i\\) für alle die \\(x\\)-Werte die auch zum fitten des Modells benutzt wurden. Die Reihenfolge der Werte \\(\\hat{y}_i\\) enspricht dabei den Werten im Original-data.frame().\n\npredict(mod)[1:5] \n\n       1        2        3        4        5 \n4.523537 4.725140 4.856256 4.761778 5.416207 \n\n\nWir haben uns hier nur die ersten fünf Werte ausgeben lassen, da nur demonstriert werden soll wie die predict()-Funktion angewendet werden kann. Um eine Anwendung zu geben, so können mittels predict() die Residuen auch von Hand ohne die resid()-Funktion erhalten werden.\n\n(jump$jump_m - predict(mod))[1:5]\n\n          1           2           3           4           5 \n-0.16267721 -0.41248842 -0.29359256 -0.01047071  0.09927500 \n\nresid(mod)[1:5]\n\n          1           2           3           4           5 \n-0.16267721 -0.41248842 -0.29359256 -0.01047071  0.09927500 \n\n\nWiederum nur zur Demonstration die ersten fünf Wert um die Äquivalenz der beiden Methoden zu demonstrieren.\nMeistens liegt das Interesse jedoch weniger auf den vorhergesagten Werten \\(\\hat{y}_i\\) für die gemessenen Werte, sondern es sollen Werte vorhergesagt werden für \\(x\\)-Werte die nicht im Datensatz enthalten sind. Operational ändert sich nichts, es wird immer noch das gefittete Modell verwendetet und es müssen lediglich neue \\(x\\)-Werte übergeben werden.\nIn R kann dies mittels des zweite Parameter in predict() erreicht werden. Soll zum Beispiel die Sprungweite für eine Anlaufgeschwindigkeit von \\(v = 11.5[m/s]\\) berechnen werden, muss zunächst ein neues tibble() erstellt werden, welches den gewünschten \\(x\\)-Wert enthält. Dabei muss der Spaltenname in dem neuen tibble() demjenigen im Original-tibble() entsprechen. Ansonsten funktioniert die Anwendung von predict() nicht.\n\ndf &lt;- tibble(v_ms = 11.5)\ndf\n\n# A tibble: 1 × 1\n   v_ms\n  &lt;dbl&gt;\n1  11.5\n\n\nDieses tibble() kann nun zusammen mit dem lm()-Objekt an predict() übergeben werden.\n\npredict(mod, newdata = df)\n\n       1 \n8.614136 \n\n\nD.h., bei einer Anlaufgeschwindigkeit von \\(v = 11.5[m/s]\\) ist anhand des Modells eine Sprungweite von \\(8.6m\\) zu erwarten."
  },
  {
    "objectID": "slm_prediction.html#unsicherheit-in-der-vorhersage",
    "href": "slm_prediction.html#unsicherheit-in-der-vorhersage",
    "title": "15  Vorhersage",
    "section": "15.2 Unsicherheit in der Vorhersage",
    "text": "15.2 Unsicherheit in der Vorhersage\nWie schon angesprochen ist unser Modell natürlich mit Unsicherheiten behaftet. Diese drücken sich in den Standardfehler für die beiden Koeffizienten \\(\\hat{\\beta_0}\\) und \\(\\hat{\\beta_1}\\) (siehe Tabelle 15.1).\n\n\n\n\nTabelle 15.1: Modellparameter und Standardfehler\n\n\n\nSchätzer\n\\(s_e\\)\n\n\n\n\n(Intercept)\n-0.14\n0.23\n\n\nv_ms\n0.76\n0.02\n\n\n\n\n\n\nDer vorhergesagte Wert \\(\\hat{y}\\) ist daher für sich alleine ist noch nicht brauchbar, da auch Informationen über dessen Unsicherheit notwendig sind um die Ergebnisse korrekt zu interpretieren.\nEs können zwei unterschiedliche Anwendungsfälle voneinander unterschieden werden.\n\nDer mittlere, erwartete Wert \\(\\hat{\\bar{y}}_{neu}\\)\nDie Vorhersage eines einzelnen Wertes \\(\\bar{y}_{neu}\\)\n\nIm konkreten Fall werden damit zwei unterschiedliche Fragestellungen beantwortet. Im 1. Fall lautet die Frage, ich habe eine Trainingsgruppe und möchte wissen was der mittlere Wert der Gruppe anhand des Modells ist, wenn alle eine bestimmte Anlaufgeschwindigkeit \\(v_{neu}\\) haben. Im 2. Fall lautet die Frage welche Weite eine einzelne Athletin für die Anlaufgeschwindigkeit \\(v_{neu}\\) springen sollte. In beiden Fällen werden keiner genau den Wert des Regressionsmodells treffen, aber im 1. Fall der Gruppe werden sich Streuungen nach oben bzw. nach unten gegenseitig im Schnitt ausbalancieren während im 2. Fall der einzelnen Athletin dies nicht der Fall ist. Daher hat die Vorhersage im 2. Fall eine höhere Unsicherheit. Diese Unterschied sollte sich dementsprechend in den Varianzen der beiden Vorhersagen wiederspiegeln.\nWie bereits erwähnt, der vorhergesagte Wert \\(\\hat{y}_{neu}\\) ist in beiden Fällen gleich und entsprecht der oben beschriebenen Methode anhand des Modell \\(y_{neu} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times x_{\\text{neu}}\\).\nFür den erwarteten Mittelwert errechnet sich die Varianz nach:\n\\[\\begin{equation}\nVar(\\hat{\\bar{y}}_{neu}) = \\hat{\\sigma}^2 \\left[\\frac{1}{n} + \\frac{(x_{neu} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\\right] = \\hat{\\sigma}_{\\hat{\\bar{y}}_{neu}}^2\n\\end{equation}\\]\nDas dazugehörige Konfidenzintervall errechnet sich danach mittels:\n\\[\\begin{equation}\n\\hat{\\bar{y}}_{neu} \\pm q_{t(1-\\alpha/2;n-2)} \\times \\hat{\\sigma}_{\\hat{\\bar{y}}_{neu}}\n\\end{equation}\\]\nDie Varianz für die Vorhersage eines einzelnen Wertes errechnet sich:\n\\[\\begin{equation}\nVar(\\hat{y}_{neu}) = \\hat{\\sigma}^2 \\left[1 + \\frac{1}{n} + \\frac{(x_{neu} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\\right] = \\hat{\\sigma}^2 + \\hat{\\sigma}_{\\hat{\\bar{y}}_{neu}}^2 = \\hat{\\sigma}_{\\hat{y}_{neu}}^2\n\\end{equation}\\]\nWas wiederum zu dem folgenden Konfidenzintervall führt:\n\\[\\begin{equation}\n\\hat{y}_{neu} \\pm q_{t(1-\\alpha/2;n-2)} \\times \\hat{\\sigma}_{\\hat{y}_{neu}}\n\\end{equation}\\]\nIn beiden Fällen ist der Term\n\\[\n\\frac{(x_{neu} - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}\n\\]\nenthalten. Anhand des Zählers kann abgeleitet werden, dass die Unsicherheit der Vorhersage mit dem Abstand vom Mittelwert der \\(x\\)-Werte zunimmt. Rein heuristisch macht dies Sinn, da davon ausgegangen werden kann, dass um den Mittelwert der \\(x\\)-Werte auch die meiste Information über \\(y\\) vorhanden ist und dementsprechend umso weiter die Werte sich vom \\(\\bar{x}\\) entfernen die Information abnimmt. Im Nenner ist wiederum wie auch beim Standardfehler \\(\\sigma_{\\beta_1}\\) des Steigungskoeffizienten \\(\\beta_1\\) zu sehen, dass die Varianz abnimmt mit der Streuung der \\(x\\)-Werte. Daher, wenn eine Vorhersage in einem bestimmten Bereich von \\(x\\)-Werten durchgeführt werden soll, dann sollte darauf geachtet werden möglichst diesen Bereich auch zu samplen um die Unsicherheit so klein wie möglich zu halten."
  },
  {
    "objectID": "slm_prediction.html#vorhersagen-in-r-mit-predict",
    "href": "slm_prediction.html#vorhersagen-in-r-mit-predict",
    "title": "15  Vorhersage",
    "section": "15.3 Vorhersagen in R mit predict()",
    "text": "15.3 Vorhersagen in R mit predict()\n\n15.3.1 Erwarteter Mittelwert\n\ndf &lt;- data.frame(v_ms = 11.5) # oder tibble(v_ms = 11.5)\npredict(mod, newdata = df, interval = 'confidence')\n\n       fit      lwr      upr\n1 8.614136 8.482039 8.746234\n\n\n\n\n15.3.2 Individuelle Werte\n\npredict(mod, newdata = df, interval = 'prediction')\n\n       fit      lwr      upr\n1 8.614136 8.118445 9.109827"
  },
  {
    "objectID": "slm_prediction.html#konfidenzintervalle-graphisch",
    "href": "slm_prediction.html#konfidenzintervalle-graphisch",
    "title": "15  Vorhersage",
    "section": "15.4 Konfidenzintervalle graphisch",
    "text": "15.4 Konfidenzintervalle graphisch\n\n\n\n\n\nWeiterführende Literatur sind Kutner u. a. (2005)"
  },
  {
    "objectID": "slm_prediction.html#r2-und-root-mean-square",
    "href": "slm_prediction.html#r2-und-root-mean-square",
    "title": "15  Vorhersage",
    "section": "15.5 \\(R^2\\) und Root-mean-square",
    "text": "15.5 \\(R^2\\) und Root-mean-square"
  },
  {
    "objectID": "slm_prediction.html#einfaches-modell",
    "href": "slm_prediction.html#einfaches-modell",
    "title": "15  Vorhersage",
    "section": "15.6 Einfaches Modell",
    "text": "15.6 Einfaches Modell\n\nmod0 &lt;- lm(y ~ x, simple)\nsummary(mod0)\n\n\nCall:\nlm(formula = y ~ x, data = simple)\n\nResiduals:\n      1       2       3       4 \n-0.5817  0.9898 -0.2345 -0.1736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   1.8414     0.7008   2.628    0.119\nx             0.4574     0.3746   1.221    0.346\n\nResidual standard error: 0.8376 on 2 degrees of freedom\nMultiple R-squared:  0.4271,    Adjusted R-squared:  0.1406 \nF-statistic: 1.491 on 1 and 2 DF,  p-value: 0.3465"
  },
  {
    "objectID": "slm_prediction.html#nochmal-abweichungen",
    "href": "slm_prediction.html#nochmal-abweichungen",
    "title": "15  Vorhersage",
    "section": "15.7 Nochmal Abweichungen",
    "text": "15.7 Nochmal Abweichungen\n\nGesamtvarianz: \\[\nSSTO := \\sum_{i=1}^N (y_i - \\bar{y})^2\n\\]\nRegressionsvarianz: \\[\nSSR :=\\sum_{i=1}^N(\\hat{y}_i - \\bar{y})^2\n\\]\nResidualvarianz: \\[\nSSE := \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\\]\n\n\n\n\n\n\nMinimalmodell der Abweichungen"
  },
  {
    "objectID": "slm_prediction.html#verhältnis-von-ssr-zu-ssto",
    "href": "slm_prediction.html#verhältnis-von-ssr-zu-ssto",
    "title": "15  Vorhersage",
    "section": "15.8 Verhältnis von \\(SSR\\) zu \\(SSTO\\)",
    "text": "15.8 Verhältnis von \\(SSR\\) zu \\(SSTO\\)\n\n\n\n\n\nPerfekter Zusammenhang\n\n\n\n\n\\[\n\\frac{SSR}{SSTO} = 1\n\\]\n\n\n\n\n\nKein Zusammenhang\n\n\n\n\n\\[\n\\frac{SSR}{SSTO} = 0\n\\]"
  },
  {
    "objectID": "slm_prediction.html#determinationskoeffizient-r2",
    "href": "slm_prediction.html#determinationskoeffizient-r2",
    "title": "15  Vorhersage",
    "section": "15.9 Determinationskoeffizient \\(R^2\\)",
    "text": "15.9 Determinationskoeffizient \\(R^2\\)\nEs gilt: \\(SSTO = SSR + SSE\\)\n\\[\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO} \\in [0,1]\n\\] 1\n\n15.9.1 Korrigierter Determinationskoeffizient \\(R_a^2\\)\n\\[\nR_a^2 = 1 - \\frac{\\frac{SSE}{n-p}}{\\frac{SSTO}{n-1}} = 1 - \\frac{n-1}{n-p}\\frac{SSE}{SSTO}\n\\]\n\n\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York."
  },
  {
    "objectID": "slm_prediction.html#footnotes",
    "href": "slm_prediction.html#footnotes",
    "title": "15  Vorhersage",
    "section": "",
    "text": "Bei der einfachen Regression gilt: \\(r_{xy} = \\pm\\sqrt{R^2}\\)↩︎"
  },
  {
    "objectID": "mlm_title.html",
    "href": "mlm_title.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "Im folgenden wird das Modell der einfachen linearen Regression erweitert indem zusätzliche Terme in das Modell aufgenommen werden. Die Prinzipien bleiben dabei jedoch weitestgehendst gleich und können direkt auf den komplizierteren Fall der multiplen Regression übertragen werden. Im Laufe der Erweiterung des Modells wird sich dabei wird herausstellen, dass neben mehreren kontinuierlichen Variablen auch nominale Faktoren in das Modell intergriert werden können. Daraus entsteht ein sehr flexibler Modellapparat, der in den verschiedensten Zusammenhängen angewendet werden kann."
  },
  {
    "objectID": "mlm_basics.html#bedeutung-der-koeffizienten-bei-der-multiplen-regression",
    "href": "mlm_basics.html#bedeutung-der-koeffizienten-bei-der-multiplen-regression",
    "title": "16  Einführung",
    "section": "16.1 Bedeutung der Koeffizienten bei der multiplen Regression",
    "text": "16.1 Bedeutung der Koeffizienten bei der multiplen Regression\nUm die Bedeutung der Regressionskoeffzienten bei der multiple Regression besser zu verstehen ist es von Vorteil sich noch einmal die Bedeutung der Koeffizienten im einfachen Regressionsmodell zu vergegenwärtigen (siehe Abbildung 16.1).\n\n\n\n\n\nAbbildung 16.1: Beispiel für eine einfache Regression und der resultierenden Regressiongeraden\n\n\n\n\nBei der einfachen Regression haben mittels der Methode der kleinsten Quadrate eine Regressiongerade durch unsere Punktwolke gelegt. Dabei haben wir die Regressionsgerade so gewählt, dass die senkrechten Abstände der beobachteten Punkte von der Regressionsgerade minimiert werden bzw. die Abstände zwischen denen auf der Gerade liegenden, vorhergesagten Werte \\(\\hat{y}_i\\) und den beobachteten Wert \\(y_i\\).\nWenn wir nun den Übergang von einer Prädiktorvariablenzum nächstkomplizierteren Fall nehmen mit zwei Prädiktorvariablen \\(x_1\\) und \\(x_2\\), dann wäre eine mögliche Darstellungsform der Daten eine Punktwolke im dreidimensionalen Raum (siehe Abbildung 16.2 (a)).\n\n\n\n\n\n\n\n(a) 3D Punktwolke\n\n\n\n\n\n\n\n(b) 3D Punktwolke mit gefitteter Ebene\n\n\n\n\nAbbildung 16.2: Punktwolken bei der multiple Regression\n\n\nDa jetzt eine einzelne Gerade nicht mehr in der Lage ist die Daten zu fitten, ist die nächst Möglichkeit eine Ebene die in die Punktwolke gelegt wird (siehe Abbildung 16.2 (b)). Dies ermöglicht dann genau die gleiche Herangehensweise wie bei der einfachen linearen Regression anzuwenden. Als Zielgröße wird aus den möglichen Ebenen diejenigen gesucht deren vorhergesagten, auf der Ebene liegenden Punkte \\(\\hat{y}_i\\) die geringsten senkrechten Abstand zu den beobachteten Punkten \\(y_i\\) haben. Anders, wir suchen diejenigen Ebene durch die Punktwolke deren Summe der quadrierten Residuen \\(e_i = y_i - \\hat{y}_i\\) minimal ist.\nDiese Herangehensweise hat den Vorteil, dass sie zum einem die einfache lineare Regression als Spezialfall mit \\(K=1\\) beinhaltet und sich beliebig erweitern lässt mit der Einschränkung, dass bei \\(K&gt;2\\) die dreidimenionale Darstellung mittels einer Grafik nicht mehr möglich ist. Das Prinzip der Minimierung der Abweichungen von \\(\\hat{y}_i\\) zu \\(y\\) bleibt aber immer erhalten. Zusammenfassend hat dieser Ansatz somit die folgenden Vorteile:\n\nDie Berechnungen bleiben alle gleich\nAbweichungen \\(\\hat{\\epsilon_i}\\) sind jetzt nicht mehr Abweichungen von einer Gerade sondern von einer \\(K\\)-dimensionalen Hyperebene. Die Eigenschaften der Residuen bleiben aber alle erhalten.\nDie Modellannahmen bleiben gleich: Unabhängige \\(y_i\\) und \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) iid\nInferenz für die Koeffizienten mittels \\(t_k = \\frac{\\hat{\\beta}_k}{s_k} \\sim t(N-K-1)\\) (Konfidenzintervall dito)\nKonzepte für die Vorhersage bleiben erhalten\nModelldiagnosetools bleiben alle erhalten\n\nAls nächster Schritt versuchen wir nun die Interpretation der Koeffizienten im multiplen Regressionsmodell besser zu verstehen."
  },
  {
    "objectID": "mlm_basics.html#einfaches-beispiel",
    "href": "mlm_basics.html#einfaches-beispiel",
    "title": "16  Einführung",
    "section": "16.2 Einfaches Beispiel",
    "text": "16.2 Einfaches Beispiel\n\\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\epsilon_i \\\\\n\\beta_0 &= 1 ,\\beta_1 = 3, \\beta_2 = 0.7 \\\\\n\\epsilon_i &\\sim N(0,\\sigma = 0.5)\n\\end{align*}\\]\n\nN &lt;- 50 # Anzahl Datenpunkte\nbeta_0 &lt;- 1\nbeta_1 &lt;- 3\nbeta_2 &lt;- 0.7\nsigma &lt;- 0.5\nset.seed(123)\ndf &lt;- tibble(\n  x1 = runif(N, -2, 2),\n  x2 = runif(N, -2, 2),\n  y = beta_0 + beta_1*x1 + beta_2*x2 + \n    rnorm(N, 0, sigma)) \n\n\n\n\n\n\nEinfacher Zusammenhang y~x1\n\n\n\n\n\n\n\n\n\nEinfacher Zusammenhang y~x2"
  },
  {
    "objectID": "mlm_basics.html#wie-sieht-der-fit-aus",
    "href": "mlm_basics.html#wie-sieht-der-fit-aus",
    "title": "16  Einführung",
    "section": "16.3 Wie sieht der Fit aus?",
    "text": "16.3 Wie sieht der Fit aus?\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.20883 -0.26741 -0.00591  0.27315  1.01322 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.07674    0.06552   16.43  &lt; 2e-16 ***\nx1           2.96537    0.05604   52.91  &lt; 2e-16 ***\nx2           0.70815    0.05961   11.88 9.27e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4604 on 47 degrees of freedom\nMultiple R-squared:  0.9849,    Adjusted R-squared:  0.9842 \nF-statistic:  1529 on 2 and 47 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "mlm_basics.html#was-bedeuten-die-einzelnen-koeffizienten",
    "href": "mlm_basics.html#was-bedeuten-die-einzelnen-koeffizienten",
    "title": "16  Einführung",
    "section": "16.4 Was bedeuten die einzelnen Koeffizienten?",
    "text": "16.4 Was bedeuten die einzelnen Koeffizienten?\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n1.077\n0.066\n\n\nx1\n2.965\n0.056\n\n\nx2\n0.708\n0.060\n\n\n\n\n\nDer Unterschied in der abhängigen Variablen, wenn zwei Objekte sich in \\(x_i\\) um eine Einheit unterscheiden und die paarweise gleichen Werte in den verbleibenden \\(x_j, j \\neq i\\) annehmen."
  },
  {
    "objectID": "mlm_basics.html#was-bedeuten-die-koeffizienten-in-kombination",
    "href": "mlm_basics.html#was-bedeuten-die-koeffizienten-in-kombination",
    "title": "16  Einführung",
    "section": "16.5 Was bedeuten die Koeffizienten in Kombination?",
    "text": "16.5 Was bedeuten die Koeffizienten in Kombination?\n\n16.5.1 Full model\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n1.077\n0.066\n\n\nx1\n2.965\n0.056\n\n\nx2\n0.708\n0.060\n\n\n\n\n\n\n\n16.5.2 um x2 bereinigt\n\nmod_x1_x2 &lt;- lm(x1 ~ x2, df)\nres_mod_x1_x2 &lt;- resid(mod_x1_x2)\nmod_x1_res &lt;- lm(y ~ res_mod_x1_x2, df)\n\n\n\n              Estimate Std. Error t value\n(Intercept)       1.25       0.16    7.61\nres_mod_x1_x2     2.97       0.14   20.97\n\n\n\n\n16.5.3 um x1 bereinigt\n\nmod_x2_x1 &lt;- lm(x2 ~ x1, df)\nres_mod_x2_x1 &lt;- resid(mod_x2_x1)\nmod_x2_res &lt;- lm(y ~ res_mod_x2_x1, df)\n\n\n\n              Estimate Std. Error t value\n(Intercept)       1.25       0.51    2.44\nres_mod_x2_x1     0.71       0.47    1.51"
  },
  {
    "objectID": "mlm_basics.html#was-bedeuten-die-koeffizienten-in-kombination-1",
    "href": "mlm_basics.html#was-bedeuten-die-koeffizienten-in-kombination-1",
    "title": "16  Einführung",
    "section": "16.6 Was bedeuten die Koeffizienten in Kombination?",
    "text": "16.6 Was bedeuten die Koeffizienten in Kombination?\n\n\\(\\hat{\\beta}_1\\): Wenn ich \\(x_2\\) weiß, welche zusätzlichen Informationen bekomme ich durch \\(x_1\\)\n\\(\\hat{\\beta}_2\\): Wenn ich \\(x_1\\) weiß, welche zusätzlichen Informationen bekomme ich durch \\(x_2\\)\n\nIn Beispiel nicht problematisch, weil nach Konstruktion \\(x_1\\) und \\(x_2\\) unabhängig voneinander sind:\n\nround(cor(df),3)\n\n      x1    x2     y\nx1 1.000 0.078 0.969\nx2 0.078 1.000 0.289\ny  0.969 0.289 1.000"
  },
  {
    "objectID": "mlm_basics.html#added-variable-plots",
    "href": "mlm_basics.html#added-variable-plots",
    "title": "16  Einführung",
    "section": "16.7 Added-variable plots",
    "text": "16.7 Added-variable plots\n\n\n\n\n\nZusammenhang zwischen y und x2 bereinigt um den Einfluß von x1."
  },
  {
    "objectID": "mlm_basics.html#added-variable-plots-mit-caravplots",
    "href": "mlm_basics.html#added-variable-plots-mit-caravplots",
    "title": "16  Einführung",
    "section": "16.8 Added-variable plots mit car::avPlots()",
    "text": "16.8 Added-variable plots mit car::avPlots()\n\ncar::avPlots(mod, ~x2)"
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-ich-einen-prädiktor-weg-lasse",
    "href": "mlm_basics.html#was-passiert-wenn-ich-einen-prädiktor-weg-lasse",
    "title": "16  Einführung",
    "section": "16.9 Was passiert wenn ich einen Prädiktor weg lasse?",
    "text": "16.9 Was passiert wenn ich einen Prädiktor weg lasse?\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n1.077\n0.066\n\n\nx1\n2.965\n0.056\n\n\nx2\n0.708\n0.060\n\n\n\n\n\n\ncoef(lm(y ~ x1, df))\n\n(Intercept)          x1 \n   1.007466    3.017589 \n\ncoef(lm(y ~ x2, df))\n\n(Intercept)          x2 \n  1.3377771   0.9555316 \n\n\nIn unserem Beispiel wieder nicht viel, da die Variablen unabhängig (orthogonal) voneinander sind."
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren",
    "href": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren",
    "title": "16  Einführung",
    "section": "16.10 Was passiert wenn Prädiktoren stark miteinander korrelieren?",
    "text": "16.10 Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n\n\nAusschnitt von Körperfettdaten\n\n\ntriceps\nthigh\nmidarm\nbody_fat\n\n\n\n\n19.5\n43.1\n29.1\n11.9\n\n\n24.7\n49.8\n28.2\n22.8\n\n\n30.7\n51.9\n37.0\n18.7\n\n\n29.8\n54.3\n31.1\n20.1\n\n\n19.1\n42.2\n30.9\n12.9\n\n\n25.6\n53.9\n23.7\n21.7\n\n\n\n\n\n1"
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-1",
    "href": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-1",
    "title": "16  Einführung",
    "section": "16.11 Was passiert wenn Prädiktoren stark miteinander korrelieren?",
    "text": "16.11 Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\nGGally::ggpairs(bodyfat) + theme(text = element_text(size = 10))\n\n\n\n\nKorrelationsmatrize"
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-2",
    "href": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-2",
    "title": "16  Einführung",
    "section": "16.12 Was passiert wenn Prädiktoren stark miteinander korrelieren?",
    "text": "16.12 Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n# Alle drei Prädiktoren\nmod_full &lt;- lm(body_fat ~ triceps + thigh + midarm, bodyfat)\n# ohne Arm\nmod_wo_midarm &lt;- lm(body_fat ~ triceps + thigh, bodyfat)\n# Ohne Oberschenkel\nmod_wo_thigh &lt;- lm(body_fat ~ triceps + midarm, bodyfat)\n# Ohne Triceps\nmod_wo_triceps &lt;- lm(body_fat ~ thigh + midarm, bodyfat)"
  },
  {
    "objectID": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-3",
    "href": "mlm_basics.html#was-passiert-wenn-prädiktoren-stark-miteinander-korrelieren-3",
    "title": "16  Einführung",
    "section": "16.13 Was passiert wenn Prädiktoren stark miteinander korrelieren?",
    "text": "16.13 Was passiert wenn Prädiktoren stark miteinander korrelieren?\n\n\n\nfull model\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n117.085\n99.782\n\n\ntriceps\n4.334\n3.016\n\n\nthigh\n-2.857\n2.582\n\n\nmidarm\n-2.186\n1.595\n\n\n\n\n\n\n\n\nw/o midarm\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n-19.174\n8.361\n\n\ntriceps\n0.222\n0.303\n\n\nthigh\n0.659\n0.291\n\n\n\n\n\n\n\n\nw/o thigh\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n6.792\n4.488\n\n\ntriceps\n1.001\n0.128\n\n\nmidarm\n-0.431\n0.177\n\n\n\n\n\n\n\n\nw/o triceps\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n-25.997\n6.997\n\n\nthigh\n0.851\n0.112\n\n\nmidarm\n0.096\n0.161"
  },
  {
    "objectID": "mlm_basics.html#multikollinearität",
    "href": "mlm_basics.html#multikollinearität",
    "title": "16  Einführung",
    "section": "16.14 Multikollinearität2",
    "text": "16.14 Multikollinearität2\n\nGroße Änderungen in den Koeffizienten wenn Prädiktoren ausgelassen/eingefügt werden\nKoeffizienten haben eine andere Richtung als erwartet\nHohe (einfache) Korrelationen zwischen Prädiktoren\nBreite Konfidenzintervalle für “wichtige” Prädiktoren \\(b_j\\)\n\n\\[\n\\widehat{\\text{Var}}(b_j) = \\frac{\\hat{\\sigma}^2}{(n-1)s_j^2}\\frac{1}{1-R_j^2}\n\\]\n\\(R_j^2\\) = Multipler Korrelationskoeffizient der Prädiktoren auf Prädiktorvariable \\(j\\)."
  },
  {
    "objectID": "mlm_basics.html#variance-inflation-factor-vif",
    "href": "mlm_basics.html#variance-inflation-factor-vif",
    "title": "16  Einführung",
    "section": "16.15 Variance Inflation Factor (VIF)",
    "text": "16.15 Variance Inflation Factor (VIF)\n\\[\n\\text{VIF}_j = \\frac{1}{1-R_j^2}\n\\]\n\n\n\n\n\n\nTipp\n\n\n\nWenn VIF &gt; 10 ist, dann deutet dies auf hohe Multikollinearität hin.\n\n\n3"
  },
  {
    "objectID": "mlm_basics.html#variance-inflation-factor-vif-1",
    "href": "mlm_basics.html#variance-inflation-factor-vif-1",
    "title": "16  Einführung",
    "section": "16.16 Variance Inflation Factor (VIF)",
    "text": "16.16 Variance Inflation Factor (VIF)\n\ncar::vif(mod_full) \n\n triceps    thigh   midarm \n708.8429 564.3434 104.6060 \n\n\n4\nÜblicherweise wird der größte Wert betrachtet um die Multikollinearität zu bewerten."
  },
  {
    "objectID": "mlm_basics.html#wenn-prädiktoren-sich-gegenseitig-maskieren",
    "href": "mlm_basics.html#wenn-prädiktoren-sich-gegenseitig-maskieren",
    "title": "16  Einführung",
    "section": "16.17 Wenn Prädiktoren sich gegenseitig maskieren5",
    "text": "16.17 Wenn Prädiktoren sich gegenseitig maskieren5\n\n\n\n\n\nx_pos maskiert den Einfluss von x_neg"
  },
  {
    "objectID": "mlm_basics.html#wenn-prädiktoren-sich-gegenseitig-maskieren-1",
    "href": "mlm_basics.html#wenn-prädiktoren-sich-gegenseitig-maskieren-1",
    "title": "16  Einführung",
    "section": "16.18 Wenn Prädiktoren sich gegenseitig maskieren",
    "text": "16.18 Wenn Prädiktoren sich gegenseitig maskieren\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n0.235\n0.135\n\n\nx_pos\n0.218\n0.147\n\n\n\n\n\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n0.228\n0.116\n\n\nx_neg\n-0.618\n0.103\n\n\n\n\n\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\n\n\n\n\n(Intercept)\n0.135\n0.096\n\n\nx_pos\n0.850\n0.123\n\n\nx_neg\n-0.976\n0.099"
  },
  {
    "objectID": "mlm_basics.html#multiple-regression",
    "href": "mlm_basics.html#multiple-regression",
    "title": "16  Einführung",
    "section": "16.19 Multiple Regression",
    "text": "16.19 Multiple Regression\nAus der einfachen Regression\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\nwird\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_K x_{Ki} + \\epsilon_i\n\\]\nmit K Prädiktorvariablen und Multikollinearität."
  },
  {
    "objectID": "mlm_basics.html#zum-nacharbeiten",
    "href": "mlm_basics.html#zum-nacharbeiten",
    "title": "16  Einführung",
    "section": "16.20 Zum Nacharbeiten",
    "text": "16.20 Zum Nacharbeiten\nAltman und Krzywinski (2015) Kutner u. a. (2005, p.278–288) Fox (2011, p.325–327)\n\n\n\n\nAltman, Naomi, und Martin Krzywinski. 2015. „Points of significance: Multiple linear regression“. Nature Methods 12 (12): 1103–4.\n\n\nFox, John. 2011. An R companion to applied regression. 2. Aufl. SAGE Publication Inc., Thousand Oaks.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York.\n\n\nMcElreath, Richard. 2016. Statistical rethinking, A Bayesian Course with Examples in R and Stan. 1. Aufl. Boca Raton: CRC Press."
  },
  {
    "objectID": "mlm_basics.html#footnotes",
    "href": "mlm_basics.html#footnotes",
    "title": "16  Einführung",
    "section": "",
    "text": "Beispiel nach Kutner u. a. (2005)↩︎\ninformell nach Kutner u. a. (2005, pp. 407)↩︎\nManchmal wird auch Tolerance = \\(\\frac{1}{VIF}\\) betrachtet.↩︎\ncar::vif berechnet generalized variance inflation factor wenn Prädiktoren Faktoren oder Polynome sind (Fox 2011.) ↩︎\nadaptiert nach McElreath (2016)↩︎"
  },
  {
    "objectID": "mlm_interactions.html#beispieldaten",
    "href": "mlm_interactions.html#beispieldaten",
    "title": "17  Interaktionseffekte",
    "section": "17.1 Beispieldaten1",
    "text": "17.1 Beispieldaten1\n\n\n\nBeispieldaten (synthetisch)\n\n\nVelocity[m/s]\nbody mass[kg]\narm span[cm]\n\n\n\n\n185.42\n68.71\n20.14\n\n\n184.08\n73.85\n21.29\n\n\n200.74\n89.43\n27.57\n\n\n170.34\n84.97\n19.88\n\n\n176.89\n82.40\n20.51\n\n\n200.68\n91.57\n29.22"
  },
  {
    "objectID": "mlm_interactions.html#beispieldaten---deskriptiv",
    "href": "mlm_interactions.html#beispieldaten---deskriptiv",
    "title": "17  Interaktionseffekte",
    "section": "17.2 Beispieldaten - Deskriptiv",
    "text": "17.2 Beispieldaten - Deskriptiv\n\n\n\nDeskriptive Statistik der Handballdaten\n\n\n\nMean\nStd.Dev\nMin\nMax\n\n\n\n\narm_span\n184.3\n7.7\n169.4\n200.7\n\n\nbody_mass\n77.5\n10.3\n58.0\n101.1\n\n\nvel\n21.9\n2.3\n18.5\n29.2"
  },
  {
    "objectID": "mlm_interactions.html#beispieldaten-1",
    "href": "mlm_interactions.html#beispieldaten-1",
    "title": "17  Interaktionseffekte",
    "section": "17.3 Beispieldaten",
    "text": "17.3 Beispieldaten\n\n\n\n\n\nGeschwindigkeit gegen Körpergewicht\n\n\n\n\n\n\n\n\n\nGeschwindigkeit gegen Armspannweite"
  },
  {
    "objectID": "mlm_interactions.html#beispieldaten---startmodell",
    "href": "mlm_interactions.html#beispieldaten---startmodell",
    "title": "17  Interaktionseffekte",
    "section": "17.4 Beispieldaten - Startmodell",
    "text": "17.4 Beispieldaten - Startmodell\n\\[\nY_{i} = \\beta_0 + \\beta_1 \\times \\textrm{bm}_i + \\beta_2 \\times \\textrm{as}_i + \\epsilon_i\n\\]\n\nmod_1 &lt;- lm(vel ~ body_mass + arm_span, handball)\n\n\n\n\nModell 1\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n-1.768\n7.632\n-0.232\n0.818\n\n\nbody_mass\n0.077\n0.033\n2.359\n0.024\n\n\narm_span\n0.096\n0.044\n2.192\n0.035\n\n\n\\(\\hat{\\sigma}\\)\n1.996"
  },
  {
    "objectID": "mlm_interactions.html#modellfit",
    "href": "mlm_interactions.html#modellfit",
    "title": "17  Interaktionseffekte",
    "section": "17.5 Modellfit",
    "text": "17.5 Modellfit\n\n\n\n\n\n3D Streudiagramm"
  },
  {
    "objectID": "mlm_interactions.html#zentrierung",
    "href": "mlm_interactions.html#zentrierung",
    "title": "17  Interaktionseffekte",
    "section": "17.6 Zentrierung",
    "text": "17.6 Zentrierung\n\nhandball &lt;- dplyr::mutate(handball,\n                          body_mass_c = body_mass - mean(body_mass),\n                          arm_span_c = arm_span - mean(arm_span))\n\n\n\n\nDeskriptive Statistik\n\n\n\nMean\nStd.Dev\n\n\n\n\narm_span\n184.29\n7.72\n\n\narm_span_c\n0.00\n7.72\n\n\nbody_mass\n77.46\n10.26\n\n\nbody_mass_c\n0.00\n10.26\n\n\nvel\n21.85\n2.31"
  },
  {
    "objectID": "mlm_interactions.html#modell-mit-zentrierten-variablen",
    "href": "mlm_interactions.html#modell-mit-zentrierten-variablen",
    "title": "17  Interaktionseffekte",
    "section": "17.7 Modell mit zentrierten Variablen",
    "text": "17.7 Modell mit zentrierten Variablen\n\nmod_2 &lt;- lm(vel ~ body_mass_c + arm_span_c, handball)\n\n\n\n\nModell 2\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n21.852\n0.316\n69.247\n&lt;0.001\n\n\nbody_mass_c\n0.077\n0.033\n2.359\n0.024\n\n\narm_span_c\n0.096\n0.044\n2.192\n0.035\n\n\n\\(\\hat{\\sigma}\\)\n1.996"
  },
  {
    "objectID": "mlm_interactions.html#residuen-im-zentrierten-additiven-modell",
    "href": "mlm_interactions.html#residuen-im-zentrierten-additiven-modell",
    "title": "17  Interaktionseffekte",
    "section": "17.8 Residuen im zentrierten, additiven Modell",
    "text": "17.8 Residuen im zentrierten, additiven Modell\n\n\n\n\n\nResiduenplot"
  },
  {
    "objectID": "mlm_interactions.html#added-variable-plot",
    "href": "mlm_interactions.html#added-variable-plot",
    "title": "17  Interaktionseffekte",
    "section": "17.9 Added-variable plot",
    "text": "17.9 Added-variable plot\n\n\n\n\n\nAbbildung 17.1: Added-variable Graph mit car::avPlots()"
  },
  {
    "objectID": "mlm_interactions.html#was-passiert-wenn-die-effekte-nicht-mehr-nur-additiv-sind",
    "href": "mlm_interactions.html#was-passiert-wenn-die-effekte-nicht-mehr-nur-additiv-sind",
    "title": "17  Interaktionseffekte",
    "section": "17.10 Was passiert wenn die Effekte nicht mehr nur additiv sind?",
    "text": "17.10 Was passiert wenn die Effekte nicht mehr nur additiv sind?\n\n\n\n\n\nUnterteilung von Körpergewicht und Armspannweite in Kategorien"
  },
  {
    "objectID": "mlm_interactions.html#was-passiert-wenn-die-effekte-nicht-mehr-nur-additiv-sind-1",
    "href": "mlm_interactions.html#was-passiert-wenn-die-effekte-nicht-mehr-nur-additiv-sind-1",
    "title": "17  Interaktionseffekte",
    "section": "17.11 Was passiert wenn die Effekte nicht mehr nur additiv sind?",
    "text": "17.11 Was passiert wenn die Effekte nicht mehr nur additiv sind?\n\n17.11.1 Neues Modell mit Interaktionen:\n\\[\nY_{i} = \\beta_0 + \\beta_1 \\times \\textrm{bm}_i + \\beta_2 \\times \\textrm{as}_i + \\beta_3 \\times \\textrm{bm}_i \\times \\textrm{as}_i + \\epsilon_i\n\\]"
  },
  {
    "objectID": "mlm_interactions.html#modellierung",
    "href": "mlm_interactions.html#modellierung",
    "title": "17  Interaktionseffekte",
    "section": "17.12 Modellierung",
    "text": "17.12 Modellierung\n\nmod_3 &lt;- lm(vel ~ body_mass_c * arm_span_c, handball) \n\n\n\n\nModell 3\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n21.346\n0.143\n149.296\n&lt;0.001\n\n\nbody_mass_c\n0.119\n0.015\n8.133\n&lt;0.001\n\n\narm_span_c\n0.083\n0.019\n4.380\n&lt;0.001\n\n\nbody_mass_c:arm_span_c\n0.021\n0.002\n12.633\n&lt;0.001\n\n\n\\(\\hat{\\sigma}\\)\n0.868\n\n\n\n\n\n\n\n\n2"
  },
  {
    "objectID": "mlm_interactions.html#einfache-steigungen-in-vergleich",
    "href": "mlm_interactions.html#einfache-steigungen-in-vergleich",
    "title": "17  Interaktionseffekte",
    "section": "17.13 Einfache Steigungen in Vergleich",
    "text": "17.13 Einfache Steigungen in Vergleich\n\n\n\n\n\nModell ohne Interaktionen\n\n\n\n\n\n\n\n\n\nModell mit Interaktionen"
  },
  {
    "objectID": "mlm_interactions.html#interaktionen-sind-symmetrisch",
    "href": "mlm_interactions.html#interaktionen-sind-symmetrisch",
    "title": "17  Interaktionseffekte",
    "section": "17.14 Interaktionen sind symmetrisch",
    "text": "17.14 Interaktionen sind symmetrisch\n\n\n\n\n\nVeränderung mit der Körpergewicht\n\n\n\n\n\n\n\n\n\nVeränderung mit dem Armspannweite"
  },
  {
    "objectID": "mlm_interactions.html#warum-das-model-sinn-macht",
    "href": "mlm_interactions.html#warum-das-model-sinn-macht",
    "title": "17  Interaktionseffekte",
    "section": "17.15 Warum das Model Sinn macht",
    "text": "17.15 Warum das Model Sinn macht\n\n\n\n\n\nVeränderung mit dem Körpergewicht\n\n\n\n\n\n\n\nEinfache Steigungen\n\n\narm span\\centered\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\n\n\n\n10\n22.18\n0.33\n\n\n0\n21.35\n0.12\n\n\n-10\n20.51\n-0.09"
  },
  {
    "objectID": "mlm_interactions.html#warum-das-modell-sinn-macht",
    "href": "mlm_interactions.html#warum-das-modell-sinn-macht",
    "title": "17  Interaktionseffekte",
    "section": "17.16 Warum das Modell Sinn macht",
    "text": "17.16 Warum das Modell Sinn macht\n\n\n\nEinfache Steigungen\n\n\narm span\\centered\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\n\n\n\n10\n22.18\n0.33\n\n\n0\n21.35\n0.12\n\n\n-10\n20.51\n-0.09\n\n\n\n\n\n\n\n\nModellkoeffizienten\n\n\n\nbetas\n\n\n\n\nb0\n21.35\n\n\nbm_c\n0.12\n\n\nas_c\n0.08\n\n\nbm_c:as_c\n0.02"
  },
  {
    "objectID": "mlm_interactions.html#interpretation-der-koeffizienten",
    "href": "mlm_interactions.html#interpretation-der-koeffizienten",
    "title": "17  Interaktionseffekte",
    "section": "17.17 Interpretation der Koeffizienten",
    "text": "17.17 Interpretation der Koeffizienten\n\\[\nY = b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2 + b_3 \\cdot x_1 \\cdot x_2 + \\epsilon_i\n\\]\n\n\\(b_0\\): (y-Achsenabschnitt) der Wert von \\(\\hat{Y}\\) wenn \\(x_1 = 0\\) und \\(x_2 = 0\\) gilt.\n\\(b_1\\): Der Unterschied in \\(\\hat{Y}\\) wenn zwei Objekte sich in \\(x_1\\) um eine Einheit unterscheiden und \\(x_2 = 0\\) ist.\n\\(b_2\\): Der Unterschied in \\(\\hat{Y}\\) wenn zwei Objekte sich in \\(x_2\\) um eine Einheit unterscheiden und \\(x_1 = 0\\) ist.\n\\(b_3\\): (Interaktionskoeffizient) Die Veränderung des Effekts von \\(x_1\\) auf \\(\\hat{Y}\\) wenn \\(x_2\\) um eine Einheit größer wird bzw. genau andersherum für \\(x_2\\)."
  },
  {
    "objectID": "mlm_interactions.html#aus-der-ebene-wird-eine-gekrümmte-fläche",
    "href": "mlm_interactions.html#aus-der-ebene-wird-eine-gekrümmte-fläche",
    "title": "17  Interaktionseffekte",
    "section": "17.18 Aus der Ebene wird eine gekrümmte Fläche",
    "text": "17.18 Aus der Ebene wird eine gekrümmte Fläche\n\n\n\n\n\n3D Streudiagramm des Interaktionsmodells"
  },
  {
    "objectID": "mlm_interactions.html#residuenvergleich",
    "href": "mlm_interactions.html#residuenvergleich",
    "title": "17  Interaktionseffekte",
    "section": "17.19 Residuenvergleich",
    "text": "17.19 Residuenvergleich\n\n\n\n\n\nResiduen im additiven Modell\n\n\n\n\n\n\n\n\n\nResiduen im Interaktionsmodell"
  },
  {
    "objectID": "mlm_interactions.html#residuenvergleich---qq-plot",
    "href": "mlm_interactions.html#residuenvergleich---qq-plot",
    "title": "17  Interaktionseffekte",
    "section": "17.20 Residuenvergleich - qq-Plot",
    "text": "17.20 Residuenvergleich - qq-Plot\n\n\n\n\n\nadditives Modell\n\n\n\n\n\n\n\n\n\nInteraktionsmodell"
  },
  {
    "objectID": "mlm_interactions.html#take-away",
    "href": "mlm_interactions.html#take-away",
    "title": "17  Interaktionseffekte",
    "section": "17.21 Take-away",
    "text": "17.21 Take-away\nInteraktionsmodell\n\nErhöht die Flexibilität des linearen Modells.\nBei Interaktionen hängt der Einfluss der einzelnen Variablen immer von den Werten der anderen Variablen ab.\nAchtung: Interpretation der einfachen Haupteffekte nicht mehr möglich bzw. sinnvoll!"
  },
  {
    "objectID": "mlm_interactions.html#zuschlag",
    "href": "mlm_interactions.html#zuschlag",
    "title": "17  Interaktionseffekte",
    "section": "17.22 Zuschlag",
    "text": "17.22 Zuschlag\nWas passiert im Interaktionsmodell mit den Koeffizienten wenn die \\(x_{ki}\\)s zentriert werden?\n\\[\\begin{align*}\ny_i &= \\beta_0 + \\beta_1 (x_{1i} - \\bar{x}_1) + \\beta_2 (x_{2i} - \\bar{x}_2) + \\beta_3 (x_{1i}-\\bar{x}_1)(x_{2i}-\\bar{x}_2) \\\\\n&= \\beta_0 + \\beta_1 x_{1i} - \\beta_1 \\bar{x}_1 + \\beta_2 x_{2i} - \\beta_2 \\bar{x}_2 + \\beta_3 x_{1i} x_{2i} - \\beta_3 x_{1i} \\bar{x}_2 - \\beta_3 \\bar{x}_1 x_{2i} + \\beta_3 \\bar{x}_1 \\bar{x}_2 \\\\\n&= \\beta_0 - \\beta_1 \\bar{x}_1 - \\beta_2 \\bar{x}_2 + \\beta_3 \\bar{x}_1 \\bar{x}_2 + \\beta_1 x_{1i}- \\beta_3 \\bar{x}_2 x_{1i} + \\beta_2 x_{2i} - \\beta_3 \\bar{x}_1 x_{2i} + \\beta_3 x_{1i} x_{2i} \\\\\n&= \\underbrace{\\beta_0 - \\beta_1 \\bar{x}_1 - \\beta_2 \\bar{x}_2 + \\beta_3 \\bar{x}_1 \\bar{x}_2}_{\\beta_0} + \\underbrace{(\\beta_1 - \\beta_3 \\bar{x}_2) x_{1i}}_{\\beta_1 x_{1i}} + \\underbrace{(\\beta_2 - \\beta_3 \\bar{x}_1) x_{2i}}_{\\beta_2 x_{2i}} + \\beta_3 x_{1i} x_{2i}\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_interactions.html#zum-nacharbeiten",
    "href": "mlm_interactions.html#zum-nacharbeiten",
    "title": "17  Interaktionseffekte",
    "section": "17.23 Zum Nacharbeiten",
    "text": "17.23 Zum Nacharbeiten\nKutner u. a. (2005, p.306–313)\n\n\n\n\nDebanne, Thierry, und Guillaume Laffaye. 2011. „Predicting the throwing velocity of the ball in handball with anthropometric variables and isotonic tests“. Journal of Sports Sciences 29 (7): 705–13.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York."
  },
  {
    "objectID": "mlm_interactions.html#footnotes",
    "href": "mlm_interactions.html#footnotes",
    "title": "17  Interaktionseffekte",
    "section": "",
    "text": "Debanne und Laffaye (2011)↩︎\nA*B wird von R ausmultipliziert in A + B + A:B. Hätte auch lm(vel ~ body_mass_c + arm_span_c + body_mass_c:arm_span_c) verwenden können.↩︎"
  },
  {
    "objectID": "mlm_dummy_coding.html#vergleich-von-zwei-gruppen",
    "href": "mlm_dummy_coding.html#vergleich-von-zwei-gruppen",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.1 Vergleich von zwei Gruppen",
    "text": "18.1 Vergleich von zwei Gruppen\nBeginnen wir mit einem einfachen Beispiel. Wir wollen die Unterschiede zwischen Männern und Frauen in Bezug auf die Körpergröße untersuchen. In Abbildung 18.1 ist ein hypothetischer Datensatz von Körpergrößen von Frauen und Männern abgebildet. Wenig überraschen, da der Datensatz so erstellt wurde, sind Männer im Mittel größer als Frauen.\n\n\n\n\n\nAbbildung 18.1: Simulierte Daten: Verteilung von Körpergrößen nach Geschlecht für Männer (m) und Frauen (f)\n\n\n\n\nIn Tabelle 18.1 ist eine Ausschnit aus den Daten tabellarisch dargestellt. Wenig überraschend, haben wir zwei Datenspalten. In der ersten Spalte stehen die Körpergrößen, während in der zweiten Spalte eine Indikatorvariable steht die entweder den Wert \\(m\\) für Männer oder \\(f\\) für Frauen annimmt.\n\n\n\n\nTabelle 18.1: Ausschnitt aus den Daten.\n\n\ncm\ngender\n\n\n\n\n174.4\nm\n\n\n177.7\nm\n\n\n195.6\nm\n\n\n171.3\nf\n\n\n164.0\nf\n\n\n176.0\nf\n\n\n\n\n\n\nIn Tabelle 18.2 sind dann auch noch einmal die deskriptiven Statistiken der Körpergrößendaten abgebildet die auch noch einmal den Eindruck aus Abbildung 18.1 bestätigen.\n\n\n\n\nTabelle 18.2: Deskriptive Statistiken der Körpergrößendaten.\n\n\ngender\nm\nsd\n\n\n\n\nf\n168.8\n8.4\n\n\nm\n179.5\n9.8\n\n\n\n\n\n\nWir müssen jetzt allerdings erst einmal eine kurze Detour nehmen und verstehen, wie nominale Werte in R repräsentiert werden."
  },
  {
    "objectID": "mlm_dummy_coding.html#nominale-variablen-in-r-detour",
    "href": "mlm_dummy_coding.html#nominale-variablen-in-r-detour",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.2 Nominale Variablen in R (detour)",
    "text": "18.2 Nominale Variablen in R (detour)\nNominale Variablen werden in R mit einen speziellen Typ repräsentiert dem sogenannten factor. Erstellt werden kann ein Faktor mit der factor()-Funktion. Die Funktion hat drei wichtige Parameter. Der erste Parameter bezeichnet die Werte, der zweite die möglichen Faktorstufen (levels) und der dritte Parameter die dazugehörigen Bezeichnungen (labels). Ein einfaches Beispiel sieht dann so aus:\n\ngender &lt;- factor(c(0,0,1,1),\n                 levels = c(0,1),\n                 labels = c('m','f'))\ngender\n\n[1] m m f f\nLevels: m f\n\n\nD.h. wir haben einen Datenvektor mit den Elemente \\((0,0,1,1)\\). Wir spezifizieren die levels dementsprechend mit \\(0\\) und \\(1\\) und definieren die dazugehörigen labels mit \\(m\\) und \\(f\\). Dabei sind jeweils Vektoren übergeben worden (siehe c()). Wenn wir die neue Variable gender aufrufen erhalten wird der Datenvektor mit den entsprechenden labels ausgegeben und zusätzlich gibt R die möglichen labels an.\nWenn wir den Parameter levels nicht angegeben hätten, dann extrahiert factor() die eineindeutigen Werte selbst und führt die Abbidlung auf die labels entsprechend der Sortierung aus.\n\ngender &lt;- factor(c(0,0,1,1),\n                 labels = c('m','f'))\ngender\n\n[1] m m f f\nLevels: m f\n\nstr(gender)\n\n Factor w/ 2 levels \"m\",\"f\": 1 1 2 2\n\n\nDabei muss darauf geachtet werden, dass die Abbildung auch tatsächlich diejenige ist, die gewünscht ist.\n\ngender &lt;- factor(c(0,0,1,1),\n                 labels = c('f','m'))\ngender\n\n[1] f f m m\nLevels: f m\n\n\nDaher ist es fast immer sinnvoll labels und levels immer zusammen zu nehmen. Wenn die Parameter nicht angegeben werden, dann führt factor die Abbildung wiederum automatisch durch und für die labels werden die Datenwerte übernommen.\n\ngender &lt;- factor(c(0,0,1,1))\ngender\n\n[1] 0 0 1 1\nLevels: 0 1\n\n\n\n\n\n\n\n\nWarnung\n\n\n\nAchtung, die Variable gender sieht zwar aus wie ein numerischer Vektor, sie ist es aber nicht.\n\nis.numeric(gender)\n\n[1] FALSE\n\n\nIntern wird eine Faktorvariable von R zwar als ein numerischer Vektor abgelegt. Aber die “sichtbaren” Werte sind nun die Zeichenketten der labels, die daher auch angezeigt werden. Die interne numerische Repräsentation muss auch nicht mehr den ursprünglichen Datenwerten entsprechen.\n\nas.numeric(gender)\n\n[1] 1 1 2 2\n\n\nDie Datenwerte waren ursprünglich \\((0,1)\\) und sind jetzt auf \\((1,2)\\) abgebildet worden. Erinnert euch an die Eigenschaft von nominalen Variablen. Nominale Variablen sind einfach voneinander unterscheidbare Werte die jedoch in keiner Ordnung stehen.\n\n\nDie automatische Konvertierung von factor() funktioniert am intuitivsten mit Zeichenkettenvektoren.\n\ngender &lt;- factor(c('m','f','m','f'))\ngender\n\n[1] m f m f\nLevels: f m\n\nstr(gender)\n\n Factor w/ 2 levels \"f\",\"m\": 2 1 2 1\n\n\nfactor() ermittelt zunächst die eineindeutigen Werte und sortiert diese dann entsprechend des Typen. In diesem Fall wird die Zeichenkette alphabetisch sortiert. Dann erfolgt die Abbildung der Werte auf die labels. Dies führt in diesem Fall dazu, dass die Werte m intern den Wert \\(2\\) zugeordnet bekommen, obwohl der erste Wert in den Daten m ist. Diese Sortierung der Daten wird später noch einmal von Bedeutung werden.\nDie Abfolge der levels kann durch eine explizite Angabe der Reihenfolge selbst bestimmt werden.\n\ngender &lt;- factor(c('m','f','m','f'),\n                 levels = c('m','f'))\ngender\n\n[1] m f m f\nLevels: m f\n\nstr(gender)\n\n Factor w/ 2 levels \"m\",\"f\": 1 2 1 2\n\ngender &lt;- factor(c('m','f','m','f'),\n                 levels = c('f','m'))\ngender\n\n[1] m f m f\nLevels: f m\n\nstr(gender)\n\n Factor w/ 2 levels \"f\",\"m\": 2 1 2 1\n\n\n\n\n\n\n\n\nTipp\n\n\n\nIm package forcats sind eine Reihe von Funktionen hinterlegt, mit denen die Eigenschaften von factor-Variablen einfach manipuliert werden können. Zum Beispiel, wenn die Reihenfolge von Faktorstufen geändert werden soll kann die Funktion fct_relevel() verwendet.\n\n\n[1] m f m f\nLevels: f m\n\n\n[1] m f m f\nLevels: m f\n\n\nSchaut euch die ausführliche Dokumentation der Funktionen und die Beispiel an, wenn ihr auf Probleme mit factor-Variablen stoßt.\n\n\n\n\n\n\n\n\nTipp\n\n\n\nViele Funktionen in R, wie z.B. lm(), transformieren Vektoren mit Zeichenketten automatisch in einen factor() um. Wird in lm() in der Formel beispielsweise y ~ gender benutzt und gender ist eine Datenspalte die aus den Zeichenketten c('m','m','f','f') besteht, dann ruft lm() intern die Funktion factor() für diese Daten auf und führt dann die Berechnung mit dem Faktor durch.\nDies erleichtert natürlich oft den Umgang mit den Daten, hat aber den Nachteil das immer klar sein muss, dass die automatische Konvertierung auch tatsächlich diejenige ist, dich auch gewünscht ist."
  },
  {
    "objectID": "mlm_dummy_coding.html#vergleich-von-zwei-gruppen-continued",
    "href": "mlm_dummy_coding.html#vergleich-von-zwei-gruppen-continued",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.3 Vergleich von zwei Gruppen (continued)",
    "text": "18.3 Vergleich von zwei Gruppen (continued)\nKommen wir zurück zum Körpergrößenvergleich. Normalerweise würden wir die Unterschiede zwischen den beiden Gruppen mit einem t-Test für unabhängige Stichproben untersuchen. In R können wird dies mit der t.test()-Funktion durchführen.\n\nt.test(cm ~ gender, data=height, var.equal=T)\n\n\n    Two Sample t-test\n\ndata:  cm by gender\nt = -4.5683, df = 58, p-value = 2.617e-05\nalternative hypothesis: true difference in means between group f and group m is not equal to 0\n95 percent confidence interval:\n -15.45403  -6.03713\nsample estimates:\nmean in group f mean in group m \n       168.7834        179.5290 \n\n\nWenig überraschend finden wir ein statisch signifikantes Ergebnis."
  },
  {
    "objectID": "mlm_dummy_coding.html#modellformulierung-beim-t-test-n_w-n_m",
    "href": "mlm_dummy_coding.html#modellformulierung-beim-t-test-n_w-n_m",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.4 Modellformulierung beim t-Test \\((n_w = n_m)\\)",
    "text": "18.4 Modellformulierung beim t-Test \\((n_w = n_m)\\)\n\\[\\begin{align*}\nY_{if} &= \\mu_{f} + \\epsilon_{if}, \\quad \\epsilon_{if} \\sim \\mathcal{N}(0,\\sigma^2) \\\\\nY_{im} &= \\mu_{m} + \\epsilon_{im}, \\quad \\epsilon_{im} \\sim \\mathcal{N}(0,\\sigma^2)\n\\end{align*}\\]\n\n18.4.1 Hypothesen\n\\[\\begin{align*}\nH_0&: \\delta = 0 \\\\\nH_1&: \\delta \\neq 0\n\\end{align*}\\]\n\n\n18.4.2 Teststatistik\n\\[\nt = \\frac{\\bar{y}_m - \\bar{y}_w}{\\sqrt{\\frac{s_m^2 + s_w^2}{2}}\\sqrt{\\frac{2}{n}}}\n\\]\n\n\n18.4.3 Referenzverteilung\n\\[\nt \\sim t_{df=2n-2}\n\\]\n\n\n\n\n\nt-Verteilung mit \\(df=58\\)"
  },
  {
    "objectID": "mlm_dummy_coding.html#kann-ich-aus-dem-t-test-ein-lineares-modell-machen",
    "href": "mlm_dummy_coding.html#kann-ich-aus-dem-t-test-ein-lineares-modell-machen",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.5 Kann ich aus dem t-Test ein lineares Modell machen?",
    "text": "18.5 Kann ich aus dem t-Test ein lineares Modell machen?\n\n18.5.1 t-Test\n\\[\\begin{align*}\nY_{if} &= \\mu_{f} + \\epsilon_{if}, \\quad \\epsilon_{if} \\sim \\mathcal{N}(0,\\sigma^2) \\\\\nY_{im} &= \\mu_{m} + \\epsilon_{im}, \\quad \\epsilon_{im} \\sim \\mathcal{N}(0,\\sigma^2) \\\\\nt &= \\frac{\\bar{y}_m - \\bar{y}_w}{\\sqrt{\\frac{s_m^2 + s_w^2}{2}}\\sqrt{\\frac{2}{n}}} \\\\\nt &\\sim t_{df=2n-2}\n\\end{align*}\\]\n\n\n18.5.2 Lineares Modell\n\\[\\begin{align*}\nY_i &= \\beta_0 + \\beta_1 \\times x_i + \\epsilon_i \\\\\n\\Delta_m &= \\mu_m - \\mu_f \\\\\nY_i &= \\beta_0 + \\beta_1 \\times x_{??} + \\epsilon_i \\\\\nY_i &= \\mu_f + \\Delta_{m} \\times x_{??} + \\epsilon_i\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#dummy--oder-indikatorkodierung",
    "href": "mlm_dummy_coding.html#dummy--oder-indikatorkodierung",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.6 Dummy- oder Indikatorkodierung",
    "text": "18.6 Dummy- oder Indikatorkodierung\n\\[\\begin{align*}\nY_i &= \\mu_f + \\Delta_{m} \\times x_{1i} + \\epsilon_i \\\\\n\\Delta_m &= \\mu_m - \\mu_f \\\\\nx_1 &=\n\\begin{cases}\n0\\text{ wenn weiblich}\\\\\n1\\text{ wenn männlich}\n\\end{cases}\n\\end{align*}\\]\nFür eine nominale Variable wird eine Indikatorvariablen (Dummyvariable) definiert. Über diese Indikatorvariable kann die Zugehörigkeit eines Messwerts \\(Y_i\\) zu einer Faktorstufe \\(k\\) bestimmt werden. Eine Faktorstufe ist dabei immer die Referenzstufe bei der die Indikatorvariable gleich \\(0\\) ist."
  },
  {
    "objectID": "mlm_dummy_coding.html#einfach-mal-stumpf-in-lm-eingeben",
    "href": "mlm_dummy_coding.html#einfach-mal-stumpf-in-lm-eingeben",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.7 Einfach mal stumpf in lm() eingeben",
    "text": "18.7 Einfach mal stumpf in lm() eingeben\n\nmod &lt;- lm(cm ~ gender, height)\n\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n168.783\n1.663\n101.477\n&lt;0.001\n\n\ngenderm\n10.746\n2.352\n4.568\n&lt;0.001\n\n\n\n\n\n1"
  },
  {
    "objectID": "mlm_dummy_coding.html#vergleich-der-konfidenzintervalle",
    "href": "mlm_dummy_coding.html#vergleich-der-konfidenzintervalle",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.8 Vergleich der Konfidenzintervalle",
    "text": "18.8 Vergleich der Konfidenzintervalle\n\n18.8.1 Lineares Modell\n\nconfint(mod)\n\n                2.5 %    97.5 %\n(Intercept) 165.45401 172.11276\ngenderm       6.03713  15.45403\n\n\n\n\n18.8.2 t-Test\n\nt.test(cm ~ gender,\n       data = height,\n       var.equal=T)$conf\n\n[1] -15.45403  -6.03713\nattr(,\"conf.level\")\n[1] 0.95\n\n\n2"
  },
  {
    "objectID": "mlm_dummy_coding.html#auf-welchen-werten-wird-ein-lineares-modell-gerechnet",
    "href": "mlm_dummy_coding.html#auf-welchen-werten-wird-ein-lineares-modell-gerechnet",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.9 Auf welchen Werten wird ein lineares Modell gerechnet???",
    "text": "18.9 Auf welchen Werten wird ein lineares Modell gerechnet???\n\n\n\nRepräsentation der Faktorvariablen\n\n\ncm\ngender\n\\(x_1\\)\n\n\n\n\n174.40\nm\n1\n\n\n177.70\nm\n1\n\n\n195.59\nm\n1\n\n\n160.05\nf\n0\n\n\n164.92\nf\n0\n\n\n154.35\nf\n0"
  },
  {
    "objectID": "mlm_dummy_coding.html#residuen",
    "href": "mlm_dummy_coding.html#residuen",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.10 Residuen",
    "text": "18.10 Residuen\n\n\n\n\n\nResiduen"
  },
  {
    "objectID": "mlm_dummy_coding.html#wens-interessiert---t-wert",
    "href": "mlm_dummy_coding.html#wens-interessiert---t-wert",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.11 Wen’s interessiert - t-Wert",
    "text": "18.11 Wen’s interessiert - t-Wert\nSeien beide Gruppen gleich groß (\\(n\\)) mit \\(N = n_m + n_w = 2 \\times n\\). Der t-Wert für \\(\\beta_1\\) berechnet sich aus \\(t = \\frac{b_1}{s_b}\\) mit:\n\\[\ns_b = \\sqrt{\\frac{\\sum_{i=1}^N (y_i - \\bar{y})^2}{N-2}\\frac{1}{\\sum_{i=1}^N(x_i-\\bar{x})^2}}\n\\] Dadurch, das die \\(x_i\\) entweder gleich \\(0\\) oder \\(1\\) sind, ist \\(\\bar{x}=0.5\\) und die Abweichungsquadrate im zweiten Term sind alle gleich \\(\\frac{1}{4}\\).\n\\[\n\\sum_{i=1}^N(x_i - \\bar{x})^2=\\sum_{i=1}^N\\left(x_i - \\frac{1}{2}\\right)^2 = \\sum_{i=1}^N\\frac{1}{4}=\\frac{N}{4}=\\frac{2n}{4}=\\frac{n}{2}\n\\]\nDer ersten Term kann mit etwas Algebra und der Definition für die Stichprobenvarianz \\(s^2\\) auf die gewünschte Form gebracht werden.\n\\[\n\\frac{\\sum_{i=1}^N(y_i-\\hat{y})^2}{N-2}=\\frac{\\sum_{i=1}^n(\\overbrace{y_{im} - \\bar{y}_m}^{Männer})^2+\\sum_{i=1}^n(\\overbrace{y_{iw}-\\bar{y}_w}^{Frauen})^2}{2(n-1)}=\\frac{(n-1)s_m^2+(n-1)s_w^2}{2(n-1)}=\\frac{s_m^2+s_w^2}{2}\n\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#wens-interessiert---beta_1-mu_w---mu_m",
    "href": "mlm_dummy_coding.html#wens-interessiert---beta_1-mu_w---mu_m",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.12 Wen’s interessiert - \\(\\beta_1 = \\mu_w - \\mu_m\\)",
    "text": "18.12 Wen’s interessiert - \\(\\beta_1 = \\mu_w - \\mu_m\\)\nMit \\(s_x^2 = \\frac{N\\frac{1}{4}}{N-1} = \\frac{N}{4(N-1)}\\) \\[\\begin{align*}\n    b_1 &= \\frac{cov(x,y)}{s_x^2} \\\\\n    &= \\frac{\\sum_{i=1}^N(y_i - \\bar{y})(x_i - \\bar{x})}{N-1} \\frac{4(N-1)}{N} \\\\\n    &= 4\\frac{\\sum_{i=1}^n(y_{im}-\\bar{y})\\frac{-1}{2}+\\sum(y_{iw}-\\bar{y})\\frac{1}{2}}{N} \\\\\n    &= \\frac{4}{2}\\frac{\\sum_{i=1}^n(y_{iw}-\\bar{y}) - \\sum_{i=1}^n(y_{im}-\\bar{y})}{2n} \\\\\n    &= \\frac{\\sum_{i=1}^n y_{iw}}{n} - \\frac{n\\bar{y}}{n} - \\frac{\\sum_{i=1}^n y_{im}}{n} + \\frac{n\\bar{y}}{n} \\\\\n    &= \\bar{y}_w - \\bar{y}_m = \\Delta\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#wens-interessiert---beta_0-mu_m",
    "href": "mlm_dummy_coding.html#wens-interessiert---beta_0-mu_m",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.13 Wen’s interessiert - \\(\\beta_0 = \\mu_m\\)",
    "text": "18.13 Wen’s interessiert - \\(\\beta_0 = \\mu_m\\)\nMit \\(b_1 = \\Delta = \\bar{y}_w - \\bar{y}_m\\): \\[\\begin{align*}\nb_0 &= \\bar{y} - \\Delta \\times \\bar{x} \\\\\n&= \\frac{\\sum_{i=1}^N y_i}{N} - \\Delta \\times \\frac{1}{2} \\\\\n&= \\frac{\\sum_{i=1}^n y_{im} + \\sum_{i=1}^n y_{iw}}{2n} - \\frac{1}{2}(\\bar{y}_w - \\bar{y}_m)  \\\\\n&= \\frac{1}{2}\\frac{\\sum_{i=1}^ny_{im}}{n} + \\frac{1}{2}\\frac{\\sum_{i=1}^ny_{iw}}{n} - \\frac{1}{2}\\bar{y}_w + \\frac{1}{2}\\bar{y}_m \\\\\n&= \\frac{1}{2}\\bar{y}_m + \\frac{1}{2}\\bar{y}_w - \\frac{1}{2}\\bar{y}_w + \\frac{1}{2}\\bar{y}_m \\\\\n&= \\bar{y}_m\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#können-auch-mehr-als-zwei-stufen-verwendet-werden",
    "href": "mlm_dummy_coding.html#können-auch-mehr-als-zwei-stufen-verwendet-werden",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.14 Können auch mehr als zwei Stufen verwendet werden?",
    "text": "18.14 Können auch mehr als zwei Stufen verwendet werden?\n\n\n\n\n\nEin Reaktionszeitexperiment mit vier Stufen A, B, C und D"
  },
  {
    "objectID": "mlm_dummy_coding.html#deskriptive-daten",
    "href": "mlm_dummy_coding.html#deskriptive-daten",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.15 Deskriptive Daten",
    "text": "18.15 Deskriptive Daten"
  },
  {
    "objectID": "mlm_dummy_coding.html#reaktionszeitexperiment-als-lineares-modell",
    "href": "mlm_dummy_coding.html#reaktionszeitexperiment-als-lineares-modell",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.16 Reaktionszeitexperiment als lineares Modell",
    "text": "18.16 Reaktionszeitexperiment als lineares Modell\n\n18.16.1 Modell\n\\[\ny_i = \\mu_A + \\Delta_{B-A} x_1 + \\Delta_{C-A} x_2 + \\Delta_{D-A} x_3 + \\epsilon_i\n\\]\n\n\n18.16.2 Dummyvariablen"
  },
  {
    "objectID": "mlm_dummy_coding.html#nochmal-allgemeiner",
    "href": "mlm_dummy_coding.html#nochmal-allgemeiner",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.17 Nochmal allgemeiner",
    "text": "18.17 Nochmal allgemeiner\nMit \\(K\\) Faktorstufen werden (K-1) Dummyvariablen \\(x_1, x_2, \\ldots, x_{K-1}\\) benötigt. Eine Stufe wird als Referenz definiert. Die \\(x_1\\) bis \\(x_{K-1}\\) kodieren die Abweichungen der anderen Stufen von dieser Stufe.3"
  },
  {
    "objectID": "mlm_dummy_coding.html#reaktionszeitexperiment-mit-lm",
    "href": "mlm_dummy_coding.html#reaktionszeitexperiment-mit-lm",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.18 Reaktionszeitexperiment mit lm()",
    "text": "18.18 Reaktionszeitexperiment mit lm()\n\nmod &lt;- lm(rt ~ group, data)\n\n\n\n\nModellfit\n\n\n\n$\\hat{\\beta}$\n$s_e$\nt\np\n\n\n\n\n(Intercept)\n509.526\n10.235\n49.784\n&lt;0.001\n\n\ngroupB\n90.150\n14.474\n6.228\n&lt;0.001\n\n\ngroupC\n197.414\n14.474\n13.639\n&lt;0.001\n\n\ngroupD\n295.561\n14.474\n20.420\n&lt;0.001"
  },
  {
    "objectID": "mlm_dummy_coding.html#ausblick",
    "href": "mlm_dummy_coding.html#ausblick",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.19 Ausblick",
    "text": "18.19 Ausblick\n\nanova(mod)\n\n\n\n\nANOVA-Tabelle\n\n\n\nDf\nSSQ\nMSQ\nF\np\n\n\n\n\ngroup\n3\n988935.1\n329645.04\n157.35\n&lt;0.001\n\n\nResiduals\n76\n159221.0\n2095.01"
  },
  {
    "objectID": "mlm_dummy_coding.html#kombination-von-kontinuierlichen-und-nominalen-variablen",
    "href": "mlm_dummy_coding.html#kombination-von-kontinuierlichen-und-nominalen-variablen",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.20 Kombination von kontinuierlichen und nominalen Variablen",
    "text": "18.20 Kombination von kontinuierlichen und nominalen Variablen\n\n\n\n\n\nHypothetische Leistungsentwicklung in Abhängigkeit vom Alter und Gender"
  },
  {
    "objectID": "mlm_dummy_coding.html#modellansatz",
    "href": "mlm_dummy_coding.html#modellansatz",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.21 Modellansatz",
    "text": "18.21 Modellansatz\n\nAus gender (K = 2) wird eine Dummyvariable\nFrauen werden (zufällig) als Referenz genommen\n\n\\[\\begin{align*}\nY_i &= \\beta_{ta = 0,x_1=0} + \\Delta_m \\times x_1 + \\beta_{ta} \\times ta + \\epsilon_i \\\\\nx_1 &=\n\\begin{cases}\n0\\text{ wenn weiblich}\\\\\n1\\text{ wenn männlich}\n\\end{cases} \\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#modellieren-mit-lm",
    "href": "mlm_dummy_coding.html#modellieren-mit-lm",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.22 Modellieren mit lm()",
    "text": "18.22 Modellieren mit lm()\n\nmod &lt;- lm(perf ~ gender_f + ta, lew)\n\n\n\n\nModellfit\n\n\n\n$\\hat{\\beta}$\n$s_e$\n\n\n\n\n(Intercept)\n41.181\n1.083\n\n\ngender\\_fm\n-10.877\n0.805\n\n\nta\n1.927\n0.145\n\n\n$\\hat{\\sigma}$\n2.845"
  },
  {
    "objectID": "mlm_dummy_coding.html#die-resultierenden-graden",
    "href": "mlm_dummy_coding.html#die-resultierenden-graden",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.23 Die resultierenden Graden",
    "text": "18.23 Die resultierenden Graden\n\n\n\n\n\nLeistungsentwicklung in Abhängigkeit vom Alter und Gender"
  },
  {
    "objectID": "mlm_dummy_coding.html#interaktion-zwischen-kontinuierlichen-und-nominalen-variablen",
    "href": "mlm_dummy_coding.html#interaktion-zwischen-kontinuierlichen-und-nominalen-variablen",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.24 Interaktion zwischen kontinuierlichen und nominalen Variablen",
    "text": "18.24 Interaktion zwischen kontinuierlichen und nominalen Variablen\n\n\n\n\n\nLeistungsentwicklung in Abhängigkeit vom Alter und Gender"
  },
  {
    "objectID": "mlm_dummy_coding.html#ansatz-für-ein-interaktionsmodell",
    "href": "mlm_dummy_coding.html#ansatz-für-ein-interaktionsmodell",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.25 Ansatz für ein Interaktionsmodell",
    "text": "18.25 Ansatz für ein Interaktionsmodell\nDas vorhergehendes Modell wird um einen Interaktionsterm erweitert.\n\\[\ny_i = \\beta_{ta=0,x_1=0} + \\Delta_m \\times x_1 + \\beta_{ta} \\times ta + \\beta_{ta \\times gender} \\times x_1 \\times ta + \\epsilon_i\n\\]"
  },
  {
    "objectID": "mlm_dummy_coding.html#interaktionsmodell-mit-lm",
    "href": "mlm_dummy_coding.html#interaktionsmodell-mit-lm",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.26 Interaktionsmodell mit lm()",
    "text": "18.26 Interaktionsmodell mit lm()\n\nmod &lt;- lm(perf ~ gender_f * ta, lew)\n\n\n\n\nModellfit\n\n\n\n$\\hat{\\beta}$\n$s_e$\n\n\n\n\n(Intercept)\n31.354\n1.370\n\n\ngender\\_fm\n8.575\n2.010\n\n\nta\n1.763\n0.195\n\n\ngender\\_fm:ta\n2.362\n0.290\n\n\n$\\hat{\\sigma}$\n2.828"
  },
  {
    "objectID": "mlm_dummy_coding.html#regressionsgeraden",
    "href": "mlm_dummy_coding.html#regressionsgeraden",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.27 Regressionsgeraden",
    "text": "18.27 Regressionsgeraden\n\n\n\n\n\nLeistungsentwicklung in Abhängigkeit vom Alter und Gender"
  },
  {
    "objectID": "mlm_dummy_coding.html#zum-nacharbeiten",
    "href": "mlm_dummy_coding.html#zum-nacharbeiten",
    "title": "18  Integration von nominalen Variablen",
    "section": "18.28 Zum Nacharbeiten",
    "text": "18.28 Zum Nacharbeiten\nKutner u. a. (2005, p.313–319) \n\n\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, und William Li. 2005. Applied Linear Statistical Models. 5. Aufl. McGraw-Hill Irwin New York."
  },
  {
    "objectID": "mlm_dummy_coding.html#footnotes",
    "href": "mlm_dummy_coding.html#footnotes",
    "title": "18  Integration von nominalen Variablen",
    "section": "",
    "text": "R gibt die Faktorstufe nach dem Namen des Faktors an. Im Beispiel steht genderm für Stufe m im Faktor gender.↩︎\nMit t.test()$conf.int kann auf das berechnete Konfidenzintervall zugegriffen werden.↩︎\nDiese Art der Kodierung wird auch als treatment Kodierung bezeichnet.↩︎"
  },
  {
    "objectID": "mlm_hierarchies.html#einfaches-modell",
    "href": "mlm_hierarchies.html#einfaches-modell",
    "title": "19  Modellhierarchien",
    "section": "19.1 Einfaches Modell",
    "text": "19.1 Einfaches Modell\nWir beginnen mit einem einfachen Modell, das wiederum nur eine \\(x\\) und eine \\(y\\)-Variable hat.\n\nmod0 &lt;- lm(y ~ x, simple)\nsummary(mod0)\n\n\nCall:\nlm(formula = y ~ x, data = simple)\n\nResiduals:\n      1       2       3       4 \n-0.5817  0.9898 -0.2345 -0.1736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   1.8414     0.7008   2.628    0.119\nx             0.4574     0.3746   1.221    0.346\n\nResidual standard error: 0.8376 on 2 degrees of freedom\nMultiple R-squared:  0.4271,    Adjusted R-squared:  0.1406 \nF-statistic: 1.491 on 1 and 2 DF,  p-value: 0.3465\n\n\nD.h. hier ist jetzt zunächst einmal nichts Neues dazukommen. Schauen wir uns aber noch einmal genauer die Residuen, d.h. die Abweichungen von der Regressionsgeraden, an. In der Besprechung des Determinationskoeffizienten \\(R^2\\) haben wir schon Quadratsummen und deren Unterteilung kennengelernt. Dort hatten wird die Aufteilung der Varianz von \\(Y\\), bezeichnet als \\(SSTO\\), in die beiden Komponenten Regressionsvarianz \\(SSR\\) und Fehlervarianz \\(SSE\\) besprochen. Es gilt.\n\\[\\begin{equation}\nSSTO = SSR + SSE\n\\end{equation}\\]\nDie Fehlerquadratsumme SSE, die Summe der quadrierten Abweichungen zwischen dem beobachteten Wert \\(y_i\\) und dem vorhergesagten Wert \\(\\hat{y}_i\\) ist definiert mittels:\n\\[\\begin{equation}\nSSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\end{equation}\\]\nUm die Werte \\(\\hat{y}_i\\) berechnen zu können, benötigen wir unser Modell bzw. die Modellkoeffizienten. Das einfache Regressionsmodell hat zwei Parameter, die beiden Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\). Formalisierung wir die Parameteranzahl indem ihr ein Symbol spendieren, per Konvention meistens das Symbol \\(p\\). In unserem Fall ist gilt daher \\(p = 2\\). Die Anzahl der Parameter \\(p\\) ist verknüpft mit den sogenannten Freiheitsgraden \\(df\\) (degrees of freedom). Die Freiheitsgrade von \\(SSE\\) berechnen sich mittels der Formel \\(N-p\\), wobei \\(N\\) die Anzahl der Beobachtungen, der Datenpunkte ist.\n\\[\\begin{equation}\ndf_E := n - p\n\\end{equation}\\]\nDie Freiheitsgerade bestimmen die effektive Anzahl der Beobachtungen die zur Verfügung stehen um die Varianz \\(\\sigma^2\\) des Modells abzuschätzen. Dadurch, dass zwei Parameter anhand der Daten für das Modell bestimt werden, fallen zwei Datenpunkt als unabhängige Informationsquellen weg. Anders ausgedrückt, wenn ich die beiden Modellparameter, in unseren Fall \\(\\beta_0\\) und \\(\\beta_1\\) kenne, dann sind nur noch \\(N-2\\) Datenpunkt frei variierbar. Sobald ich die Werte von \\(N-2\\) Datenpunkten und die beiden Parameter kenne, kann ich die verbleibenden beiden Werte berechnen. Daher die Begriff der Freiheitsgrade.\nWir nun \\(SSE\\) durch die Anzahl der Freiheitsgerade teilen, dann lässt sich zeigen, das dieser Wert ein erwartungstreuer Schätzer für die Residualvarianz \\(\\sigma^2\\) unter der Verteilungsannahme \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) der Daten ist. Das Verhältnis von \\(SSE\\) zu \\(df\\) wird als Mean squared error (\\(MSE\\)) bezeichnet.\n\\[\\begin{equation}\nMSE = \\frac{SSE}{df_{E}} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p} = \\hat{\\sigma}^2\n\\end{equation}\\]\nIm ersten Moment erscheint diese Begründung etwas undurchsichtig, aber tatsächlich ist diese Formel schon eine alte Bekannte die uns in Form der Stichprobenvarianz \\(s^2\\) begegnet ist.\nWenn wir eine Stichprobe der Größe \\(N\\) mit Werten \\(y_i\\) haben, dann haben wir die Stichprobenvarianz mittels der Formel:\n\\[\\begin{equation}\n\\hat{\\sigma}^2 = s^2 = \\frac{1}{n-1}\\sum_{i=1}^2(y_i - \\bar{y})^2\n\\end{equation}\\]\nberechnet. Was bei dieser Formel schon immer etwas quer gesessen hat, ist der Nenner mit \\(N-1\\) anstatt einfach \\(N\\) wie wir es vom Mittelwert kennen. Aber, um die Varianz zu berechnen benötigen wir einen Parameter, den Mittelwert \\(\\hat{y}\\) den wir anhand der Daten berechnen. Dies führt dazu, dass nur \\(N-1\\) Werte frei variiert werden können. Sobald wir, neben dem Mittelwert \\(\\bar{y}\\), \\(N-1\\) Werte kennen, können den verbleibenden Wert \\(N\\)-ten Wert berechnen."
  },
  {
    "objectID": "mlm_hierarchies.html#genereller-linearer-modell-testansatz",
    "href": "mlm_hierarchies.html#genereller-linearer-modell-testansatz",
    "title": "19  Modellhierarchien",
    "section": "19.1 Genereller Linearer Modell Testansatz",
    "text": "19.1 Genereller Linearer Modell Testansatz\nWir beginnen mit einem einfachen Modell, das wiederum nur eine \\(x\\) und eine \\(y\\)-Variable hat.\n\nmod0 &lt;- lm(y ~ x, simple)\nsummary(mod0)\n\n\nCall:\nlm(formula = y ~ x, data = simple)\n\nResiduals:\n      1       2       3       4 \n-0.5817  0.9898 -0.2345 -0.1736 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   1.8414     0.7008   2.628    0.119\nx             0.4574     0.3746   1.221    0.346\n\nResidual standard error: 0.8376 on 2 degrees of freedom\nMultiple R-squared:  0.4271,    Adjusted R-squared:  0.1406 \nF-statistic: 1.491 on 1 and 2 DF,  p-value: 0.3465\n\n\nD.h. hier ist jetzt zunächst einmal nichts Neues dazukommen. Schauen wir uns aber noch einmal genauer die Residuen, d.h. die Abweichungen von der Regressionsgeraden, an. In der Besprechung des Determinationskoeffizienten \\(R^2\\) haben wir schon Quadratsummen und deren Unterteilung kennengelernt. Dort hatten wird die Aufteilung der Varianz von \\(Y\\), bezeichnet als \\(SSTO\\), in die beiden Komponenten Regressionsvarianz \\(SSR\\) und Fehlervarianz \\(SSE\\) besprochen. Es gilt.\n\\[\\begin{equation}\nSSTO = SSR + SSE\n\\end{equation}\\]\nDie Fehlerquadratsumme SSE, die Summe der quadrierten Abweichungen zwischen dem beobachteten Wert \\(y_i\\) und dem vorhergesagten Wert \\(\\hat{y}_i\\) ist definiert mittels:\n\\[\\begin{equation}\nSSE = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\end{equation}\\]\nUm die Werte \\(\\hat{y}_i\\) berechnen zu können, benötigen wir unser Modell bzw. die Modellkoeffizienten. Das einfache Regressionsmodell hat zwei Parameter, die beiden Koeffizienten \\(\\beta_0\\) und \\(\\beta_1\\). Formalisierung wir die Parameteranzahl indem ihr ein Symbol spendieren, per Konvention meistens das Symbol \\(p\\). In unserem Fall ist gilt daher \\(p = 2\\). Die Anzahl der Parameter \\(p\\) ist verknüpft mit den sogenannten Freiheitsgraden \\(df\\) (degrees of freedom). Die Freiheitsgrade von \\(SSE\\) berechnen sich mittels der Formel \\(N-p\\), wobei \\(N\\) die Anzahl der Beobachtungen, der Datenpunkte ist.\n\\[\\begin{equation}\ndf_E := n - p\n\\end{equation}\\]\nDie Freiheitsgerade bestimmen die effektive Anzahl der Beobachtungen die zur Verfügung stehen um die Varianz \\(\\sigma^2\\) des Modells abzuschätzen. Dadurch, dass zwei Parameter anhand der Daten für das Modell bestimt werden, fallen zwei Datenpunkt als unabhängige Informationsquellen weg. Anders ausgedrückt, wenn ich die beiden Modellparameter, in unseren Fall \\(\\beta_0\\) und \\(\\beta_1\\) kenne, dann sind nur noch \\(N-2\\) Datenpunkt frei variierbar. Sobald ich die Werte von \\(N-2\\) Datenpunkten und die beiden Parameter kenne, kann ich die verbleibenden beiden Werte berechnen. Daher die Begriff der Freiheitsgrade.\nWir nun \\(SSE\\) durch die Anzahl der Freiheitsgerade teilen, dann lässt sich zeigen, das dieser Wert ein erwartungstreuer Schätzer für die Residualvarianz \\(\\sigma^2\\) unter der Verteilungsannahme \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) der Daten ist. Das Verhältnis von \\(SSE\\) zu \\(df\\) wird als Mean squared error (\\(MSE\\)) bezeichnet.\n\\[\\begin{equation}\nMSE = \\frac{SSE}{df_{E}} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p} = \\hat{\\sigma}^2\n\\end{equation}\\]\nIm ersten Moment erscheint diese Begründung etwas undurchsichtig, aber tatsächlich ist diese Formel schon eine alte Bekannte die uns in Form der Stichprobenvarianz \\(s^2\\) begegnet ist.\nWenn wir eine Stichprobe der Größe \\(N\\) mit Werten \\(y_i\\) haben, dann haben wir die Stichprobenvarianz mittels der Formel:\n\\[\\begin{equation}\n\\hat{\\sigma}^2 = s^2 = \\frac{1}{n-1}\\sum_{i=1}^2(y_i - \\bar{y})^2\n\\end{equation}\\]\nberechnet. Was bei dieser Formel schon immer etwas quer gesessen hat, ist der Nenner mit \\(N-1\\) anstatt einfach \\(N\\) wie wir es vom Mittelwert kennen. Aber, um die Varianz zu berechnen benötigen wir einen Parameter, den Mittelwert \\(\\hat{y}\\) den wir anhand der Daten berechnen. Dies führt dazu, dass nur \\(N-1\\) Werte frei variiert werden können. Sobald wir, neben dem Mittelwert \\(\\bar{y}\\), \\(N-1\\) Werte kennen, können den verbleibenden Wert \\(N\\)-ten Wert berechnen.\nNach dieser Wiederholung sind wir nun in der Lage eine neue Teststatistik entwickeln. Das Ziel ist dabei eine Metrik zu entwickeln mit der wir abschätzen können, ob die Hinzunahme von Modellparametern zu einer Verbesserung der Modellvorhersage führt. Die Verbesserung werden wir mittels der Reduktion der Fehlervarianz \\(SSE\\) abschätzen. Wir werden in diesem Zusammenhang sehen, dass Modelle in eine Hierarchie zueinander in Beziehung gesetzt werden können bei einfachere Modellen als Teilmodelle von komplexeren Modellen interpretiert werden können.\nWir müssen dazu zunächst die Unterscheidung in ein volles Modell (\\(F\\)ull model) und ein reduziertes Modell (\\(R\\)educed model) verstehen. Beim Beispiel der einfachen lineare Regression ist das full model das uns schon bekannte:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\n\\]\nDie Residualvarianz \\(SSE(F)\\) berechnet sich wie oben wiederholt mittels:\n\\[\\begin{equation}\n\\textrm{SSE(F)}  = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n [y_i - (\\beta_0 + \\beta_1 x_i)]^2\n\\label{eq-mlm-hier-ssef}\n\\end{equation}\\]\nDa wir \\(p = 2\\) Modellparameter haben, hat das Modell \\(dfE(F) = n - 2\\) Freiheitsgerade. So weit ist bisher noch nichts Neues dazugekommen. Wir könnten uns jetzt die Frage stellen, ob wir tatsächlich den Modellparameter \\(\\beta_1\\) benötigen. Vielleicht zeigt die \\(x\\)-Variable gar keinen Zusammenhang mit der \\(y\\)-Variable und wir fitten nur Rauschen im Modell. Aus dieser Überlegung heraus, können wir jetzt ein reduziertes Modell formulieren bei dem der Parameter \\(\\beta_1\\) fehlt.\n\\[\nY_i = \\beta_0 + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\n\\]\nDie Residualvarianz SSE(R) berechnet sich jetzt mittels.\n\\[\n\\textrm{SSE(R)} = \\sum_{i=1}^n (y_i - \\beta_0)^2 = \\sum_{i=1}^n(y_i - \\bar{y})^2 = \\textrm{SSTO}\n\\]\nUns Modell hat jetzt nur noch einen Paramter \\(\\beta_0\\) gilt nun \\(p = 1\\) und entsprechend \\(dfE(R) = n - 1\\). Der Modellparameter \\(\\beta_0\\) ist jetzt also nichts anderes als der Mittelwert \\(\\bar{y}\\) der beobachteten \\(y\\)-Werte.\nMit etwas Algebra lässt sich zeigen, dass im Allgemeinen \\(SSE(F) \\leq SSE(R)\\) gilt. Dieser Zusammenhang lässt sich auch heuristisch herleiten. Wenn es keinen Zusammenhang zwischen \\(x\\) und \\(y\\) gibt, dann wird der \\(\\beta_1\\) im full model nahezu \\(0\\) sein und Formel \\(\\eqref{eq-mlm-hier-ssef}\\) wird zu SSE(R). Im realistischen Fall wird aber, selbst wenn kein Zusammenhang besteht, ein Teil des Rauschens mittels \\(\\beta_1\\) gefittet, so dass der beschriebene Zusammenhang zwischen SSE(F) und SSE(R) entsteht.\n\n19.1.1 Reduziertes Modell und Stichprobenvarianz(*)\nZwischen der Residualvarianz im reduzierten Modell SSE(R), dem optimalen Modellparameter \\(\\beta_0\\) und der Stichprobenvarianz besteht ein enger Zusammenhang bzw. Identität wie sich anhand der folgenden Herleitung sehen lässt. Wir wollen den Modellparameter unter der Minimierung der Summer der Quadrate der Abweichung, sprich SSE, ermitteln.\n\\[\\begin{align*}\nSSE &= \\sum_{i=1}^n(y_i - \\beta_0)^2 \\\\\n&= \\sum_{i=1}^n (y_i^2 - 2y_i\\beta_0 + \\beta_0^2)\n\\end{align*}\\]\nWir wollen entsprechend \\(min[SSE]\\) bestimmen, wie wir das schon vorher immer beim Regressionmodell gemacht haben. Daher setzen wir den Term \\(=0\\) und leiten nach dem Modellparameter \\(\\beta_0\\) ab. Ein bisschen Algebra führt zu:\n\\[\\begin{align*}\n0 &= \\frac{\\mathrm{d}}{\\mathrm{d} \\beta_0}\\sum_{i=1}^n (y_i^2 - 2y_i\\beta_0 + \\beta_0^2) \\\\\n0 &= \\sum_{i=1}^n (-2y_i + 2 \\beta_0) = -2\\sum_{i=1}^n y_i + 2\\sum_{i=1}^n \\beta_0\\\\\nn\\beta_0 &= \\sum_{i=1}^n y_i \\\\\n\\beta_0 &= \\frac{\\sum_{i=1}^n y_i}{n}\n\\end{align*}\\]\nSomit ist Wert von \\(\\beta_0\\) der Abweichungen minimiert, der Mittelwert \\(\\bar{y}\\). Daraus folgt allerdings, das unsere Schätzer für die \\(\\sigma^2\\),\n\\[\\begin{equation*}\n\\hat{\\sigma}^2 = \\frac{SSE}{n-p} = \\frac{SSE(R)}{n-1} = \\sum_{i=1}^n (y_i - \\bar{y})^2 =  s^2\n\\end{equation*}\\]\neinfach nur unser bekannter Schätzer der Stichprobenvarianz \\(s^2\\) ist.\nKommen wir zurück zur Entwicklung unsere Metrik um das volle und das reduzierte Modell miteinander zu vergleichen. Gehen wir davon aus, das das reduzierte Modell ist korrekt. D.h. die Hinzunahme von \\(X\\) sollte keine Verbesserung des Modells nach sich ziehen. Konkret bedeutet dies, dass \\(SSE(R)\\) und \\(SSE(F)\\) in etwas gleich sind bzw. \\(SSE(F)\\) nur wenig besser ist als \\(SSE(R)\\). Wenn ich jetzt die Differenz zwischen diesen beiden Werte nehme, dann sollte der Wert eher klein sein.\n\\[\\begin{equation}\n\\textrm{SSE(R)} - \\textrm{SSE(F)}\n\\label{eq-mlm-hier-div}\n\\end{equation}\\]\nBeide Modelle haben einen gleich guten fitten die Daten in etwas gleich gut. Das volle Modell etwas besser, das es durch den zusätzlichen Parameter etwas flexibler als das reduzierte Modell ist.\nGehen wir von nun von der entgegengesetzen Annahme aus. Das reduzierte Modell ist falsch und wir benötigen die Variable \\(X\\) um die Varianz in \\(Y\\) aufzuklären. In diesem Fall sollte die Differenz \\(\\eqref{eq-mlm-hier-div}\\) einen deutlich größeren Wert annehmen, da das reduzierte Modell denjenigen Teil der Varianz von \\(Y\\) nicht aufklären kann, der durch \\(X\\) entsteht. Im vollem Modell kann diese Varianz durch den zusätzlichen Modellparameter \\(\\beta_1\\) dagegen erklärt werden.\nZusammengefasst haben wir heuristisch eine Metrik hergeleitet, die uns erlaubt verschiedene Modell miteinander zu vergleichen. Wenn das reduzierte, einfachere Modell ausreicht um die Daten zu fitten, dann wird die Differenz \\(\\eqref{eq-mlm-hier-div}\\) eher klein ausfallen. Wenn die zusätzlichen Parameter im vollem Modell benötigt werden, dann wird der Unterschied \\(\\eqref{eq-mlm-hier-div}\\) eher groß.\nWir bringen jetzt noch eine zusätzliche Parameter in unseren Modellvergleich ein. Die Bedeutsamkeit des Unterschieds zwischen den beiden Modellen ist auch noch abhängig davon in wie vielen Parameter sich die beiden Modelle voneinander unterscheiden. Wenn im vollen Modell \\(p = 10\\) Parameter sind und im reduzierten Modell eben nur \\(p = 1\\) Parameter ist, dann ist ein gegebener Unterschied in den \\(SSE\\)s zwischen den Modell anders zu bewerten, als wenn im vollem Modell \\(p = 2\\) Parameter geschätzt werden. Bei gleichem Unterschied zwischen den Modell ist der Unterschied im ersteren Fall weniger Bedeutsam im Vergleich zum letzteren Fall. Daher wird der Unterschied noch anhand des Unterschieds in der Anzahl der Parameter kalibriert. Anders formuliert, schauen wir uns die Veränderung in der Varianzverkleinerung pro Freiheitsgrad an. In unserem einfachen Fall passiert da nichts, da der Unterschied in der Parameteranzahl \\(= 1\\) ist. Wir werden aber später sehen, dass auch Modelle mit größeren Unterschieden in der Parameteranzahl \\(p\\) miteinander verglichen werden können.\nMit \\(p_F\\) = Anzahl der Parameter im vollen Modell, \\(p_R\\) = Anzahl der Parameter im reduzierten Modell, $df_{E(F)} = gilt:\n\\[\np_{F} - p_{R} = p_{F} - p_{R} + N - N = N - p_{R} - (N - p_{F}) = df_{E(R)} - df_{E(F)}\n\\]\nSomit schreiben wir den Unterschied zwischen den beiden Modellen folgendermaßen auf und gegen dem Term auch noch einen Namen \\(MS_{\\textrm{test}}\\) für mean squared test.\n\\[\\begin{equation}\nMS_{\\textrm{test}} = \\frac{\\textrm{SSE(R)} - \\textrm{SSE(F)}}{df_{E(R)} - df_{E(F)}}\n\\label{eq-mlm-hier-mstest}\n\\end{equation}\\]\nUnter der Annahme, das das reduzierte Modell korrekt ist, lässt sich zeigen, dass \\(MS_{\\textrm{test}}\\) ein Schätzer für die Varianz \\(\\sigma^2\\) im Rahmen der üblichen Modellannahmen \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) ist. D.h.\n\\[\\begin{equation*}\nMS_{\\textrm{test}} = \\hat{\\sigma}^2\n\\end{equation*}\\]\nZusätzlich, wenn das reduzierte Modell korrekt ist, dann ist auch das volle Modell korrekt, hat halt einen Parameter zu viel aber der sollte wie oben ausgeführt in der Nähe von \\(0\\) sein. Daher ist auch wie schon vorher gezeigt \\(MSE(F)\\) ein Schätzer für die Varianz \\(\\sigma^2\\) im Rahmen der üblichen Modellannahmen \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) ist.\n\\[\\begin{equation}\nMS_{E(F)} = \\frac{\\textrm{SSE(F)}}{df_{E(F)}} = \\hat{\\sigma}^2\n\\end{equation}\\]\nEine Sache fehlt uns noch um die Größe von \\(MS_{\\textrm{test}}\\) einordnen zu können. Da wir es mit Varianzen zu tun haben, können wir die Größe der Quadratsummen verändern indem wir die Einheiten der abhängigen Variablen \\(Y\\) verändern. Würden wir z.B. von \\([m]\\) auf \\([cm]\\) gehen, da würde sich der Unterschied in Formel \\(\\eqref{eq-mlm-hier-mstest}\\) um den Faktor \\(10\\times 10=100\\) vergrößern ohne das wirklich eine Veränderung in den Modellen stattgefunden hat. Daher kalibrieren wir \\(MS_{\\textrm{test}}\\) indem wir den Term durch \\(MS_{E(F)}\\) teilen. Damit fallen alle Problem mit Veränderungen durch Änderungen in den Einheiten weg (siehe auch Maxwell, Delaney, und Kelley 2004, p.75).\n\\[\\begin{equation}\n\\frac{MS_{\\textrm{test}}}{MS_{E(F)}} = \\frac{\\frac{\\textrm{SSE(R)} - \\textrm{SSE(F)}}{df_{E(R)} - df_{E(F)}}}{ \\frac{\\textrm{SSE(F)}}{df_{E(F)}}}\n\\label{eq-mlm-hier-Ftest}\n\\end{equation}\\]\nZusätzlich hat dies auch noch den Vorteil, dass die entstehende Metrik unter der \\(H_0\\) das das reduzierte Modell korrekt ist, einer uns bekannten theoretische Verteilung, nämlich der \\(F\\)-Verteilung mit \\(df_{E(R)} - df_{E(F)}\\) und \\(df_{E(F)}\\) Freiheitsgeraden, folgt.\n\\[\nF = \\frac{MS_{\\textrm{test}}}{MS_{E(F)}}  \\sim F(df_{E(R)}-df_{E(F)},df_{E(F)})\n\\]\nZur Erinnerung sind in Abbildung 19.1 nochmal ein paar Beispiele für \\(F\\)-Verteilung mit verschiedenen Freiheitsgeraden abgebildet.\n\n\n\n\n\nAbbildung 19.1: Beispiele für die F-Verteilung mit verschiedenen Freiheitsgraden \\(df_1, df_2\\)\n\n\n\n\nSobald wir eine bekannte theoretische Verteilung unter einer \\(H_0\\) haben, können wir unser Hypothestinstrumentarium auf die Verteilung los lassen und entsprechend einen kritische Bereich mit \\(\\alpha\\) bestimmen. In Abbildung 19.2 haben wir dies entsprechend getan. Somit wenn wenn der beobachtete \\(F\\)-Wert in den kritischen Bereich fällt, interpretieren wird das als Evidenz gegen die \\(H_0\\). Wir sind überrascht diesen Wert unter der \\(H_0\\) zu sehen und lehen die \\(H_0\\) das das einfachere Modell korrekt ist ab und werten dies als Evidenz dafür, das das komplexere, volle Modell die Daten besser abbildet und statistisch signifikant mehr Varianz der abhängigen Variable modellieren kann.\n\n\n\n\n\nAbbildung 19.2: F-Verteilung mit \\(df_1 = 5, df_2 = 10\\) und kritischem Wert bei \\(\\alpha=0.05\\)\n\n\n\n\n\n\n19.1.2 Zusammenfassung\nDurch den Vergleich von Modellen miteinander, sind wir jetzt in der Lage, die Verbesserung/Verschlechterung der Modellvorhersage statistisch zu überprüfen. Wenn der \\(F\\)-Test statistisch signifikant ist, dann werten wir dies als Evidenz dafür, das das volle Modell die Daten so viel besser modelliert, das wir dieses Modell dem reduzierten Modell vorziehen."
  },
  {
    "objectID": "mlm_hierarchies.html#link-reduziertes-modell-und-stichprobenvarianz",
    "href": "mlm_hierarchies.html#link-reduziertes-modell-und-stichprobenvarianz",
    "title": "19  Modellhierarchien",
    "section": "19.3 Link: Reduziertes Modell und Stichprobenvarianz(*)",
    "text": "19.3 Link: Reduziertes Modell und Stichprobenvarianz(*)\nZwischen der Residualvarianz im reduzierten Modell SSE(R) und der Stichprobenvarianz bestehht ein enger Zusammenhang bzw. Identität wie sich anhand er folgenden Herleitung sehen lässt.\n\\[\\begin{align*}\nSSE &= \\sum_{i=1}^n(y_i - \\beta_0)^2 = \\sum_{i=1}^n (y_i^2 - 2y_i\\beta_0 + \\beta_0^2) \\\\\n0 &= \\frac{\\mathrm{d}}{\\mathrm{d} \\beta_0}\\sum_{i=1}^n (y_i^2 - 2y_i\\beta_0 + \\beta_0^2) \\\\\n0 &= \\sum_{i=1}^n (-2y_i + 2 \\beta_0) = -2\\sum_{i=1}^n y_i + 2\\sum_{i=1}^n \\beta_0\\\\\nn\\beta_0 &= \\sum_{i=1}^n y_i \\\\\n\\beta_0 &= \\frac{\\sum_{i=1}^n y_i}{n} = \\bar{y} \\rightarrow \\frac{SSE}{n-1} = \\hat{\\sigma}^2 = s^2\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_hierarchies.html#genereller-linearer-modell-testansatz-1",
    "href": "mlm_hierarchies.html#genereller-linearer-modell-testansatz-1",
    "title": "19  Modellhierarchien",
    "section": "19.3 Genereller Linearer Modell Testansatz",
    "text": "19.3 Genereller Linearer Modell Testansatz\nKommen wir zurück zur Entwicklung unsere Metrik um das volle und das reduzierte Modell miteinander zu vergleichen. Gehen wir davon aus, das das reduzierte Modell ist korrekt. D.h. die Hinzunahme von \\(X\\) sollte keine Verbesserung des Modells nach sich ziehen. Konkret bedeutet dies, dass \\(SSE(R)\\) und \\(SSE(F)\\) in etwas gleich sind bzw. \\(SSE(F)\\) nur wenig besser ist als \\(SSE(R)\\). Wenn ich jetzt die Differenz zwischen diesen beiden Werte nehme, dann sollte der Wert eher klein sein.\n\\[\\begin{equation}\n\\textrm{SSE(R)} - \\textrm{SSE(F)}\n\\label{eq-mlm-hier-div}\n\\end{equation}\\]\nBeide Modelle haben einen gleich guten fitten die Daten in etwas gleich gut. Das volle Modell etwas besser, das es durch den zusätzlichen Parameter etwas flexibler als das reduzierte Modell ist.\nGehen wir von nun von der entgegengesetzen Annahme aus. Das reduzierte Modell ist falsch und wir benötigen die Variable \\(X\\) um die Varianz in \\(Y\\) aufzuklären. In diesem Fall sollte die Differenz \\(\\eqref{eq-mlm-hier-div}\\) einen deutlich größeren Wert annehmen, da das reduzierte Modell denjenigen Teil der Varianz von \\(Y\\) nicht aufklären kann, der durch \\(X\\) entsteht. Im vollem Modell kann diese Varianz durch den zusätzlichen Modellparameter \\(\\beta_1\\) dagegen erklärt werden.\nZusammengefasst haben wir heuristisch eine Metrik hergeleitet, die uns erlaubt verschiedene Modell miteinander zu vergleichen. Wenn das reduzierte, einfachere Modell ausreicht um die Daten zu fitten, dann wird die Differenz \\(\\eqref{eq-mlm-hier-div}\\) eher klein ausfallen. Wenn die zusätzlichen Parameter im vollem Modell benötigt werden, dann wird der Unterschied \\(\\eqref{eq-mlm-hier-div}\\) eher groß.\nWir bringen jetzt noch eine zusätzliche Parameter in unseren Modellvergleich ein. Die Bedeutsamkeit des Unterschieds zwischen den beiden Modellen ist auch noch abhängig davon in wie vielen Parameter sich die beiden Modelle voneinander unterscheiden. Wenn im vollen Modell \\(p = 10\\) Parameter sind und im reduzierten Modell eben nur \\(p = 1\\) Parameter ist, dann ist ein gegebener Unterschied in den \\(SSE\\)s zwischen den Modell anders zu bewerten, als wenn im vollem Modell \\(p = 2\\) Parameter geschätzt werden. Bei gleichem Unterschied zwischen den Modell ist der Unterschied im ersteren Fall weniger Bedeutsam im Vergleich zum letzteren Fall. Daher wird der Unterschied noch anhand des Unterschieds in der Anzahl der Parameter kalibriert. In unserem Fall passiert da nichts, da der Unterschied \\(= 1\\) ist. Wir werden aber später sehen, dass auch Modelle mit größeren Unterschieden miteinander verglichen werden können.\nMit \\(p_F\\) = Anzahl der Parameter im vollen Modell, \\(p_R\\) = Anzahl der Parameter im reduzierten Modell, $df_{E(F)} = gilt:\n\\[\np_{F} - p_{R} = p_{F} - p_{R} + N - N = N - p_{R} - (N - p_{F}) = df_{E(R)} - df_{E(F)}\n\\]\nSomit schreiben wir den Unterschied zwischen den beiden Modellen folgendermaßen auf und gegen dem Term auch noch einen Namen \\(MS_{\\textrm{test}}\\) für mean squared test.\n\\[\\begin{equation}\nMS_{\\textrm{test}} = \\frac{\\textrm{SSE(R)} - \\textrm{SSE(F)}}{df_{E(R)} - df_{E(F)}}\n\\label{eq-mlm-hier-mstest}\n\\end{equation}\\]\nUnter der Annahme, das das reduzierte Modell korrekt ist, lässt sich zeigen, dass \\(MS_{\\textrm{test}}\\) ein Schätzer für die Varianz \\(\\sigma^2\\) im Rahmen der üblichen Modellannahmen \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) ist. D.h.\n\\[\\begin{equation*}\nMS_{\\textrm{test}} = \\hat{\\sigma}^2\n\\end{equation*}\\]\nZusätzlich, wenn das reduzierte Modell korrekt ist, dann ist auch das volle Modell korrekt, hat halt einen Parameter zu viel aber der sollte wie oben ausgeführt in der Nähe von \\(0\\) sein. Daher ist auch wie schon vorher gezeigt \\(MSE(F)\\) ein Schätzer für die Varianz \\(\\sigma^2\\) im Rahmen der üblichen Modellannahmen \\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\) ist.\n\\[\\begin{equation}\nMS_{E(F)} = \\frac{\\textrm{SSE(F)}}{df_{E(F)}} = \\hat{\\sigma}^2\n\\end{equation}\\]\nEine Sache fehlt uns noch um die Größe von \\(MS_{\\textrm{test}}\\) einordnen zu können. Da wir es mit Varianzen zu tun haben, können wir die Größe der Quadratsummen verändern indem wir die Einheiten der abhängigen Variablen \\(Y\\) verändern. Würden wir z.B. von \\([m]\\) auf \\([cm]\\) gehen, da würde sich der Unterschied in Formel \\(\\eqref{eq-mlm-hier-mstest}\\) um den Faktor \\(10\\times 10=100\\) vergrößern ohne das wirklich eine Veränderung in den Modellen stattgefunden hat. Daher kalibrieren wir \\(MS_{\\textrm{test}}\\) indem wir den Term durch \\(MS_{E(F)}\\) teilen. Damit fallen alle Problem mit Veränderungen durch Änderungen in den Einheiten weg.\n\\[\\begin{equation}\n\\frac{MS_{\\textrm{test}}}{MS_{E(F)}} = \\frac{\\frac{\\textrm{SSE(R)} - \\textrm{SSE(F)}}{df_{E(R)} - df_{E(F)}}}{ \\frac{\\textrm{SSE(F)}}{df_{E(F)}}}\n\\label{eq-mlm-hier-Ftest}\n\\end{equation}\\]\nZusätzlich hat dies auch noch den Vorteil, dass die entstehende Metrik unter der \\(H_0\\) das das reduzierte Modell korrekt ist, einer uns bekannten theoretische Verteilung, nämlich der \\(F\\)-Verteilung mit \\(df_{E(R)} - df_{E(F)}\\) und \\(df_{E(F)}\\) Freiheitsgeraden, folgt.\n\\[\nF = \\frac{MS_{\\textrm{test}}}{MS_{E(F)}}  \\sim F(df_{E(R)}-df_{E(F)},df_{E(F)})\n\\]\nZur Erinnerung sind in Abbildung 19.1 nochmal ein paar Beispiele abgebildet.\n\n\n\n\n\nAbbildung 19.1: Beispiele für die F-Verteilung mit verschiedenen Freiheitsgraden \\(df_1, df_2\\)\n\n\n\n\nSobald wir eine bekannte theoretische Verteilung unter einer \\(H_0\\) haben, können wir unser Hypothestinstrumentarium auf die Verteilung los lassen und entsprechend einen kritische Bereich mit \\(\\alpha\\) bestimmen. In Abbildung 19.2 haben wir dies entsprechend getan. Somit wenn wenn der beobachtete \\(F\\)-Wert in den kritischen Bereich fällt, interpretieren wird das als Evidenz gegen die \\(H_0\\). Wir sind überrascht diesen Wert unter der \\(H_0\\) zu sehen und lehen die \\(H_0\\) das das einfachere Modell korrekt ist ab und werten dies als Evidenz dafür, das das komplexere, volle Modell die Daten besser abbildet und signifikanz mehr Varianz der abhängigen Variable modellieren kann.\n\n\n\n\n\nAbbildung 19.2: F-Verteilung mit \\(df_1 = 5, df_2 = 10\\) und kritischem Wert bei \\(\\alpha=0.05\\)"
  },
  {
    "objectID": "mlm_hierarchies.html#genereller-linearer-modell-testansatz---teststatistik",
    "href": "mlm_hierarchies.html#genereller-linearer-modell-testansatz---teststatistik",
    "title": "19  Modellhierarchien",
    "section": "19.4 Genereller Linearer Modell Testansatz - Teststatistik",
    "text": "19.4 Genereller Linearer Modell Testansatz - Teststatistik\nWenn das reduzierte Modell korrekt ist, dann lässt sich zeigen, dass: \\[\nMS_{\\textrm{test}} = \\frac{\\textrm{SSE(R)} - \\textrm{SSE(F)}}{\\textrm{dfE(R)} - \\textrm{dfE(F)}}\n\\] ein Schätzer für die Varianz \\(\\sigma^2\\) (\\(\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\)) ist.\nWenn das reduzierte Modell korrekt ist, dann ist auch das volle Modell korrekt. Daher ist dann:\n\\[\n\\textrm{MSE(F)} = \\frac{\\textrm{SSE(F)}}{\\textrm{dfE(F)}}\n\\] auch ein Schätzer für \\(\\sigma^2\\)"
  },
  {
    "objectID": "mlm_hierarchies.html#f-wert-als-teststatistik",
    "href": "mlm_hierarchies.html#f-wert-als-teststatistik",
    "title": "19  Modellhierarchien",
    "section": "19.4 F-Wert als Teststatistik",
    "text": "19.4 F-Wert als Teststatistik\n\\[\nF = \\frac{MS_{\\textrm{test}}}{MSE(F)}= \\frac{\\frac{\\textrm{SSE(R)} - \\textrm{SSE(F)}}{\\textrm{dfE(R)} - \\textrm{dfE(F)}}}{ \\frac{\\textrm{SSE(F)}}{\\textrm{dfE(F)}}}\n\\]"
  },
  {
    "objectID": "mlm_hierarchies.html#verteilung-der-f-statistik",
    "href": "mlm_hierarchies.html#verteilung-der-f-statistik",
    "title": "19  Modellhierarchien",
    "section": "19.5 Verteilung der F-Statistik",
    "text": "19.5 Verteilung der F-Statistik\n\\[\nF = \\frac{MS_{\\textrm{test}}}{MSE(F)}  \\sim F(\\textrm{dfE(R)}-\\textrm{dfE(F)},\\textrm{dfE(F)})\n\\]\n\n\n\n\n\nBeispiele für die F-Verteilung mit verschiedenen Freiheitsgraden \\(df_1, df_2\\)"
  },
  {
    "objectID": "mlm_hierarchies.html#hypothesentest-mit-f-wert",
    "href": "mlm_hierarchies.html#hypothesentest-mit-f-wert",
    "title": "19  Modellhierarchien",
    "section": "19.4 Hypothesentest mit F-Wert",
    "text": "19.4 Hypothesentest mit F-Wert\n\n\n\n\n\nF-Verteilung mit \\(df_1 = 5, df_2 = 10\\) und kritischem Wert bei \\(\\alpha=0.05\\)\n\n\n\n\n1"
  },
  {
    "objectID": "mlm_hierarchies.html#teilziel",
    "href": "mlm_hierarchies.html#teilziel",
    "title": "19  Modellhierarchien",
    "section": "19.4 Teilziel",
    "text": "19.4 Teilziel\n\nDurch den Vergleich von Modellen kann die Verbesserung/Verschlechterung der Modellvorhersage statistisch Überprüft werden\nAlternativ: Brauchen ich zusätzliche Parameter oder reicht mir das einfache Modell?"
  },
  {
    "objectID": "mlm_hierarchies.html#beispiel-candy-problem",
    "href": "mlm_hierarchies.html#beispiel-candy-problem",
    "title": "19  Modellhierarchien",
    "section": "19.2 Beispiel: Candy-Problem",
    "text": "19.2 Beispiel: Candy-Problem\nSchauen wir uns hergeleitete Metrik in Aktion an einem konkreten Beispiel an. Dabei werden hier auch noch den Begriff der Modellhierarchien einführen. In Abbildung 19.3 ist eine exemplarischer Datensatz abgebildet.\n\n\n\n\n\nAbbildung 19.3: Zusammenhang zwischen der Präferenz für ein Bonbon und dem Süßgrad (g pro Bonbon/100) für verschiedene Saftanteile 0% - 20%\n\n\n\n\nIn einer Studie wurde der Zusammenhang zwischen wie gut ein Bonbon bewertet wurde (like, Skala 0-100) und dem Süßegrad und dem Saftanteil untersucht. Wir sehen, dass Bonbons umso besser bewertet werden umso höher der Süßegrad war, aber das dieser Effekt durch den Saftanteil beeinflusst wird und umso stärker ist, umso höher der Saftanteil ist. Daher spricht dies für ein Interaktionsmodell.\nDas volle Modell kann dementsprechend mit \\(x_1\\) = Süßegrad und \\(x_2\\) = Saftanteil folgendermaßen modelliert werden.\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i}x_{2i} + \\epsilon_i\n\\]\nIm vollen Modell ist daher \\(p = 4\\) und wir können drei verschiedenen reduzierte Modelle definieren. Das einfachste Modell bezeichnen wir der Einfachheit halber als \\(m_0\\) und dementsprechend ansteigend bis zum vollen Modell \\(m_3\\).\n\\[\\begin{align*}\nm_0&: y_i = \\beta_0 + \\epsilon_i \\\\\nm_1&: y_i = \\beta_0 + \\beta_1 x_{1i} + \\epsilon_i \\\\\nm_2&: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\\\\nm_3&: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i}x_{2i} + \\epsilon_i\n\\end{align*}\\]\nDas Modell \\(m_0\\) besitzt nur einen \\(y\\)-Achsenabschnitt \\(\\beta_0\\), der wie wir oben gesehen haben zu \\(\\bar{y}\\) wird. Modell \\(m_1\\) hat einen zusätzlichen Parameter mit einem Steigungskoeffizienten \\(\\beta_1\\) für den Süßegrad, \\(m_1\\) hat zusätzlich noch einen Parameter \\(\\beta_2\\) für den Saftanteil. Wir können nun diese Modelle in folgender Hierarchie anordnen.\n\\[\\begin{equation*}\nm_0 \\subseteq m_1 \\subseteq m_2 \\subseteq m_3\n\\end{equation*}\\]\nEs gilt ebenfalls\n\\[\\begin{equation*}\np_{m_0} &lt; p_{m_1} &lt; p_{m_2} &lt; p_{m_3}\n\\end{equation*}\\]\nIn R können wir die entsprechenden Modelle fitten.\n\nmod_0 &lt;- lm(like ~ 1, candy)\nmod_1 &lt;- lm(like ~ sweetness, candy)\nmod_2 &lt;- lm(like ~ sweetness + moisture, candy)\nmod_3 &lt;- lm(like ~ sweetness * moisture, candy)\n\nDen Vergleich \\(m_0\\) gegen \\(m_1\\)\n\\[\\begin{align*}\nm_0: y_i &= \\beta_0 + \\epsilon_i \\\\\nm_1: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\epsilon_i\n\\end{align*}\\]\nkönnen wir mit der anova() Funktion machen, indem wir die beiden gefitteten Modelle übergeben (tatsächlich können auch mehr Modelle übergeben werden).\n\nanova(mod_0, mod_1)\n\nAnalysis of Variance Table\n\nModel 1: like ~ 1\nModel 2: like ~ sweetness\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     77 48509                                  \n2     76 26222  1     22287 64.596 9.346e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDie Einträge unter ResDf sind die jeweiligen \\(df_{E}\\), RSS die jeweiligen Quadratsummen, unter Df ist der Unterschied in der Anzahl der Modellparameter angeben, gefolgt von \\(MS_{\\text{test}}\\) und dem resultierenden \\(F\\)-Wert und des p-Werts unter der \\(H_0\\). Im Beispiel sehen wir wenig überraschend einen statistische signifikanten p-Wert.\nParallel dazu können wir \\(m_1\\) gegen \\(m_2\\) testen.\n\\[\\begin{align*}\nm_1: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\epsilon_i \\\\\nm_2: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_1, mod_2)\n\nAnalysis of Variance Table\n\nModel 1: like ~ sweetness\nModel 2: like ~ sweetness + moisture\n  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     76 26221.7                                 \n2     75  6115.1  1     20107 246.6 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWiederum deutet der Test einen darauf, das das komplexere Modell das beide Variablen enthält des statisch besseren fit ermöglicht.\nLetztendlich erfolgt der Test von \\(m_3\\) gegen \\(m_2\\).\n\\[\\begin{align*}\nm_2: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i  \\\\\nm_3: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_2, mod_3)\n\nAnalysis of Variance Table\n\nModel 1: like ~ sweetness + moisture\nModel 2: like ~ sweetness * moisture\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     75 6115.1                                  \n2     74  311.4  1    5803.7 1379.1 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDieser Test zeigt ebenfalls ein statistisch signifikantes Ergebnis.\nWir sind aber nicht darauf beschränkt immer nur einen zusätzlichen Parameter zu testen. In diesem Fall ist der \\(F\\)-Test äquivalent zum \\(t\\)-Wert den wir unter summary() angezeigt bekommen. Der \\(F\\) ist gleich dem quadrierten \\(t\\)-Wert.\n\nsummary(mod_3)\n\n\nCall:\nlm(formula = like ~ sweetness * moisture, data = candy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2547 -1.4818 -0.1531  1.0703  4.1625 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.395065   0.903822   1.544    0.127    \nsweetness          0.063198   0.051052   1.238    0.220    \nmoisture           0.056554   0.076683   0.738    0.463    \nsweetness:moisture 0.158653   0.004272  37.136   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.051 on 74 degrees of freedom\nMultiple R-squared:  0.9936,    Adjusted R-squared:  0.9933 \nF-statistic:  3818 on 3 and 74 DF,  p-value: &lt; 2.2e-16\n\n\nWir können nun aber ebenso das Modell \\(m_3\\) mit \\(p = 4\\) Parametern gegen das \\(m_0\\) Modell mit \\(p = 1\\) Parameter vergleichen.\n\\[\\begin{align*}\nm_0: y_i &= \\beta_0 + \\epsilon_i  \\\\\nm_3: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_0, mod_3)\n\nAnalysis of Variance Table\n\nModel 1: like ~ 1\nModel 2: like ~ sweetness * moisture\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     77 48509                                  \n2     74   311  3     48197 3817.6 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn diesem Fall wird getestet ob die Hinzunahme der Parameter \\(\\beta_1, \\beta_2\\) und \\(\\beta_3\\) eine statistisch signifikante Verbesserung im Modellfit bedeutet. Tatsächlich wird dieser Test in der Ausgabe von summary() angegeben.\n\nsummary(mod_3)\n\n\nCall:\nlm(formula = like ~ sweetness * moisture, data = candy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2547 -1.4818 -0.1531  1.0703  4.1625 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.395065   0.903822   1.544    0.127    \nsweetness          0.063198   0.051052   1.238    0.220    \nmoisture           0.056554   0.076683   0.738    0.463    \nsweetness:moisture 0.158653   0.004272  37.136   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.051 on 74 degrees of freedom\nMultiple R-squared:  0.9936,    Adjusted R-squared:  0.9933 \nF-statistic:  3818 on 3 and 74 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "mlm_hierarchies.html#modelle-als-hierarchien-auffassen",
    "href": "mlm_hierarchies.html#modelle-als-hierarchien-auffassen",
    "title": "19  Modellhierarchien",
    "section": "19.6 Modelle als Hierarchien auffassen",
    "text": "19.6 Modelle als Hierarchien auffassen\n\n19.6.1 Full model\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i}x_{2i} + \\epsilon_i\n\\]\n\n\n19.6.2 Hierarchie\n\\[\\begin{align*}\nm_0&: y_i = \\beta_0 + \\epsilon_i \\\\\nm_1&: y_i = \\beta_0 + \\beta_1 x_{1i} + \\epsilon_i \\\\\nm_2&: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\\\\nm_3&: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i}x_{2i} + \\epsilon_i\n\\end{align*}\\]\nEs gilt: \\(m_0 \\subseteq m_1 \\subseteq m_2 \\subseteq m_3\\)"
  },
  {
    "objectID": "mlm_hierarchies.html#modelle-als-hierarchien-auffassen-in-r",
    "href": "mlm_hierarchies.html#modelle-als-hierarchien-auffassen-in-r",
    "title": "19  Modellhierarchien",
    "section": "19.3 Modelle als Hierarchien auffassen in R",
    "text": "19.3 Modelle als Hierarchien auffassen in R\nIn R:\n\nmod_0 &lt;- lm(like ~ 1, candy)\nmod_1 &lt;- lm(like ~ sweetness, candy)\nmod_2 &lt;- lm(like ~ sweetness + moisture, candy)\nmod_3 &lt;- lm(like ~ sweetness * moisture, candy)"
  },
  {
    "objectID": "mlm_hierarchies.html#vergleich-m_0-gegen-m_1",
    "href": "mlm_hierarchies.html#vergleich-m_0-gegen-m_1",
    "title": "19  Modellhierarchien",
    "section": "19.4 Vergleich \\(m_0\\) gegen \\(m_1\\)",
    "text": "19.4 Vergleich \\(m_0\\) gegen \\(m_1\\)\n\\[\\begin{align*}\nm_0: y_i &= \\beta_0 + \\epsilon_i \\\\\nm_1: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_0, mod_1)\n\n\n\n\nVergleich der Modellfits\n\n\nModel\nResDF\nDF\nSS\nF\np-val\n\n\n\n\nModel 1: like ~ 1\n77\n\n\n\n\n\n\nModel 2: like ~ sweetness\n76\n1\n21723.88\n56.31\n0"
  },
  {
    "objectID": "mlm_hierarchies.html#vergleich-m_1-gegen-m_2",
    "href": "mlm_hierarchies.html#vergleich-m_1-gegen-m_2",
    "title": "19  Modellhierarchien",
    "section": "19.3 Vergleich \\(m_1\\) gegen \\(m_2\\)",
    "text": "19.3 Vergleich \\(m_1\\) gegen \\(m_2\\)\n\\[\\begin{align*}\nm_1: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\epsilon_i \\\\\nm_2: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_1, mod_2)\n\n\n\n\nVergleich der Modellfits\n\n\n\n\n\n\n\n\n\n\nModel\nResDF\nDF\nSS\nF\np-val\n\n\n\n\nModel 1: like ~ sweetness\n76\n\n\n\n\n\n\nModel 2: like ~ sweetness + moisture\n75\n1\n19350.27\n208.37\n0"
  },
  {
    "objectID": "mlm_hierarchies.html#vergleich-m_2-gegen-full-model-m_3",
    "href": "mlm_hierarchies.html#vergleich-m_2-gegen-full-model-m_3",
    "title": "19  Modellhierarchien",
    "section": "19.4 Vergleich \\(m_2\\) gegen full model \\(m_3\\)",
    "text": "19.4 Vergleich \\(m_2\\) gegen full model \\(m_3\\)\n\\[\\begin{align*}\nm_2: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i  \\\\\nm_3: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_2, mod_3)\n\n\n\n\nVergleich der Modellfits\n\n\n\n\n\n\n\n\n\n\nModel\nResDF\nDF\nSS\nF\np-val\n\n\n\n\nModel 1: like ~ sweetness + moisture\n75\n\n\n\n\n\n\nModel 2: like ~ sweetness * moisture\n74\n1\n6738.75\n2203.93\n0"
  },
  {
    "objectID": "mlm_hierarchies.html#vergleich-full-model-m_3-gegen-minmales-modell-m_0",
    "href": "mlm_hierarchies.html#vergleich-full-model-m_3-gegen-minmales-modell-m_0",
    "title": "19  Modellhierarchien",
    "section": "19.3 Vergleich full model \\(m_3\\) gegen minmales Modell \\(m_0\\)",
    "text": "19.3 Vergleich full model \\(m_3\\) gegen minmales Modell \\(m_0\\)\n\\[\\begin{align*}\nm_0: y_i &= \\beta_0 + \\epsilon_i  \\\\\nm_3: y_i &= \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} x_{2i} + \\epsilon_i\n\\end{align*}\\]\n\nanova(mod_0, mod_3)\n\n\n\n\nVergleich der Modellfits\n\n\n\n\n\n\n\n\n\n\nModel\nResDF\nDF\nSS\nF\np-val\n\n\n\n\nModel 1: like ~ 1\n77\n\n\n\n\n\n\nModel 2: like ~ sweetness * moisture\n74\n3\n33361.03\n2998.71\n0"
  },
  {
    "objectID": "mlm_hierarchies.html#in-summary-m_3-gegen-m_0",
    "href": "mlm_hierarchies.html#in-summary-m_3-gegen-m_0",
    "title": "19  Modellhierarchien",
    "section": "19.4 In summary() \\(m_3\\) gegen \\(m_0\\)",
    "text": "19.4 In summary() \\(m_3\\) gegen \\(m_0\\)\n\n\n\nCall:\nlm(formula = like ~ sweetness * moisture, data = candy)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9396 -1.2715  0.0762  1.3218  4.7059 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.911895   0.805668   2.373   0.0202 *  \nsweetness          0.075328   0.046242   1.629   0.1076    \nmoisture           0.053380   0.060656   0.880   0.3817    \nsweetness:moisture 0.157025   0.003894  40.325   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.926 on 74 degrees of freedom\nMultiple R-squared:  0.9918,    Adjusted R-squared:  0.9915 \nF-statistic:  2999 on 3 and 74 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "mlm_hierarchies.html#eine-nominale-variable-mit-vier-stufen",
    "href": "mlm_hierarchies.html#eine-nominale-variable-mit-vier-stufen",
    "title": "19  Modellhierarchien",
    "section": "19.3 Eine nominale Variable mit vier Stufen",
    "text": "19.3 Eine nominale Variable mit vier Stufen\nSchauen wir uns als Nächstes an, was passiert wenn wir eine nominale Variable in unser Modell aufnehmen. In Abbildung 19.4 haben wir ein hypothetisches Beispiel. Es wurde ein Reaktionszeitexperiment mit vier verschiedenen Konditionen A, B, C und D gemacht.\n\n\n\n\n\nAbbildung 19.4: Ein Reaktionszeitexperiment mit vier Stufen A, B, C und D\n\n\n\n\nIm Bachelor haben wir dazu gelernt eine Analysis of Variance (ANOVA) anzuwenden um diese Art von Daten zu analysieren. Ausgangspunkt war die Zerlegung der Varianz in:\n\\[\\begin{equation*}\nSS_{total} = SS_{between} + SS_{error}\n\\end{equation*}\\]\nMit etwas motiviertem Starren fällt uns natürlich auf, das das ziemlich ähnlich unserer Regressionszerlegung in \\(SSR\\) und \\(SSE\\) aussieht. Damals sind die folgenden Formeln zur Berechnung verwendet worden.\n\\[\\begin{align*}\ns_{between}^2 &= \\frac{1}{K-1}\\sum_{j=1}^K N_j (\\bar{y}_{j.}-\\bar{y})^2 \\\\\ns_{within}^2 &= \\frac{1}{N-K}\\sum_{j=1}^K\\sum_{i=1}^{N_j}(y_{ji}-\\bar{y}_{j.})^2 = \\frac{1}{N-K}\\sum_{j=1}^K(N_j-1)s_j^2 \\\\\nF &= \\frac{\\hat{\\sigma}_{between}^2} {\\hat{\\sigma}_{within}^2} \\sim F(K-1,N-K)\n\\end{align*}\\]\nDiese lassen wir zunächst einmal links liegen und greifen sie erst wieder später im Rahmen von CR-Designs wieder auf.\nIn R berechnen wir eine ANOVA mit der aov() Funktion. Die Modellformulierung ist gleich derjenigen wie mit lm().\n\nmod_aov &lt;- aov(rt ~ group, rt_tbl)\n\nAnwendung von summary() auf das gefittete aov-Modell liefert die übliche ANOVA-Tabelle.\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \ngroup        3 988935  329645   157.3 &lt;2e-16 ***\nResiduals   76 159221    2095                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWie sieht das Ganz aus, wenn wir den Ansatz mit Modellhierarchien anwenden? Nun, wir kennen das Beispiel schon und wissen das wir eine nominale Variable mittels Dummy-Variablen abbilden können. Daher formulieren das vollständige Modell als dasjenige, bei dem wir für beispielsweise Stufe A als Referenz nehmen und mittels dreier Dummy-Variablen (\\(K-1\\)) die Abweichungen der Stufen B-D von A modellieren.\n\\[\\begin{equation*}\ny_i = \\beta_0 + \\beta_{\\Delta_{B-A}} x_1 + \\beta_{\\Delta_{C-A}} x_2 + \\beta_{\\Delta_{D-A}} x_3 + \\epsilon_i\n\\end{equation*}\\]\nD.h. \\(p = 4\\). Als reduziertes Modell wählen wir das Modell mit nur einem Parameter mit \\(p = 1\\).\n\\[\\begin{equation*}\ny_i = \\beta_0 + \\epsilon_i\n\\end{equation*}\\]\nWenn das reduzierte Modell die Daten gleich gut fittet wie das vollständige Modell dann bedeutet dass, das die zusätzliche Information über die verschiedenen Konditionstufen meine Vorhersage von \\(y_i\\) nicht verbessert.\n\nmod &lt;- lm(rt ~ group, rt_tbl)\nsummary(mod)\n\n\nCall:\nlm(formula = rt ~ group, data = rt_tbl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-120.261  -25.789   -0.857   27.520  114.994 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   509.53      10.23  49.784  &lt; 2e-16 ***\ngroupB         90.15      14.47   6.228  2.4e-08 ***\ngroupC        197.41      14.47  13.639  &lt; 2e-16 ***\ngroupD        295.56      14.47  20.420  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45.77 on 76 degrees of freedom\nMultiple R-squared:  0.8613,    Adjusted R-squared:  0.8559 \nF-statistic: 157.3 on 3 and 76 DF,  p-value: &lt; 2.2e-16\n\n\nWenn wir uns die letzte Zeile im summary() Ausdruck anschauen, dann sehen wir dort die gleichen Werte wie wir mit aov() erhalten haben. Wenn wir anova() das gefittete lm Modell übergeben erhalten wir sogar genau die gleiche ANOVA Tabelle.\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: rt\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ngroup      3 988935  329645  157.35 &lt; 2.2e-16 ***\nResiduals 76 159221    2095                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTatsächlich verwendet aov() im Hintergrund auch nichts anderes als die lm() Funktion. Insgesamt bedeutet das für uns, das die ANOVA und Regression letztendlich auf das gleiche Modell zurück gehen, nämlich auf das Allgemeine Lineare Modell."
  },
  {
    "objectID": "mlm_hierarchies.html#früher---analysis-of-variance-anova-bzw.-aov",
    "href": "mlm_hierarchies.html#früher---analysis-of-variance-anova-bzw.-aov",
    "title": "19  Modellhierarchien",
    "section": "19.4 Früher - Analysis of Variance (ANOVA bzw. AOV)",
    "text": "19.4 Früher - Analysis of Variance (ANOVA bzw. AOV)\n\\[\\begin{align*}\ns_{zwischen}^2 &= \\frac{1}{K-1}\\sum_{j=1}^K N_j (\\bar{x}_{j.}-\\bar{x})^2 \\\\\ns_{innerhalb}^2 &= \\frac{1}{N-K}\\sum_{j=1}^K\\sum_{i=1}^{N_j}(x_{ji}-\\bar{x}_{j.})^2 = \\frac{1}{N-K}\\sum_{j=1}^K(N_j-1)s_j^2 \\\\\nF &= \\frac{\\hat{\\sigma}_{zwischen}^2} {\\hat{\\sigma}_{innerhalb}^2} \\sim F(K-1,N-K)\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_hierarchies.html#anova-in-r",
    "href": "mlm_hierarchies.html#anova-in-r",
    "title": "19  Modellhierarchien",
    "section": "19.4 ANOVA in R",
    "text": "19.4 ANOVA in R\n\nmod_aov &lt;- aov(rt ~ group, rt_tbl)\nsummary(mod_aov)\n\n\n\n\nAusgabe mit aov()\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ngroup\n3\n988935.1\n329645\n157.3\n0\n\n\nResiduals\n76\n159221.0\n2095"
  },
  {
    "objectID": "mlm_hierarchies.html#ansatz-mittels-modellhierarchien",
    "href": "mlm_hierarchies.html#ansatz-mittels-modellhierarchien",
    "title": "19  Modellhierarchien",
    "section": "19.5 Ansatz mittels Modellhierarchien",
    "text": "19.5 Ansatz mittels Modellhierarchien\n\n19.5.1 Full model\n\\[\ny_i = \\beta_0 + \\beta_{\\Delta_{B-A}} x_1 + \\beta_{\\Delta_{C-A}} x_2 + \\beta_{\\Delta_{D-A}} x_3 + \\epsilon_i\n\\]\n\n\n19.5.2 Reduced model\n\\[\ny_i = \\beta_0 + \\epsilon_i\n\\]\nWenn das reduced model die Daten gleich gut fittet wie das full model \\(\\Rightarrow\\) Information über das Treatment verbessert meine Vorhersage von \\(y_i\\) nicht."
  },
  {
    "objectID": "mlm_hierarchies.html#model-fit---full-model",
    "href": "mlm_hierarchies.html#model-fit---full-model",
    "title": "19  Modellhierarchien",
    "section": "19.6 Model fit - Full model",
    "text": "19.6 Model fit - Full model\n\nmod &lt;- lm(rt ~ group, rt_tbl)\n\n\n\n\nModellfit\n\n\n\n\\(\\hat{\\beta}\\)\n\\(s_e\\)\nt\np\n\n\n\n\n(Intercept)\n509.526\n10.235\n49.784\n&lt;0.001\n\n\ngroupB\n90.150\n14.474\n6.228\n&lt;0.001\n\n\ngroupC\n197.414\n14.474\n13.639\n&lt;0.001\n\n\ngroupD\n295.561\n14.474\n20.420\n&lt;0.001"
  },
  {
    "objectID": "mlm_hierarchies.html#anova-mit-nur-einem-modell",
    "href": "mlm_hierarchies.html#anova-mit-nur-einem-modell",
    "title": "19  Modellhierarchien",
    "section": "19.4 anova() mit nur einem Modell",
    "text": "19.4 anova() mit nur einem Modell\n\nanova(mod)\n\n\n\n\nÄquivalent zum Vergleich full gegen reduced model\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\ngroup\n3\n988935.1\n329645\n157.3\n0\n\n\nResiduals\n76\n159221.0\n2095"
  },
  {
    "objectID": "mlm_hierarchies.html#zum-nacharbeiten",
    "href": "mlm_hierarchies.html#zum-nacharbeiten",
    "title": "19  Modellhierarchien",
    "section": "19.4 Zum Nacharbeiten",
    "text": "19.4 Zum Nacharbeiten\nChristensen (2018, p.57–64)\n\n\n\n\nChristensen, Ronald. 2018. Analysis of variance, design, and regression: Linear modeling for unbalanced data. CRC Press.\n\n\nMaxwell, Scott E, Harold D Delaney, und Ken Kelley. 2004. Designing experiments and analyzing data: A model comparison perspective. 2. Aufl. Routledge."
  },
  {
    "objectID": "mlm_hierarchies.html#footnotes",
    "href": "mlm_hierarchies.html#footnotes",
    "title": "19  Modellhierarchien",
    "section": "",
    "text": "In R: df(), pf(), qf(), rf()↩︎"
  },
  {
    "objectID": "ed_crd.html#das-modell",
    "href": "ed_crd.html#das-modell",
    "title": "21  Completely Randomized Design",
    "section": "21.1 Das Modell",
    "text": "21.1 Das Modell\nUm diesem Design ein statistisches Modell anzupassen, müssen wir uns nun Gedanken über den DGP machen. Zunächst einmal wir jede(r) Teilnehmer(in) eine bestimmte Balancierfähigkeit haben. Bzw. wenn wir die Kondition mit dem festen Untergrund als eine Referenzkondition festlegen, können wir davon ausgehen, dass es eine mittlere Balancierfähigkeit gibt und die einzelnen Personen aus der Population um diesen Mittelwert schwanken.\n\\[\nY_{ij} = \\mu + \\tau_i + \\epsilon_{ij}, \\qquad \\epsilon_{ij}\\sim \\mathcal{N}(0,\\sigma^2)\n\\]\ni = Gruppenindikator, j = experimental unit-indikator, \\(\\mu\\) = Gesamtmittelwert,"
  },
  {
    "objectID": "ed_crd.html#annahmen",
    "href": "ed_crd.html#annahmen",
    "title": "21  Completely Randomized Design",
    "section": "21.2 Annahmen",
    "text": "21.2 Annahmen\n\nUnabhängige Experimental Units (EU)\nDie EUs sind zufällig in die k Gruppen eingeteilt worden\nDie Varianzen \\(\\sigma_i\\) in jeder Gruppe \\(i\\) sind gleich\nDie Werte in jeder Gruppe sind Normalverteilt \\(Y_{ij} \\sim \\mathcal{N}(\\mu_i, \\sigma)\\)"
  },
  {
    "objectID": "ed_crd.html#analyse-mittels-modellhierarchien",
    "href": "ed_crd.html#analyse-mittels-modellhierarchien",
    "title": "21  Completely Randomized Design",
    "section": "21.3 Analyse mittels Modellhierarchien",
    "text": "21.3 Analyse mittels Modellhierarchien\n\n21.3.1 Full model\n\\[\nY_{ij} = \\mu^* + \\tau_2^* x_{1j} + \\ldots + \\tau_k^* x_{(k-1)j} + \\epsilon_{ij}\n\\]\nmit \\[\\begin{align*}\n\\mu^* &= \\mu + \\tau_1 \\quad\\textrm{Referenzstufe} \\\\\n\\tau^*_i &= \\tau_i - \\tau_1 \\quad\\Delta\\textrm{ von Stufe i zur Referenzstufe}\n\\end{align*}\\] und \\(x_{ij}\\) als Dummyvariablen.\n\n\n21.3.2 Reduced model\n\\[\nY_{ij} = \\mu + \\epsilon_{ij}\n\\]"
  },
  {
    "objectID": "ed_crd.html#teststatistik",
    "href": "ed_crd.html#teststatistik",
    "title": "21  Completely Randomized Design",
    "section": "21.4 Teststatistik",
    "text": "21.4 Teststatistik\n\\[\nF = \\frac{\\textrm{SSE(R)} - \\textrm{SSE(F)}}{\\textrm{df}_R - \\textrm{df}_F} \\frac{\\textrm{df}_F}{\\textrm{SSE(F)}} \\sim F(\\textrm{df}_R-\\textrm{df}_F,\\textrm{df}_F)\n\\]\n\n21.4.1 Statistische Hypothesen\n\\[\\begin{align*}\nH_0:& \\tau_1 = \\tau_2 = \\ldots = \\tau_k = 0 \\\\\nH_1:& \\exists\\tau_i \\neq \\tau_j\\ \\textrm{mit}\\ i \\neq j, i,j \\in \\{1,2,\\ldots,k\\}\n\\end{align*}\\]"
  },
  {
    "objectID": "ed_crd.html#verbindung-modellhierarchien-und-ba-anova",
    "href": "ed_crd.html#verbindung-modellhierarchien-und-ba-anova",
    "title": "21  Completely Randomized Design",
    "section": "21.5 Verbindung Modellhierarchien und BA-ANOVA",
    "text": "21.5 Verbindung Modellhierarchien und BA-ANOVA\nIm Bachelor wurde das completely randomized design aller Wahrscheinlichkeit nach unter der Bezeichnung Einfaktorielle ANOVA eingeführt. Was eigentlich nicht viel Sinn macht, da es sich bei der einfaktoriellen ANOVA um eine Analysemethode und streng genommen nicht um ein experimentelles Design handelt. Wie auch immer,\n\\[\\begin{equation}\n\\sum_i (y_i - \\hat{y}) = \\sum_i y_i - n \\hat{y} = n \\hat{y} - n \\hat{y} = 0\n\\label{eq-ed-crd-bar-sum}\n\\end{equation}\\]\n\\[\\begin{align}\n\\begin{split}\n\\sum_i \\sum_j 2 (y_{ij} - \\hat{y}_j)(\\hat{y}_j - \\hat{y}) &= \\sum_j \\sum_i 2 (y_{ij} - \\hat{y}_j)(\\hat{y}_j - \\hat{y}) \\\\\n&= 2 \\sum_j (\\hat{y}_j - \\hat{y}) \\underbrace{\\sum_i (y_{ij} - \\hat{y}_j)}_{\\text{mit} ~(\\ref{eq-ed-crd-bar-sum})=0}\\\\\n&= 2 \\sum_j (\\hat{y}_j - \\hat{y}) 0  = 0 \\\\\n\\end{split}\n\\label{eq-ed-crd-cross-sum}\n\\end{align}\\]\n\\[\\begin{align}\n\\begin{split}\n\\underbrace{\\sum_i \\sum_j (y_{ij} - \\hat{y})^2}_{SS_{\\text{Total}}} &= \\sum_i \\sum_j (y_{ij} - \\hat{y}_j + \\hat{y}_j - \\hat{y})^2 \\\\ &= \\sum_i \\sum_j (y_{ij} - \\hat{y}_j)^2 + \\sum_i \\sum_j (\\hat{y}_j - \\hat{y})^2 - \\underbrace{\\sum_i \\sum_j 2 (y_{ij} - \\hat{y}_j)(\\hat{y}_j - \\hat{y})}_{\\text{mit}~(\\ref{eq-ed-crd-cross-sum})=0}\\\\\n&= \\underbrace{\\sum_i \\sum_j (y_{ij} - \\hat{y}_j)^2}_{SS_{\\text{Error}}} + \\underbrace{\\sum_i \\sum_j (\\hat{y}_j - \\hat{y})^2}_{SS_{\\text{Regression}}}\n\\end{split}\n\\label{eq-ed-crd-sse-total}\n\\end{align}\\]\n\\[\\begin{align*}\nSSE(R) - SSE(F) &= \\sum_i \\sum_j (y_{ij} - \\hat{y})^2 - \\sum_i \\sum_j (y_{ij} - \\hat{y}_j)^2 \\\\\n&= \\underbrace{\\sum_i \\sum_j (y_{ij} - \\hat{y}_j)^2 + \\sum_i \\sum_j (\\hat{y}_j - \\hat{y})^2}_{\\text{mit}~\\eqref{eq-ed-crd-sse-total}}  - \\sum_i \\sum_j (y_{ij} - \\hat{y}_j)^2 \\\\\n&= \\sum_i \\sum_j (\\hat{y}_j - \\hat{y})^2 \\\\\n\\end{align*}\\]\n\\[\nQS_{zwischen} = \\sum_{j=1}^K \\sum_{i=1}^{N_j} (\\bar{x}_{j}-\\bar{x})^2\n\\]"
  },
  {
    "objectID": "ed_crd.html#beispieldaten",
    "href": "ed_crd.html#beispieldaten",
    "title": "21  Completely Randomized Design",
    "section": "21.6 Beispieldaten",
    "text": "21.6 Beispieldaten\n\n\n\n\n\nEinfluss von Koffeine auf die Laufleistung über 8km"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatur",
    "section": "",
    "text": "Altman, Douglas G, and J Martin Bland. 1995. “Statistics Notes:\nAbsence of Evidence Is Not Evidence of Absence.” Bmj 311\n(7003): 485.\n\n\nAltman, Naomi, and Martin Krzywinski. 2015a. “Points of\nSignificance: Multiple Linear Regression.” Nature\nMethods 12 (12): 1103–4.\n\n\n———. 2015b. “Points of Significance: Simple Linear\nRegression.” Nature Methods 12 (11).\n\n\n———. 2016a. “Points of Significance: Analyzing Outliers:\nInfluential or Nuisance.” Nature Methods 13 (4): 281–82.\n\n\n———. 2016b. “Points of Significance: Regression\nDiagnostics.” Nature Methods 13 (5): 385–86.\n\n\nChambers, John M. 2008. Software for Data Analysis: Programming with\nr. Vol. 2. 1. Springer.\n\n\nChristensen, Ronald. 2018. Analysis of Variance, Design, and\nRegression: Linear Modeling for Unbalanced Data. CRC Press.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. 2nd ed. Routledge.\n\n\nCumming, Geoff. 2013. Understanding the New Statistics: Effect\nSizes, Confidence Intervals, and Meta-Analysis. Routledge.\n\n\nDalgaard, Peter. 2008. Introductory Statistics with r.\nSpringer.\n\n\nDebanne, Thierry, and Guillaume Laffaye. 2011. “Predicting the\nThrowing Velocity of the Ball in Handball with Anthropometric Variables\nand Isotonic Tests.” Journal of Sports Sciences 29 (7):\n705–13.\n\n\nFox, John. 2011. An r Companion to Applied Regression. 2nd ed.\nSAGE Publication Inc., Thousand Oaks.\n\n\nGross, Benedict, Joe Harris, and Emily Riehl. 2019. Fat Chance:\nProbability from 0 to 1. Cambridge University Press.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li.\n2005. Applied Linear Statistical Models. 5th ed. McGraw-Hill\nIrwin New York.\n\n\nMcElreath, Richard. 2016. Statistical Rethinking, a Bayesian Course\nwith Examples in r and Stan. 1st ed. Boca Raton: CRC Press.\n\n\nPeng, Roger D. 2016. R Programming for Data Science. Leanpub\nVictoria, BC, Canada.\n\n\nSpiegelhalter, David. 2019. The Art of Statistics: Learning from\nData. Penguin UK.\n\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA\nStatement on p-Values: Context, Process, and Purpose.” Taylor\n& Francis.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media, Inc.\".\n\n\nWild, Christopher J, and Georg AF Seber. 2000. Chance Encounters: A\nFirst Course in Data Analysis and Inference. Wiley Press.\n\n\nWilkinson, G. N., and C. E. Rogers. 1973. “Symbolic Description of\nFactorial Models for Analysis of Variance.” Applied\nStatistics 22 (3): 392–99.\n\n\nYoung, Alwyn. 2019. “Channeling Fisher: Randomization Tests and\nthe Statistical Insignificance of Seemingly Significant Experimental\nResults.” The Quarterly Journal of Economics 134 (2):\n557–98."
  },
  {
    "objectID": "mlm_hierarchies.html#reduziertes-modell-und-stichprobenvarianz-.unnumbered",
    "href": "mlm_hierarchies.html#reduziertes-modell-und-stichprobenvarianz-.unnumbered",
    "title": "19  Modellhierarchien",
    "section": "19.3 Reduziertes Modell und Stichprobenvarianz(*) {.unnumbered}",
    "text": "19.3 Reduziertes Modell und Stichprobenvarianz(*) {.unnumbered}\nZwischen der Residualvarianz im reduzierten Modell SSE(R) und der Stichprobenvarianz bestehht ein enger Zusammenhang bzw. Identität wie sich anhand er folgenden Herleitung sehen lässt.\n\\[\\begin{align*}\nSSE &= \\sum_{i=1}^n(y_i - \\beta_0)^2 = \\sum_{i=1}^n (y_i^2 - 2y_i\\beta_0 + \\beta_0^2) \\\\\n0 &= \\frac{\\mathrm{d}}{\\mathrm{d} \\beta_0}\\sum_{i=1}^n (y_i^2 - 2y_i\\beta_0 + \\beta_0^2) \\\\\n0 &= \\sum_{i=1}^n (-2y_i + 2 \\beta_0) = -2\\sum_{i=1}^n y_i + 2\\sum_{i=1}^n \\beta_0\\\\\nn\\beta_0 &= \\sum_{i=1}^n y_i \\\\\n\\beta_0 &= \\frac{\\sum_{i=1}^n y_i}{n} = \\bar{y} \\rightarrow \\frac{SSE}{n-1} = \\hat{\\sigma}^2 = s^2\n\\end{align*}\\]"
  },
  {
    "objectID": "mlm_hierarchies.html#beispiel-nominale-variable",
    "href": "mlm_hierarchies.html#beispiel-nominale-variable",
    "title": "19  Modellhierarchien",
    "section": "19.3 Beispiel: Nominale Variable",
    "text": "19.3 Beispiel: Nominale Variable\nSchauen wir uns als Nächstes an, was passiert wenn wir eine nominale Variable in unser Modell aufnehmen. In Abbildung 19.4 haben wir ein hypothetisches Beispiel. Es wurde ein Reaktionszeitexperiment mit vier verschiedenen Konditionen A, B, C und D gemacht.\n\n\n\n\n\nAbbildung 19.4: Ein Reaktionszeitexperiment mit vier Stufen A, B, C und D\n\n\n\n\nIm Bachelor haben wir dazu gelernt eine Analysis of Variance (ANOVA) anzuwenden um diese Art von Daten zu analysieren. Ausgangspunkt war die Zerlegung der Varianz in:\n\\[\\begin{equation*}\nSS_{total} = SS_{between} + SS_{error}\n\\end{equation*}\\]\nMit etwas motiviertem Starren fällt uns natürlich auf, das das ziemlich ähnlich unserer Regressionszerlegung in \\(SSR\\) und \\(SSE\\) aussieht. Damals sind die folgenden Formeln zur Berechnung verwendet worden.\n\\[\\begin{align*}\ns_{between}^2 &= \\frac{1}{K-1}\\sum_{j=1}^K N_j (\\bar{y}_{j.}-\\bar{y})^2 \\\\\ns_{within}^2 &= \\frac{1}{N-K}\\sum_{j=1}^K\\sum_{i=1}^{N_j}(y_{ji}-\\bar{y}_{j.})^2 = \\frac{1}{N-K}\\sum_{j=1}^K(N_j-1)s_j^2 \\\\\nF &= \\frac{\\hat{\\sigma}_{between}^2} {\\hat{\\sigma}_{within}^2} \\sim F(K-1,N-K)\n\\end{align*}\\]\nDiese lassen wir zunächst einmal links liegen und greifen sie erst wieder später im Rahmen von CR-Designs wieder auf.\nIn R berechnen wir eine ANOVA mit der aov() Funktion. Die Modellformulierung ist gleich derjenigen wie mit lm().\n\nmod_aov &lt;- aov(rt ~ group, rt_tbl)\n\nAnwendung von summary() auf das gefittete aov-Modell liefert die übliche ANOVA-Tabelle.\n\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \ngroup        3 988935  329645   157.3 &lt;2e-16 ***\nResiduals   76 159221    2095                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWie sieht das Ganz aus, wenn wir den Ansatz mit Modellhierarchien anwenden? Nun, wir kennen das Beispiel schon und wissen das wir eine nominale Variable mittels Dummy-Variablen abbilden können. Daher formulieren das vollständige Modell als dasjenige, bei dem wir für beispielsweise Stufe A als Referenz nehmen und mittels dreier Dummy-Variablen (\\(K-1\\)) die Abweichungen der Stufen B-D von A modellieren.\n\\[\\begin{equation*}\ny_i = \\beta_0 + \\beta_{\\Delta_{B-A}} x_1 + \\beta_{\\Delta_{C-A}} x_2 + \\beta_{\\Delta_{D-A}} x_3 + \\epsilon_i\n\\end{equation*}\\]\nD.h. \\(p = 4\\). Als reduziertes Modell wählen wir das Modell mit nur einem Parameter mit \\(p = 1\\).\n\\[\\begin{equation*}\ny_i = \\beta_0 + \\epsilon_i\n\\end{equation*}\\]\nWenn das reduzierte Modell die Daten gleich gut fittet wie das vollständige Modell dann bedeutet dass, das die zusätzliche Information über die verschiedenen Konditionstufen meine Vorhersage von \\(y_i\\) nicht verbessert.\n\nmod &lt;- lm(rt ~ group, rt_tbl)\nsummary(mod)\n\n\nCall:\nlm(formula = rt ~ group, data = rt_tbl)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-120.261  -25.789   -0.857   27.520  114.994 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   509.53      10.23  49.784  &lt; 2e-16 ***\ngroupB         90.15      14.47   6.228  2.4e-08 ***\ngroupC        197.41      14.47  13.639  &lt; 2e-16 ***\ngroupD        295.56      14.47  20.420  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45.77 on 76 degrees of freedom\nMultiple R-squared:  0.8613,    Adjusted R-squared:  0.8559 \nF-statistic: 157.3 on 3 and 76 DF,  p-value: &lt; 2.2e-16\n\n\nWenn wir uns die letzte Zeile im summary() Ausdruck anschauen, dann sehen wir dort die gleichen Werte wie wir mit aov() erhalten haben. Wenn wir anova() das gefittete lm Modell übergeben erhalten wir sogar genau die gleiche ANOVA Tabelle.\n\nanova(mod)\n\nAnalysis of Variance Table\n\nResponse: rt\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ngroup      3 988935  329645  157.35 &lt; 2.2e-16 ***\nResiduals 76 159221    2095                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTatsächlich verwendet aov() im Hintergrund auch nichts anderes als die lm() Funktion. Insgesamt bedeutet das für uns, das die ANOVA und Regression letztendlich auf das gleiche Modell zurück gehen, nämlich auf das Allgemeine Lineare Modell."
  }
]