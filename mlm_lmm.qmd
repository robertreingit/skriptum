# Gemischte Lineare Modelle 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

```{r}
library(purrr)
library(broom)
library(lme4)
tidy_lm <- function(mod) {
  broom::tidy(mod) |> add_row(term = 'sigma',
                       estimate = sigma(mod))
}
```

In allen bisherigen Modellen, war eine zentrale Annahme, dass die Objekte **unabhängig** voneinander sind. Ob bei der einfachen, der multiplen oder der logistischen Regression, die Annahme war in allen Fällen, dass die einzelnen Datenpunkte unabhängig voneinander sind. Allerdings kommt es natürlich in der Realität vor, dass mehrere Messwerte von den gleichen Punkten ermittelt werden, um beispielsweise die Präzision zu erhöhen. Oder es sollen zwei Gruppen vor und nach einem Treatment miteinander verglichen werden. In diesen Fällen ist die Annahme der unabhängig verletzt. In diesem Fall werden sogenannte gemischte lineare Modelle verwendet. Gemischte lineare Modelle werden auch als hierarchische Modelle oder Mehrebenenmodelle bezeichnet. Konzeptionell stellen die gemischten Modelle einer Erweiterung der Modelle die bisher behandelt wurden dar oder anders herum, die multiple lineare Regression ist ein Spezialfall der gemischten linearen Modelle. Vorteil ist auch wieder, dass die bereits gelernten Konzepte angewendet werden können.

## Setup 

```{r}
#| layout-ncol: 2

df <- tibble(
  x = 1:4,
  y = c(1.1,1.6,3.3,4.1)
)
kable(df)
ggplot(df, aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=F, col='red')
```

## Kovarianz und Korrelation

\begin{align*}
\sigma_{XY} &= E[(X-\mu_X)(Y-\mu_Y)] \\
\text{cov}(x,y) &= \frac{\sum_{i=1}^N (x_i - \bar{x})(y_i - \bar{y})}{N-1}
\end{align*}

\begin{align*}
\rho_{XY} &= \frac{\sigma_{XY}}{\sigma_X \sigma_Y} \\
r_{xy} &= \frac{\text{cov}(x,y)}{s_x s_y} 
\end{align*}

$$
\sigma_{XY} = \sigma_X \cdot \sigma_Y \cdot \rho_{XY}
$$

## Residuenannahme - Einfache Lineare Regression

$$
\epsilon_i \sim \mathcal{N}(0,\sigma^2), i \in [1,2,3,4], \textrm{iid}
$$

### Identisch verteilt
$$
\begin{matrix}
\epsilon_1 \sim \mathrm{N}(0,\sigma^2) \\
\epsilon_2 \sim \mathrm{N}(0,\sigma^2) \\
\epsilon_3 \sim \mathrm{N}(0,\sigma^2) \\
\epsilon_4 \sim \mathrm{N}(0,\sigma^2) \\
\end{matrix}
$$

### Unabhängig
$$
\text{cov}(\epsilon_i,\epsilon_j) = 0, \quad i,j \in [1,2,3,4], i\neq j
$$

## Varianz-Kovarianzmartrix der Residuen


$$
\Sigma = \begin{pmatrix}
\sigma^2 & 0 & 0 & 0 \\
0 & \sigma^2 & 0 & 0 \\
0 & 0 & \sigma^2 & 0 \\
0 & 0 & 0 & \sigma^2
\end{pmatrix}
$$

## Verteilungsannahme der Residuen in Matrizenschreibweise

Mit 
$$
\mathbf{0} = \begin{pmatrix}
0 & 0 & 0 & 0 
\end{pmatrix}^\top 
$$

$$
\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\Sigma)
$$

## Andere Schreibweise der Varianz-Kovarianzmatrix

### Einheitsmatrix

$$
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
$$

### Varianz-Kovarianzmatrix

$$
\Sigma = \sigma^2\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
$$

## Allgemeiner Fall der Varianz-Kovarianzmatrix

### Multivariate Normalverteilung

$$
\mu = \begin{pmatrix}
\mu_1 \\ \mu_2 \\ \mu_3 \\ \mu_4
\end{pmatrix} \quad
$$

$$
\Sigma = \begin{pmatrix} 
\sigma_1^2 & \sigma_{12} & \sigma_{13} & \sigma_{14} \\
\sigma_{21} & \sigma_2^2 & \sigma_{23} & \sigma_{24} \\
\sigma_{31} & \sigma_{32} & \sigma_3^2 & \sigma_{34} \\
\sigma_{41} & \sigma_{42} & \sigma_{43} & \sigma_4^2 \\
\end{pmatrix} =
\begin{pmatrix} 
\sigma_1^2 & \rho_{12}\sigma_1\sigma_2 & \rho_{13}\sigma_1\sigma_{3} & \rho_{14}\sigma_1\sigma_{4} \\
\rho_{21}\sigma_2\sigma_{1} & \sigma_2^2 & \rho_{23}\sigma_2\sigma_{3} & \rho_{24}\sigma_2\sigma_{4} \\
\rho_{31}\sigma_3\sigma_{1} & \rho_{32}\sigma_3\sigma_{2} & \sigma_3^2 & \rho_{34}\sigma_3\sigma_{4} \\
\rho_{41}\sigma_4\sigma_{1} & \rho_{42}\sigma_4\sigma_{2} & \rho_{43}\sigma_4\sigma_{3} & \sigma_4^2 \\
\end{pmatrix}
$$

## Alternative Schreibweise - einfache Regression

### Indexschreibweise

$$
\begin{matrix}
\epsilon_i = y_i - \hat{y}_i & | + \hat{y}_i & \sim \mathcal{N}(0,\sigma^2) \\
\Leftrightarrow \epsilon + \hat{y}_i = y_i &  & \sim \mathcal{N}(0,\sigma^2) +\hat{y} \\
& & = \mathcal{N}(\hat{y},\sigma^2) \\
& & = \mathcal{N}(\beta_0 + \beta_1\cdot x_i,\sigma^2) \\
\end{matrix}
$$

$$
y_i \sim \mathcal{N}(\beta_0 + \beta_1\cdot x_i,\sigma^2)  
$$

### Matrizenschreibweise

$$
\mathbf{Y} = \mathcal{N}(\mathbf{X}\boldsymbol{\beta},\Sigma)
$$

## Wofür? {.t}

z.B. Daten von Schülern aus Klassen aus Schulen

## Patterned covarianz matrices

$$
\Sigma = \begin{pmatrix}
\Sigma_1 & 0 \\ 0 & \Sigma_2
\end{pmatrix}
$$

$$
\Sigma_i = \begin{pmatrix}
\sigma_1^2 & \rho_{12}\sigma_1\sigma_2 \\
\rho_{21}\sigma_1\sigma_2 & \sigma_2^2  \\
\end{pmatrix}
$$

- `id` = unique musician identification number
- `na` = negative affect score from PANAS
- `perf_type` = type of performance (Solo, Large Ensemble, or Small Ensemble)
- `instrument` = Voice, Orchestral, or Piano


```{r}
df <- readr::read_csv('data/roback_musicians.csv')
set.seed(1)
df |> sample_n(10) |> kable()
```

## Explorative Analyse Univariate - Individual vs. Grouped

```{r}
#| layout-ncol: 2
#| fig-height: 4

ggplot(df, aes(na)) + geom_histogram(col='black',fill='white',bins=12)
df |> group_by(id) |> summarize(na = mean(na)) |> 
  ggplot(aes(na)) + geom_histogram(col='black',fill='white',bins=12)

```


## Explorative Analyse Bivariate - Average `na`

```{r}
df |> group_by(id) |> 
  summarize(
    na_bar = mean(na),
    na_sd = sd(na),
    n = n(),
    na_se = na_sd/sqrt(n)) |> 
  mutate(id_r = rank(na_bar)) |> 
  ggplot(aes(na_bar, id_r)) +
  geom_label(aes(x = na_bar+2*na_se, label = id), alpha=.5) +
  geom_pointrange(aes(xmin=na_bar-2*na_se, xmax=na_bar+2*na_se)) +
  scale_x_continuous('Average na (CI)', breaks=seq(10,26,2)) +
  scale_y_continuous('Musician', breaks=NULL)
```


## Explorative Analyse Bivariate - Individual vs. Grouped

```{r}
#| layout-ncol: 2
#| fig-height: 4

ggplot(df, aes(factor(instrument_f), na)) + geom_boxplot()
df |> group_by(instrument_f, id) |> summarize(na = mean(na)) |> 
  ggplot(aes(factor(instrument_f), na)) + geom_boxplot()
```

## Multilevel modelling - Terminologie


- **Level one** refers to the **individual-level data**, capturing variation **within groups** (e.g., students within classes).
- **Level two** refers to the **group-level data**, capturing variation **between groups** (e.g., differences between classes or schools).


## Two-stage modelling - Level One

### Musiker No. 22

\begin{align*}
Y_{22j} &= a_{22}+b_{22}\textrm{large}_{22j}+\epsilon_{22j} \quad \epsilon_{22j}\sim N(0,\sigma^2)\\ 
\textrm{large}_{j} &= \begin{cases}  
1  & \textrm{if perf-type = Large Ensemble} \\
0  & \textrm{if perf-type = Solo or Small Ensemble}
\end{cases}
\end{align*}

## Musiker No. 22

```{r}
#| echo: true

df_22 <- df |> filter(id == 22)
mod_22 <- lm(na ~ large, df_22)
```

```{r}
tidy_lm(mod_22) |> kable(digits=2) 
```


## Level One für alle Musiker

```{r}
#| fig-height: 4
#| layout-ncol: 2

l_1 <- df |> 
  nest(data = -c(id, instrument_f)) |> 
  mutate(fit = map(data, ~lm(na ~ large, .x)),
         fits = map(fit, tidy)) |> 
  unnest(fits) |>  
  select(c(id, instrument_f), term, estimate, instrument_f) |>  
  pivot_wider(id_cols=c(id, instrument_f), names_from=term, values_from=estimate) |> 
  rename(intercept=3, slope=large)

ggplot(l_1, aes(slope)) +
  geom_histogram(bins=7,fill='white', color='black') 
ggplot(l_1, aes(intercept)) +
  geom_histogram(bins=7, fill='white', color='black')
```


## Two-stage modelling - Level Two 

\begin{align*}
a_{i} & = \alpha_{0}+\alpha_{1}\textrm{Orch}_{i}+u_{i} \\
b_{i} & = \beta_{0}+\beta_{1}\textrm{Orch}_{i}+v_{i}
\end{align*}

## Two-stage modelling - Level Two fit

\begin{align*}
\hat{a}_{i} & = 16.3+1.4\textrm{Orch}_{i}+u_{i} \\
\hat{b}_{i} & = -0.8-1.4\textrm{Orch}_{i}+v_{i}
\end{align*}

## Multilevel-Approach

- Level One:
\begin{equation*}
Y_{ij} = a_{i}+b_{i}\textrm{LargeEns}_{ij}+\epsilon_{ij}
\end{equation*}
- Level Two:
\begin{align*}
a_{i} & = \alpha_{0}+\alpha_{1}\textrm{Orch}_{i}+u_{i} \\
b_{i} & = \beta_{0}+\beta_{1}\textrm{Orch}_{i}+v_{i},
\end{align*}

\begin{align*}
Y_{ij} & = [\alpha_{0}+\alpha_{1}\textrm{Orch}_{i}+\beta_{0}\textrm{LargeEns}_{ij}+\beta_{1}\textrm{Orch}_{i}\textrm{LargeEns}_{ij}] \\
 & \textrm{} + [u_{i}+v_{i}\textrm{LargeEns}_{ij}+\epsilon_{ij}]
\end{align*}

## Model Building - Random Intercepts

- Level One:
$$
Y_{ij} = a_{i}+\epsilon_{ij} \textrm{ where } \epsilon_{ij}\sim N(0,\sigma^2)
$$
- Level Two:
$$
a_{i} = \alpha_{0}+u_{i} \textrm{ where } u_{i}\sim N(0,\sigma_{u}^{2})
$$

$$
Y_{ij}=\alpha_{0}+u_{i}+\epsilon_{ij}
$$

## Model Building - Random Intercepts

```{r}
#| echo: true

mod_0 <- lmer(na ~ 1 + (1|id), data = df)
```


## Intraclass correlation cofficient

\begin{align*}
\rho &=\frac{\textrm{Between-person variability}}{\textrm{Total variability}} \\
 &= \frac{\hat{\sigma}_{u}^{2}}{\hat{\sigma}_{u}^{2}+\hat{\sigma}^2} 
\end{align*}

$$
\hat{\rho} =\frac{5.0}{5.0+22.5} = .182.
$$

## Model Building - Random Slopes and Intercepts

- Level One:
\begin{equation*}
Y_{ij} = a_{i}+b_{i}\textrm{LargeEns}_{ij}+\epsilon_{ij}
\end{equation*}
- Level Two:
\begin{align*}
a_{i} & = \alpha_{0}+u_{i} \\
b_{i} & = \beta_{0}+v_{i}
\end{align*}

\begin{equation*}
Y_{ij}=[\alpha_{0}+\beta_{0}\textrm{LargeEns}_{ij}]+[u_{i}+v_{i}\textrm{LargeEns}_{ij}+\epsilon_{ij}]
\end{equation*}

$$
\epsilon_{ij}\sim N(0,\sigma^2)\quad\left[ \begin{array}{c}
            u_{i} \\ v_{i}
          \end{array}  \right] \sim N \left( \left[
          \begin{array}{c}
            0 \\ 0
          \end{array} \right], \left[
          \begin{array}{cc}
            \sigma_{u}^{2} & \\
            \rho\sigma_{u}\sigma_{v} & \sigma_{v}^{2}
          \end{array} \right] \right). 
$$

## Model Building - Random Slopes and Intercepts

```{r}
#| echo: true

mod_1 <- lmer(na ~ large + (large|id), data = df)
```

## Pseudo $R^2$

\begin{align*}
\textrm{Pseudo }R^2_{L1} &= \frac{\hat{\sigma}^{2}(\textrm{Model A})-\hat{\sigma}^{2}(\textrm{Model B})}{\hat{\sigma}^{2}(\textrm{Model A})} \\
&= \frac{22.5-21.8}{22.5} = 0.031
\end{align*}

## Eine Kovariate auf Level Two hinzufügen

- Level One:
\begin{equation*}
Y_{ij} = a_{i}+b_{i}\textrm{LargeEns}_{ij}+\epsilon_{ij}
\end{equation*}

- Level Two:
\begin{align*}
a_{i} & = \alpha_{0}+\alpha_{1}\textrm{Orch}_{i}+u_{i} \\
b_{i} & = \beta_{0}+\beta_{1}\textrm{Orch}_{i}+v_{i},
\end{align*}
$\epsilon_{ij}\sim N(0,\sigma^2)$ and

$$
 \left[ \begin{array}{c}
            u_{i} \\ v_{i}
          \end{array}  \right] \sim N \left( \left[
          \begin{array}{c}
            0 \\ 0
          \end{array} \right], \left[
          \begin{array}{cc}
            \sigma_{u}^{2} & \\
            \rho\sigma_{u}\sigma_{v} & \sigma_{v}^{2}
          \end{array} \right] \right). 
$$


## Eine Kovariate auf Level Two hinzufügen

```{r}
#| echo: true

mod_2 <- lmer(na ~ large + instrument_f + large:instrument_f + 
                (1|id), data = df)
```

## Finales Modell

```{r}
#| echo: true

mod_3 <- lmer(na ~ large + instrument_f + large:instrument_f + 
                (large|id), data = df)
```

## Modellvergleiche

```{r}
#| echo: true

anova(mod_2, mod_3)
```

## Pooling

```{r}
#| fig-cap: "Two-stage versus MLM"

y_bar <- sum(fixef(mod_3)[c(1,3)])
l_1 <- l_1 |> 
  mutate(intercept_mlm = ranef(mod_3)$id[,1] + y_bar,
         n = df |> group_by(id) |> summarize(n = n()) |> pull(n))
l_1 |> select(id, intercept, intercept_mlm) |> 
  pivot_longer(-id, names_to = 'Modell') |> 
  ggplot(aes(value, id)) +
  geom_vline(xintercept = y_bar, linetype = 'dashed') +
  geom_line(aes(group=id), linetype='dotted') +
  geom_point(aes(color = Modell)) +
  scale_x_continuous("Intercept")
```

