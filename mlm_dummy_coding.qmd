# Integration von nominalen Variablen 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
library(codingMatrices)
```


```{r defs_dummy}
N <- 30
set.seed(123)
height <- tibble::tibble(cm = rnorm(2*N,rep(c(180,167),each=N),10),
                 gender = factor(rep(c('m','f'),each=N)))
mu_s <- height |> dplyr::group_by(gender) |> dplyr::summarize(m = mean(cm))
mu_f <- mu_s |> dplyr::filter(gender == 'f') |> pull() |> round(2) 
mu_m <- mu_s |> dplyr::filter(gender == 'm') |> pull() |> round(2)
Delta <- round(mu_m - mu_f, 2)
N <- 20
K <- 4
set.seed(1)
data <- tibble::tibble(
  group = gl(K, N, labels = c('A','B','C','D')),
  rt = rnorm(K * N, mean = rep(seq(500,800,100), each=N), sd = 50)
)
```

In den bishergen Betrachtungen zur multiplen linearen Regression sind nur kontinuierliche, beziehungsweise metrische Variablen als Prädiktorvariablen verwendet worden. Im Folgenden werden gezeigt, dass mit einem kleinem Trick genauso nominale Variablen, also z.B. TREATMENT versus CONTROL, in das Modell integriert werden können. Dabei wird sich zeigen, dass dadurch im Prinzip keine fundamental neuen Konzept erlernt werden müssen. Die bisherige Betrachtungsweise von Modellparametern als Steigungen von Geraden bzw. Ebenen ist immer noch ausreichend um die Modellparameter interpretieren zu können. Der Einstieg beginnt zunächst mit dem kanonischen Vergleich von zwei unabhängigen Gruppen.

## Vergleich von zwei unabhängigen Gruppen

Es sollen im Folgenden die Unterschiede zwischen Männern und Frauen in Bezug auf die Körpergröße statistisch untersuchen werden. In @fig-mlm-dummy-gender ist ein hypothetischer Datensatz von Körpergrößen von Frauen und Männern abgebildet. Der Datensatz ist so konstruiert, dass Männer im Mittel größer als Frauen sind.

```{r}
#| fig-cap: "Simulierte Daten: Verteilung von Körpergrößen nach Geschlecht für Männer (m) und Frauen (f). Rote Punkte sind individuelle Datenpunkte"
#| label: fig-mlm-dummy-gender

ggplot(height, aes(gender,cm)) + 
  see::geom_violindot(size_dots=20, fill_dots='red') +
  labs(x = 'Gender', y = 'Körpergröße[cm]') 
```

In @tbl-mlm-dummy-gender ist ein Ausschnitt der Daten tabellarisch dargestellt. Die Daten sind unterteilt in zwei Datenspalten. In der ersten Spalte stehen die Körpergrößen, während in der zweiten Spalte die nominale Variable `gender` steht. Als eine nominale Variable nimmt sie entweder den Wert `m` für Männer oder `f` für Frauen an.

```{r}
#| label: tbl-mlm-dummy-gender
#| tbl-cap: "Ausschnitt aus den Daten."

height[c(1:3,31:33),] |> 
  knitr::kable(
    booktabs = T,
    caption = "Ausschnitt aus den Daten",
    digits = 1,
    linesep = ''
  )
```

In @tbl-mlm-dummy-gender-desc sind die deskriptiven Statistiken der Körpergrößendaten abgebildet. Hier ist zu erkennen, dass die Standardabweichung in etwa die gleiche Größen haben, während die Mittelwerte mit $\bar{y}_{f} = 169$ bzw. $\bar{y}_{m} = 180$ entsprechend unterschiedlich sind.

```{r}
#| label: tbl-mlm-dummy-gender-desc
#| tbl-cap: "Deskriptive Statistiken der Körpergrößendaten."
 
x_hat <- height |> dplyr::group_by(gender) |>
  dplyr::summarize(m = round(mean(cm),1), sd = round(sd(cm),1))
knitr::kable(x_hat, booktabs=TRUE,
             caption = "Deskriptive Werte",
             col.names = c('Gender', "$\\bar{x}$", "SD"),
             escape = FALSE)
```

Um die Repräsentation von nominalen Variablen in einem lineare Modell zu verstehen ist allerdings zunächst noch eine kurze Detour notwendig um zu verstehen wie nominale Werte in `R` repräsewerden.

## Nominale Variablen in `R` 

Nominale Variablen werden in `R` mittels eines eigenem Datentyps `factor` repräsentiert. Erstellt werden kann ein Faktor mittels der `factor()`-Funktion. Die Funktion hat drei wichtige Parameter. Der erste Parameter bezeichnet die Werte, der zweite die möglichen Faktorstufen (`levels`) und der dritte Parameter die dazugehörigen Bezeichnungen (`labels`). Ein einfaches Beispiel mit einer nominalen Variablen die die beiden Werte `m` und `f` annehmen kann, kann in `R` wie folgt umgesetzt werden. 

```{r}
#| echo: true
gender <- factor(c(0,0,1,1),
                 levels = c(0,1),
                 labels = c('m','f'))
gender
```

D.h. ein Faktor besteht aus einem Datenvektor mit den Elemente $(0,0,1,1)$. Auf diesem Datenvektor werden die Stufen `levels` mit $0$ und $1$ und definiert und die dazugehörigen `labels` mit $m$ und $f$ definiert. Wenn die neue Variable `gender` aufgerufen wird, gibt `R` einen Vektor mit den entsprechenden `labels` zurück. Zusätzlich gibt `R` die möglichen `labels` auch noch einmal explizit als `Levels` an.

Wenn der Parameter `levels` nicht angegeben wird, dann extrahiert `factor()` die eineindeutigen Werte selbst und führt die Abbildung auf die `labels` entsprechend den Sortierungsregeln von `R` aus.

```{r}
#| echo: true
gender <- factor(c(0,0,1,1),
                 labels = c('m','f'))
gender
str(gender)
```

Dabei muss darauf geachtet werden, dass die Abbildung auch tatsächlich diejenige ist, die gewünscht ist.

```{r}
#| echo: true
gender <- factor(c(0,0,1,1),
                 labels = c('f','m'))
gender
```

Daher ist es fast immer sinnvoll `labels` und `levels` immer zusammen zu spezifizieren. Wenn die Parameter nicht angegeben werden, dann führt `factor` die Abbildung automatisch durch und für die `labels` werden die Datenwerte übernommen. 

```{r}
#| echo: true
gender <- factor(c(0,0,1,1))
gender
```

::: {.callout-warning}
Achtung, die Variable `gender` sieht zwar aus wie ein numerischer Vektor, sie ist es aber nicht.

```{r}
#| echo: true

is.numeric(gender)
gender + 1
```

Intern wird eine Faktorvariable von `R` zwar als ein numerischer Vektor abgelegt. Aber die *sichtbaren* Werte sind nun die Zeichenketten der `labels` definiert, die daher auch angezeigt werden. Die interne numerische Repräsentation muss auch nicht mehr den ursprünglichen Datenwerten entsprechen.

```{r}
#| echo: true

as.numeric(gender)
```

Die Datenwerte waren ursprünglich $(0,1)$ und sind jetzt auf $(1,2)$ abgebildet worden. Dazu sei noch mal an die Eigenschaft von nominalen Variablen erinnert. Nominale Variablen sind voneinander unterscheidbare Werte die jedoch in **keiner Reihenfolge zueinander** stehen.
:::

Die automatische Konvertierung von `factor()` funktioniert am intuitivsten mit Zeichenkettenvektoren.

```{r}
#| echo: true

gender <- factor(c('m','f','m','f'))
gender
str(gender)
```

`factor()` ermittelt zunächst die eineindeutigen Werte und sortiert diese dann entsprechend des Typen. In diesem Fall wird die Zeichenkette alphabetisch sortiert. Dann erfolgt die Abbildung der Werte auf die `labels`. Dies führt in diesem Fall dazu, dass die Werte `m` intern den Wert $2$ zugeordnet bekommen, obwohl der erste Wert in den Daten `m` ist. Diese Sortierung der Daten wird später noch einmal von Bedeutung werden.

Die Reihenfolge der Faktorstufen wird durch die Angabe des `levels` explizit bestimmt werden und ist daher auch noch mal ein Argument dafür das `levels`-Argument zu verwenden.

```{r}
#| echo: true

gender <- factor(c('m','f','m','f'),
                 levels = c('m','f'))
gender
str(gender)
gender <- factor(c('m','f','m','f'),
                 levels = c('f','m'))
gender
str(gender)
```

::: {.callout-tip}
Im Package `forcats` sind eine Reihe von Funktionen hinterlegt, mit denen die Eigenschaften von `factor`-Variablen einfach manipuliert werden können. Zum Beispiel, wenn die Reihenfolge von Faktorstufen geändert werden soll kann die Funktion `fct_relevel()` verwendet.

```{r}
gender <- factor(c('m','f','m','f'),
                 levels = c('f','m'))
gender
forcats::fct_relevel(gender, 'm')
```

Wie immer ist die ausführliche Dokumentation der Funktionen mit Beispiel die erste Addresse, wenn Probleme im Zusammenhang mit `factor`-Variablen auftauchen.
:::

::: {.callout-tip}
Viele Funktionen in `R`, wie z.B. `lm()`, transformieren Vektoren mit Zeichenketten automatisch in einen `factor()` um. Wird in `lm()` in der Formel beispielsweise `y ~ gender` benutzt und `gender` ist eine Datenspalte die aus den Zeichenketten `c('m','m','f','f')` besteht, dann ruft `lm()` intern die Funktion `factor()` für diese Daten auf und führt dann die Berechnung mit der resultierenden Faktorvariable durch.

Dies erleichtert natürlich oft den Umgang mit den Daten, hat aber den Nachteil das immer klar sein muss, dass die automatische Konvertierung auch tatsächlich diejenige ist, dich gewünscht ist.
:::

## Vergleich von zwei Gruppen (continued)

Zurück zum Körpergrößenvergleich. Normalerweise würden die Unterschiede zwischen den beiden Gruppen mit einem klassischen t-Test für unabhängige Stichproben unter der Annahme der Varianzgleichheit untersucht werden. In `R` kann dazu die `t.test()`-Funktion verwendet werden.

```{r}
#| echo: true

t.test(cm ~ gender, data=height, var.equal=T)
```

Dem Output von `t.test()` zeigt an, dass der Unterschied zwischen den Gruppen statistisch Signifikant ist.

Der Anwendung des t-Tests unterliegt, wie bei allen statistischen Tests, ein Modell. Bei gleich großen Gruppen $(n_w = n_m)$ folgt die übliche Formulierung.

\begin{equation}
\begin{aligned}
Y_{if} &= \mu_{f} + \epsilon_{if}, \quad \epsilon_{if} \sim \mathcal{N}(0,\sigma^2) \\
Y_{im} &= \mu_{m} + \epsilon_{im}, \quad \epsilon_{im} \sim \mathcal{N}(0,\sigma^2)
\end{aligned}
\label{eq-mlm-dummy-tTest-model}
\end{equation}

Dem Modell folgend, sollten beide Gruppen normalverteilt sein mit gleicher Varianz $\sigma^2$ und den Erwartungswerten $\mu_f$ für die weiblichen Teilnehmerinnen und $\mu_m$ für die männlichen Teilnehmer. Die Hypothesen beim t-Test sind unter der Nullhypothese die übliche Annahme der Gleichheit der Mittelwerte.

\begin{equation*}
H_0: \delta = 0 
\end{equation*}

Daraus resultiert die folgende Teststatistik:

\begin{equation}
t = \frac{\bar{y}_m - \bar{y}_w}{\sqrt{\frac{s_m^2 + s_w^2}{2}}\sqrt{\frac{2}{n}}}
\label{eq-tTest}
\end{equation}

Unter der $H_0$ folgt die Referenz- bzw. Stichprobenverteilung einer t-Verteilung mit $N - 2$ Freiheitsgraden.

\begin{equation*}
t|H_0 \sim t_{df=2n-2}
\end{equation*}

Die entsprechende Dichtefunktion hat die folgende graphische Darstellung (siehe @fig-mlm-dummy-tDist). 

```{r}
#| label: fig-mlm-dummy-tDist
#| fig-cap: "Dichtefunktion der t-Verteilung mit $df=58$ Freiheitsgraden."

n_fig_pts <- 100
t_dist <- tibble::tibble(
  x = seq(-3,3,length.out=n_fig_pts),
  t = dt(x,58)
) 
ggplot(t_dist, aes(x,t)) +
  geom_ribbon(aes(ymin=0, ymax=t), alpha=0.3, fill='red') +
  geom_line() + 
  labs(x = 't-Werte', y = 'Dichte') 
```

Soweit nicht Neues. Im Beispiel wurde ein $t$-Wert von $t = -4.6$ beobachtet der derart extrem ist, dass er in @fig-mlm-dummy-tDist schon gar nicht mehr dargestellt wird. Im Folgenden soll nun gezeigt werden, das das gleiche Ergebnis mittels eines linearen Modellansatzes berechnet werden. 

Das bekannte lineares Modell hat im einfachsten Fall die folgende Form für eine einfache Regression:

\begin{equation}
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i 
\label{eq-mlm-dummy-lm}
\end{equation}

Die Frage ist nun, ob dieses Modell auf die Formeln \eqref{eq-mlm-dummy-tTest-model} und \eqref{eq-tTest} abgebildet werden kann?

Tatsächlich gibt es eine relativ einfache Möglichkeit mittels eines Ansatzes mit sogenannten **Indikatorvariablen** (oft auch als **Dummyvariablen** bezeichnet). Die $Y$-Variable, die abhängige Variable ist die gleiche wie beim t-Test. Im Beispiel also die Körpergrößen. Daher bleibt für die $X$-Variable nur noch die unabhängige Variable, in dem Beispiel entsprechend die `gender`-Variable. Jetzt entsteht allerdings das Problem,  dass $x_i$ nur zwei verschiedene Werte annehmen kann. Diese sind im Beispiel entweder `m` oder `f` . Offensichtlich macht die folgende Formulierung wenig Sinn, da der Steigungskoeffizient $\beta_1$ nicht mit einem *Wert* `m` multipliziert werden kann.

\begin{equation*}
y_i = \beta_0 + \beta_1 \cdot m + \epsilon_i
\end{equation*}

Die Lösung besteht nun darin $X_i$, also im Beispiel `m`, in eine Indikatorvariable umzuwandeln. Eine Indikatorvariable kann nur zwei verschiedene numerische Werte einnehmen. Entweder den Wert $0$ oder den Wert $1$ (es gibt auch noch andere Systeme, aber diese seien im Weiteren ignoriert). Dies führt zur folgenden Formel.

\begin{align*}
y_i &= \beta_0 + \beta_1 \cdot 0 + \epsilon_i \quad \text{oder} \\
y_i &= \beta_0 + \beta_1 \cdot 1 + \epsilon_i \quad
\end{align*}

Diese Formulierung ist nun wieder sinnvoll, da $\beta_1$ mit einem numerischen Wert multipliziert wird. Nun wird noch eine Abbildung von den Werten `m` und `f` auf die Werte $0$ und $1$ benötigt, um die Gruppenzugehörigkeit zu kodieren. Dazu wird eine sogenannte **Referenzstufe** festgelegt. Der Referenzstufe wird der Wert $x_i = 0$ zugewiesen. Im Beispiel sei die Referenzstufe die weibliche Gruppe. Dies führt zu der folgenden Vorschrift:

| Wenn der Wert $y_i$ aus der Gruppe weiblich (`f`) kommt, dann wird $x_i = 0$ gesetzt
| Wenn der Wert $y_i$ aus der Gruppe männlich (`m`) kommt, dann wird $x_i = 1$ gesetzt.

Formal:

\begin{equation*}
x_i = \begin{cases}
0\text{ wenn weiblich}\\
1\text{ wenn männlich}
\end{cases} 
\end{equation*}

Was jetzt noch fehlt, ist eine sinnvolle Belegung des $Y$-Achsenabschnittes $\beta_0$ und des *Steigungskoeffizienten* $\beta_1$ in Formel \eqref{eq-mlm-dummy-lm} im Zusammenhang mit dem Indikatorvariablenansatz. In der weiblichen Gruppe mit $x_i = 0$ folgt für \eqref{eq-mlm-dummy-lm}:

\begin{equation*}
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i = \beta_0 + \beta_1 \cdot 0 = \beta_0 + \epsilon
\end{equation*}

Daraus folgt, dass für alle $Y$-Werte aus der Referenzgruppe, im Beispiel aus der Gruppe `f`, anhand des Modells der gleiche Wert $\hat{y}_i$ vorhergesagt wird. Formal:

\begin{equation*}
\hat{y}_{f} = \beta_0
\end{equation*}

Bezogen auf die übliche Interpretation von $\beta_0$ als der $Y$-Achsenabschnitt beschreibt der $Y$-Achsenabschnitt den Vorhersagewert $\hat{y}_i$ für all Werte aus der Referenzstufe. Die Frage ist nun welcher Wert $\hat{y}_{f}$ wird vorhergesagt? Es lässt sich zeigen (siehe unten), dass $\beta_0$ den Mittelwert der Referenzgruppe annimmt. Rein inituitiv macht das auch Sinn. Bei der linearen Regression werden die Modellkoeffizienten so gewählt, dass die Quadrate der Abweichungen, die Residuen, von der Geraden (Ebene) minimiert werden. In diesem Fall kommen alle Werte aus der Referenzgruppe, die durch die Wahl der Indikatorvariable ($=0$) genau auf der $Y$-Achse liegen. Derjenige Wert der die Abweichungen minimiert ist der Mittelwert $\bar{y}_{f}$ der Gruppe. Wenn nun noch die Annahmen der linearen Regression bezüglich der Residuen dazu genommen werden, folgt:

\begin{equation*}
\hat{y}_{f} = \beta_0 + \epsilon_i, \quad \epsilon \sim \mathcal{N}(0,\sigma)
\end{equation*}

Diese Formulierung ist genau gleich derjenigen die im t-Test-Ansatz verwendet wurde (siehe \eqref{eq-mlm-dummy-tTest-model}). Formal wird also der Mittelwert $\mu_f$ mit dem $Y$-Achsenabschnitt $\beta_0$ identifiziert: 

\begin{equation*}
\beta_0 = \mu_f
\end{equation*}

Da die Indikatorvariable $X_i$ für Werte aus der Gruppe `f` den Wert $0$ zugewiesen bekommt, wird für $Y$-Werte aus der Gruppe `m` entsprechend der Indikatorwert $1$ zugewiesen. Formal führt dies also zu: 

\begin{equation*}
Y_i = \beta_0 + \beta_1 X_i = \beta_0 + \beta_1 \cdot 1 = \beta_0 + \beta_1
\end{equation*}

D.h. der vorhergesagte Wert $\hat{y}_i$ ist wieder nur eine Konstante für alle Werte von $i$, die sich jetzt aber aus beiden Modellparametern $\beta_0$ und $\beta_1$ zusammensetzt. Mit der gleichen Argumentation wie oben, ist der Wert der unter der Randbedingung des Minimums der mittleren, quadrierten Abweichungen der Mittelwert der Werte aus der Gruppe `m`. Daraus folgt, zusammen mit der Festsetzung von eben $\beta_0 = \bar{y}_{f}:

\begin{align*}
\bar{y}_{m} &= \beta_0 + \beta_1 = \bar{y}_{f} + \beta_1 \\
\Leftrightarrow \beta_1 &= \bar{y}_{m} - \bar{y}_{f}
\end{align*}

Daraus folgt, dass der Modellparameter $\beta_1$, der *Steigungskoeffizient*, den Unterschied der Mittelwerten $\Delta = \bar{y}_{m} - \bar{y}_{f}$ zwischen den beiden Gruppen modelliert.

Insgesamt resultieren daraus die folgenden Zusammenhänge wenn noch die Annnahme der normalverteilten Residuen $\epsilon_i \sim \mathcal{N}(0,\sigma)$ dazugenommen werden.

\begin{align*}
y_i &= \beta_0 + \beta_1 \cdot 0 + \epsilon_i = \mu_f + \Delta \cdot 0 + \epsilon_i = \mu_f + \epsilon_i \\
y_i &= \beta_0 + \beta_1 \cdot 1 + \epsilon_i = \mu_f + \Delta \cdot 1 + \epsilon_i = \mu_f + (\mu_m - \mu_f) + \epsilon_i = \mu_m + \epsilon_i
\end{align*}

Insgesamt zeigt dies, dass mittels der Indikatorvariablen eine Abbildung des einfachen linearen Regressionsmodells auf den t-Test (Formel \eqref{eq-mlm-dummy-tTest-model}) möglich ist. Der t-Test ist kann daher als *Spezialfall* der einfachen Regression bzw. des einfachen linearen Modells betrachtet werden.

Die Voraussetzungen im t-Test der Normalverteilung der Werte, bedeutet hierbei letztendlich nichts anderes, als die Normalverteilung der Residuen im Regressionsmodell. Die Voraussetzung der Varianzgleichheit ist dabei nichts anderes als die Homoskedastizität im linearen Modell. 

Um den Ansatz einmal konkret zu machen, sei zum Beispiel den ersten Wert aus @tbl-mlm-dummy-gender $y_{1m} = 174.4$ in seine Komponenten aufgeteilt. Es folgt $\Delta = `r mu_m` - `r mu_f` = `r mu_m - mu_f`, e_{1m} = 174.4 - `r mu_m`= -5.13$ und $x_{1m} = 1$.

\begin{align*}
y_{1m} = 174.4 &= \mu_f + \Delta \cdot x_{1m} + e_{1m} \\
&= 168.78 + 10.74 \cdot 1 - 5.13 
\end{align*}

Das Gleiche für den ersten weiblichen Wert in @tbl-mlm-dummy-gender $y_{1f} = 171.3, \Delta = `r Delta`, e_{1f} = 171.3 - `r mu_f` = 2.52$ und $x_{1f} = 0$.

\begin{align*}
y_{1f} = 171.3 &= \mu_f + \Delta \cdot x_{1f} + e_{1f} \\
&= 168.78 + 10.74 \cdot 0 + 2.52
\end{align*}

Zusammenfassend, mit Hilfe von Indikatorvariable ist eine Möglichkeit gefunden worden, mit der das t-Test Modell auf das lineare Modell abgebildet werden kann.

In @fig-mlm-dummy-dummymodel ist die *Regressionsgerade* für das Modell zusammen mit den Rohdaten mittels der Indikatorvariablen abgetragen.

```{r}
#| label: fig-mlm-dummy-dummymodel
#| fig-cap: "Regressionsgerade des einfachen linearen Modell mit Indikatorvariablen."

mod <- lm(cm ~ gender, height)
height |> dplyr::mutate(gender_num = dplyr::if_else(gender == 'm', 1, 0)) |> 
  ggplot(aes(gender_num, cm)) +
  geom_point() +
  geom_abline(intercept = coef(mod)[1], slope = coef(mod)[2]) +
  scale_x_continuous("Gender", breaks = 0:1)
```

In @fig-mlm-dummy-dummymodel ist zu erkennen, wie der Steigungskoeffizient $\beta_0$ tatsächlich als eine Steigung zu interpretieren ist. Da die beiden Gruppen um eine $X$-Einheit von $1$ voneinander entfernt liegen, ist der Unterschied zwischen den beiden Gruppenmittelwerten genau die *Steigung* der Gerade.

### Verwendung von Indikatorvariablen in `lm()`

In `lm()` kann eine nominale Variable genauso wie die bisherigen kontinuierlichen Variablen verwendet werden. 

```{r}
#| eval: false
#| echo: true

mod <- lm(cm ~ gender, height)
```
```{r}
mod <- lm(cm ~ gender, height)
lm_tbl_knitr(mod)
```

`lm()` gibt die Faktorstufe nach dem Namen der Faktorvariable an. Im Beispiel steht `genderm` für Stufe `m` aus dem Faktor `gender`. Der Wert für $\hat{\beta}_0$ entspricht genau demjenigen in @tbl-mlm-dummy-gender-desc. Weiterhin, der Wert für den t-Wert für $\beta_1 = \Delta$, ist zu beobachten, dass der Wert exakt mit t-Wert übereinstimmt, der mittels des t-Test berechnet wurde . Lediglich das Vorzeichen ist in der anderen Richtung was aber nur damit zusammenhängt welcher Gruppenmittelwert von dem anderen Gruppenmittelwert subtrahiert wird.

Sei nun noch das Konfidenzintervall für den *Steigungskoeffizienten* $\beta_1$ betrachtet:

```{r}
#| echo: true

confint(mod)
```

Wieder stimmt, bis auf das Vorzeichen, das Konfidenzintervall exakt mit demjenigen des t-Test überein. 

Damit lässt sich auf Basis der vorhergehenden Ausführung auch direkt nachvollziehen wie `lm()` intern mit nominalen Variablen umgeht. Bei der Modellevaluierung erkennt `lm()` `gender` als nominale Variable und ersetzt es durch eine Indiktaorvariable (siehe @tbl-mlm-dummy-dummy-lm).

```{r}
#| label: tbl-mlm-dummy-dummy-lm
#| tbl-cap: "Repräsentation der Faktorvariablen"

cbind(height, (0:1)[as.numeric(height$gender)])[c(1:3,41:43),] |> 
  knitr::kable(booktabs = TRUE,
               col.names = c('cm', 'gender', '$x_1$'),
               row.names = F,
               escape = F,
               linesep = '',
               digits = 2)
```

`lm()` ersetzt `gender` durch die Indikatorvariable, also durch Zahlenwerte, und dann kann einfach wieder die bekannte Maschinerie des linearen Modell angeworfen werden. Als letzten Schritt nun noch eine Betrachtung des resultierenden Residuenplots (siehe @fig-mlm-dummy-resid-plot).

```{r}
#| label: fig-mlm-dummy-resid-plot 
#| fig-cap: "Residuenplot des Körpergrößenmodells"

height |> dplyr::mutate(y_hat = predict(mod),
                 e_hat = resid(mod)) |> 
  ggplot(aes(y_hat, e_hat)) +
  geom_point() +
  geom_hline(yintercept = 0, col = 'red', linetype = 'dashed') +
  labs(x = expression(paste('Vorhergesagte Werte ', hat(y)[i])),
       y = expression(paste('Residuen ', hat(e)[i]))) 
```

Der Plot sieht etwas speziell aus, da die Residuen nur für zwei vorhergesagte Werte $\hat{y}_i$ angezeigt werden. Diese spezielle Form, ist letztendlich ein Ergebnis dessen, dass nur zwei verschiedene $\hat{y}$-Werte vorhergesagt werden.

## Dummyvariablen bei mehr als zwei Faktorstufen

Nachdem ein Beispiel mit zwei Faktorstufen durchgegangen, stellt sich die Frage ob dieser Ansatz auch für mehr als zwei Faktorstufen einer nominalen Variablen möglich ist? Sei dazu, zunächst ein einfaches synthetisches Beispiel betrachtet (siehe @fig-mlm-dummy-reaction-01)

```{r}
#| label: fig-mlm-dummy-reaction-01
#| fig-cap: "Ein Reaktionszeitexperiment mit vier Stufen A, B, C und D"

ggplot(data, aes(group, rt)) + geom_boxplot() +
  geom_point(alpha=.1, col='red') +
  labs(x = 'Gruppe', y = 'Reaktionszeit') 
```

Es wurde ein Datensatz ähnlich einem Reaktionszeitexperiment erstellt, bei dem Probanden unter vier verschiedenen Konditionen $A,B,C$ und $D$ beobachtet wurden. Die deskriptive Daten sind @tbl-mlm-dummy-reaction-01 einsehbar. Der Einfachheit halber sind die Daten so generiert, dass die Gruppenmittelwert von $A$ nach $D$ immer größer werden.

```{r}
#| label: tbl-mlm-dummy-reaction-01
#| tbl-cap: "Gruppenmittelwerte, Standardabweichung und Unterschiede zu Stufe A"
tmp <- data |> dplyr::group_by(group) |>
  dplyr::summarize(y_hat = mean(rt), sd = sd(rt))
mat <-  t(matrix(c(-1,1,0,0,-1,0,1,0,-1,0,0,1),nr=4))
tmp |> dplyr::mutate(delta = c(NA,mat %*% y_hat)) |>
  knitr::kable(booktabs = TRUE,
               col.names = c('Gruppe','$\\bar{y}_j$', '$s_j$', '$\\Delta_{j-A}$'),
               digits = 2,
               escape=F)
```

Die letzten Spalte zeigt die Abweichungen der Gruppenmittelwerte der Gruppen $B,C,D$ von der Gruppe $A$ an. Das sollte jetzt auch schon direkt eine Möglichkeit aufzeigen, wie diese Daten in ein lineares Modell überführt werden können. Dazu wird wiederum eine Referenzgruppe festgelegt, z.B. Gruppe $A$. Anstatt nun nur ein einzelnes $Delta$ zu berechnen, werden drei $\Delta$s eingeführt. Die $\Delta$s repräsentieren dabei jeweils die Abweichungen der spezifischen Faktorstufen von der Referenzstufe. Für jedes dieser $\Delta_i$s wird nun eine Indikatorvariable $x_{ij}$ eingeführt. Die Indikatorvariable indiziert jeweils über die $0$ und $1$ die Zugehörig des $Y_i$ Wertes zu der Gruppe. Ein lineares Modell lässt nun wie folgt formulieren:

\begin{equation*}
y_i = \mu_A + \Delta_{B-A} x_{1i} + \Delta_{C-A} x_{2i} + \Delta_{D-A} x_{3i} + \epsilon_i
\end{equation*}

Für die Abweichungen von der Referenzgruppe sind drei Indikatorvarialben $x_{1i}, x_{2i}$ und $x_{3i}$ eingeführt worden. Die Modellkoeffizienten bilden somit jeweils die Abweichungen der Mittelwerte von Gruppe $A$ ab. Dies führt dann zu der folgenden Kodierung der Daten (siehe @tbl-mlm-dummy-reaction-02).

```{r}
#| label: tbl-mlm-dummy-reaction-02
#| tbl-cap: "Kodierung der Dummyvariablen für das Reaktionszeitexperiment."

mat <- contrasts(data$group)
colnames(mat) <- c('x1','x2','x3')
knitr::kable(mat, booktabs=TRUE) |>
  kableExtra::kable_styling(position = 'center')
```

Wenn ein Wert $Y_i$ aus Gruppe $A$, der Referenzgruppe, kommt, dann werden die dire Indikatorvariablen $X_{1i}, X_{2i}, X_{3i}$ mit den Werten $(0,0,0)$ belegt. Wenn ein Wert $Y_i$ aus Gruppe $B$ kommt, dann mit $(1,0,0)$ belegt usw..

Dieser Ansatz lässt sich wie folgt Verallgemeinern (siehe @tbl-mlm-dummy-coding).

|     | $x_1$ | $x_2$ | $\ldots$ | $x_{K-1}$ | 
| --- | --- | --- | --- | --- |
| Referenz ($j=1$) | 0 | 0 |  | 0 | 
| $j=2$ | 1 | 0 | $\ldots$  | 0 | 
| $j=3$ | 0 | 1 | $\ldots$  | 0 |
| $j=K$ | 0 | 0 | $\ldots$  | 1 | 

: Indikatorkodierung bei $k$-Faktorstufen. {#tbl-mlm-dummy-coding}

Bei Integration einer nominalen Variable mit $K$ Faktorstufen werden $K-1$ Indikatorvariablen $X_1, X_2, \ldots, X_{K-1}$ benötigt. Eine der Faktorstufen wird als Referenzstufe definiert. Die Indikatorvariablen $X_1$ bis $X_{K-1}$ kodieren dann die Abweichungen der anderen Stufen von der Referenzstufe. Diese Art der Kodierung wird in der Literatur auch als *treatment* Kodierung bezeichnet. Die Indikatorvariablen werden oft auch als Dummyvariablen bezeichnet

::: {#def-dummy}
## Indikator-/Dummyvariablen \index{Dummyvariablen} \index{Indikatorvariablen}

Eine Dummyvariable, auch Indikatorvariable genannt, ist eine binäre Prädiktorvariable die nur die Werte $0$ oder $1$ annimmt. Mit einer Dummyvariable kann die An- bzw. Abwesenheit eines kategorialen Effekts modelliert werden.
:::

Das lineare Modell in unserem Reaktionszeitexperiment ergibt den folgenden Modellfit (siehe @tbl-mlm-dummy-reaction-03).

```{r}
#| echo: true
#| eval: false

mod <- lm(rt ~ group, data)
```

```{r}
#| label: tbl-mlm-dummy-reaction-03
#| tbl-cap: "Koeffizientenmatrix für die Dummykodierung des Reaktionszeitexperiments."

mod <- lm(rt ~ group, data)
lm_tbl_knitr(mod)
```

Als Steigungskoeffizienten werden genau die Abweichungen der Mittelwerte der jeweiligen Faktorstufen von der Referenzstufe erhalten (siehe @tbl-mlm-dummy-reaction-02). Zusammenfassend ermöglicht somit die Verwendung der Indikatorvarialben die Möglichkeit eine nominale Variable mit beliebig vielen Faktorstufen in einem linearen Modell abzubilden. Damit ist dann auch die Unterscheidung in ANOVA und linear Regression als überfällig anzusehen, da beide letztendlich auf dem gleichem Modell, nämlich dem linearen Modell, beruhen.

Es folgt wieder eine kurze Betrachtung des Residuenplots (siehe @fig-mlm-dummy-reaction-02).

```{r}
#| label: fig-mlm-dummy-reaction-02
#| fig-cap: "Residuenplot für das Modell der Reaktionszeitdaten."

data |> mutate(y_hat = predict(mod), e_i = resid(mod)) |> 
ggplot(aes(y_hat, e_i)) + 
  geom_point() +
  geom_hline(yintercept = 0, col = 'red', linetype = 'dashed') +
  labs(x = expression(paste('Vorhergesagte Werte ', hat(y)[i])),
       y = expression(paste('Residuen ', hat(e)[i]))) 
```

Es ist wieder zu sehen, dass nur vier verschiedene $\hat{y}_i$ Werte vorhergesagt werden, nämlich die Mittelwerte $\bar{y}_k$ in den jeweiligen $K$ Gruppen. Die Residuen $e_i$ streuen daher nur um diese vorhergesagten Werte.

In `R` kann die Dummy-Kodierung explizit mittels der `dummy.coef()`-Funktion erhalten werden.

```{r}
#| echo: true

dummy.coef(mod)
```

## Kombination von kontinuierlichen und nominalen Prädiktorvariablen

Da durch die Verwendung von Indikatorvariablen die Unterscheidung zwischen nominalen und kontinuierlichen Variablen in der Modellierung aufgehoben wird, können beide Typen von Variablen in einem Modell integriert werden. In @fig-mlm-dummy-combi-01 ist ein hypothetischen Zusammenhang zwischen der körperlichen Leistung dem Trainingsalter und dem Gender abgetragen.

```{r}
#| label: fig-mlm-dummy-combi-01
#| fig-cap: "Hypothetische Leistungsentwicklung in Abhängigkeit vom Alter und Gender"

N <- 50
set.seed(1)
lew <- tibble::tibble(
  ta = sample(10, N, replace=T),
  gender = sample(0:1, N, replace=T),
  perf = rnorm(N, 30, 3) + 2 * ta + 10*gender,
  gender_f = factor(gender, levels = c(1,0), labels=c('f','m'))
)
ggplot(lew, aes(ta, perf, color = gender_f)) +
  geom_point(size=2) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  scale_color_discrete('Gender') 
```

Das Trainingsalter (`ta`) geht wie bisher als kontinuierliche Variable in das Modell ein, während `gender` als nominale Variable über eine Dummyvariable modelliert wird. Weibliche Teilnehmerinnen der Studie werden als Referenzstufe festgelegt. Das resultiert dann in dem folgenden linearen Modell:

\begin{align*}
Y_i &= \beta_{ta = 0,x_{1i}=0} + \Delta_m \times x_{1i} + \beta_{ta} \times ta + \epsilon_i \\
x_1 &= 
\begin{cases}
0\text{ wenn weiblich}\\
1\text{ wenn männlich}
\end{cases} \\
\end{align*}

Modellieren mit `lm()` führt dann zu:

```{r}
#| echo: true

mod <- lm(perf ~ gender_f + ta, lew)
```
```{r}
#| label: tbl-mlm-dummy-combi-01
#| tbl-cap: "Modellfit für das Modell mit Trainingsalter und Gender."

broom::tidy(mod) |> knitr::kable(
  booktabs = TRUE,
  digits = 3,
)
```

Die Variable `gender` ist durch `lm()` als Dummyvariable definiert worden und der Steigungskoeffizient $\beta_1=$ `gender_fm` kodiert den mittleren Unterschied zwischen männlichen und weiblichen Studienteilnehmerinnen. Der $Y$-Achsenabschnitt kodiert daher die Leistung von weiblichen Teilnehmerinnen mit einem Trainingsalter von `ta`$= 0$. (Warum?). Der Graph der Datenpunkte zusammen mit den resultierenden Modellgraphen der vorhergesagten $\hat{y}_i$ zeigt an wie das Modell die Daten modelliert (siehe @fig-mlm-dummy-combi-02).

```{r}
#| label: fig-mlm-dummy-combi-02
#| fig-cap: "Leistungsentwicklung in Abhängigkeit vom Alter und Gender"

new_lew <- data.frame(ta = c(1,10,1,10),
           gender_f = factor(c(0,0,1,1),
                             levels=0:1,
                             labels=c('f','m')))
new_lew <- new_lew |> dplyr::mutate(y_hat=predict(mod, new_lew))
ggplot(lew, aes(ta, perf, color = gender_f)) + geom_point() +
  geom_line(data = new_lew, aes(y = y_hat)) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  scale_color_discrete('Gender') 
```

Es wurden zwei Geraden modelliert. Eine Gerade für die weiblichen und eine für die männlichen Teilnehmer. Die Geraden sind parallel zueinander und um den Wert von $\beta_1$ gegeneinander verschoben.

Natürlich ist es auch nun möglich mit dem Indikatorvariablenansatz auch eine Interaktion zwischen kontinuierlichen und nominalen Variablen zu modellieren. Wenn die Daten zum Beispiel dem Trend in @fig-mlm-dummy-combi-03 folgen würden.

```{r}
#| label: fig-mlm-dummy-combi-03
#| fig-cap: "Leistungsentwicklung in Abhängigkeit vom Alter und Gender"

N <- 50
set.seed(1)
lew <- tibble::tibble(
  ta = sample(10, N, replace=T),
  gender = sample(0:1, N, replace=T),
  perf = rnorm(N, 30, 3) + 2 * ta + 10*gender + 2*ta*gender,
  gender_f = factor(gender, levels = c(0,1), labels=c('m','f'))
)
ggplot(lew, aes(ta, perf, color = gender_f)) +
  geom_point(size=2) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  scale_color_discrete('Gender') 
```

Hier ist zu sehen, dass die Zunahme der Leistung mit dem Trainingsalter nicht gleich ist in beiden `gender` Gruppen. Bei den männlichen Teilnehmern ist der Zuwachs mit dem Trainingsalter größer. D.h der Einfluss der Prädiktorvariable Trainingsalter hängt mit der Ausprägung der Prädiktorvariablen `gender` zusammen. Es liegt ein Interaktionseffekt vor. Übertragen auf ein lineares Modell könnte der Modellansatz wie folgt aussehen:

\begin{equation*}
y_i = \beta_{ta=0,x_{1i}=0} + \Delta_m \times x_{1i} + \beta_{ta} \times ta + \beta_{ta \times gender} \times x_{1i} \times ta + \epsilon_i
\end{equation*}

In `lm()` ausgedrückt:

```{r}
#| echo: true

mod <- lm(perf ~ gender_f * ta, lew)
```
```{r}
#| label: tbl-mlm-dummy-combi-02
#| tbl-cap: "Modellfit für das Interaktionsmodell mit Trainingsalter und Gender."

broom::tidy(mod) |> knitr::kable(
  booktabs = TRUE,
  digits = 3
)
```

Die gefitteten Regressionsgeraden nehmen dann die folgende Form an (siehe @fig-mlm-dummy-combi-04).

```{r}
#| label: fig-mlm-dummy-combi-04 
#| fig-cap: "Leistungsentwicklung in Abhängigkeit vom Alter und Gender"

new_lew <- data.frame(ta = c(1,10,1,10),
           gender_f = factor(c(0,0,1,1),
                             levels=0:1,
                             labels=c('f','m')))
new_lew <- new_lew |> dplyr::mutate(y_hat = predict(mod,new_lew))
ggplot(lew, aes(ta, perf, color = gender_f)) +
  geom_point() +
  geom_line(data = new_lew, aes(y = y_hat)) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  scale_color_discrete('Gender') 
# für später
ta_cen <- 6
```

Zunächst sollen Modellkoeffizienten mit Hilfe des Graphen interpretiert werden. Dazu wird die Prädiktorvariable für das Alters `ta` um den Wert $\bar{ta} = `r ta_cen`$ zentriert und die Modellanpassung erfolgt mittels der zentrierten Variable. 

```{r}
#| echo: true

lew <- lew |> mutate(ta_c = ta - ta_cen)
mod_c <- lm(perf ~ gender_f * ta_c, lew)

```

In @tbl-mlm-dummy-combi-03 sind die neuen Koeffizienten zu sehen. Sie unterscheiden sich nur für $\beta_0$ und $\beta_1$ zu den Koeffizienten von @tbl-mlm-dummy-combi-02 (Warum?).

```{r}
#| label: tbl-mlm-dummy-combi-03
#| tbl-cap: "Modellfit für das Interaktionsmodell mit dem zentrierten Trainingsalter (`ta_c`) und Gender."

beta_s <- round(coef(mod_c), 2)

broom::tidy(mod_c)[,1:2] |> 
    add_column(coef = paste0("$\\beta_", 0:3, "$"), .before=1)|> 
  knitr::kable(
    booktabs = TRUE,
    digits = 3,
    escape=F
  )
```

Wie sind die Modellkoeffizienten zu interpretieren? Die Referenzgruppe sind die Datenpunkte der männlichen Gruppe. Daher ist der $Y$-Achsenabschnitt $\beta_0 = `r beta_s[1]`$ die Performance einer durchschnittlichen, männlichen Versuchsperson mit einem Trainingsalter von `ta`$= 6$. Der zweite Koeffizient $\beta_1 = `r beta_s[2]`$ kodiert hier den Unterschied zwischen einer männlichen und einer weiblichen Versuchsperson jeweils mit einem Trainingsalter von `ta`$=6$. Der Steigungskoeffizient $\beta_2 = `r beta_s[3]`$ dagegen, kodiert den Unterschied zwischen zwei männlichen Versuchspersonen, die sich um ein Trainingsjahr `ta` voneinander unterscheiden. Der letzte Koeffizient $\beta_3 = `r beta_s[4]`$, der Interaktionskoeffizient kodiert nun den Unterschied in den Steigungen zwischen den männlichen und den weiblichen Versuchspersonen. Um den Unterschied zwischen zwei weiblichen Versuchsperson zu erhalten, die sich um ein Trainingsjahr `ta` voneinander Unterscheiden, müssen daher $\beta_2$ und $\beta_3$ miteinander addiert werden $`r round(sum(beta_s[3:4]), 3)`$. 

```{r}
#| label: fig-mlm-dummy-combi-05 
#| fig-cap: "Leistungsentwicklung in Abhängigkeit vom Alter und Gender und die Modellkoeffizienten im zentrierten Modell."

ggplot(lew, aes(ta, perf)) +
  geom_point(aes(color = gender_f)) +
  geom_line(data = new_lew, aes(y = y_hat, color=gender_f)) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  geom_segment(data = tibble(ta = 6, perf = 0),
    aes(xend=ta, yend=beta_s[1]),
    arrow=arrow(length=unit(.1,'inches'), type='closed')) +
  geom_segment(data = tibble(ta = 6, perf = beta_s[1]),
    aes(xend=ta, yend=beta_s[1] + beta_s[2]),
    arrow=arrow(length=unit(.1,'inches'), type='closed')) +
  geom_segment(data = tibble(ta = 6, perf = beta_s[1]),
    aes(xend=ta+1, yend=sum(beta_s[c(1,3)])),
    arrow=arrow(length=unit(.1,'inches'), type='closed')) +
  geom_segment(data = tibble(ta = 6, perf = sum(beta_s[1:2])),
    aes(xend=ta+1, yend=sum(beta_s)),
    arrow=arrow(length=unit(.1,'inches'), type='closed')) +
  geom_label(data = tibble(ta=c(5.7,5.7,6.5,6.5), perf=c(25,56,48,74),
                           labels=c(expression(beta[0]),expression(beta[1]),
                                    expression(beta[2]), expression(beta[2]+beta[3]))),
    aes(label = labels), parse=T) +
  scale_x_continuous("Trainingsalter", breaks=1:10) +
  scale_y_continuous('Gender', limits = c(0,90))
```

In @fig-mlm-dummy-combi-05 sind die einzelnen Modellkoeffizienten aus @tbl-mlm-dummy-combi-02 in der Grafik miteingefügt. Die Interpretation der Werte sollte damit klarer werden.

## Verschiedene Kodierschemas für die Dummy-Variablen.

In der Herleitung der Integration von nominalen Faktoren in die multiple Regression ist bisher ein bestimmtes Schema verwendet worden, wie die einzelnen Faktorstufen auf $X$-Variablen abgebildet werden. Dazu wurde eine Referenzstufe welche durch einen $Y$-Achsenabschnitt modelliert wird und die anderen $\beta_i$-Koeffizienten bilden Abweichungen in den Faktorstufenmittelwerten von dieser Referenzstufe ab.

Wenn die $\beta_i$-Koeffizienten in einem Vektor $\boldsymbol{\beta}$ zusammengefasst.

\begin{equation*}
\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_K)^T
\end{equation*}

und die Mittelwerte $\mu_i$ der Faktorstufen $i = 1,2,\ldots,K$ in einem Vektor $\boldsymbol{\mu}$

\begin{equation*}
\boldsymbol{\mu} = (\mu_1, \mu_2, \ldots, \mu_K)^T
\end{equation*}

Dann können die $\beta$-Koeffzienten im Sinne der Mittelwerte der Faktorstufen als eine Abbildung von $\boldsymbol{\beta} \rightarrow \boldsymbol{\mu}$ aufgefasst werden. Sie diese Abbildung mit dem Symbol $\boldsymbol{B}$ abgebildet.

\begin{equation*}
\boldsymbol{\mu} = \boldsymbol{B\beta}
\end{equation*}

::: {.callout-tip}
Wenn ihr euch mit Matrizen auskennt, dann werdet ihr gemerkt haben, dass wir hier um lineare Abbildungen herumtanzen. Wenn euch das nichts sagt, dann einfach weiterlesen.
:::

### Treatment-Kodierung

Die Abbildung mit einer Referenzstufe und Abweichungen von dieser Stufe, wird als **treatment coding** \index{treatment coding} bezeichnet. In `R` kann diese Kodierung mit Hilfe der Funktion `contr.treatment()` erstellt werden, bzw. ist standardmäßig bei `R` eingestellt. 

Sei eine Faktorvariable $A$ hat drei Stufen $K = 3, A, B, C$ gegeben. Mit der Funktion `mean_contrasts()` aus dem package `codingMatrices` können die Abbildung dargestellt werden (Im Code werden noch die Zeilen- und Spaltenbeschriftungen geändert um den Output leichter interpretieren zu können).

```{r}
#| echo: true

f_A <- c("A","B","C")
beta_to_mu <- mean_contrasts(contr.treatment(f_A))
dimnames(beta_to_mu)[[1]] <- paste0("beta_",0:2)
dimnames(beta_to_mu)[[2]] <- paste0("mu_",f_A)
beta_to_mu
```

Im Ausdruck bezeichnen die Zeilen die $\beta$-Koeffizienten im multiplen Regressionsmodell, während die Spalten die Mittelwerte $\mu_i$ der Faktorstufen $A,B$ und $C$ spezifizieren. Die Punkte `.` stehen für den Wert $0$. Es ist zu sehen, dass in der ersten Zeile für $\beta_0$ eine $1$ unter $\mu_1$ eingetragen ist, während unter $\mu_2$ und $\mu_3$ jeweils eine $0$ eingetragen sind. D.h., der Koeffizient $\beta_0$ repräsentiert den Mittelwert der Faktorstufe $A$, der Referenzstufe. In der zweiten Zeile für $\beta_1$ dagegen, steht unter dem Mittelwert $\mu_A$ eine $-1$, unter $\mu_B$ eine 1 und unter $\mu_C$ eine $0$. D.h., wie schon bekannt, der Koeffizient $\beta_1$ repräsentiert den Unterschied zwischen den Mittelwerten $\mu_A$ und $\mu_B$ der dazugehörenden Faktorstufen $A$ und $B$. Das gleiche gilt für die dritte Zeile für $\beta_2$ für den Unterschied $\mu_C - \mu_A$. Insgesamt, ist hier direkt die Kodierung der Faktorstufen wie sie oben hergeleitet wurde zu erkennen. Die Kodierung ist auch in der dazugehörenden Modellmatrix zu sehen welche die Funktion `contr.treatment()` verwendet.

```{r}
#| echo: true

set.seed(1)
height_4 <- height |> sample_n(4)
model.matrix(~gender, height_4)
```

### Sum-to-zero Kodierung

Die Treatmentkodierung \index{Abweichungskodierung} \index{sum-to-zero coding} ist nicht die einzig mögliche Kodierung, sondern es geht noch zahlreiche andere bzw. es können auch eigene Abbildungen definiert werden. Ein weiteres Kodierungsschema ist die Abweichungskodierung (engl. deviation) welche auch als *sum-to-zero* Kodierung bezeichnet wird.  In `R` kann dieses Schema mit der Funktion `contr.sum()` erzeugt werden und das resultierende Abbildungsschema sieht wie folgt aus.

```{r}
#| echo: true

beta_to_mu <- mean_contrasts(contr.sum(f_A))
dimnames(beta_to_mu)[[1]] <- paste0("beta_",0:2)
dimnames(beta_to_mu)[[2]] <- paste0("mu_",f_A)
beta_to_mu
```

Die erste Zeile ist direkt ablesbar. Der Koeffizient $\beta_0$ stellt den Mittelwert $\bar{\mu}$ der Stufenmittelwerte dar. Die zweite und dritte Zeile für $\beta_1$ und $\beta_2$ sind etwas undurchsichtiger. Die $\beta_i$-Koffizienten repräsentieren jeweils die Abweichung der Stufe $\mu_i$ vom Gesamtmittelwert $\bar{\mu}$. Formal:

\begin{equation*}
\beta_i = \mu_i - \bar{\mu} \quad i = 2,3,\ldots,K
\end{equation*}

Wie resultiert daraus diese Kodierung? Sei zunächst $\beta_1$ betrachtet.

\begin{equation*}
\begin{aligned}
\beta_1 &= \mu_1 - \bar{\mu} = \mu_1 - \frac{\sum_{i=1}^3\mu_i}{3} \\
        &= \mu_A - \frac{1}{3}\mu_A - \frac{1}{3}\mu_B - \frac{1}{3}\mu_C = \frac{2}{3}\mu_A - \frac{1}{3}\mu_B - \frac{1}{3}\mu_C
\end{aligned}
\end{equation*}

Die Koeffizienten aus dem Output sind wieder zu erkennen. Das gleiche Schema trifft auch für $\beta_2$ zu. Übertragen auf das Schema im Beispiel der Reaktionszeiten ergibt sich.

```{r}
#| echo: true

mu_i <- data |> group_by(group) |> summarize(m = mean(rt))
mu_bar <- mean(data$rt)
mu_i |> add_row(group="mu_bar", m = mu_bar)
```

Beziehungsweise für die Abweichungen der Mittelwerte $\mu_i$ vom Gesamtmittelwert $\bar{\mu}$.

```{r}
#| echo: true

mu_i$m - mu_bar
```

Nun wird eine Modellanpassung mit `lm()` durchgeführt, wobei `contr.sum()` anstatt `contr.treatment()` verwendet werden soll. Dies kann direkt im Aufruf von `lm()` über den Parameter `contrasts` spezifiziert werden. Der Parameter erwartet eine Liste mit den Namen der nominalen Variablen zusammen mit der Kodierungsfunktion als Zeichenkette (Es gibt noch weitere Möglichkeiten).

```{r}
#| echo: true

mod_sum <- lm(rt ~ group, data, contrasts = list(group = 'contr.sum'))
coef(mod_sum)
```

Es ist zu sehen, dass die $\beta$-Koeffizienten tatsächlich den Gesamtmittelwert und die Abweichungen der Faktormittelwerte $\mu_i$ vom Gesamtmittelwert $\bar{\mu}$ kodieren. Es fehlt allerdings der letzte Abweichungswert. Hier kommt die Eigenschaft dieser Kodierung ins Spiel, die der Kodierung ihren Namen sum-to-zero gegeben hat. Per Konstruktion haben die Abweichungen die Eigenschaft das sie aufsummiert gleich $0$ ergeben. Wie bereits bei den Residuen kennengelernt, die Abweichungen von Werten von deren Mittelwert müssen $0$ ergeben. Daher gilt:

\begin{equation*}
\begin{aligned}
& \sum_{i=1}^K \beta_i = 0 \\
\Leftrightarrow & \beta_K = -\sum_{i=1}^{K-1}
\end{aligned}
\end{equation*}

D.h. durch die Summation der Abweichungen und der Invertierung des Vorzeichens kann der letzte Abweichungswert erhalten werden. Übertragen auf das Beispiel:

```{r}
#| echo: true

-sum(coef(mod_sum)[2:4])
```

Die Anwendung von `contr.sum()` wirkt sich entsprechend auf die Konstruktion der Modellmatrix aus (Es wird eine verkürzter Datensatz mit nur acht Datenpunkten zu leicheteren Darstellung erstellt).

```{r}
#| echo: true

data_8 <- data |> sample_n(8)
model.matrix(~group, data_8)
model.matrix(~group, data_8, contrasts=list(group='contr.sum'))
```


### Differenzkodierung

Abschließend sei noch eine weitere Möglichkeit betachtet um die Faktorstufen zu kodieren, die Differenzkodierung \index{Differenzkodierung}. Bei der Differenzkodierung werden jeweils aufeinanderfolgende Stufen miteinander verglichen. In `R` kann dieses Kodierungsssystem mittels der Funktion `contr.diff()` aus dem package `codingMatrices` erstellen werden. Angewendet auf den Faktor `f_A` ergibt sich.

```{r}
#| echo: true

beta_to_mu <- mean_contrasts(contr.diff(f_A))
dimnames(beta_to_mu)[[1]] <- paste0("beta_",0:2)
dimnames(beta_to_mu)[[2]] <- paste0("mu_",f_A)
beta_to_mu
```

Der Koeffizient $\beta_0$ ist wieder gleich dem Mittelwert aus Gruppe $A$. Der Koeffiziente $\beta_1$ kodiert ähnlich wie beim Treatmentkoding den Unterschied zwischen den Mittelwerten der Faktoren $A$ und $B$. Im Unterschied zum Treatmentkoding kodiert $\beta_2$ dagegen den Unterschied zwischen den Mittelwerten der Faktoren $C$ und $B$.

Angewendet auf das Reaktionszeitexperiment:

```{r}
#| echo: true

mod_diff <- lm(rt ~ group, data, contrasts = list(group = 'contr.diff'))
coef(mod_diff)
```

Die Koeffizienten entsprechen den aufeinanderfolgenden Unterschieden der Gruppenmittelwerten.

```{r}
#| echo: true

diff(mu_i$m)
```

Entsprechend folgt wieder eine andere Modellmatrize.

```{r}
#| echo: true

model.matrix(~group, data_8, contrasts = list(group = 'contr.diff'))
```

### Zusammenfassung

Zusammenfassend zeigen diese Beispiele, dass die verschiedenen Kodierungssysteme dazu führen, dass die Bedeutung der Regressionskoeffizienten $\beta_i$ sich unterscheidet je nachdem welches Kodierungssystem verwendet wird. Daher, um die Werte der Koeffizienten interpretieren zu können, muss bekannt sein, welches Kodierungssystem bei der Analyse verwendet wurde. Leider ist dies nicht die einzige Implikation, sondern die unterschiedlichen Kodierungssysteme führen zu weiteren, subtilen Unterschieden die ganz und gar nicht offensichtlich sind. Vor allem später, bei der Behandlung von faktoriellen, experimentellen Untersuchungsdesigns werden die Details der Kodierungssysteme noch einmal stärker von Relevanz werden. In Bezug auf den allgemeinen Fit des Modells haben die Unterschiedlichen Systeme allerdings keinen Einfluss. Die angepasste Regressionsgerade (Ebene) wird daher durch die Wahl der Kodierungssysteme ebenfalls nicht beeinflusst. Somit sind auch die Residuen $e_i$ unter allen Kodierungssystemen gleich.

Durch die Wahl von speziellen Kodierungssysteme ist es möglich spezifisch gewollte Vergleiche zwischen Faktorenstufen direkt zu kodieren und dann in Form der Koeffizienten statistisch zu überprüfen. Unter der Treatmentkodierung bilden die Koeffizienten direkt die Unterschiede der Faktorenstufen von der Referenzstufe ab. Daher sagt die statistische Signifikanz der Koeffizienten $\beta_i, i = 2,3,\ldots,K$ direkt schon etwas über die statistische Signifikanz dieser Stufen von der ersten Stufe aus. Wird zum Beispiel als die Referenzstufe eine Kontrollkondition gewählt und die anderen Stufen bilden unterschiedliche Interventionsstufen ab, dann kann über die Koeffizienten direkt der Unterschied der Interventionsstufen von der Kontrollstufe überprüft werden. Dadurch überschneidet die Thematik der Kodierungssysteme sich mit dem der Kontraste bzw. Mehrfachvergleiche, dass später im Kapitel zu Linearen Kontrasten eingeführt wird.

Dieser kurze Überblick über die Kodierungssysteme basiert hauptsächlich auf @venables2023. Dies ist auch die primäre Quelle um sich in die Thematik noch einmal tiefer einzulesen. 

## Zusammenfassung

In diesem Kapitel wurde gezeigt, dass auch nominale Prädiktorvariablen ohne Probleme in das lineare Modell integrieren können. Dabei können die Variablen rein additiv oder auch als interaktive Effekte eingehen. Damit ist der Prozess um Daten zu modellieren deutlich flexibler geworden. Dabei bleibt aber immer die bekannten Rückführung auf Punkt-Steigungs-Modell erhalten.

## Herleitung der Identität von t-Test und linearen Modell (optional)

Seien beide Gruppen gleich groß ($n$) mit $N = n_m + n_w = 2 \times n$. Der t-Wert für $\beta_1$ berechnet sich aus $t = \frac{b_1}{s_b}$ mit:

$$
s_b = \sqrt{\frac{\sum_{i=1}^N (y_i - \bar{y})^2}{N-2}\frac{1}{\sum_{i=1}^N(x_i-\bar{x})^2}}
$$
Dadurch, das die $x_i$ entweder gleich $0$ oder $1$ sind, ist $\bar{x}=0.5$ und die Abweichungsquadrate im zweiten Term sind alle gleich $\frac{1}{4}$.

$$
\sum_{i=1}^N(x_i - \bar{x})^2=\sum_{i=1}^N\left(x_i - \frac{1}{2}\right)^2 = \sum_{i=1}^N\frac{1}{4}=\frac{N}{4}=\frac{2n}{4}=\frac{n}{2}
$$

Der ersten Term kann mit etwas Algebra und der Definition für die Stichprobenvarianz $s^2$ auf die gewünschte Form gebracht werden.

$$
\frac{\sum_{i=1}^N(y_i-\hat{y})^2}{N-2}=\frac{\sum_{i=1}^n(\overbrace{y_{im} - \bar{y}_m}^{Männer})^2+\sum_{i=1}^n(\overbrace{y_{iw}-\bar{y}_w}^{Frauen})^2}{2(n-1)}=\frac{(n-1)s_m^2+(n-1)s_w^2}{2(n-1)}=\frac{s_m^2+s_w^2}{2}
$$

Die Herleitung für $\beta_1 = \Delta = \mu_w - \mu_m$ ist ebenfalls relativ geradlinig wenn wir uns an den Zusammenhang zwischen $\beta_1$ und der Kovarianz zwischen $x$ und $y$ erinnern. Mit $s_x^2 = \frac{N\frac{1}{4}}{N-1} = \frac{N}{4(N-1)}$ folgt:

\begin{align*}
    b_1 &= \frac{cov(x,y)}{s_x^2} \\
    &= \frac{\sum_{i=1}^N(y_i - \bar{y})(x_i - \bar{x})}{N-1} \frac{4(N-1)}{N} \\
    &= 4\frac{\sum_{i=1}^n(y_{im}-\bar{y})\frac{-1}{2}+\sum(y_{iw}-\bar{y})\frac{1}{2}}{N} \\
    &= \frac{4}{2}\frac{\sum_{i=1}^n(y_{iw}-\bar{y}) - \sum_{i=1}^n(y_{im}-\bar{y})}{2n} \\
    &= \frac{\sum_{i=1}^n y_{iw}}{n} - \frac{n\bar{y}}{n} - \frac{\sum_{i=1}^n y_{im}}{n} + \frac{n\bar{y}}{n} \\
    &= \bar{y}_w - \bar{y}_m = \Delta
\end{align*}

Zu guter Letzt noch die Herleitung für $\beta_0 = \mu_m$.

Mit $b_1 = \Delta = \bar{y}_w - \bar{y}_m$:
\begin{align*}
b_0 &= \bar{y} - \Delta \times \bar{x} \\
&= \frac{\sum_{i=1}^N y_i}{N} - \Delta \times \frac{1}{2} \\
&= \frac{\sum_{i=1}^n y_{im} + \sum_{i=1}^n y_{iw}}{2n} - \frac{1}{2}(\bar{y}_w - \bar{y}_m)  \\
&= \frac{1}{2}\frac{\sum_{i=1}^ny_{im}}{n} + \frac{1}{2}\frac{\sum_{i=1}^ny_{iw}}{n} - \frac{1}{2}\bar{y}_w + \frac{1}{2}\bar{y}_m \\
&= \frac{1}{2}\bar{y}_m + \frac{1}{2}\bar{y}_w - \frac{1}{2}\bar{y}_w + \frac{1}{2}\bar{y}_m \\
&= \bar{y}_m
\end{align*}


## Zum Nach- und Weiterlesen

In [@kutner2005, p.313-319] sind die Herleitungen zur Indexkodierung etwas ausführlicher erklärt.

