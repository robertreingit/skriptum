# Integration von nominalen Variablen 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
library(codingMatrices)
```


```{r defs_dummy}
N <- 30
set.seed(123)
height <- tibble::tibble(cm = rnorm(2*N,rep(c(180,167),each=N),10),
                 gender = factor(rep(c('m','f'),each=N)))
mu_s <- height |> dplyr::group_by(gender) |> dplyr::summarize(m = mean(cm))
mu_f <- mu_s |> dplyr::filter(gender == 'f') |> pull() |> round(2) 
mu_m <- mu_s |> dplyr::filter(gender == 'm') |> pull() |> round(2)
Delta <- round(mu_m - mu_f, 2)
N <- 20
K <- 4
set.seed(1)
data <- tibble::tibble(
  group = gl(K, N, labels = c('A','B','C','D')),
  rt = rnorm(K * N, mean = rep(seq(500,800,100), each=N), sd = 50)
)
```

Bisher haben wir nur kontinuierliche, beziehungsweise metrische Variablen in unsere linearen Modelle aufgenommen. Im Folgenden werden wir sehen, dass wir mit einem kleinem Trick genauso nominale Variablen, also z.B. TREATMENT versus CONTROL, in das Modell integrieren können, ohne dass wir dafür etwas fundamental neues lernen müssen.

## Vergleich von zwei Gruppen

Beginnen wir mit einem einfachen Beispiel. Wir wollen die Unterschiede zwischen Männern und Frauen in Bezug auf die Körpergröße untersuchen statistisch untersuchen. In @fig-mlm-dummy-gender ist ein hypothetischer Datensatz von Körpergrößen von Frauen und Männern abgebildet. Wenig überraschend, da der Datensatz so erstellt wurde, sind Männer im Mittel größer als Frauen.

```{r}
#| fig-cap: "Simulierte Daten: Verteilung von Körpergrößen nach Geschlecht für Männer (m) und Frauen (f)"
#| label: fig-mlm-dummy-gender

ggplot(height, aes(gender,cm)) + 
  see::geom_violindot(size_dots=20, fill_dots='red') +
  labs(x = 'Gender', y = 'Körpergröße[cm]') 
```

In @tbl-mlm-dummy-gender ist ein Ausschnit der Daten tabellarisch dargestellt. Wir haben zwei Datenspalten. In der ersten Spalte stehen die Körpergrößen, während in der zweiten Spalte die nominale Variable `gender` steht die entweder den Wert $m$ für Männer oder $f$ für Frauen annimmt.

```{r}
#| label: tbl-mlm-dummy-gender
#| tbl-cap: "Ausschnitt aus den Daten."

height[c(1:3,31:33),] |> 
  knitr::kable(
    booktabs = T,
    caption = "Ausschnitt aus den Daten",
    digits = 1,
    linesep = ''
  )
```

In @tbl-mlm-dummy-gender-desc sind dann auch noch einmal die deskriptiven Statistiken der Körpergrößendaten abgebildet die auch noch einmal den Eindruck aus @fig-mlm-dummy-gender bestätigen.

```{r}
#| label: tbl-mlm-dummy-gender-desc
#| tbl-cap: "Deskriptive Statistiken der Körpergrößendaten."
 
x_hat <- height |> dplyr::group_by(gender) |>
  dplyr::summarize(m = round(mean(cm),1), sd = round(sd(cm),1))
knitr::kable(x_hat, booktabs=TRUE,
             caption = "Deskriptive Werte",
             col.names = c('Gender', "$\\bar{x}$", "SD"),
             escape = FALSE)
```

Wir müssen zunächst einmal eine kurze Detour nehmen und uns einmal genauer anschauen wie nominale Werte in `R` repräsentiert werden.

## Nominale Variablen in `R` (detour)

Nominale Variablen werden in `R` mittels eines eigenem Datentyps `factor` repräsentiert. Erstellt werden kann ein Faktor mit der `factor()`-Funktion. Die Funktion hat drei wichtige Parameter. Der erste Parameter bezeichnet die Werte, der zweite die möglichen Faktorstufen (`levels`) und der dritte Parameter die dazugehörigen Bezeichnungen (`labels`). Ein einfaches Beispiel sieht dann so aus:

```{r}
#| echo: true
gender <- factor(c(0,0,1,1),
                 levels = c(0,1),
                 labels = c('m','f'))
gender
```

D.h. wir haben einen Datenvektor mit den Elemente $(0,0,1,1)$. Wir spezifizieren die `levels` dementsprechend mit $0$ und $1$ und definieren die dazugehörigen `labels` mit $m$ und $f$. Dabei sind jeweils Vektoren übergeben worden (siehe `c()`). Wenn wir die neue Variable `gender` aufrufen erhalten wir den Datenvektor mit den entsprechenden `labels`. Zusätzlich gibt `R` die möglichen `labels` auch noch einmal explizit als `Levels` an.

Wenn wir den Parameter `levels` nicht angegeben hätten, dann extrahiert `factor()` die eineindeutigen Werte selbst und führt die Abbidlung auf die `labels` entsprechend der standard Sortierungsregeln von `R` aus.

```{r}
#| echo: true
gender <- factor(c(0,0,1,1),
                 labels = c('m','f'))
gender
str(gender)
```

Dabei muss darauf geachtet werden, dass die Abbildung auch tatsächlich diejenige ist, die gewünscht ist.

```{r}
#| echo: true
gender <- factor(c(0,0,1,1),
                 labels = c('f','m'))
gender
```

Daher ist es fast immer sinnvoll `labels` und `levels` immer zusammen zu spezifizieren. Wenn die Parameter nicht angegeben werden, dann führt `factor` die Abbildung automatisch durch und für die `labels` werden die Datenwerte übernommen. 

```{r}
#| echo: true
gender <- factor(c(0,0,1,1))
gender
```

::: {.callout-warning}
Achtung, die Variable `gender` sieht zwar aus wie ein numerischer Vektor, sie ist es aber nicht.

```{r}
#| echo: true

is.numeric(gender)
gender + 1
```

Intern wird eine Faktorvariable von `R` zwar als ein numerischer Vektor abgelegt. Aber die "sichtbaren" Werte sind nun die Zeichenketten der `labels`, die daher auch angezeigt werden. Die interne numerische Repräsentation muss auch nicht mehr den ursprünglichen Datenwerten entsprechen.

```{r}
#| echo: true

as.numeric(gender)
```

Die Datenwerte waren ursprünglich $(0,1)$ und sind jetzt auf $(1,2)$ abgebildet worden. Erinnert euch an die Eigenschaft von nominalen Variablen. Nominale Variablen sind einfach voneinander unterscheidbare Werte die jedoch in keiner Ordnung stehen.
:::

Die automatische Konvertierung von `factor()` funktioniert am intuitivsten mit Zeichenkettenvektoren.

```{r}
#| echo: true

gender <- factor(c('m','f','m','f'))
gender
str(gender)
```

`factor()` ermittelt zunächst die eineindeutigen Werte und sortiert diese dann entsprechend des Typen. In diesem Fall wird die Zeichenkette alphabetisch sortiert. Dann erfolgt die Abbildung der Werte auf die `labels`. Dies führt in diesem Fall dazu, dass die Werte `m` intern den Wert $2$ zugeordnet bekommen, obwohl der erste Wert in den Daten `m` ist. Diese Sortierung der Daten wird später noch einmal von Bedeutung werden.

Die Reihenfolge der Faktorstufen wird durch die Angabe des `levels` explizit bestimmt werden und ist daher auch noch mal ein Argument dafür das `levels`-Argument zu verwenden.

```{r}
#| echo: true

gender <- factor(c('m','f','m','f'),
                 levels = c('m','f'))
gender
str(gender)
gender <- factor(c('m','f','m','f'),
                 levels = c('f','m'))
gender
str(gender)
```

::: {.callout-tip}
Im package `forcats` sind eine Reihe von Funktionen hinterlegt, mit denen die Eigenschaften von `factor`-Variablen einfach manipuliert werden können. Zum Beispiel, wenn die Reihenfolge von Faktorstufen geändert werden soll kann die Funktion `fct_relevel()` verwendet.

```{r}
gender <- factor(c('m','f','m','f'),
                 levels = c('f','m'))
gender
forcats::fct_relevel(gender, 'm')
```

Schaut euch die ausführliche Dokumentation der Funktionen und die Beispiel an, wenn ihr auf Probleme mit `factor`-Variablen stoßt.

:::

::: {.callout-tip}
Viele Funktionen in `R`, wie z.B. `lm()`, transformieren Vektoren mit Zeichenketten automatisch in einen `factor()` um. Wird in `lm()` in der Formel beispielsweise `y ~ gender` benutzt und `gender` ist eine Datenspalte die aus den Zeichenketten `c('m','m','f','f')` besteht, dann ruft `lm()` intern die Funktion `factor()` für diese Daten auf und führt dann die Berechnung mit dem Faktor durch.

Dies erleichtert natürlich oft den Umgang mit den Daten, hat aber den Nachteil das einem immer klar sein muss, dass die automatische Konvertierung auch tatsächlich diejenige ist, dich auch gewünscht ist.
:::

## Vergleich von zwei Gruppen (continued)

Kommen wir zurück zum Körpergrößenvergleich. Normalerweise würden wir die Unterschiede zwischen den beiden Gruppen mit einem klassischen t-Test für unabhängige Stichproben unter der Annahme der Varianzgleichheit untersuchen. In `R` können wird dies mit der `t.test()`-Funktion durchführen.

```{r}
#| echo: true

t.test(cm ~ gender, data=height, var.equal=T)
```

Dem Output von `t.test() ist ein statistisch signifikantes Ergebnis zu entnehmen.

Erinnern wir uns noch einmal an die Modellformulierung beim t-Test bei gleich großen Gruppen $(n_w = n_m)$.

\begin{equation}
\begin{aligned}
Y_{if} &= \mu_{f} + \epsilon_{if}, \quad \epsilon_{if} \sim \mathcal{N}(0,\sigma^2) \\
Y_{im} &= \mu_{m} + \epsilon_{im}, \quad \epsilon_{im} \sim \mathcal{N}(0,\sigma^2)
\end{aligned}
\label{eq-mlm-dummy-tTest-model}
\end{equation}

Beide Gruppen sollten normalverteilt sein mit gleicher Varianz $\sigma^2$ und den Erwartungswerten $\mu_f$ für die weiblichen Teilnehmerinnen und $\mu_m$ für die männlichen Teilnehmer. Die Hypothesen beim t-Test sind unter der Nullhypothese die übliche Annahme der Gleichheit der Mittelwerte bzw. unter der Alternativhypothese die Ungleichheit der Mittelwerte.

\begin{align*}
H_0&: \delta = 0 \\
H_1&: \delta \neq 0
\end{align*}

Daraus resultiert die folgende Teststatistik:

\begin{equation}
t = \frac{\bar{y}_m - \bar{y}_w}{\sqrt{\frac{s_m^2 + s_w^2}{2}}\sqrt{\frac{2}{n}}}
\label{eq-tTest}
\end{equation}

Unter der $H_0$ folgt die Referenz- bzw. Stichprobenverteilung einer t-Verteilung mit $N - 2$ Freiheitsgraden.

\begin{equation*}
t \sim t_{df=2n-2}
\end{equation*}

Die entsprechende Dichtefunktion hat die folgende graphische Darstellung (siehe @fig-mlm-dummy-tDist). 

```{r}
#| label: fig-mlm-dummy-tDist
#| fig-cap: "Dichtefunktion der t-Verteilung mit $df=58$"

n_fig_pts <- 100
t_dist <- tibble::tibble(
  x = seq(-3,3,length.out=n_fig_pts),
  t = dt(x,58)
) 
ggplot(t_dist, aes(x,t)) +
  geom_ribbon(aes(ymin=0, ymax=t), alpha=0.3, fill='red') +
  geom_line() + 
  labs(x = 't-Werte', y = 'Dichte') 
```

Soweit nicht Neues. Im Beispiel haben wir einen $t$-Wert von $t = -4.6$ beobachtet der derart extrem ist, dass wir ihn in @fig-mlm-dummy-tDist schon gar nicht mehr unterbringen können. 

Da wir aber faul sind und uns nicht verschiedene Tests merken wollen schauen wir uns jetzt an, wie Wir auf das gleiche Ergebnis mittels eines linearen Modellansatzes kommen können. Die Frage ist daher nun:

## Kann ich aus dem t-Test ein lineares Modell machen? 

Unser altbekanntes lineares Modell hat im einfachsten Fall die folgende Form für eine einfache Regression:

\begin{equation}
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i 
\label{eq-mlm-dummy-lm}
\end{equation}

Können wir diese Modell irgendwie auf die Formeln \eqref{eq-mlm-dummy-tTest-model} und \eqref{eq-tTest} abbilden?

Tatsächlich gibt es eine relativ einfache Möglichkeit mittels eines Ansatzes mit sogenannten Dummy- oder auch Indikatorvariablen. Die $Y$-Variable, die abhängige Variable ist die gleiche wie im t-Test. Im Beispiel also die Körpergrößen. Daher bleibt für die $X$-Variable nur noch die unabhängige Variable, in unserem Beispiel die `gender`-Variable. Jetzt haben wir aber das Problem,  dass $x_i$ nur zwei verschiedene Werte annehmen kann und diese auch noch entweder `m` oder `f` im Beispiel sind. Offensichtlich macht die folgende Formulierung wenig Sinn:

\begin{equation*}
y_i = \beta_0 + \beta_1 \cdot m + \epsilon_i
\end{equation*}

Wir machen aus $x_i$ ein Indikatorvariable. Eine Indikatorvariable kann nur zwei verschiedene numerische Werte einnehmen. Entweder den Wert $0$ oder den Wert $1$ (es gibt auch andere Schemata, aber die Ignoriren wir mal). Jetzt macht die Formel wieder mehr Sinn.

\begin{align*}
y_i &= \beta_0 + \beta_1 \cdot 0 + \epsilon_i \quad \text{oder} \\
y_i &= \beta_0 + \beta_1 \cdot 1 + \epsilon_i \quad
\end{align*}

Wir brauchen jetzt nur noch eine Abbildung von unseren Werten `m` und `f` auf die Werte $0$ und $1$ um die Gruppenzugehörigkeit zu kodieren. Dazu legen wir eine sogenannte Referenzgruppe fest die denn Wert $x_i = 0$ zugewiesen bekommt. Im Beispiel sei die weibliche Gruppe die Referenzgruppe. Dies führt zu der folgenden Vorschrift: Wenn ein Wert $y_i$ aus der Gruppe weiblich (`f`) kommt, dann wird $x_i = 0$ gesetzt, während wenn ein Wert $y_i$ aus der Gruppe männlich (`m`) kommt, dann wird $x_i = 1$ gesetzt.

\begin{equation*}
x_i = \begin{cases}
0\text{ wenn weiblich}\\
1\text{ wenn männlich}
\end{cases} 
\end{equation*}

So weit so gut. Jetzt können wir uns mit den Koeffizienten $\beta_0$ und $\beta_1$ in Formel \eqref{eq-mlm-dummy-lm} beschäftigen und deren Bedeutung unter dem Indikatorvariablenansatz verstehen. In der weiblichen Gruppe mit $x_i = 0$ folgt für \eqref{eq-mlm-dummy-lm}:

\begin{equation*}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i = \beta_0 + \epsilon
\end{equation*}

D.h. der $y$-Achsenabschnitt $\beta_0$ wird zum Mittelwert der Referenzgruppe, im Beispiel der `f`. Die Formulierung ist genau die gleich wie diejenige die wir im t-Test-Ansatz verwendet haben (siehe \eqref{eq-mlm-dummy-tTest-model}). Formal also 

\begin{equation*}
\beta_0 = \mu_f
\end{equation*}

Schauen wir uns $\beta_1$ nun an. Wenn ein Datenpunkt aus der Gruppe `m` kommt dann wird $x_i = 1$ gesetzt. Formal:

\begin{equation*}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i = \beta_0 + \beta_1 + \epsilon
\end{equation*}

Das sieht jetzt etwas unverständlich aus. Machen wir einfach mal die folgende Festlegung: $\beta_1 = \mu_m - \mu_f = \Delta$. D.h. wir setzen als $\beta_1$ der Unterschied zwischen den beiden Gruppenmittelwerten. Setzen wir das mal ein und schauen ob uns das weiter bringt.

\begin{align*}
y_i &= \beta_0 + \beta_1 \cdot 0 + \epsilon_i = \mu_f + \Delta \cdot 0 + \epsilon_i = \mu_f + \epsilon_i \\
y_i &= \beta_0 + \beta_1 \cdot 1 + \epsilon_i = \mu_f + \Delta \cdot 1 + \epsilon_i = \mu_f + (\mu_m - \mu_f) + \epsilon_i = \mu_m + \epsilon_i
\end{align*}

D.h. wir sind mittels der Indikatorvariablen in der Lage das Modell des t-Test (Formel \eqref{eq-mlm-dummy-tTest-model}) in ein lineares Modell eine einfache lineare Regression zu übersetzen.

\begin{align*}
Y_i &= \mu_f + \Delta_{m} \times x_{i} + \epsilon_i \\
\Delta_m &= \mu_m - \mu_f \\
\end{align*}

Eine der Voraussetzungen im t-Test ist die Normalverteilung der Wert. Letztendlich bedeutet dies, dass wenn ich von den $y_i$-Werten eine Konstante abziehen, das der Rest auch wieder Normalverteilt ist. Der Rest ist jetzt nichts anderes als die Residuen, wenn ich von den $y$-Werten die jeweiligen Gruppenmittelwerte abziehe. 

\begin{align*}
e_{if} &= y_{if} - \mu_f \\
e_{im} &= y_{im} - \mu_m
\end{align*}

D.h. somit, dass die Residuen $e_i$ Normalverteilt. Das ist aber auch eine der Voraussetzungen für die Inferenz im linearen Modell. Im linearen Modell wird weiterhin Homoskedasitzität voraussgetzt, was nichts anderes als die Varianzgleichheit im t-Test ist.

Um den Ansatz einmal konkret zu machen, können wir zum Beispiel den ersten Wert aus @tbl-mlm-dummy-gender $y_{1m} = 174.4$ in seine Komponenten aufteilen. Wir erhalten $\Delta = `r mu_m` - `r mu_f` = `r mu_m - mu_f`, e_{1m} = 174.4 - `r mu_m`= -5.13$ und $x_{1m} = 1$.

\begin{align*}
y_{1m} = 174.4 &= \mu_f + \Delta \cdot x_{1m} + e_{1m} \\
&= 168.78 + 10.74 \cdot 1 - 5.13 
\end{align*}

Das Gleiche für den ersten weiblichen Wert in @tbl-mlm-dummy-gender $y_{1f} = 171.3, \Delta = `r Delta`, e_{1f} = 171.3 - `r mu_f` = 2.52$ und $x_{1f} = 0$.

\begin{align*}
y_{1f} = 171.3 &= \mu_f + \Delta \cdot x_{1f} + e_{1f} \\
&= 168.78 + 10.74 \cdot 0 + 2.52
\end{align*}

Zusammenfassend, haben wir mit der Indikatorvariable und dem $\Delta$ zwischen den Mittelwerten der beiden Gruppen eine Möglichkeit gefunden das t-Test Modell auf das lineare Modell abzubilden. Und tatsächlich, wenn das Modell in `lm()` eingeben:

```{r}
#| eval: false
#| echo: true

mod <- lm(cm ~ gender, height)
```
```{r}
mod <- lm(cm ~ gender, height)
lm_tbl_knitr(mod)
```

Dann sehen wir, dass wir genau diese Werte auch herausbekommen. `lm()` gibt die Faktorstufe nach dem Namen der Faktorvariable an. Im Beispiel steht `genderm` für Stufe `m` aus dem Faktor `gender`. Wenn wir uns die Werte für den t-Wert für $\beta_1 = \Delta$ anschauen, dann sehen wir, dass der Wert exakt mit demjenigen übereinstimmt, den wir mittels des t-Test berechnet haben. Lediglich das Vorzeichen ist in der anderen Richtung was aber nur damit zusammenhängt welcher Gruppenmittelwert von dem anderen Gruppenmittelwert subtrahiert werden. Schauen wir uns das Konfidenzintervall für den *Steigungskoeffizienten* $\beta_1$ an:

```{r}
#| echo: true

confint(mod)
```

Dann sehen wir, dass auch das Konfidenzintervall exakt demjenigen des t-Test, bis auf das Vorzeichen, entspricht. D.h. nochmal wir können einen t-Test als ein lineares Modell verstehen und anders herum wir können nun ohne Probleme nominale Variablen in ein lineares Modell mit aufnehmen.

Damit haben wir auch schon eine sehr gute Idee wie 'lm()' intern gemacht hat. Bei der Modellevaluierung erkennt `lm()` gender als nominale Variable und ersetzt es durch die Dummykodierung (siehe @tbl-mlm-dummy-dummy-lm).

```{r}
#| label: tbl-mlm-dummy-dummy-lm
#| tbl-cap: "Repräsentation der Faktorvariablen"

cbind(height, (0:1)[as.numeric(height$gender)])[c(1:3,41:43),] |> 
  knitr::kable(booktabs = TRUE,
               col.names = c('cm', 'gender', '$x_1$'),
               row.names = F,
               escape = F,
               linesep = '',
               digits = 2)
```

`lm()` ersetzt `gender` durch die Dummy-Variable, also durch Zahlenwerte, und dann kann einfach wieder die uns bekannte Maschinerie des linearen Modell angeworfen werden. Schauen wir uns an, wie ein Residuenplot bei einer Dummyvariablen aussieht (siehe @fig-mlm-dummy-resid-plot).

```{r}
#| label: fig-mlm-dummy-resid-plot 
#| fig-cap: "Residuenplot des Körpergrößenmodells"

height |> dplyr::mutate(y_hat = predict(mod),
                 e_hat = resid(mod)) |> 
  ggplot(aes(y_hat, e_hat)) +
  geom_point() +
  geom_hline(yintercept = 0, col = 'red', linetype = 'dashed') +
  labs(x = expression(paste('Vorhergesagte Werte ', hat(y)[i])),
       y = expression(paste('Residuen ', hat(e)[i]))) 
```

Der Plot sieht etwas komisch aus, da die Residuen nur für zwei vorhergesagte Werte $\hat{y}_i$ angezeigt werden. Kurzes überlegen sollte uns aber davon überzeugen, dass dies korrekt ist, da wir nun noch zwei verschiedene $y$-Wert vorhersagen. Nämlich für die Werte $x_i = 0$ und $x_i = 1$, eben die Mittelwerte aus den beiden Gruppen.


## Dummyvariablen bei mehr als zwei Faktorstufen

Nachdem wir ein Beispiel mit zwei Faktorstufen durchgegangen sind stellt sich die Frage ob wir auch mehr als zwei Faktorstufen bei einer nominalen Variablen in ein lineares Modell überführen können?

Schauen wir uns dazu wiederum ein einfaches synthetisches Beispiel an (siehe @fig-mlm-dummy-reaction-01)

```{r}
#| label: fig-mlm-dummy-reaction-01
#| fig-cap: "Ein Reaktionszeitexperiment mit vier Stufen A, B, C und D"

ggplot(data, aes(group, rt)) + geom_boxplot() +
  geom_point(alpha=.1, col='red') +
  labs(x = 'Gruppe', y = 'Reaktionszeit') 
```

Wir haben Daten zu einem Reaktionszeitexperiment bei dem Probanden unter vier verschiedenen Konditionen $A,B,C$ und $D$ beobachtet wurden. Die deskriptive Daten sind @tbl-mlm-dummy-reaction-01 einsehbar. Der Einfachheit halber sind die Daten so generiert, dass die Gruppenmittelwert von $A$ nach $D$ immer größer werden.

```{r}
#| label: tbl-mlm-dummy-reaction-01
#| tbl-cap: "Gruppenmittelwerte, Standardabweichung und Unterschiede zu Stufe A"
tmp <- data |> dplyr::group_by(group) |>
  dplyr::summarize(y_hat = mean(rt), sd = sd(rt))
mat <-  t(matrix(c(-1,1,0,0,-1,0,1,0,-1,0,0,1),nr=4))
tmp |> dplyr::mutate(delta = c(NA,mat %*% y_hat)) |>
  knitr::kable(booktabs = TRUE,
               col.names = c('Gruppe','$\\bar{y}_j$', '$s_j$', '$\\Delta_{j-A}$'),
               digits = 2,
               escape=F)
```

Die letzten Spalte zeigt die Abweichungen der Gruppenmittelwerte der Gruppen $B,C,D$ von der Gruppe $A$ an. Das sollte jetzt auch schon direkt eine Möglichkeit aufzeigen, wie wir diese Daten in ein lineares Modell überführen. Wir setzen wieder eine Referenzgruppe fest, z.B. Gruppe $A$ und anstatt ein einzelnes $Delta$ führen wir drei $\Delta$s ein, die jeweils die Abweichungen der Faktorstufen von der Referenzstufe repräsentieren. Für jedes dieser $\Delta_i$s führen wir jetzt eine Dummyvariable $x_{ij}$ ein, mit der wir jeweils über die $0$ und $1$ die Zugehörig des $y_i$ Wertes zu der Gruppe kodieren. Ein lineares Modell lässt nun wie folgt formulieren:

$$
y_i = \mu_A + \Delta_{B-A} x_{1i} + \Delta_{C-A} x_{2i} + \Delta_{D-A} x_{3i} + \epsilon_i
$$

Wir haben drei Dummyvariablen $x_{1i}, x_{2i}$ und $x_{3i}$ eingeführt und die Modellkoeffizienten bilden jeweils die Abweichungen der Mittelwerte von Gruppe $A$ ab. Dies führt dann zu der folgenden Kodierung der Daten (siehe @tbl-mlm-dummy-reaction-02).

```{r}
#| label: tbl-mlm-dummy-reaction-02
#| tbl-cap: "Kodierung der Dummyvariablen für das Reaktionszeitexperiment."

mat <- contrasts(data$group)
colnames(mat) <- c('x1','x2','x3')
knitr::kable(mat, booktabs=TRUE) |>
  kableExtra::kable_styling(position = 'center')
```

Wenn ein Wert $y_i$ aus Gruppe $A$ kommt, dann werden die Dummyvariablen $x_{1i}, x_{2i}, x_{3i}$ mit den Werten $(0,0,0)$ belegt. Wenn ein Wert $y_i$ aus Gruppe $B$ kommt, dann $(1,0,0)$ belegt usw..

Dieser Ansatz lässt sich wie folgt Verallgemeinern (siehe @tbl-mlm-dummy-coding).

|     | $x_1$ | $x_2$ | $\ldots$ | $x_{K-1}$ | 
| --- | --- | --- | --- | --- |
| Referenz ($j=1$) | 0 | 0 |  | 0 | 
| $j=2$ | 1 | 0 | $\ldots$  | 0 | 
| $j=3$ | 0 | 1 | $\ldots$  | 0 |
| $j=K$ | 0 | 0 | $\ldots$  | 1 | 

: Dummykodierung bei $k$-Faktorstufen. {#tbl-mlm-dummy-coding}

Bei Integration einer nominalen Variable mit $K$ Faktorstufen werden $K-1$ Dummyvariablen $x_1, x_2, \ldots, x_{K-1}$ benötigt. Eine der Faktorstufen wird als Referenzstufe definiert. Die Dummyvariablen $x_1$ bis $x_{K-1}$ kodieren dann die Abweichungen der anderen Stufen von der Referenzstufe. Diese Art der Kodierung wird in der Literatur auch als *treatment* Kodierung bezeichnet.

::: {#def-dummy}
## Dummyvariablen \index{Dummyvariablen}

Eine Dummyvariable, auch Indikatorvariable, ist eine binäre Prädiktorvariable die nur die Werte $0$ oder $1$ annimmt in einem linearem Modell einnimmt. Mit einer Dummyvariable kann die An- bzw. Abwesenheit eines kategorialen Effekts modelliert werden.
:::

Das lineare Modell in unserem Reaktionszeitexperiment ergibt den folgenden Modellfit (siehe @tbl-mlm-dummy-reaction-03).

```{r}
#| echo: true
#| eval: false

mod <- lm(rt ~ group, data)
```

```{r}
#| label: tbl-mlm-dummy-reaction-03
#| tbl-cap: "Koeffizientenmatrix für die Dummykodierung des Reaktionszeitexperiments."

mod <- lm(rt ~ group, data)
lm_tbl_knitr(mod)
```

Wir erhalten als Steigungskoeffizienten eben genau die Abweichungen der Mittelwerte der jeweiligen Faktorstufen von der Referenzstufe (siehe @tbl-mlm-dummy-reaction-02). D.h. insgesamt haben wir also die Möglichkeit eine nominale Variable mit beliebig vielen Faktorstufen in einem linearen Modell abzubilden. Damit ist dann auch die Unterscheidung in ANOVA und linear Regression als überfällig anzusehen, da beide letztendlich auf dem gleichem Modell, nämlich dem linearen Modell, beruhen.

Schauen wir uns noch den Residuenplot an (siehe @fig-mlm-dummy-reaction-02).

```{r}
#| label: fig-mlm-dummy-reaction-02
#| fig-cap: "Residuenplot für das Modell der Reaktionszeitdaten."

data |> mutate(y_hat = predict(mod), e_i = resid(mod)) |> 
ggplot(aes(y_hat, e_i)) + 
  geom_point() +
  geom_hline(yintercept = 0, col = 'red', linetype = 'dashed') +
  labs(x = expression(paste('Vorhergesagte Werte ', hat(y)[i])),
       y = expression(paste('Residuen ', hat(e)[i]))) 
```

Hier sehen wir wieder, dass nur vier verschiedene $\hat{y}_i$ Werte vorhergesagt werden, nämlich die Mittelwerte $\bar{y}_k$ in den jeweiligen $K$ Gruppen. Die Residuen $e_i$ streuen daher nur um diese vorhergesagten Werte.

Wir können die Dummy-Kodierung explizit von `R` mittels der `dummy.coef()`-Funktion erhalten.

```{r}
#| echo: true

dummy.coef(mod)
```


## Kombination von kontinuierlichen und nominalen Prädiktorvariablen

Schauen wir uns nun als weiteren Fall ein Modell an, bei dem beide Typen von Variablen, nominale und kontinuierliche, integriert sind. In @fig-mlm-dummy-combi-01 haben wir einen hypothetischen Zusammenhang zwischen der körperlichen Leistung dem Trainingsalter und dem Gender.

```{r}
#| label: fig-mlm-dummy-combi-01
#| fig-cap: "Hypothetische Leistungsentwicklung in Abhängigkeit vom Alter und Gender"

N <- 50
set.seed(1)
lew <- tibble::tibble(
  ta = sample(10, N, replace=T),
  gender = sample(0:1, N, replace=T),
  perf = rnorm(N, 30, 3) + 2 * ta + 10*gender,
  gender_f = factor(gender, levels = c(1,0), labels=c('f','m'))
)
ggplot(lew, aes(ta, perf, color = gender_f)) +
  geom_point(size=2) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  scale_color_discrete('Gender') 
```

Das Trainingsalter geht wie bisher als kontinuierliche Variable in das Modell ein, während wir gender als nominale Variable über eine Dummyvariable modellieren. Weibliche Teilnehmerinnen der Studie legen wir als Referenzstufe fest. Das resultiert in dem folgenden linearen Modell:

\begin{align*}
Y_i &= \beta_{ta = 0,x_{1i}=0} + \Delta_m \times x_{1i} + \beta_{ta} \times ta + \epsilon_i \\
x_1 &= 
\begin{cases}
0\text{ wenn weiblich}\\
1\text{ wenn männlich}
\end{cases} \\
\end{align*}

Modellieren mit `lm()` führt dann zu:

```{r}
#| echo: true

mod <- lm(perf ~ gender_f + ta, lew)
```
```{r}
#| label: tbl-mlm-dummy-combi-01
#| tbl-cap: "Modellfit für das Modell mit Trainingsalter und Gender."

broom::tidy(mod) |> knitr::kable(
  booktabs = TRUE,
  digits = 3,
)
```

Die Variable `gender` ist durch `lm()` als Dummyvariable definiert worden und der Steigungskoeffizient $\beta_1=$`gender_fm` kodiert den mittleren Unterschied zwischen männlichen und weiblichen Studienteilnehmerinnen. Der $y$-Achsenabschnitt kodiert daher die Leistung von weiblichen Teilnehmerinnen mit einem Trainingsalter von `ta`$= 0$. (Warum?). Schauen wir uns dazu die resultierenden Graden der vorhergesagten $\hat{y}_i$ an (siehe @fig-mlm-dummy-combi-02).

```{r}
#| label: fig-mlm-dummy-combi-02
#| fig-cap: "Leistungsentwicklung in Abhängigkeit vom Alter und Gender"

new_lew <- data.frame(ta = c(1,10,1,10),
           gender_f = factor(c(0,0,1,1),
                             levels=0:1,
                             labels=c('f','m')))
new_lew <- new_lew |> dplyr::mutate(y_hat=predict(mod, new_lew))
ggplot(lew, aes(ta, perf, color = gender_f)) + geom_point() +
  geom_line(data = new_lew, aes(y = y_hat)) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  scale_color_discrete('Gender') 
```

Wir haben zwei Geraden eine für die weiblichen und eine für die männlichen Teilnehmer mittels des Modells gefittet. Die Geraden sind parallel und um den Wert von $\beta_1$ gegeneinander verschoben.

Nichts hält uns davon ab auch eine Interaktion zwischen kontinuierlichen und nominalen Variablen zu modellieren. Wenn die Daten zum Beispiel dem Trend in @fig-mlm-dummy-combi-03 folgen.

```{r}
#| label: fig-mlm-dummy-combi-03
#| fig-cap: "Leistungsentwicklung in Abhängigkeit vom Alter und Gender"

N <- 50
set.seed(1)
lew <- tibble::tibble(
  ta = sample(10, N, replace=T),
  gender = sample(0:1, N, replace=T),
  perf = rnorm(N, 30, 3) + 2 * ta + 10*gender + 2*ta*gender,
  gender_f = factor(gender, levels = c(0,1), labels=c('m','f'))
)
ggplot(lew, aes(ta, perf, color = gender_f)) +
  geom_point(size=2) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  scale_color_discrete('Gender') 
```

Hier sehen wir, dass die Zunahme der Leistung mit dem Trainingsalter nicht gleich ist in beiden gendern. Bei männlichen Teilnehmern ist der Zuwachs mit dem Trainingsalter größer. D.h der Einfluss der Prädiktorvariable Trainingsalter hängt mit der Ausprägung der Prädiktorvariablen gender zusammen. Es liegt ein Interaktionseffekt vor. Übertragen auf ein lineares Modell könnte der Modellansatz wie folgt aussehen:

\begin{equation*}
y_i = \beta_{ta=0,x_{1i}=0} + \Delta_m \times x_{1i} + \beta_{ta} \times ta + \beta_{ta \times gender} \times x_{1i} \times ta + \epsilon_i
\end{equation*}

In `lm()` ausgedrückt:

```{r}
#| echo: true

mod <- lm(perf ~ gender_f * ta, lew)
```
```{r}
#| label: tbl-mlm-dummy-combi-02
#| tbl-cap: "Modellfit für das Interaktionsmodell mit Trainingsalter und Gender."

broom::tidy(mod) |> knitr::kable(
  booktabs = TRUE,
  digits = 3
)
```

Die gefitteten Regressionsgeraden nehmen dann die folgende Form an (siehe @fig-mlm-dummy-combi-04).

```{r}
#| label: fig-mlm-dummy-combi-04 
#| fig-cap: "Leistungsentwicklung in Abhängigkeit vom Alter und Gender"

new_lew <- data.frame(ta = c(1,10,1,10),
           gender_f = factor(c(0,0,1,1),
                             levels=0:1,
                             labels=c('f','m')))
new_lew <- new_lew |> dplyr::mutate(y_hat = predict(mod,new_lew))
ggplot(lew, aes(ta, perf, color = gender_f)) +
  geom_point() +
  geom_line(data = new_lew, aes(y = y_hat)) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  scale_color_discrete('Gender') 
# für später
ta_cen <- 6
```

Interpretieren wir noch einmal die Regressionskoeffizienten mit Hilfe des Graphen. Dazu zentrieren wir zunächst einmal die Altersvariable um den Wert $ta = `r ta_cen`$ und fitten das Modell mittels der zentrierten Variable. 

```{r}
#| echo: true

lew <- lew |> mutate(ta_c = ta - ta_cen)
mod_c <- lm(perf ~ gender_f * ta_c, lew)

```

In @tbl-mlm-dummy-combi-03 sind die neuen Koeffizienten zu sehen. Sie unterscheiden sich nur für $\beta_0$ und $\beta_1$ zu den Koeffizienten von @tbl-mlm-dummy-combi-02 (Warum?).

```{r}
#| label: tbl-mlm-dummy-combi-03
#| tbl-cap: "Modellfit für das Interaktionsmodell mit dem zentrierten Trainingsalter (`ta_c`) und Gender."

beta_s <- round(coef(mod_c), 2)

broom::tidy(mod_c)[,1:2] |> 
    add_column(coef = paste0("$\\beta_", 0:3, "$"), .before=1)|> 
  knitr::kable(
    booktabs = TRUE,
    digits = 3,
    escape=F
  )
```

Was bedeuten nun die einzelnen Koeffizienten noch einmal genau? Die Referenzgruppe sind die Datenpunkte der männlichen Gruppe. Daher ist der y-Achsenabschnitt $\beta_0 = `r beta_s[1]`$ die Performance einer durchschnittlichen, männlichen Versuchsperson mit einem Trainingsalter von `ta`$= 6$. Der zweite Koeffizient $\beta_1 = `r beta_s[2]`$ kodiert hier den Unterschied zwischen einer männlichen und einer weiblichen Versuchsperson jeweils mit einem Trainingsalter von `ta`$=6$. Der Steigungskoeffizient $\beta_2 = `r beta_s[3]`$ dagegen, kodiert den Unterschied zwischen zwei männlichen Versuchspersonen, die sich um ein Trainingsjahr `ta` voneinander unterscheiden. Der letzte Koeffiziente $\beta_3 = `r beta_s[4]`$, der Interaktionskoeffizient kodiert nun den Unterschied in den Steigungen zwischen den männlichen und den weiblichen Versuchspersonen. Um den Unterschied zwischen zwei weiblichen Versuchsperson zu erhalten, die sich um ein Trainingsjahr `ta` voneinander Unterscheiden, müssen daher $\beta_2$ und $\beta_3$ miteinander addiert werden $`r round(sum(beta_s[3:4]), 3)`$. 

```{r}
#| label: fig-mlm-dummy-combi-05 
#| fig-cap: "Leistungsentwicklung in Abhängigkeit vom Alter und Gender und die Modellkoeffizienten im zentrierten Modell."

ggplot(lew, aes(ta, perf)) +
  geom_point(aes(color = gender_f)) +
  geom_line(data = new_lew, aes(y = y_hat, color=gender_f)) +
  labs(x = 'Trainingsalter', y = 'Performance') +
  geom_segment(data = tibble(ta = 6, perf = 0),
    aes(xend=ta, yend=beta_s[1]),
    arrow=arrow(length=unit(.1,'inches'), type='closed')) +
  geom_segment(data = tibble(ta = 6, perf = beta_s[1]),
    aes(xend=ta, yend=beta_s[1] + beta_s[2]),
    arrow=arrow(length=unit(.1,'inches'), type='closed')) +
  geom_segment(data = tibble(ta = 6, perf = beta_s[1]),
    aes(xend=ta+1, yend=sum(beta_s[c(1,3)])),
    arrow=arrow(length=unit(.1,'inches'), type='closed')) +
  geom_segment(data = tibble(ta = 6, perf = sum(beta_s[1:2])),
    aes(xend=ta+1, yend=sum(beta_s)),
    arrow=arrow(length=unit(.1,'inches'), type='closed')) +
  geom_label(data = tibble(ta=c(5.7,5.7,6.5,6.5), perf=c(25,56,48,74),
                           labels=c(expression(beta[0]),expression(beta[1]),
                                    expression(beta[2]), expression(beta[2]+beta[3]))),
    aes(label = labels), parse=T) +
  scale_x_continuous("Trainingsalter", breaks=1:10) +
  scale_y_continuous('Gender', limits = c(0,90))
```

In @fig-mlm-dummy-combi-05 sind die einzelnen Modellkoeffizienten aus @tbl-mlm-dummy-combi-02 in der Grafik miteingefügt. Die Interpretation der Werte sollte damit klarer werden.


## Verschiedene Kodierschemas für die Dummy-Variablen.

In der Herleitung der Integration von nominalen Faktoren in die multiple Regression haben wir ein bestimmtes Schema kennengelernt, wie die einzelnen Faktorstufen auf $X$-Variablen abgebildet werden. Wir haben eine Referenzstufe welche durch einen $y$-Achsenabschnitt modelliert wird und die anderen $\beta_i$-Koeffizienten bilden Abweichungen in den Faktorstufenmittelwerten von dieser Referenzstufe ab.

Wenn wir die $\beta_i$-Koeffizienten in einem Vektor $\boldsymbol{\beta}$ zusammenfassen.
\begin{equation*}
\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots, \beta_K)^T
\end{equation*}

und die Mittelwerte $\mu_i$ der Faktorstufen $i = 1,2,\ldots,K$ in einem Vektor $\boldsymbol{\mu}$
\begin{equation*}
\boldsymbol{\mu} = (\mu_1, \mu_2, \ldots, \mu_K)^T
\end{equation*}

Dann können wir die Interpretation der $\beta$-Koeffzienten im Sinne der Mittelwerte der Faktorstufen als eine Abbildung von $\boldsymbol{\beta} \rightarrow \boldsymbol{\mu}$ auffassen. Bezeichnen wir diese Abbildung mit dem Symbol $\boldsymbol{B}$.

\begin{equation*}
\boldsymbol{\mu} = \boldsymbol{B\beta}
\end{equation*}

::: {.callout-tip}
Wenn ihr euch mit Matrizen auskennt, dann werdet ihr gemerkt haben, dass wir hier um lineare Abbildungen herumtanzen. Wenn euch das nichts sagt, dann einfach weiterlesen.
:::

### Treatment-Kodierung

Die Abbildung mit einer Referenzstufe und Abweichungen von dieser Stufe, die wir kennengelernt haben, wird als **treatment coding** \index{treatment coding}. Und kann mit Hilfe der Funktion `contr.treatment()` erstellt werden, bzw. ist standardmäßig bei `R` eingestellt. Schauen wir `R` dazu etwas unter die Motorhaube.

Unsere Faktorvariable $A$ hat beispielsweise drei Stufen $K = 3, A, B, C$. Mit der Funktion `mean_contrasts()` aus dem package `codingMatrices` können wir uns die Abbildung darstellen lassen (Im Code werden noch die Zeilen- und Spaltenbeschriftungen geändert um den Output leichter interpretieren zu können).

```{r}
#| echo: true

f_A <- c("A","B","C")
beta_to_mu <- mean_contrasts(contr.treatment(f_A))
dimnames(beta_to_mu)[[1]] <- paste0("beta_",0:2)
dimnames(beta_to_mu)[[2]] <- paste0("mu_",f_A)
beta_to_mu
```

Im Ausdruck bezeichnen die Zeilen die $\beta$-Koeffizienten im multiplen Regressionsmodell, während die Spalten die Mittelwerte $\mu_i$ der Faktorstufen $A,B$ und $C$ spezifizieren. Die Punkte `.` stehen für den Wert $0$. Wir sehen, dass in der ersten Zeile für $\beta_0$ eine $1$ unter $\mu_1$ eingetragen ist, während unter $\mu_2$ und $\mu_3$ jeweils eine $0$ eingetragen sind. D.h., der Koeffizient $\beta_0$ repräsentiert den Mittelwert der Faktorstufe $A$, der Referenzstufe. In der zweiten Zeile für $\beta_1$ dagegen, steht unter dem Mittelwert $\mu_A$ eine $-1$, unter $\mu_B$ eine 1 und unter $\mu_C$ eine $0$. D.h., wie schon bekannt, der Koeffizient $\beta_1$ repräsentiert den Unterschied zwischen den Mittelwerten $\mu_A$ und $\mu_B$ der dazugehörenden Faktorstufen $A$ und $B$. Das gleiche gilt für die dritte Zeile für $\beta_2$ für den Unterschied $\mu_C - \mu_A$. Insgesamt, erkennen wir hier direkt die Kodierung der Faktorstufen wie wir sie oben hergeleitet haben.

### Sum-to-zero Kodierung

Allerdings ist die Treatmentkodierung \index{Abweichungskodierung} \index{sum-to-zero coding} nicht die einzig mögliche Kodierung, sondern es geht noch zahlreiche Andere bzw. es können auch eigene Abbildungen definiert werden, worauf wir hier aber nicht eingehen. Ein weiteres Kodierungsschema ist die Abweichungskodierung (engl. deviation) welche auch als sum-to-zero Kodierung bezeichnet wird. Schauen wir uns zunächst an, wie das Schema aussieht. In `R` können wir dieses Schema mit der Funktion `contr.sum()` generieren.

```{r}
#| echo: true

beta_to_mu <- mean_contrasts(contr.sum(f_A))
dimnames(beta_to_mu)[[1]] <- paste0("beta_",0:2)
dimnames(beta_to_mu)[[2]] <- paste0("mu_",f_A)
beta_to_mu
```

Die erste Zeile ist direkt ablesbar. Der Koeffizient $\beta_0$ stellt den Mittelwert $\bar{\mu}$ der Stufenmittelwerte dar. Die zweite und dritte Zeile für $\beta_1$ und $\beta_2$ sind etwas undurchsichtiger. Deswegen direkt die Lösung und danach die Herleitung. Die $\beta_i$-Koffizienten repräsentieren jeweils die Abweichung der Stufe $\mu_i$ vom Gesamtmittelwert $\bar{\mu}$. Formal:

\begin{equation*}
\beta_i = \mu_i - \bar{\mu} \quad i = 2,3,\ldots,K
\end{equation*}

Wie kommen wir dann auf diese Kodierung? Schauen wir uns $\beta_1$ an.

\begin{equation*}
\begin{aligned}
\beta_1 &= \mu_1 - \bar{\mu} = \mu_1 - \frac{\sum_{i=1}^3\mu_i}{3} \\
        &= \mu_A - \frac{1}{3}\mu_A - \frac{1}{3}\mu_B - \frac{1}{3}\mu_C = \frac{2}{3}\mu_A - \frac{1}{3}\mu_B - \frac{1}{3}\mu_C
\end{aligned}
\end{equation*}

Wir erkennen unsere Koeffizienten aus dem Output wieder. Das gleiche Schema trifft auch für $\beta_2$ zu. Schauen wir uns das Schema in unserem Beispiel der Reaktionszeiten an.

```{r}
#| echo: true

mu_i <- data |> group_by(group) |> summarize(m = mean(rt))
mu_bar <- mean(data$rt)
mu_i |> add_row(group="mu_bar", m = mu_bar)
```

Beziehungsweise für die Abweichungen der Mittelwerte $\mu_i$ vom Gesamtmittelwert $\bar{\mu}$.

```{r}
#| echo: true

mu_i$m - mu_bar
```

Fitten wir nun das Modell mit `lm()`, verwenden nun aber `contr.sum()` anstatt `contr.treatment()`. Wir können dies direkt im Aufruf von `lm()` spezifizieren über den Parameter `contrasts` dem eine Liste mit den Namen der nominalen Variablen zusammen mit der Kodierungsfunktion als Zeichenkette übergeben wird (Es gibt noch weitere Möglichkeiten).

```{r}
#| echo: true

mod_sum <- lm(rt ~ group, data, contrasts = list(group = 'contr.sum'))
coef(mod_sum)
```

Wir sehen, dass die $\beta$-Koeffizienten tatsächlich den Gesamtmittelwert und die Abweichungen der Faktormittelwerte $\mu_i$ vom Gesamtmittelwert $\bar{\mu}$ kodieren. Es fehlt allerdings der letzte Abweichungswert. Wie können wir diesen erhalten? Hier kommt die Eigenschaft dieser Kodierung ins Spiel, die der Kodierung ihren Namen sum-to-zero gegeben hat. Per Konstruktion haben die Abweichungen die Eigenschaft das sie aufsummiert gleich $0$ ergeben. Wie bereits bei den Residuen kennengelernt, die Abweichungen von Werten von deren Mittelwert müssen $0$ ergeben. Daher gilt:

\begin{equation*}
\begin{aligned}
& \sum_{i=1}^K \beta_i = 0 \\
\Leftrightarrow & \beta_K = -\sum_{i=1}^{K-1}
\end{aligned}
\end{equation*}

D.h. durch die Aufsummation der Abweichungen und der Invertierung des Vorzeichens bekommen wir den letzten Abweichungswert. Übertragen auf unser Beispiel:

```{r}
#| echo: true

-sum(coef(mod_sum)[2:4])
```

Und tatsächlich erhalten wir den gesuchten Wert.

### Differenzkodierung

Schauen wir uns noch eine weitere Möglichkeit an die Faktorstufen zu kodieren, die Differenzkodierung \index{Differenzkodierung}. Bei der Differenzkodierung werden jeweils aufeinanderfolgende Stufen miteinander verglichen. In `R` können wir dieses Kodierungsssystem mittels der Funktion `contr.diff()` aus dem package `codingMatrices` erstellen. Wenden wir das Kodierungssystem wieder zunächst auf unseren Faktor `f_A` an.

```{r}
#| echo: true

beta_to_mu <- mean_contrasts(contr.diff(f_A))
dimnames(beta_to_mu)[[1]] <- paste0("beta_",0:2)
dimnames(beta_to_mu)[[2]] <- paste0("mu_",f_A)
beta_to_mu
```

Der Koeffizient $\beta_0$ ist wieder gleich dem Mittelwert aus Gruppe $A$. Der Koeffiziente $\beta_1$ kodiert ähnlich wie beim Treatmentkoding den Unterschied zwischen den Mittelwerten der Faktoren $A$ und $B$. Im Unterschied zum Treatmentkoding kodiert $\beta_2$ dagegen den Unterschied zwischen den Mittelwerten der Faktoren $C$ und $B$.

Angewendet auf das Reaktionszeitexperiment erhalten wir.

```{r}
#| echo: true

mod_diff <- lm(rt ~ group, data, contrasts = list(group = 'contr.diff'))
coef(mod_diff)
```

Die Koeffizienten entsprechen den aufeinanderfolgenden Unterschieden der Gruppenmittelwerten.

```{r}
#| echo: true

diff(mu_i$m)
```

### Zusammenfassung

Zusammenfassend, die verschiedenen Kodierungssysteme führen dazu, dass die Bedeutung der Regressionskoeffizienten $\beta_i$ sich unterscheidet. Daher, um die Koeffizienten interpretieren zu können, muss immer bekannt sein, welches Kodierungssystem bei der Analyse verwendet wurde. Leider ist dies nicht die einzige Implikation, sondern die unterschiedlichen Kodierungssysteme relativ zu weiteren subtilen Unterschieden die ganz und gar nicht offensichtlich sind. Vor allem später, bei der Behandlung von faktoriellen, experimentellen Designs werden die Details der Kodierungssysteme noch einmal stärker von Relevanz werden. In Bezug auf den Fit des Modells haben die Unterschiedlichen Systeme allerdings keinen Einfluss. Die Regressionsgerade (Ebene) wird durch die Wahl der Kodierungssysteme nicht beeinflusst. Somit sind auch die Residuen $e_i$ unter allen Kodierungssystemen gleich.  In der gegensätzlichen Richtung ermöglichen die Kodierungssysteme gewollte Vergleiche zwischen Faktorenstufen direkt zu kodieren und dann in Form der Koeffizienten statistisch zu überprüfen. Unter der Treatmentkodierung bilden die Koeffizienten direkt die Unterschiede der Faktorenstufen von der Referenzstufe ab. Daher sagt die statistische Signifikanz der Koeffizienten $\beta_i, i = 2,3,\ldots,K$ direkt schon etwas über die statistische Signifikanz dieser Stufen von der ersten Stufe aus. Wird zum Beispiel als die Referenzstufe eine Kontrollkondition gewählt und die anderen Stufen bilden unterschiedliche Interventionsstufen ab, dann kann über die Koeffizienten direkt der Unterschied der Interventionsstufen von der Kontrollstufe überprüft werden. Dadurch überschneidet die Thematik der Kodierungssysteme sich mit dem der Kontraste das wir später im Kapitel zu Linearen Kontrasten kennenlernen werden.

Dieser kurze Überblick über die Kodierungssysteme basiert hauptsächlich auf @venables2023. Dies ist auch die primäre Quelle um sich in die Thematik noch einmal tiefer einzulesen. 

## Zusammenfassung

In diesem Kapitel haben wir gesehen, dass wir ohne Probleme auch nominale Prädiktorvariablen in das lineare Modell integrieren können. Dabei können die Variablen rein additiv oder auch als interaktive Effekte eingehen. Damit haben wir eine extrem flexiblen Prozess um Daten zu modellieren und dabei immer in dem bekannten Punkt-Steigungs-Modell bleiben.

## Herleitung der Identität von t-Test und linearen Modell (optional)

Seien beide Gruppen gleich groß ($n$) mit $N = n_m + n_w = 2 \times n$. Der t-Wert für $\beta_1$ berechnet sich aus $t = \frac{b_1}{s_b}$ mit:

$$
s_b = \sqrt{\frac{\sum_{i=1}^N (y_i - \bar{y})^2}{N-2}\frac{1}{\sum_{i=1}^N(x_i-\bar{x})^2}}
$$
Dadurch, das die $x_i$ entweder gleich $0$ oder $1$ sind, ist $\bar{x}=0.5$ und die Abweichungsquadrate im zweiten Term sind alle gleich $\frac{1}{4}$.

$$
\sum_{i=1}^N(x_i - \bar{x})^2=\sum_{i=1}^N\left(x_i - \frac{1}{2}\right)^2 = \sum_{i=1}^N\frac{1}{4}=\frac{N}{4}=\frac{2n}{4}=\frac{n}{2}
$$

Der ersten Term kann mit etwas Algebra und der Definition für die Stichprobenvarianz $s^2$ auf die gewünschte Form gebracht werden.

$$
\frac{\sum_{i=1}^N(y_i-\hat{y})^2}{N-2}=\frac{\sum_{i=1}^n(\overbrace{y_{im} - \bar{y}_m}^{Männer})^2+\sum_{i=1}^n(\overbrace{y_{iw}-\bar{y}_w}^{Frauen})^2}{2(n-1)}=\frac{(n-1)s_m^2+(n-1)s_w^2}{2(n-1)}=\frac{s_m^2+s_w^2}{2}
$$

Die Herleitung für $\beta_1 = \Delta = \mu_w - \mu_m$ ist ebenfalls relativ geradlinig wenn wir uns an den Zusammenhang zwischen $\beta_1$ und der Kovarianz zwischen $x$ und $y$ erinnern. Mit $s_x^2 = \frac{N\frac{1}{4}}{N-1} = \frac{N}{4(N-1)}$ folgt:

\begin{align*}
    b_1 &= \frac{cov(x,y)}{s_x^2} \\
    &= \frac{\sum_{i=1}^N(y_i - \bar{y})(x_i - \bar{x})}{N-1} \frac{4(N-1)}{N} \\
    &= 4\frac{\sum_{i=1}^n(y_{im}-\bar{y})\frac{-1}{2}+\sum(y_{iw}-\bar{y})\frac{1}{2}}{N} \\
    &= \frac{4}{2}\frac{\sum_{i=1}^n(y_{iw}-\bar{y}) - \sum_{i=1}^n(y_{im}-\bar{y})}{2n} \\
    &= \frac{\sum_{i=1}^n y_{iw}}{n} - \frac{n\bar{y}}{n} - \frac{\sum_{i=1}^n y_{im}}{n} + \frac{n\bar{y}}{n} \\
    &= \bar{y}_w - \bar{y}_m = \Delta
\end{align*}

Zu guter Letzt noch die Herleitung für $\beta_0 = \mu_m$.

Mit $b_1 = \Delta = \bar{y}_w - \bar{y}_m$:
\begin{align*}
b_0 &= \bar{y} - \Delta \times \bar{x} \\
&= \frac{\sum_{i=1}^N y_i}{N} - \Delta \times \frac{1}{2} \\
&= \frac{\sum_{i=1}^n y_{im} + \sum_{i=1}^n y_{iw}}{2n} - \frac{1}{2}(\bar{y}_w - \bar{y}_m)  \\
&= \frac{1}{2}\frac{\sum_{i=1}^ny_{im}}{n} + \frac{1}{2}\frac{\sum_{i=1}^ny_{iw}}{n} - \frac{1}{2}\bar{y}_w + \frac{1}{2}\bar{y}_m \\
&= \frac{1}{2}\bar{y}_m + \frac{1}{2}\bar{y}_w - \frac{1}{2}\bar{y}_w + \frac{1}{2}\bar{y}_m \\
&= \bar{y}_m
\end{align*}


## Zum Nach- und Weiterlesen

In [@kutner2005, p.313-319] sind die Herleitungen zur Indexkodierung etwas ausführlicher erklärt.

