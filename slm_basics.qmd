# Einführung 

```{r}
#| echo: false
#| warning: false
#| message: false
source('_common.R')
```

## Back to school

Wir beginnen mit ein Konzept das wir schon alle kennen. Nämlich der Punkt-Steigungsform aus der Schule (siehe @eq-slm-psform-1).

$$
y = m x + b
$${#eq-slm-psform-1}

Wir haben eine abhängige Variable $y$ und eine lineare Formel $mx + b$ die den funktionalen Zusammenhang zwischen den Variablen $y$ und $x$ beschreibt. Um das Ganze einmal konkret zu machen setzen wir $m = 2$ und $b = 3$ fest. Die Formel @eq-slm-psform-1 wird dann zu:

```{r}
m <- 2
b <- 3
```

$$
y = `r m` x + `r b`
$${#eq-slm-psform-2}

Um ein paar Werte für $y$ zu erhalten setzen wir jetzt verschiedene Wert für $x$ ein indem wir $x$ in Einserschritten zwischen $[0, \ldots, 5]$ erhöhen. Um die Werte darzustellen verwenden wir zunächst eine Tabelle (vlg. @tbl-slm-psform)

```{r}
#| tbl-cap: "Tabelle der Daten"
#| label: tbl-slm-psform

tib_psf <- tibble(x = 0:5, y = m * x + b)
kable(tib_psf,
      booktabs = T,
      linesep = '')

```

Wenig überraschend nimmt $y$ für den Wert $x = 0$ den Wert $`r b`$ an und z.B. für den Wert $x = 3$ nimmt $y$ den Wert $`r m` \cdot 3 + `r b` = `r m*3+b`$ an. 

Eine andere Darstellungsform ist naturlich eine graphische Darstellung in dem wir die Werte von $y$ gegen $x$ auf einem Graphen abtragen (siehe @fig-slm-psform-1).

```{r}
#| fig.cap: "Graphische Darstellung der Daten aus @tbl-slm-psform"
#| label: fig-slm-psform-1

ggplot(tib_psf, aes(x,y)) +
  geom_line() + 
  geom_point(size=4)

```

Wiederum wenig überraschen sehen wir einen linearen Zuwachs der $y$-Wert mit den größerwerdenden $x$-Werte. Da in der Definition der Formel @eq-slm-psform-2 nirgends festgelegt wurde, dass diese nur für ganzzahlige $x$-Werte gilt, haben wir direkt eine Gerade durch die Punkte gelegt. Hier wird auch die Bedeutung von $m$ und $b$ direkt klar. Die Variable $m$ bestimmt die Steigung der Gleichung während $b$ den y-Achsenabschnitt beschreibt. 

::: {#def-slm-basics-y-intercept}
## $y$-Achsenabschnitt

Der y-Achsenabschnitt ist der Wert den $y$ einnimmt wenn $x$ den Wert $0$ annimmt. Sei $y$ durch eine lineare Gleichung $y = mx + b$ definiert, dann wird der y-Achsenabschnitt durch den Wert $b$ bestimmt.

:::

Die Variable $m$ dahingehend bestimmt die Steigung der Gerade. 

::: {#def-slm-basics-slope}

Wenn $y$ durch eine lineare Gleichung $y = mx + b$ definiert ist, dann bestimmt die Variable $m$ die Steiung der dazugehörenden Gerade. D.h. wenn sich die Variable $x$ um einen Einheit vergrößert (verkleinert) wird der Wert von $y$ um $m$ Einheiten größer (kleiner). Gilt $m < 0$ dann umgekehrt.

:::

Diese beiden trivialen Konzepte mit eigenen Definitionen zu versehen erscheint im ersten Moment vielleicht etwas übertrieben. Wie sich allerdings später zeigen wird, sind diese beiden Einsichten immer wieder zentral wenn es um die Interpretation von linearen statistischen Modellen geht.

Soweit so gut. Führen wir direkt ein paar Symbole ein, die uns später noch behilflich sein werden. Sei jetzt die Menge der $x$-Werte geben $x = [0, 1, 2, 3, 4, 5]$. Strenggenommen handelt es sich wieder um ein Tupel, da wir jetzt die Reihenfolge nicht mehr ändern. Wir führen nun einen Index $i$ ein, um einzelne Werte in dem Tupel über ihre Position zu bestimmen und wir hängen diesen Index $i$ an $x$ an. Dann wird aus $x$, $x_i$.

```{r}
#| tbl-cap: "$x$-Werte und ihr Index $i$"
#| label: tbl-slm-basic-index

kable(tibble(i = 1:6, x = 0:5),
      booktabs = T,
      linesep = '',
      col.names = c('Index $i$', '$x$-Wert'))
```

Damit können wir jetzt einen speziellen Wert zum Beispiel den dritten Wert mit $x_3 = 2$ bestimmen. Wenden wir unseren Index auf unsere @eq-slm-psform-1 an, folgt daraus, dass $y$ jetzt auch einen Index $i$ erhält.

$$
y_i = m x_i + b \qquad i \text{ in } [1,2,3,4,5,6]
$$

Wir bezeichnen die beiden Variablen $m$, die Steigung, und $b$, den y-Achsenabschnitt, jetzt auch mit neuen Variablen die auch noch einen Index erhalten. Aus $m$ wird $\beta_1$ und aus $b$ wird $\beta_0$. Damit wird der y-Achsenabschnitt mit $\beta_0$ bezeichnet und die Steigung wird mit $\beta_1$ bezeichnet. Dann wir aus unserer Gleichung:

$$
y_i = \beta_0 + \beta_1 x_i
$$ {#eq-slm-psform-beta}

Das ist immer noch unsere einfache Punkt-Steigungsform, wir haben lediglich den Index $i$ eingeführt um unterschiedliche $y-x$-Wertepaare zu bezeichnen und wir haben den $y$-Achsenabschnitt und die Steigung mit neuen Symbolen versehen.

Bei dem bisherigen Zusammenhang handelt es sich um einen funktionalen Zusammenhang\index{Funktionaler Zusammenhang} zwischen den beiden Variablen $x$ und $y$. Funktional deswegen, weil wir eine definiertes mathematisches Modell angeben können, d.h. wir haben eine mathematische Funktion welche die Beziehung zwischen den beiden Variablen beschreibt. Wenn wir den Wert für $x$ kenne, dann können wir den präzisen Wert für $y$ ausreichen, indem wir ihn in @eq-slm-psform-1 einsetzen. Aus der Schule kennen wir auch noch die Darstellung $y = f(x)$. Streng genommen ist diese Darstellung für @eq-slm-psform-1 nicht ausreichend, denn um den Wert für $y$ auszurechnen benötigen wir auch noch Kenntnis über die Werte $m$ und $b$, bzw. in unsere weiteren Darstellung $\beta_0$ und $\beta_1$. Daher sollte der Zusammenhang eigentlich mit $y = f(x, \beta_0, \beta_1)$ bezeichnet werden. Es gilt aber immernoch, für gegebene $x, \beta_0$ und $\beta_1$ ist der Wert für $y$ fest determiniert.

Wenn wir mit realen Daten arbeiten, dann funktioniert dieser Ansatz leider nicht ganz. Selbst wenn wir ein Experiment gleich durchführen werden wir immer etwas unterschiedliche Werte im Sinne der Messungenauigkeit messen. Wenn wir biologische Systeme messen, kommt dazu das diese in den seltensten Fällen zeitstabil sind sondern immer bestimmte Veränderungen von einem Zeitpunkt zum nächsten auftauchen. In @fig-slm-basics-jump-1 sind Sprungweiten von mehreren Weitspringerinnen gegen die Anlaufgeschwindigkeit abgetragen. Bei der Betrachtung der Daten erscheint ein linearer Zusammenhang zwischen diesen beiden Variablen durchaus als plausibel. 

```{r defs_reg_01}
jump <- readr::read_delim('data/running_jump.csv',
                          delim=';',
                          col_types = 'dd')  
jump <- jump |> add_column(i = 1:dim(jump)[1], .before=1)
mod_jump <- lm(jump_m ~ v_ms, jump)
jump <- jump |> mutate(y_hat = predict(mod_jump))
```

```{r}
#| fig.cap: "Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite beim Weitsprung"
#| label: fig-slm-basics-jump-1 
#| fig.height: 4

id_mark <- which(jump$v_ms > 9 & jump$v_ms < 9.1)[-2]
ggplot(jump, aes(v_ms, jump_m)) +
  geom_point(data = jump[id_mark,], color = 'red', size = 3) +
  geom_point(size=2) + 
  labs(x = 'Anlaufgeschwindigkeit[m/s]',
       y = 'Sprungweite[m]') 
```

In @fig-slm-basics-jump-1 sind zwei Punkte rot markiert. Die beiden Werte haben praktisch die gleichen $x$-Werte allerdings unterscheiden sich die $y$-Werte deutlich von einander. Und dies sind nicht die einzigen Beispielpaare bei denen die $x$-Werte nahe beiandern liegen, während die $y$-Werte deutlich weiter voneiander entfernt liegen als bei einen funktionalen Zusammenhang nach @eq-slm-psform-1 zu erwarten wäre. Diese Abweichungen kommen durch zufällige Einflussfaktoren wie eben zum Beispiel die Veränderungen angesprochener biologischer Faktoren, Messunsicherheiten, beim Weitsprung draußen sind auch immer externe Einflüsse mögliche, vielleicht wenn es sich um den gleichen Springer handelt, hat er auch beim zweiten Mal keine Lust mehr gehabt. Wenn die Punkte zwei unterschiedliche Springer sind, dann kommt auch dazu, dass zwei Weitspringer bei identischer Anlaufgeschwindigkeit unterschiedliche Sprungfähigkeiten haben oder auch technisch nicht gleich gesprungen sind und so weiter und so fort. Insgesamt führen alle diese Einflüsse dazu, dass wir nicht mehr einen streng funktionalen Zusammenhang zwischen unseren beiden Variablen $x$ der Anlaufgeschwindigkeit und $y$ der Sprungweite vorfinden. Wie wir mit diesen Einflüssen umgehen ist das zentrale Thema des nächsten Abschnitts und markiert auch unseren Eingang zur einfachen linearen Regression.

## Die einfache lineare Regression

Bleiben wir bei unserem Beispiel aus @fig-slm-basics-jump-1 und interpretieren das als praktisches Problem. Wir sind eine Weitsprungtrainerin und stehen jetzt vor der Aufgabe in unserem Training etwas zu verändern um die Weitsprungleistung zu verbessern. Wir haben wir haben uns dazu entschlossen am Anlauf etwas zu verbessern wissen jetzt aber nicht ob, das wirklich lohnenswert ist. Von einer befreundeten Trainerin haben wir einen Datensatz bekommen von Anlaufgeschwindigkeiten und den dazugehörigen Sprungweiten. Schauen wir uns zunächst die einmal die Struktur der Daten an.

```{r}
#| label: tbl-slm-basics-jump-1
#| tbl-cap: "Ausschnitt der Sprungdaten"

jump |> select(jump_m, v_ms) |>
  head(n = 7) |>  
  kable(booktabs=TRUE,
               linesep="",
               digits = 2)
```

In @tbl-slm-basics-jump-1 ist ein Ausschnitt Sprungdaten abgebildet. Wir haben eine einfache Struktur der Daten. Wir haben eine Tabelle mit zwei Spalten. `jump_m` bezeichnet die Sprungweiten und `v_ms` die Anlaufgeschwindigkeiten. Damit wir die Datenpaare voneinander unterscheiden bzw. identifzieren können führen wir unseren bereits besprochenen Index $i$ und können so einzelne Paare ansprechen.

```{r}
#| label: tbl-slm-basics-jump-2
#| tbl-cap: "Ausschnitt der Sprungdaten"
jump |> select(i, jump_m, v_ms) |>
  head(n = 7) |>  
  kable(booktabs=TRUE,
               linesep="",
               digits = 2)
```

Das waren bisher aber nur Formalitäten. Wir wollen jetzt denn Zusammenhang zwischen den beiden Variablen modellieren. Wir könnten wahrscheinlich auch einfach Pi-mal-Daumen abschätzen wie groß der Zusammenhang ist. Wenn wir jetzt aber einen unserer Läufer haben, der z.B. etwa $9m/s$ anläuft, welchen Vergleichswerte nehmen wir dann aus @fig-slm-basics-jump-1. Den unteren oder den oberen der beiden roten Werte? Oder vielleicht den Mittelwert? Welchen Wert nehmen wir wenn unserer Athlete $9.7m/s$ anläuft. Da haben wir leider keinen Vergleichswert in unserer Tabelle. Daher wäre es schon ganz praktisch eine Formel nach dem Muster von @eq-slm-psform-beta zu haben. Wie wir allerdings schon festgestellt haben, geht dies nicht so einfach da wir eben das Problem mit den Einflussfaktoren haben, die dazu führen, dass die Werte eben nicht streng auf eine Gerade liegen. Somit liegt die Herausforderung nun eine Gerade zu finden die möglichst *genau* die Daten wiederspiegelt.


```{r}
#| fig.cap: "Mögliche Geraden um den Zusammenhang der Anlaufgeschwindigkeit und der Sprungweite zu modellieren"
#| label: fig-slm-basics-line-1

beta_0 <- coef(mod_jump)[1]
beta_1 <- coef(mod_jump)[2]
ggplot(jump, aes(v_ms, jump_m)) + 
  geom_abline(slope = beta_1, intercept = beta_0, col = 'blue', alpha=.3)  +
  geom_abline(data = tibble(s = beta_1 + seq(-0.1, 0.1, 0.2), y_in = beta_0),
              aes(slope = s, intercept = y_in), alpha=.1, col='blue') +
  geom_abline(intercept = 3, slope = 0.3, col = 'blue', alpha=.3) +
  geom_abline(intercept = -3, slope = 1.2, col = 'blue', alpha=.3) +
  #geom_segment(aes(x = v_ms, xend = v_ms, yend = jump_m, y = y_hat),
  #             col = 'red') +
  geom_point() +
  #lims(y = c(-10, 10), x = c(-2, 12)) +
  labs(x = 'Anlaufgeschwindigkeit[m/s]',
       y = 'Sprungweite[m]') 
```

In @fig-slm-basics-line-1 sind die Daten zusammen mit verschiedenen möglichen Geraden abgebildet. Eine kurze Überlegung macht schnell klar, dass es im Prinzip unendlich viele unterschiedliche Geraden gibt die durch die Datenpunkte gelegt werden können. D.h. es gibt unendlich viele Kombinationen von $\beta_0$ und $\beta_1$, die die jeweiligen Geraden bezeichnen. Daher muss jetzt eine Kriterium gefunden werden, welches ermöglicht aus diesen unendlich vielen Geraden eine auszuwählen die im Sinne des Kriterium optimal ist.

Tatsächlich gibt es dort auch verschiedene Möglichkeiten Kriterien anzuwenden, dasjenige dass jedoch am weitesten verbreitet ist aus verschiedenen Gründen sind die quadratierten Abweichungen von der Gerade. Schauen wir uns die Herleitung dazu schrittweise an. In @fig-slm-basics-line-2 ist zur Übersicht nur ein Ausschnitt der Daten zusammen mit einer möglichen Gerade eingezeichnet. Die senkrechten Abweichungen der Geraden zu den jeweiligen Datenpunkten sind rot eingezeichnet. Es ist ersichtlich, dass für diese Wahl der Geraden es zwei Punkte gibt die tatsächlich auch ziemlich genau auf der Geraden liegen während die anderen Punkte zum Teil oberhalb bzw. unterhalb der Geraden liegen. Das Kriterium wäre jetzt dementsprechen die jenige Geraden aus den unendlich vielen zu finden, bei der diese Abweichung ein Minimum annehmen. 

$$
\text{Min}\sum_{i=1}^n y_i - (\beta_0 + \beta_1 x_i) = \sum_{i=1}^n y_i - \beta_0 - \beta_1 x_i
$$

```{r}
#| fig.cap: "Abweichungen der Gerade von der Datenpunkten für die Daten mit eine Anlaufgeschwindigkeit zwischen $8m/s$ und $10m/s$."
#| label: fig-slm-basics-line-2

beta_0 <- coef(mod_jump)[1]
beta_1 <- coef(mod_jump)[2]
ggplot(jump |> filter(v_ms > 8, v_ms < 10), aes(v_ms, jump_m)) + 
  geom_abline(slope = beta_1, intercept = beta_0, col = 'blue', alpha=.3)  +
  geom_segment(aes(x = v_ms, xend = v_ms, yend = jump_m, y = y_hat),
               col = 'red') +
  geom_point(size=3) +
  labs(x = 'Anlaufgeschwindigkeit[m/s]',
       y = 'Sprungweite[m]') 
```

Unglücklicherweise haben die einfachen Abweichungen die unhandliche Eigenschaft, dass dann die *Gerade* $y_i = \hat{y}$ optimal ist. 

$$
\sum_{i=1}^n y_i - \hat{y} = \sum_i^n y_i - \sum_{i=1}^n \hat{y} = \sum_{i=1}^n y_i - n\hat{y} = \sum_{i=1}^n y_i - n\frac{1}{n}\sum_{i=1}^n y_i = \sum_{i=1}^n y_i - \sum_{i=1}^n y_i= 0
$$

Wir können das Kriterium aber auch noch etwas schärfer machen. Wenn wir sagen, dass wir größere Abweichungen stärker gewichten wollen als kleinere Abweichungen. D.h. große Abweichungen zwischen der Gerade und den Datenpunkten sollten stärker berücksichtigt werden, als kleine Abweichungen. Dies können wir erreichen indem wir die Abweichungen noch zusätzlich quadrieren. Dies hat auch noch den Vorteil noch verschiedene andere mathematische Vorteile, unter anderem führt dies dazu, dass wir eine Gerade erhalten, die auch tatsächlich die Steigung der Punkte berücksichtigt und nicht einfache nur eine horizontale Gerade durch die Punkte zeichnet. Dementsprechend erhalten wir die folgende Funktion, die es zu minimieren gilt:

$$
\text{Min} \sum_{i=1}^n(y_i - (\beta_0 + \beta_1 x_i))^2
$$ {#eq-slm-basics-rms-1}

Die Abweichungen zwischen der zu findenden Gerade und den Datenpunkten werden als Residuen $e_i$ bezeichnet. Dementsprechend ist die Minimierungsgleichung auch als:

$$
\text{Min} \sum_{i=1}^n e_i
$$
darzustellen, mit $e_i := y_i - (\beta_0 + \beta_1 x_i)$. Führen wir noch eine weitere Bezeichnung $E$ ein, mit der wir die Minimierungsfunktion bezeichnen ($E$ nach englisch error).

$$
E = \sum_{i=1}^n e_i = \sum_{i=1}^n y_i - (\beta_0 - \beta_1 x_i)
$$

Das Minimum lässt sich jetzt finden, indem die partiellen Ableitungen von $\beta_0$ und $\beta_1$ berechnen und wie wir es aus der Schule kennen, die Ableitungen gleich Null gesetzt werden.

\begin{align*}
\frac{\partial E}{\partial \beta_0} &= -2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i) = 0 \\
\frac{\partial E}{\partial \beta_1} &= -2 \sum_{i=1}^n x_i (y_i - \beta_0 - \beta_1 x_i) = 0
\end{align*}

Diese Gleichungen lassen sich umstellen und nach $\beta_0$ und $\beta_1$ auflösen:

\begin{align*}
\hat{\beta_1} &= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} \\
\hat{\beta_0} &= \bar{y} - \hat{\beta_1} \bar{x}
\end{align*}

$\bar{x}$ und $\bar{y}$ sind wieder die Mittelwerte von $x_i$ und $y_i$. Diese beiden Gleichungen werden als die Normalengleichungen bezeichnet.

Wir führen noch einen weiteren Term ein, den vorhergesagten Wert $\hat{y}_i$ von $y_i$ anhand der Geradengleichung. Das Hütchen über $y_i$ ist dabei immer das Signal dafür, das es sich um einen abgeschätzten Wert handelt. Wenn wir $\beta_0$ und $\beta_1$ anhand der Normalengleichung bestimmen, dann sind das mit großer Wahrscheinlichkeit nicht die *wahren* Werte aus der Population, sondern wir haben sie nur anhand der Daten abgeschätzt, daher bekommen die berechneten Werte ebenfalls ein Hütchen $\hat{\beta}_0$ und $\hat{\beta}_1$. Insgesamt wird die Gleichung dann zu:

$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 * x_i
$$

Graphisch sind die $\hat{y}_i$s die Werte auf der Geraden für die gegebenen $x$-Werte.

```{r}
#| fig.cap: "Die vorhergesaten Werte $\\hat{y}_i$ auf der Gerade."
#| label: fig-slm-basics-line-3

beta_0 <- coef(mod_jump)[1]
beta_1 <- coef(mod_jump)[2]
ggplot(jump |> filter(v_ms > 8, v_ms < 10), aes(v_ms, jump_m)) + 
  geom_abline(slope = beta_1, intercept = beta_0, col = 'blue', alpha=.3)  +
  geom_segment(aes(x = v_ms, xend = v_ms, yend = jump_m, y = y_hat),
               col = 'red') +
  geom_point(size=3) +
  geom_point(aes(y = y_hat), color = 'yellow', size=3) +
  labs(x = 'Anlaufgeschwindigkeit[m/s]',
       y = 'Sprungweite[m]') 
```

Für den vorliegenden Fall der Weitsprungdaten berechnen sich die Werte für die Koeffizienten zu $\hat{\beta}_0 = -0.14$ und $\hat{\beta}_1 = 0.76$ Somit folgt für die Geradengleichung

$$
\hat{y}_i = -0.14 + 0.76 \times x_i
$$

bzw. für die graphische Darstellung.

```{r}
#| fig.cap: "Die Regressionsgerade der Sprungdaten."
#| label: fig-slm-basics-line-4

beta_0 <- coef(mod_jump)[1]
beta_1 <- coef(mod_jump)[2]
ggplot(jump, aes(v_ms, jump_m)) + 
  geom_abline(slope = beta_1, intercept = beta_0, col = 'blue', alpha=.3)  +
  geom_point(size=3) +
  labs(x = 'Anlaufgeschwindigkeit[m/s]',
       y = 'Sprungweite[m]') 
```

Um uns auch zu vergewissern, dass unsere Berechnungen korrekt sind, schauen wir uns noch einmal an, wie sich $E$ verhällt, wenn wir unterschiedliche Kombinationen von Werten für $\beta_0$ und $\beta_1$ in unserer linearen Gleichung verwenden.

```{r}
#| fig.cap: "Heatmap von $log(E)$ für verschiedene Werte von $\\beta_0$ und $\\beta_1$"
#| fig.height: 4
#| fig.width: 4
#| label: fig-slm-basics-gradient

coefs <- coef(mod_jump)
foo <- function(ab) {
  res <- numeric(nrow(ab))
  for (i in 1:nrow(ab)) {
    res[i] <- sum((jump$jump_m - ab$a[i] - ab$b[i] * jump$v_ms)**2)
  } 
  ab |> mutate(z = res)
}
min_coef <- sum( (jump$jump_m - coefs[1] - coefs[2]*jump$v_ms))
n <- 40 
a <- seq(coefs[1]-.4, coefs[1]+.4, length.out=n)
b <- seq(coefs[2]-.1, coefs[2]+.1, length.out=n)
ab <- expand_grid(a = a,
                  b = b)
ggplot(foo(ab), aes(a, b, z = log(z))) +
  geom_contour_filled(show.legend=F) +
  geom_contour(color='red', show.legend = F) +
  geom_hline(yintercept = coefs[2], linetype = 'dashed', color = 'black') +
  geom_vline(xintercept = coefs[1], linetype = 'dashed', color = 'black') +
  geom_point(data = tibble(a = coefs[1], b = coefs[2], z = min_coef), color='black', size=6) +
  theme(panel.background = element_rect(fill='white')) +
  scale_colour_manual(aesthetics = 'fill', drop = FALSE,
                      values = colorRampPalette(c('white','red'))(10)) +
  labs(x = expression(hat(beta)[0]), y = expression(hat(beta)[1]))

```

In @fig-slm-basics-gradient sind verschiedene Werte für $E$ in einer heatmap abgetragen. Die Abweichungen wurden $log$-transformiert (d.h. der Logarithmus der $E$-Werte wurde berechnet), da sonst die Unterschiede in der diagnolen Bildrichtung zu schnell wachsen und die Unterschied nicht mehr so einfach zu deuten sind. Werte näher an Weiß bedeuten kleine Werte und Werte näher an Rot bedeuten größere Werte von $E$. Das berechnete Paar von $(\hat{\beta}_0, \hat{\beta}_1)$ ist schwarz eingezeichnet. Die Abbildung zeigt, dass dieses Wertepaar tatsächlich ein Minimum bezüglich der Funktion $E$ bedeutet, da in alle Richtung weg von dem schwarzen Punkt die Werte für $E$ zunehmen.

### Detaillierte Herleitung der Normalengleichungen

Um die Herleitung der Normalengleichungen Schritt-für-Schritt nachvollziehen zu können benötigen wir zunächst einmal ein paar algebraische Tricks.

Für den Mittelwert gilt:
$$
\bar{x} = \frac{1}{n}\sum x_i \Leftrightarrow \sum x_i = n \bar{x}
$$

Bei Summen und konstanten $a$ konstant gilt:
\begin{align}
    \sum a &= n a \\
    \sum a x_i &= a \sum x_i \\
    \sum (x_i + y_i) &= \sum x_i + \sum y_i
\end{align}

Wenn eine Summe abgeleitet wird, kann in die Ableitung in die Summe reingezogen werden.
$$
\frac{d}{d x}\sum f(x) = \sum\frac{d}{d x} f(x)
$$

Hier ein zwei Umformungen bei Summen und dem Kreuzprodukt bzw. dem Quadrat.
\begin{alignat}{2}
 && \sum(x_i-\bar{x})(y_i-\bar{y}) \label{eqn:id_1} \\
 \Leftrightarrow\mkern40mu && \sum (x_iy_i-\bar{x}y_i-x_i\bar{y}+\bar{x}\bar{y}) \nonumber \\
 \Leftrightarrow\mkern40mu && \sum x_i y_i - \sum\bar{x}y_i - \sum x_i \bar{y} + \sum \bar{x} \bar{y} \nonumber \\
 \Leftrightarrow\mkern40mu&&  \sum x_iy_i - n\bar{x}\bar{y}-n\bar{x}\bar{y}+n\bar{x}\bar{y} \nonumber \\
 \Leftrightarrow\mkern40mu && \sum x_iy_i - n\bar{x}\bar{y} \nonumber
\end{alignat}
\begin{alignat}{2}
&& \sum(x_i - \bar{x})^2 \label{eqn:id_2} \\
 \Leftrightarrow\mkern40mu && \sum(x_i^2 - 2 x_i \bar{x} + \bar{x}^2) \nonumber \\
 \Leftrightarrow\mkern40mu && \sum x_i^2 - 2\bar{x}\sum x_i + \sum\bar{x}^2 \nonumber\\
 \Leftrightarrow\mkern40mu && \sum x_i^2 - 2\bar{x}n\bar{x} + n\bar{x}^2 \nonumber \\
 \Leftrightarrow\mkern40mu && \sum x_i^2 - n \bar{x}^2 \nonumber
\end{alignat}
\subsection{Herleitung}

Zurück zu unserem Problem. Es gilt $E$ zu minimieren:

\begin{alignat}{2}
&& E = \sum e_i^2 = \sum (y_i - \hat{y}_i)^2 \\
\Leftrightarrow\mkern40mu && \sum (y_i - (\beta_0 + \beta_1 x_i))^2 \nonumber \\
\Leftrightarrow\mkern40mu && \sum (y_i - \beta_0 - \beta_1 x_i)^2 \nonumber
\end{alignat}

Die Gleichung hängt von zwei Variablen $\beta_0$ und $\beta_1$. Um das Minimum der Gleichung zu erhalten, verfährt man wie in der Schule, indem man die Ableitung gleich Null setzt. Der vorliegenden Fall ist jedoch etwas komplizierter, da die Gleichung von zwei Variablen abhängt. Daher müssen wir die partiellen Ableitungen $\frac{\partial}{\partial \beta_0}$ und $\frac{\partial}{\partial \beta_1}$ verwendet. Wir erhalten dadurch ein Gleichungssystem mit zwei Gleichungen (die jeweiligen Ableitungen) in zwei Unbekannten ($\beta_0$ und $\beta_1$). Die Lösung erfolgt, indem zuerst eine Gleichung nach der einen Unbekannten umgestellt wird und das Ergebnis dann in die andere Gleichung eingesetzt wird.

Wir beginnen mit der partiellen Ableitung nach $\beta_0$ für den y-Achsenabschnitt.
(Zurück an die Schule erinnern: Äußere Ableitung mal innere Ableitung)

\begin{alignat}{2}
&& \frac{\partial \sum (y_i - \beta_0 - \beta_1 x_i)^2}{\partial \beta_0} \\
\Leftrightarrow\mkern40mu && \sum\frac{\partial}{\partial \beta_0}(y_i - \beta_0- \beta_1 x_i)^2 \nonumber \\
\Leftrightarrow\mkern40mu && \sum 2(y_i - \beta_0- \beta_1 x_i) (-1) \nonumber \\
\Leftrightarrow\mkern40mu && -2 \sum (y_i - \beta_0- \beta_1 x_i) \nonumber
\end{alignat}
Zum minimieren gleich Null setzen.
\begin{alignat}{2}
&& -2 \sum (y_i - \beta_0- \beta_1 x_i) = 0 \nonumber \\
\Leftrightarrow\mkern40mu && \sum (y_i - \beta_0- \beta_1 x_i) = 0 \nonumber \\
\Leftrightarrow\mkern40mu && \sum y_i - \sum \beta_0- \sum \beta_1 x_i = 0 \nonumber \\
\Leftrightarrow\mkern40mu && n \bar{y} - n \beta_0- \beta_1 n \bar{x} = 0 \nonumber \\
\Leftrightarrow\mkern40mu && \bar{y} - \beta_0- \beta_1 \bar{x} = 0 \nonumber \\
\Leftrightarrow\mkern40mu && \bar{y} - \beta_1 \bar{x} = \beta_0\nonumber \\
\Leftrightarrow\mkern40mu && \beta_0= \bar{y} - \beta_1 \bar{x} 
\end{alignat} 

Es folgt nach dem gleichen Prinzip die Herleitung für die Steigung $\beta_1$ und indem die Lösung für $\beta_0$ eingesetzt wird.

\begin{alignat}{2}
&& \frac{\partial \sum (y_i - \beta_0 - \beta_1x_i)^2}{\partial \beta_1} \\
\Leftrightarrow\mkern40mu && \sum\frac{\partial}{\partial b}(y_i - \beta_0 - \beta_1x_i)^2 \nonumber \\
\Leftrightarrow\mkern40mu && \sum2(y_i - \beta_0 - \beta_1x_i) -x_i \nonumber \\
\Leftrightarrow\mkern40mu && -2 \sum(y_i - \beta_0 - \beta_1x_i)x_i
\end{alignat}
Wiederum gleich Null setzen.
\begin{alignat}{2}
&& -2 \sum(y_i - \beta_0 - \beta_1x_i)x_i = 0 \nonumber\\
\Leftrightarrow\mkern40mu && \sum (y_i - \beta_0 - \beta_1x_i)x_i = 0 \nonumber \\
\Leftrightarrow\mkern40mu && \sum (y_i x_i - \beta_0 x_i - \beta_1x_i x_i) = 0 \nonumber \\
\Leftrightarrow\mkern40mu && \sum y_i x_i - \beta_0 \sum x_i - b\sum x_i^2 = 0 \nonumber \\
\Leftrightarrow\mkern40mu && \sum y_i x_i - n \beta_0 \bar{x} - \beta_1\sum x_i^2 = 0 \nonumber
\end{alignat}
Einsetzen der Lösung für $\beta_0$ führt zu:
\begin{alignat}{2}
\Leftrightarrow\mkern40mu && \sum y_i x_i - n (\bar{y} - \beta_1 \bar{x}) \bar{x} - \beta_1\sum x_i^2 = 0 \nonumber \\
\Leftrightarrow\mkern40mu && \sum y_i x_i - n\bar{y}\bar{x} + n \beta_1\bar{x}^2 - \beta_1\sum x_i^2 = 0 \nonumber \\
\Leftrightarrow\mkern40mu && \sum y_i x_i - n\bar{y}\bar{x} = \beta_1 \sum x_i^2 - \beta_1n \bar{x}^2 \nonumber \\
\Leftrightarrow\mkern40mu && \sum (x_i-\bar{x})(y_i-\bar{y}) = \beta_1 (\sum x_i^2 - n\bar{x}^2) \nonumber \\
\Leftrightarrow\mkern40mu && \frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum x_i^2 - n\bar{x}^2} = \beta_1\nonumber \\
\Leftrightarrow\mkern40mu && \beta_1= \frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2} \nonumber
\end{alignat}

Somit erhält man die beiden Normalengleichungen der Regression.

Über diese beiden Gleichungen erhalten wir die gewünschten Koeffizienten $\hat{\beta_0}$ und $\hat{\beta_1}$. Die Methode wird als die als die Methode der kleinsten Quadrate\index{Methode der kleinsten Quadrate} bezeichnet oder im Englischen Root-Mean-Square (RMS)\index{RMS}.




## Regression in `R`

### Model fitten mit `lm()`
```{r, echo=T}
mod <- lm(jump_m ~ v_ms, data = jump)
mod
```

## Formelsyntax in `lm(y ~ x, data)`


Table: Formelsyntaxbeispiele für `lm()`

| Modell | Formel |  Erklärung |
|---|---|---|
| $y=\beta_0$ | `y ~ 1` | y-Ab
| $y=\beta_0+\beta x$ | `y ~ x` | y-Ab und StKoef |
| $y=\beta_0+\beta_1x_1+\beta_2x_2$ | `y ~ x1 + x2` | y-Ab und 2 StKoe |

y-Ab = y-Achsenabshnitt, StKoef = Steigungskoeffizient

## `lm()`-fit mit `summary()` inspizieren

```{r, echo=T}
summary(mod)
```

## `lm()` und ein paar friends...

Koeffizienten und Standardschätzfehler
```{r, echo=T}
coef(mod)
sigma(mod)
```

Residuen
```{r, echo=T}
# Nur die ersten beiden
# Residuen
# damit der Ausdruck
# auf das Slide passt.
resid(mod)[1:2]
```
